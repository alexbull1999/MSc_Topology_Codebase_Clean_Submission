Starting Surface Distance Metric Analysis job...
Job ID: 181925
Node: gpuvm15
Time: Sat 12 Jul 16:23:41 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 12 16:23:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-12 16:23:55.416765
Using device: cuda

Configuration:
  Embedding type: lattice
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 100
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_162355
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_162355/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'lattice'
Output dimension will be: 768
GlobalDataLoader initialized:
  Embedding type: lattice
  Output dimension: 768
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating lattice embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated lattice embeddings: torch.Size([549367, 768])
Generating embeddings for validation...
Generating lattice embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated lattice embeddings: torch.Size([549367, 768])
Generating embeddings for test...
Generating lattice embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated lattice embeddings: torch.Size([549367, 768])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 768])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 768])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 768])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 768
Updated model input_dim to: 768
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 537
  Test batches: 539
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 549367 samples, 537 batches
  Test: 549367 samples, 539 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 768
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,089,355
Model created with 1,089,355 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,089,355
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.036 ± 0.004
    Neg distances: 0.036 ± 0.004
    Separation ratio: 1.02x
    Gap: -0.048
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9996 (C:1.9996, R:0.0028)
Batch  25/537: Loss=1.9945 (C:1.9945, R:0.0026)
Batch  50/537: Loss=1.9744 (C:1.9744, R:0.0025)
Batch  75/537: Loss=1.9587 (C:1.9587, R:0.0024)
Batch 100/537: Loss=1.9413 (C:1.9413, R:0.0022)
Batch 125/537: Loss=1.9340 (C:1.9340, R:0.0021)
Batch 150/537: Loss=1.9293 (C:1.9293, R:0.0020)
Batch 175/537: Loss=1.9334 (C:1.9334, R:0.0020)
Batch 200/537: Loss=1.9305 (C:1.9305, R:0.0019)
Batch 225/537: Loss=1.9375 (C:1.9375, R:0.0019)
Batch 250/537: Loss=1.9225 (C:1.9225, R:0.0019)
Batch 275/537: Loss=1.9199 (C:1.9199, R:0.0018)
Batch 300/537: Loss=1.9339 (C:1.9339, R:0.0018)
Batch 325/537: Loss=1.9171 (C:1.9171, R:0.0018)
Batch 350/537: Loss=1.9181 (C:1.9181, R:0.0018)
Batch 375/537: Loss=1.9171 (C:1.9171, R:0.0018)
Batch 400/537: Loss=1.9232 (C:1.9232, R:0.0018)
Batch 425/537: Loss=1.9233 (C:1.9233, R:0.0018)
Batch 450/537: Loss=1.9196 (C:1.9196, R:0.0017)
Batch 475/537: Loss=1.9129 (C:1.9129, R:0.0018)
Batch 500/537: Loss=1.9130 (C:1.9130, R:0.0018)
Batch 525/537: Loss=1.9174 (C:1.9174, R:0.0018)

============================================================
Epoch 1/100 completed in 41.4s
Train: Loss=1.9323 (C:1.9323, R:0.0020) Ratio=1.60x
Val:   Loss=1.9086 (C:1.9086, R:0.0018) Ratio=1.89x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.9086)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.9121 (C:1.9121, R:0.0018)
Batch  25/537: Loss=1.9090 (C:1.9090, R:0.0018)
Batch  50/537: Loss=1.9150 (C:1.9150, R:0.0017)
Batch  75/537: Loss=1.9282 (C:1.9282, R:0.0018)
Batch 100/537: Loss=1.9143 (C:1.9143, R:0.0018)
Batch 125/537: Loss=1.8997 (C:1.8997, R:0.0018)
Batch 150/537: Loss=1.9164 (C:1.9164, R:0.0018)
Batch 175/537: Loss=1.9136 (C:1.9136, R:0.0017)
Batch 200/537: Loss=1.9134 (C:1.9134, R:0.0017)
Batch 225/537: Loss=1.9022 (C:1.9022, R:0.0018)
Batch 250/537: Loss=1.9001 (C:1.9001, R:0.0018)
Batch 275/537: Loss=1.9164 (C:1.9164, R:0.0018)
Batch 300/537: Loss=1.9097 (C:1.9097, R:0.0018)
Batch 325/537: Loss=1.9066 (C:1.9066, R:0.0017)
Batch 350/537: Loss=1.9147 (C:1.9147, R:0.0018)
Batch 375/537: Loss=1.9170 (C:1.9170, R:0.0017)
Batch 400/537: Loss=1.9136 (C:1.9136, R:0.0018)
Batch 425/537: Loss=1.9111 (C:1.9111, R:0.0018)
Batch 450/537: Loss=1.9213 (C:1.9213, R:0.0018)
Batch 475/537: Loss=1.9171 (C:1.9171, R:0.0018)
Batch 500/537: Loss=1.9031 (C:1.9031, R:0.0018)
Batch 525/537: Loss=1.9120 (C:1.9120, R:0.0018)

============================================================
Epoch 2/100 completed in 31.4s
Train: Loss=1.9117 (C:1.9117, R:0.0018) Ratio=1.83x
Val:   Loss=1.9033 (C:1.9033, R:0.0018) Ratio=1.95x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.9033)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8976 (C:1.8976, R:0.0018)
Batch  25/537: Loss=1.9152 (C:1.9152, R:0.0018)
Batch  50/537: Loss=1.9148 (C:1.9148, R:0.0018)
Batch  75/537: Loss=1.9066 (C:1.9066, R:0.0018)
Batch 100/537: Loss=1.9091 (C:1.9091, R:0.0018)
Batch 125/537: Loss=1.9013 (C:1.9013, R:0.0017)
Batch 150/537: Loss=1.9004 (C:1.9004, R:0.0017)
Batch 175/537: Loss=1.9074 (C:1.9074, R:0.0017)
Batch 200/537: Loss=1.8933 (C:1.8933, R:0.0018)
Batch 225/537: Loss=1.8938 (C:1.8938, R:0.0017)
Batch 250/537: Loss=1.9126 (C:1.9126, R:0.0018)
Batch 275/537: Loss=1.9092 (C:1.9092, R:0.0018)
Batch 300/537: Loss=1.9270 (C:1.9270, R:0.0018)
Batch 325/537: Loss=1.9111 (C:1.9111, R:0.0018)
Batch 350/537: Loss=1.9153 (C:1.9153, R:0.0017)
Batch 375/537: Loss=1.8935 (C:1.8935, R:0.0017)
Batch 400/537: Loss=1.9007 (C:1.9007, R:0.0018)
Batch 425/537: Loss=1.9034 (C:1.9034, R:0.0018)
Batch 450/537: Loss=1.9029 (C:1.9029, R:0.0018)
Batch 475/537: Loss=1.9052 (C:1.9052, R:0.0017)
Batch 500/537: Loss=1.9202 (C:1.9202, R:0.0018)
Batch 525/537: Loss=1.9081 (C:1.9081, R:0.0018)

============================================================
Epoch 3/100 completed in 30.3s
Train: Loss=1.9062 (C:1.9062, R:0.0018) Ratio=1.90x
Val:   Loss=1.9000 (C:1.9000, R:0.0018) Ratio=2.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.9000)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.601 ± 0.697
    Neg distances: 1.199 ± 0.823
    Separation ratio: 1.99x
    Gap: -2.452
    ⚠️  Moderate global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.4547 (C:1.4547, R:0.0018)
Batch  25/537: Loss=1.4422 (C:1.4422, R:0.0018)
Batch  50/537: Loss=1.4527 (C:1.4527, R:0.0018)
Batch  75/537: Loss=1.4293 (C:1.4293, R:0.0018)
Batch 100/537: Loss=1.4094 (C:1.4094, R:0.0017)
Batch 125/537: Loss=1.4537 (C:1.4537, R:0.0018)
Batch 150/537: Loss=1.4504 (C:1.4504, R:0.0018)
Batch 175/537: Loss=1.4407 (C:1.4407, R:0.0017)
Batch 200/537: Loss=1.4818 (C:1.4818, R:0.0018)
Batch 225/537: Loss=1.4258 (C:1.4258, R:0.0018)
Batch 250/537: Loss=1.4884 (C:1.4884, R:0.0018)
Batch 275/537: Loss=1.4466 (C:1.4466, R:0.0017)
Batch 300/537: Loss=1.4351 (C:1.4351, R:0.0018)
Batch 325/537: Loss=1.4281 (C:1.4281, R:0.0018)
Batch 350/537: Loss=1.4404 (C:1.4404, R:0.0018)
Batch 375/537: Loss=1.4425 (C:1.4425, R:0.0018)
Batch 400/537: Loss=1.4316 (C:1.4316, R:0.0018)
Batch 425/537: Loss=1.4394 (C:1.4394, R:0.0018)
Batch 450/537: Loss=1.4168 (C:1.4168, R:0.0018)
Batch 475/537: Loss=1.4050 (C:1.4050, R:0.0018)
Batch 500/537: Loss=1.4221 (C:1.4221, R:0.0018)
Batch 525/537: Loss=1.4392 (C:1.4392, R:0.0017)

============================================================
Epoch 4/100 completed in 35.8s
Train: Loss=1.4381 (C:1.4381, R:0.0018) Ratio=1.96x
Val:   Loss=1.4192 (C:1.4192, R:0.0018) Ratio=2.08x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.4192)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.4084 (C:1.4084, R:0.0018)
Batch  25/537: Loss=1.4175 (C:1.4175, R:0.0018)
Batch  50/537: Loss=1.4172 (C:1.4172, R:0.0018)
Batch  75/537: Loss=1.4260 (C:1.4260, R:0.0018)
Batch 100/537: Loss=1.4050 (C:1.4050, R:0.0017)
Batch 125/537: Loss=1.4210 (C:1.4210, R:0.0018)
Batch 150/537: Loss=1.4126 (C:1.4126, R:0.0018)
Batch 175/537: Loss=1.4356 (C:1.4356, R:0.0018)
Batch 200/537: Loss=1.4298 (C:1.4298, R:0.0018)
Batch 225/537: Loss=1.4556 (C:1.4556, R:0.0018)
Batch 250/537: Loss=1.4395 (C:1.4395, R:0.0018)
Batch 275/537: Loss=1.3819 (C:1.3819, R:0.0018)
Batch 300/537: Loss=1.3841 (C:1.3841, R:0.0018)
Batch 325/537: Loss=1.4269 (C:1.4269, R:0.0018)
Batch 350/537: Loss=1.4262 (C:1.4262, R:0.0018)
Batch 375/537: Loss=1.4157 (C:1.4157, R:0.0018)
Batch 400/537: Loss=1.4173 (C:1.4173, R:0.0018)
Batch 425/537: Loss=1.4019 (C:1.4019, R:0.0018)
Batch 450/537: Loss=1.4011 (C:1.4011, R:0.0018)
Batch 475/537: Loss=1.4381 (C:1.4381, R:0.0018)
Batch 500/537: Loss=1.3924 (C:1.3924, R:0.0017)
Batch 525/537: Loss=1.3887 (C:1.3887, R:0.0018)

============================================================
Epoch 5/100 completed in 30.1s
Train: Loss=1.4245 (C:1.4245, R:0.0018) Ratio=2.06x
Val:   Loss=1.4080 (C:1.4080, R:0.0018) Ratio=2.18x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.4080)
Checkpoint saved at epoch 5
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.4157 (C:1.4157, R:0.0018)
Batch  25/537: Loss=1.4760 (C:1.4760, R:0.0018)
Batch  50/537: Loss=1.3957 (C:1.3957, R:0.0018)
Batch  75/537: Loss=1.4143 (C:1.4143, R:0.0018)
Batch 100/537: Loss=1.4388 (C:1.4388, R:0.0018)
Batch 125/537: Loss=1.4011 (C:1.4011, R:0.0017)
Batch 150/537: Loss=1.3639 (C:1.3639, R:0.0017)
Batch 175/537: Loss=1.4008 (C:1.4008, R:0.0018)
Batch 200/537: Loss=1.4362 (C:1.4362, R:0.0017)
Batch 225/537: Loss=1.3886 (C:1.3886, R:0.0018)
Batch 250/537: Loss=1.3983 (C:1.3983, R:0.0018)
Batch 275/537: Loss=1.4135 (C:1.4135, R:0.0018)
Batch 300/537: Loss=1.4270 (C:1.4270, R:0.0018)
Batch 325/537: Loss=1.4397 (C:1.4397, R:0.0018)
Batch 350/537: Loss=1.4165 (C:1.4165, R:0.0017)
Batch 375/537: Loss=1.4115 (C:1.4115, R:0.0018)
Batch 400/537: Loss=1.3808 (C:1.3808, R:0.0018)
Batch 425/537: Loss=1.4349 (C:1.4349, R:0.0018)
Batch 450/537: Loss=1.3884 (C:1.3884, R:0.0017)
Batch 475/537: Loss=1.4410 (C:1.4410, R:0.0018)
Batch 500/537: Loss=1.4069 (C:1.4069, R:0.0017)
Batch 525/537: Loss=1.4302 (C:1.4302, R:0.0018)

============================================================
Epoch 6/100 completed in 29.7s
Train: Loss=1.4144 (C:1.4144, R:0.0018) Ratio=2.10x
Val:   Loss=1.3961 (C:1.3961, R:0.0018) Ratio=2.28x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.3961)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.542 ± 0.687
    Neg distances: 1.248 ± 0.842
    Separation ratio: 2.30x
    Gap: -2.778
    ✅ Good global separation

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.3663 (C:1.3663, R:0.0018)
Batch  25/537: Loss=1.3219 (C:1.3219, R:0.0017)
Batch  50/537: Loss=1.3233 (C:1.3233, R:0.0018)
Batch  75/537: Loss=1.3461 (C:1.3461, R:0.0018)
Batch 100/537: Loss=1.3674 (C:1.3674, R:0.0018)
Batch 125/537: Loss=1.3194 (C:1.3194, R:0.0018)
Batch 150/537: Loss=1.3065 (C:1.3065, R:0.0018)
Batch 175/537: Loss=1.3735 (C:1.3735, R:0.0018)
Batch 200/537: Loss=1.3621 (C:1.3621, R:0.0018)
Batch 225/537: Loss=1.3691 (C:1.3691, R:0.0018)
Batch 250/537: Loss=1.3378 (C:1.3378, R:0.0018)
Batch 275/537: Loss=1.3467 (C:1.3467, R:0.0018)
Batch 300/537: Loss=1.3845 (C:1.3845, R:0.0018)
Batch 325/537: Loss=1.3273 (C:1.3273, R:0.0018)
Batch 350/537: Loss=1.3422 (C:1.3422, R:0.0018)
Batch 375/537: Loss=1.3370 (C:1.3370, R:0.0018)
Batch 400/537: Loss=1.3661 (C:1.3661, R:0.0018)
Batch 425/537: Loss=1.3544 (C:1.3544, R:0.0018)
Batch 450/537: Loss=1.3739 (C:1.3739, R:0.0017)
Batch 475/537: Loss=1.3392 (C:1.3392, R:0.0018)
Batch 500/537: Loss=1.3804 (C:1.3804, R:0.0018)
Batch 525/537: Loss=1.3378 (C:1.3378, R:0.0018)

============================================================
Epoch 7/100 completed in 34.4s
Train: Loss=1.3477 (C:1.3477, R:0.0018) Ratio=2.17x
Val:   Loss=1.3256 (C:1.3256, R:0.0018) Ratio=2.37x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.3256)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.3392 (C:1.3392, R:0.0018)
Batch  25/537: Loss=1.3343 (C:1.3343, R:0.0018)
Batch  50/537: Loss=1.3289 (C:1.3289, R:0.0018)
Batch  75/537: Loss=1.3071 (C:1.3071, R:0.0018)
Batch 100/537: Loss=1.3671 (C:1.3671, R:0.0018)
Batch 125/537: Loss=1.3648 (C:1.3648, R:0.0018)
Batch 150/537: Loss=1.3213 (C:1.3213, R:0.0018)
Batch 175/537: Loss=1.3458 (C:1.3458, R:0.0018)
Batch 200/537: Loss=1.3315 (C:1.3315, R:0.0018)
Batch 225/537: Loss=1.3282 (C:1.3282, R:0.0018)
Batch 250/537: Loss=1.3338 (C:1.3338, R:0.0018)
Batch 275/537: Loss=1.3576 (C:1.3576, R:0.0017)
Batch 300/537: Loss=1.3339 (C:1.3339, R:0.0017)
Batch 325/537: Loss=1.3184 (C:1.3184, R:0.0018)
Batch 350/537: Loss=1.3056 (C:1.3056, R:0.0018)
Batch 375/537: Loss=1.3487 (C:1.3487, R:0.0018)
Batch 400/537: Loss=1.3337 (C:1.3337, R:0.0018)
Batch 425/537: Loss=1.3073 (C:1.3073, R:0.0018)
Batch 450/537: Loss=1.3270 (C:1.3270, R:0.0018)
Batch 475/537: Loss=1.2925 (C:1.2925, R:0.0018)
Batch 500/537: Loss=1.3620 (C:1.3620, R:0.0018)
Batch 525/537: Loss=1.3585 (C:1.3585, R:0.0018)

============================================================
Epoch 8/100 completed in 29.2s
Train: Loss=1.3380 (C:1.3380, R:0.0018) Ratio=2.26x
Val:   Loss=1.3150 (C:1.3150, R:0.0018) Ratio=2.47x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.3150)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.3196 (C:1.3196, R:0.0018)
Batch  25/537: Loss=1.3136 (C:1.3136, R:0.0018)
Batch  50/537: Loss=1.3452 (C:1.3452, R:0.0018)
Batch  75/537: Loss=1.2983 (C:1.2983, R:0.0017)
Batch 100/537: Loss=1.3590 (C:1.3590, R:0.0018)
Batch 125/537: Loss=1.3341 (C:1.3341, R:0.0017)
Batch 150/537: Loss=1.3465 (C:1.3465, R:0.0018)
Batch 175/537: Loss=1.3322 (C:1.3322, R:0.0018)
Batch 200/537: Loss=1.3319 (C:1.3319, R:0.0018)
Batch 225/537: Loss=1.3130 (C:1.3130, R:0.0018)
Batch 250/537: Loss=1.3154 (C:1.3154, R:0.0018)
Batch 275/537: Loss=1.3256 (C:1.3256, R:0.0017)
Batch 300/537: Loss=1.3449 (C:1.3449, R:0.0017)
Batch 325/537: Loss=1.3013 (C:1.3013, R:0.0018)
Batch 350/537: Loss=1.3275 (C:1.3275, R:0.0018)
Batch 375/537: Loss=1.3225 (C:1.3225, R:0.0018)
Batch 400/537: Loss=1.3262 (C:1.3262, R:0.0018)
Batch 425/537: Loss=1.3541 (C:1.3541, R:0.0018)
Batch 450/537: Loss=1.3401 (C:1.3401, R:0.0018)
Batch 475/537: Loss=1.3398 (C:1.3398, R:0.0018)
Batch 500/537: Loss=1.3200 (C:1.3200, R:0.0018)
Batch 525/537: Loss=1.3351 (C:1.3351, R:0.0018)

============================================================
Epoch 9/100 completed in 30.1s
Train: Loss=1.3316 (C:1.3316, R:0.0018) Ratio=2.36x
Val:   Loss=1.3078 (C:1.3078, R:0.0018) Ratio=2.53x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.3078)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.526 ± 0.673
    Neg distances: 1.281 ± 0.834
    Separation ratio: 2.43x
    Gap: -2.544
    ✅ Good global separation

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.2827 (C:1.2827, R:0.0018)
Batch  25/537: Loss=1.3040 (C:1.3040, R:0.0018)
Batch  50/537: Loss=1.3123 (C:1.3123, R:0.0018)
Batch  75/537: Loss=1.3005 (C:1.3005, R:0.0018)
Batch 100/537: Loss=1.2610 (C:1.2610, R:0.0018)
Batch 125/537: Loss=1.3048 (C:1.3048, R:0.0018)
Batch 150/537: Loss=1.2932 (C:1.2932, R:0.0018)
Batch 175/537: Loss=1.3086 (C:1.3086, R:0.0018)
Batch 200/537: Loss=1.2820 (C:1.2820, R:0.0018)
Batch 225/537: Loss=1.3014 (C:1.3014, R:0.0018)
Batch 250/537: Loss=1.3107 (C:1.3107, R:0.0018)
Batch 275/537: Loss=1.2974 (C:1.2974, R:0.0018)
Batch 300/537: Loss=1.2926 (C:1.2926, R:0.0018)
Batch 325/537: Loss=1.2959 (C:1.2959, R:0.0018)
Batch 350/537: Loss=1.2858 (C:1.2858, R:0.0018)
Batch 375/537: Loss=1.3215 (C:1.3215, R:0.0018)
Batch 400/537: Loss=1.2609 (C:1.2609, R:0.0018)
Batch 425/537: Loss=1.3270 (C:1.3270, R:0.0018)
Batch 450/537: Loss=1.3095 (C:1.3095, R:0.0018)
Batch 475/537: Loss=1.2623 (C:1.2623, R:0.0018)
Batch 500/537: Loss=1.3476 (C:1.3476, R:0.0018)
Batch 525/537: Loss=1.2990 (C:1.2990, R:0.0018)

============================================================
Epoch 10/100 completed in 34.9s
Train: Loss=1.2995 (C:1.2995, R:0.0018) Ratio=2.42x
Val:   Loss=1.2752 (C:1.2752, R:0.0018) Ratio=2.63x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2752)
Checkpoint saved at epoch 10
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.2695 (C:1.2695, R:0.0018)
Batch  25/537: Loss=1.2954 (C:1.2954, R:0.0018)
Batch  50/537: Loss=1.2574 (C:1.2574, R:0.0018)
Batch  75/537: Loss=1.2898 (C:1.2898, R:0.0018)
Batch 100/537: Loss=1.3045 (C:1.3045, R:0.0018)
Batch 125/537: Loss=1.2878 (C:1.2878, R:0.0018)
Batch 150/537: Loss=1.2839 (C:1.2839, R:0.0017)
Batch 175/537: Loss=1.3068 (C:1.3068, R:0.0018)
Batch 200/537: Loss=1.2860 (C:1.2860, R:0.0017)
Batch 225/537: Loss=1.2941 (C:1.2941, R:0.0018)
Batch 250/537: Loss=1.3158 (C:1.3158, R:0.0017)
Batch 275/537: Loss=1.2828 (C:1.2828, R:0.0018)
Batch 300/537: Loss=1.3097 (C:1.3097, R:0.0018)
Batch 325/537: Loss=1.3156 (C:1.3156, R:0.0017)
Batch 350/537: Loss=1.3242 (C:1.3242, R:0.0018)
Batch 375/537: Loss=1.3219 (C:1.3219, R:0.0018)
Batch 400/537: Loss=1.3344 (C:1.3344, R:0.0018)
Batch 425/537: Loss=1.2787 (C:1.2787, R:0.0018)
Batch 450/537: Loss=1.3208 (C:1.3208, R:0.0018)
Batch 475/537: Loss=1.2767 (C:1.2767, R:0.0018)
Batch 500/537: Loss=1.2829 (C:1.2829, R:0.0018)
Batch 525/537: Loss=1.2752 (C:1.2752, R:0.0018)

============================================================
Epoch 11/100 completed in 29.2s
Train: Loss=1.2943 (C:1.2943, R:0.0018) Ratio=2.43x
Val:   Loss=1.2686 (C:1.2686, R:0.0018) Ratio=2.69x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2686)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.3143 (C:1.3143, R:0.0018)
Batch  25/537: Loss=1.2893 (C:1.2893, R:0.0017)
Batch  50/537: Loss=1.3036 (C:1.3036, R:0.0018)
Batch  75/537: Loss=1.3115 (C:1.3115, R:0.0018)
Batch 100/537: Loss=1.3131 (C:1.3131, R:0.0018)
Batch 125/537: Loss=1.3059 (C:1.3059, R:0.0018)
Batch 150/537: Loss=1.2961 (C:1.2961, R:0.0018)
Batch 175/537: Loss=1.2588 (C:1.2588, R:0.0018)
Batch 200/537: Loss=1.2637 (C:1.2637, R:0.0018)
Batch 225/537: Loss=1.2856 (C:1.2856, R:0.0018)
Batch 250/537: Loss=1.2974 (C:1.2974, R:0.0017)
Batch 275/537: Loss=1.2920 (C:1.2920, R:0.0018)
Batch 300/537: Loss=1.2569 (C:1.2569, R:0.0018)
Batch 325/537: Loss=1.2715 (C:1.2715, R:0.0018)
Batch 350/537: Loss=1.2930 (C:1.2930, R:0.0018)
Batch 375/537: Loss=1.3211 (C:1.3211, R:0.0018)
Batch 400/537: Loss=1.2914 (C:1.2914, R:0.0017)
Batch 425/537: Loss=1.3129 (C:1.3129, R:0.0018)
Batch 450/537: Loss=1.2442 (C:1.2442, R:0.0018)
Batch 475/537: Loss=1.2956 (C:1.2956, R:0.0018)
Batch 500/537: Loss=1.2958 (C:1.2958, R:0.0018)
Batch 525/537: Loss=1.2892 (C:1.2892, R:0.0018)

============================================================
Epoch 12/100 completed in 29.0s
Train: Loss=1.2876 (C:1.2876, R:0.0018) Ratio=2.46x
Val:   Loss=1.2624 (C:1.2624, R:0.0018) Ratio=2.77x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2624)
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.478 ± 0.650
    Neg distances: 1.324 ± 0.850
    Separation ratio: 2.77x
    Gap: -2.626
    ✅ Good global separation

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=1.2700 (C:1.2700, R:0.0018)
Batch  25/537: Loss=1.2656 (C:1.2656, R:0.0018)
Batch  50/537: Loss=1.2172 (C:1.2172, R:0.0018)
Batch  75/537: Loss=1.2441 (C:1.2441, R:0.0018)
Batch 100/537: Loss=1.2310 (C:1.2310, R:0.0018)
Batch 125/537: Loss=1.2273 (C:1.2273, R:0.0018)
Batch 150/537: Loss=1.2268 (C:1.2268, R:0.0018)
Batch 175/537: Loss=1.2049 (C:1.2049, R:0.0017)
Batch 200/537: Loss=1.2397 (C:1.2397, R:0.0018)
Batch 225/537: Loss=1.2534 (C:1.2534, R:0.0018)
Batch 250/537: Loss=1.2154 (C:1.2154, R:0.0018)
Batch 275/537: Loss=1.2483 (C:1.2483, R:0.0018)
Batch 300/537: Loss=1.2428 (C:1.2428, R:0.0017)
Batch 325/537: Loss=1.2281 (C:1.2281, R:0.0018)
Batch 350/537: Loss=1.2335 (C:1.2335, R:0.0018)
Batch 375/537: Loss=1.2487 (C:1.2487, R:0.0018)
Batch 400/537: Loss=1.2434 (C:1.2434, R:0.0018)
Batch 425/537: Loss=1.2193 (C:1.2193, R:0.0018)
Batch 450/537: Loss=1.2316 (C:1.2316, R:0.0018)
Batch 475/537: Loss=1.2119 (C:1.2119, R:0.0018)
Batch 500/537: Loss=1.2124 (C:1.2124, R:0.0018)
Batch 525/537: Loss=1.2231 (C:1.2231, R:0.0017)

============================================================
Epoch 13/100 completed in 35.2s
Train: Loss=1.2382 (C:1.2382, R:0.0018) Ratio=2.53x
Val:   Loss=1.2076 (C:1.2076, R:0.0018) Ratio=2.85x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2076)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=1.2424 (C:1.2424, R:0.0017)
Batch  25/537: Loss=1.2561 (C:1.2561, R:0.0018)
Batch  50/537: Loss=1.2458 (C:1.2458, R:0.0017)
Batch  75/537: Loss=1.2220 (C:1.2220, R:0.0018)
Batch 100/537: Loss=1.2464 (C:1.2464, R:0.0018)
Batch 125/537: Loss=1.2583 (C:1.2583, R:0.0018)
Batch 150/537: Loss=1.2599 (C:1.2599, R:0.0018)
Batch 175/537: Loss=1.2384 (C:1.2384, R:0.0017)
Batch 200/537: Loss=1.2539 (C:1.2539, R:0.0018)
Batch 225/537: Loss=1.2337 (C:1.2337, R:0.0018)
Batch 250/537: Loss=1.2678 (C:1.2678, R:0.0018)
Batch 275/537: Loss=1.2322 (C:1.2322, R:0.0018)
Batch 300/537: Loss=1.2340 (C:1.2340, R:0.0018)
Batch 325/537: Loss=1.2199 (C:1.2199, R:0.0018)
Batch 350/537: Loss=1.2360 (C:1.2360, R:0.0018)
Batch 375/537: Loss=1.2072 (C:1.2072, R:0.0018)
Batch 400/537: Loss=1.2542 (C:1.2542, R:0.0018)
Batch 425/537: Loss=1.2020 (C:1.2020, R:0.0017)
Batch 450/537: Loss=1.2133 (C:1.2133, R:0.0018)
Batch 475/537: Loss=1.2401 (C:1.2401, R:0.0018)
Batch 500/537: Loss=1.2054 (C:1.2054, R:0.0018)
Batch 525/537: Loss=1.2563 (C:1.2563, R:0.0018)

============================================================
Epoch 14/100 completed in 29.7s
Train: Loss=1.2337 (C:1.2337, R:0.0018) Ratio=2.56x
Val:   Loss=1.2028 (C:1.2028, R:0.0018) Ratio=2.94x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2028)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=1.2236 (C:1.2236, R:0.0017)
Batch  25/537: Loss=1.2329 (C:1.2329, R:0.0018)
Batch  50/537: Loss=1.2098 (C:1.2098, R:0.0017)
Batch  75/537: Loss=1.2186 (C:1.2186, R:0.0018)
Batch 100/537: Loss=1.2251 (C:1.2251, R:0.0018)
Batch 125/537: Loss=1.2133 (C:1.2133, R:0.0018)
Batch 150/537: Loss=1.2268 (C:1.2268, R:0.0017)
Batch 175/537: Loss=1.1974 (C:1.1974, R:0.0018)
Batch 200/537: Loss=1.2605 (C:1.2605, R:0.0018)
Batch 225/537: Loss=1.2639 (C:1.2639, R:0.0018)
Batch 250/537: Loss=1.2571 (C:1.2571, R:0.0018)
Batch 275/537: Loss=1.2420 (C:1.2420, R:0.0018)
Batch 300/537: Loss=1.2292 (C:1.2292, R:0.0018)
Batch 325/537: Loss=1.2397 (C:1.2397, R:0.0018)
Batch 350/537: Loss=1.2346 (C:1.2346, R:0.0018)
Batch 375/537: Loss=1.2350 (C:1.2350, R:0.0018)
Batch 400/537: Loss=1.2481 (C:1.2481, R:0.0018)
Batch 425/537: Loss=1.2099 (C:1.2099, R:0.0018)
Batch 450/537: Loss=1.2271 (C:1.2271, R:0.0018)
Batch 475/537: Loss=1.1979 (C:1.1979, R:0.0018)
Batch 500/537: Loss=1.2288 (C:1.2288, R:0.0018)
Batch 525/537: Loss=1.2251 (C:1.2251, R:0.0018)

============================================================
Epoch 15/100 completed in 29.3s
Train: Loss=1.2297 (C:1.2297, R:0.0018) Ratio=2.62x
Val:   Loss=1.1985 (C:1.1985, R:0.0018) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1985)
Checkpoint saved at epoch 15
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.462 ± 0.643
    Neg distances: 1.353 ± 0.855
    Separation ratio: 2.93x
    Gap: -2.679
    ✅ Good global separation

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=1.1944 (C:1.1944, R:0.0018)
Batch  25/537: Loss=1.1838 (C:1.1838, R:0.0018)
Batch  50/537: Loss=1.2001 (C:1.2001, R:0.0018)
Batch  75/537: Loss=1.2446 (C:1.2446, R:0.0017)
Batch 100/537: Loss=1.2009 (C:1.2009, R:0.0018)
Batch 125/537: Loss=1.2175 (C:1.2175, R:0.0018)
Batch 150/537: Loss=1.2161 (C:1.2161, R:0.0017)
Batch 175/537: Loss=1.1873 (C:1.1873, R:0.0018)
Batch 200/537: Loss=1.1800 (C:1.1800, R:0.0018)
Batch 225/537: Loss=1.2082 (C:1.2082, R:0.0018)
Batch 250/537: Loss=1.2021 (C:1.2021, R:0.0018)
Batch 275/537: Loss=1.1993 (C:1.1993, R:0.0018)
Batch 300/537: Loss=1.2230 (C:1.2230, R:0.0018)
Batch 325/537: Loss=1.1977 (C:1.1977, R:0.0018)
Batch 350/537: Loss=1.2469 (C:1.2469, R:0.0017)
Batch 375/537: Loss=1.2008 (C:1.2008, R:0.0018)
Batch 400/537: Loss=1.2078 (C:1.2078, R:0.0018)
Batch 425/537: Loss=1.2164 (C:1.2164, R:0.0018)
Batch 450/537: Loss=1.1712 (C:1.1712, R:0.0017)
Batch 475/537: Loss=1.2151 (C:1.2151, R:0.0018)
Batch 500/537: Loss=1.2346 (C:1.2346, R:0.0018)
Batch 525/537: Loss=1.1884 (C:1.1884, R:0.0018)

============================================================
Epoch 16/100 completed in 34.8s
Train: Loss=1.2058 (C:1.2058, R:0.0018) Ratio=2.69x
Val:   Loss=1.1719 (C:1.1719, R:0.0018) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1719)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=1.2030 (C:1.2030, R:0.0018)
Batch  25/537: Loss=1.1618 (C:1.1618, R:0.0018)
Batch  50/537: Loss=1.2101 (C:1.2101, R:0.0018)
Batch  75/537: Loss=1.2139 (C:1.2139, R:0.0018)
Batch 100/537: Loss=1.1995 (C:1.1995, R:0.0018)
Batch 125/537: Loss=1.2064 (C:1.2064, R:0.0018)
Batch 150/537: Loss=1.1649 (C:1.1649, R:0.0017)
Batch 175/537: Loss=1.2090 (C:1.2090, R:0.0017)
Batch 200/537: Loss=1.1878 (C:1.1878, R:0.0018)
Batch 225/537: Loss=1.2119 (C:1.2119, R:0.0018)
Batch 250/537: Loss=1.1948 (C:1.1948, R:0.0017)
Batch 275/537: Loss=1.1944 (C:1.1944, R:0.0018)
Batch 300/537: Loss=1.2304 (C:1.2304, R:0.0018)
Batch 325/537: Loss=1.2107 (C:1.2107, R:0.0018)
Batch 350/537: Loss=1.2012 (C:1.2012, R:0.0018)
Batch 375/537: Loss=1.1913 (C:1.1913, R:0.0018)
Batch 400/537: Loss=1.2053 (C:1.2053, R:0.0018)
Batch 425/537: Loss=1.2062 (C:1.2062, R:0.0018)
Batch 450/537: Loss=1.2371 (C:1.2371, R:0.0018)
Batch 475/537: Loss=1.2182 (C:1.2182, R:0.0018)
Batch 500/537: Loss=1.2420 (C:1.2420, R:0.0018)
Batch 525/537: Loss=1.2152 (C:1.2152, R:0.0018)

============================================================
Epoch 17/100 completed in 29.4s
Train: Loss=1.2019 (C:1.2019, R:0.0018) Ratio=2.63x
Val:   Loss=1.1685 (C:1.1685, R:0.0018) Ratio=3.12x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1685)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=1.2062 (C:1.2062, R:0.0017)
Batch  25/537: Loss=1.1577 (C:1.1577, R:0.0018)
Batch  50/537: Loss=1.2015 (C:1.2015, R:0.0017)
Batch  75/537: Loss=1.1944 (C:1.1944, R:0.0018)
Batch 100/537: Loss=1.1901 (C:1.1901, R:0.0018)
Batch 125/537: Loss=1.1893 (C:1.1893, R:0.0018)
Batch 150/537: Loss=1.2077 (C:1.2077, R:0.0018)
Batch 175/537: Loss=1.1897 (C:1.1897, R:0.0018)
Batch 200/537: Loss=1.1872 (C:1.1872, R:0.0018)
Batch 225/537: Loss=1.2086 (C:1.2086, R:0.0018)
Batch 250/537: Loss=1.2033 (C:1.2033, R:0.0018)
Batch 275/537: Loss=1.1853 (C:1.1853, R:0.0018)
Batch 300/537: Loss=1.1751 (C:1.1751, R:0.0017)
Batch 325/537: Loss=1.1918 (C:1.1918, R:0.0018)
Batch 350/537: Loss=1.2152 (C:1.2152, R:0.0018)
Batch 375/537: Loss=1.2156 (C:1.2156, R:0.0018)
Batch 400/537: Loss=1.2160 (C:1.2160, R:0.0018)
Batch 425/537: Loss=1.1597 (C:1.1597, R:0.0018)
Batch 450/537: Loss=1.2190 (C:1.2190, R:0.0018)
Batch 475/537: Loss=1.2066 (C:1.2066, R:0.0018)
Batch 500/537: Loss=1.1732 (C:1.1732, R:0.0018)
Batch 525/537: Loss=1.1652 (C:1.1652, R:0.0018)

============================================================
Epoch 18/100 completed in 29.2s
Train: Loss=1.1986 (C:1.1986, R:0.0018) Ratio=2.76x
Val:   Loss=1.1659 (C:1.1659, R:0.0018) Ratio=3.19x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1659)
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.448 ± 0.626
    Neg distances: 1.378 ± 0.859
    Separation ratio: 3.07x
    Gap: -2.528
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=1.1429 (C:1.1429, R:0.0018)
Batch  25/537: Loss=1.1975 (C:1.1975, R:0.0018)
Batch  50/537: Loss=1.1947 (C:1.1947, R:0.0018)
Batch  75/537: Loss=1.1805 (C:1.1805, R:0.0018)
Batch 100/537: Loss=1.1803 (C:1.1803, R:0.0018)
Batch 125/537: Loss=1.2014 (C:1.2014, R:0.0018)
Batch 150/537: Loss=1.1606 (C:1.1606, R:0.0018)
Batch 175/537: Loss=1.2203 (C:1.2203, R:0.0018)
Batch 200/537: Loss=1.1845 (C:1.1845, R:0.0018)
Batch 225/537: Loss=1.1488 (C:1.1488, R:0.0018)
Batch 250/537: Loss=1.1874 (C:1.1874, R:0.0018)
Batch 275/537: Loss=1.1485 (C:1.1485, R:0.0018)
Batch 300/537: Loss=1.1816 (C:1.1816, R:0.0018)
Batch 325/537: Loss=1.1687 (C:1.1687, R:0.0018)
Batch 350/537: Loss=1.1927 (C:1.1927, R:0.0018)
Batch 375/537: Loss=1.1891 (C:1.1891, R:0.0017)
Batch 400/537: Loss=1.1926 (C:1.1926, R:0.0018)
Batch 425/537: Loss=1.1640 (C:1.1640, R:0.0018)
Batch 450/537: Loss=1.1698 (C:1.1698, R:0.0017)
Batch 475/537: Loss=1.1934 (C:1.1934, R:0.0018)
Batch 500/537: Loss=1.1785 (C:1.1785, R:0.0018)
Batch 525/537: Loss=1.2070 (C:1.2070, R:0.0018)

============================================================
Epoch 19/100 completed in 35.2s
Train: Loss=1.1802 (C:1.1802, R:0.0018) Ratio=2.76x
Val:   Loss=1.1431 (C:1.1431, R:0.0018) Ratio=3.27x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1431)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=1.1730 (C:1.1730, R:0.0018)
Batch  25/537: Loss=1.1653 (C:1.1653, R:0.0018)
Batch  50/537: Loss=1.1587 (C:1.1587, R:0.0018)
Batch  75/537: Loss=1.1968 (C:1.1968, R:0.0018)
Batch 100/537: Loss=1.1785 (C:1.1785, R:0.0018)
Batch 125/537: Loss=1.1791 (C:1.1791, R:0.0018)
Batch 150/537: Loss=1.1878 (C:1.1878, R:0.0018)
Batch 175/537: Loss=1.1679 (C:1.1679, R:0.0018)
Batch 200/537: Loss=1.1614 (C:1.1614, R:0.0018)
Batch 225/537: Loss=1.2138 (C:1.2138, R:0.0018)
Batch 250/537: Loss=1.1491 (C:1.1491, R:0.0018)
Batch 275/537: Loss=1.1447 (C:1.1447, R:0.0018)
Batch 300/537: Loss=1.1515 (C:1.1515, R:0.0018)
Batch 325/537: Loss=1.1842 (C:1.1842, R:0.0018)
Batch 350/537: Loss=1.1994 (C:1.1994, R:0.0018)
Batch 375/537: Loss=1.1792 (C:1.1792, R:0.0018)
Batch 400/537: Loss=1.2039 (C:1.2039, R:0.0018)
Batch 425/537: Loss=1.2297 (C:1.2297, R:0.0018)
Batch 450/537: Loss=1.2123 (C:1.2123, R:0.0018)
Batch 475/537: Loss=1.1772 (C:1.1772, R:0.0018)
Batch 500/537: Loss=1.1719 (C:1.1719, R:0.0018)
Batch 525/537: Loss=1.2139 (C:1.2139, R:0.0018)

============================================================
Epoch 20/100 completed in 29.0s
Train: Loss=1.1771 (C:1.1771, R:0.0018) Ratio=2.78x
Val:   Loss=1.1395 (C:1.1395, R:0.0018) Ratio=3.38x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1395)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=1.1720 (C:1.1720, R:0.0018)
Batch  25/537: Loss=1.1720 (C:1.1720, R:0.0018)
Batch  50/537: Loss=1.1981 (C:1.1981, R:0.0017)
Batch  75/537: Loss=1.1899 (C:1.1899, R:0.0018)
Batch 100/537: Loss=1.1621 (C:1.1621, R:0.0018)
Batch 125/537: Loss=1.2095 (C:1.2095, R:0.0018)
Batch 150/537: Loss=1.1481 (C:1.1481, R:0.0017)
Batch 175/537: Loss=1.1550 (C:1.1550, R:0.0018)
Batch 200/537: Loss=1.1699 (C:1.1699, R:0.0017)
Batch 225/537: Loss=1.1908 (C:1.1908, R:0.0018)
Batch 250/537: Loss=1.1527 (C:1.1527, R:0.0018)
Batch 275/537: Loss=1.1744 (C:1.1744, R:0.0018)
Batch 300/537: Loss=1.1862 (C:1.1862, R:0.0018)
Batch 325/537: Loss=1.1618 (C:1.1618, R:0.0017)
Batch 350/537: Loss=1.1815 (C:1.1815, R:0.0018)
Batch 375/537: Loss=1.2078 (C:1.2078, R:0.0018)
Batch 400/537: Loss=1.2072 (C:1.2072, R:0.0018)
Batch 425/537: Loss=1.1697 (C:1.1697, R:0.0018)
Batch 450/537: Loss=1.1946 (C:1.1946, R:0.0017)
Batch 475/537: Loss=1.1681 (C:1.1681, R:0.0018)
Batch 500/537: Loss=1.1846 (C:1.1846, R:0.0018)
Batch 525/537: Loss=1.1906 (C:1.1906, R:0.0017)

============================================================
Epoch 21/100 completed in 29.2s
Train: Loss=1.1728 (C:1.1728, R:0.0018) Ratio=2.80x
Val:   Loss=1.1354 (C:1.1354, R:0.0018) Ratio=3.42x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1354)
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.407 ± 0.608
    Neg distances: 1.422 ± 0.884
    Separation ratio: 3.49x
    Gap: -2.682
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=1.1026 (C:1.1026, R:0.0018)
Batch  25/537: Loss=1.1217 (C:1.1217, R:0.0018)
Batch  50/537: Loss=1.1139 (C:1.1139, R:0.0018)
Batch  75/537: Loss=1.1440 (C:1.1440, R:0.0018)
Batch 100/537: Loss=1.0785 (C:1.0785, R:0.0018)
Batch 125/537: Loss=1.1471 (C:1.1471, R:0.0018)
Batch 150/537: Loss=1.1427 (C:1.1427, R:0.0017)
Batch 175/537: Loss=1.1194 (C:1.1194, R:0.0018)
Batch 200/537: Loss=1.1357 (C:1.1357, R:0.0018)
Batch 225/537: Loss=1.1598 (C:1.1598, R:0.0018)
Batch 250/537: Loss=1.1447 (C:1.1447, R:0.0018)
Batch 275/537: Loss=1.1486 (C:1.1486, R:0.0018)
Batch 300/537: Loss=1.1564 (C:1.1564, R:0.0018)
Batch 325/537: Loss=1.1617 (C:1.1617, R:0.0018)
Batch 350/537: Loss=1.1337 (C:1.1337, R:0.0018)
Batch 375/537: Loss=1.1503 (C:1.1503, R:0.0018)
Batch 400/537: Loss=1.1437 (C:1.1437, R:0.0018)
Batch 425/537: Loss=1.1386 (C:1.1386, R:0.0017)
Batch 450/537: Loss=1.1076 (C:1.1076, R:0.0018)
Batch 475/537: Loss=1.1596 (C:1.1596, R:0.0018)
Batch 500/537: Loss=1.1366 (C:1.1366, R:0.0018)
Batch 525/537: Loss=1.1226 (C:1.1226, R:0.0017)

============================================================
Epoch 22/100 completed in 34.2s
Train: Loss=1.1317 (C:1.1317, R:0.0018) Ratio=2.88x
Val:   Loss=1.0928 (C:1.0928, R:0.0018) Ratio=3.45x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0928)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=1.1323 (C:1.1323, R:0.0018)
Batch  25/537: Loss=1.1323 (C:1.1323, R:0.0018)
Batch  50/537: Loss=1.1364 (C:1.1364, R:0.0018)
Batch  75/537: Loss=1.1224 (C:1.1224, R:0.0018)
Batch 100/537: Loss=1.1932 (C:1.1932, R:0.0018)
Batch 125/537: Loss=1.1069 (C:1.1069, R:0.0018)
Batch 150/537: Loss=1.1002 (C:1.1002, R:0.0018)
Batch 175/537: Loss=1.1275 (C:1.1275, R:0.0018)
Batch 200/537: Loss=1.1451 (C:1.1451, R:0.0018)
Batch 225/537: Loss=1.1525 (C:1.1525, R:0.0018)
Batch 250/537: Loss=1.1268 (C:1.1268, R:0.0018)
Batch 275/537: Loss=1.1271 (C:1.1271, R:0.0018)
Batch 300/537: Loss=1.1162 (C:1.1162, R:0.0018)
Batch 325/537: Loss=1.1439 (C:1.1439, R:0.0018)
Batch 350/537: Loss=1.0913 (C:1.0913, R:0.0018)
Batch 375/537: Loss=1.1000 (C:1.1000, R:0.0018)
Batch 400/537: Loss=1.1536 (C:1.1536, R:0.0018)
Batch 425/537: Loss=1.1065 (C:1.1065, R:0.0018)
Batch 450/537: Loss=1.1324 (C:1.1324, R:0.0017)
Batch 475/537: Loss=1.1181 (C:1.1181, R:0.0018)
Batch 500/537: Loss=1.1691 (C:1.1691, R:0.0018)
Batch 525/537: Loss=1.1166 (C:1.1166, R:0.0018)

============================================================
Epoch 23/100 completed in 29.0s
Train: Loss=1.1292 (C:1.1292, R:0.0018) Ratio=2.95x
Val:   Loss=1.0892 (C:1.0892, R:0.0018) Ratio=3.55x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0892)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=1.1210 (C:1.1210, R:0.0018)
Batch  25/537: Loss=1.1524 (C:1.1524, R:0.0017)
Batch  50/537: Loss=1.0810 (C:1.0810, R:0.0018)
Batch  75/537: Loss=1.1182 (C:1.1182, R:0.0017)
Batch 100/537: Loss=1.1277 (C:1.1277, R:0.0018)
Batch 125/537: Loss=1.1501 (C:1.1501, R:0.0018)
Batch 150/537: Loss=1.1502 (C:1.1502, R:0.0017)
Batch 175/537: Loss=1.1379 (C:1.1379, R:0.0018)
Batch 200/537: Loss=1.0994 (C:1.0994, R:0.0018)
Batch 225/537: Loss=1.0868 (C:1.0868, R:0.0018)
Batch 250/537: Loss=1.1610 (C:1.1610, R:0.0018)
Batch 275/537: Loss=1.1289 (C:1.1289, R:0.0018)
Batch 300/537: Loss=1.1074 (C:1.1074, R:0.0017)
Batch 325/537: Loss=1.1405 (C:1.1405, R:0.0017)
Batch 350/537: Loss=1.1086 (C:1.1086, R:0.0018)
Batch 375/537: Loss=1.0702 (C:1.0702, R:0.0018)
Batch 400/537: Loss=1.1222 (C:1.1222, R:0.0018)
Batch 425/537: Loss=1.1361 (C:1.1361, R:0.0018)
Batch 450/537: Loss=1.1497 (C:1.1497, R:0.0018)
Batch 475/537: Loss=1.1471 (C:1.1471, R:0.0018)
Batch 500/537: Loss=1.1476 (C:1.1476, R:0.0018)
Batch 525/537: Loss=1.1357 (C:1.1357, R:0.0017)

============================================================
Epoch 24/100 completed in 29.1s
Train: Loss=1.1265 (C:1.1265, R:0.0018) Ratio=2.97x
Val:   Loss=1.0852 (C:1.0852, R:0.0018) Ratio=3.56x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0852)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.414 ± 0.605
    Neg distances: 1.423 ± 0.880
    Separation ratio: 3.44x
    Gap: -2.616
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=1.1159 (C:1.1159, R:0.0018)
Batch  25/537: Loss=1.1015 (C:1.1015, R:0.0018)
Batch  50/537: Loss=1.1415 (C:1.1415, R:0.0018)
Batch  75/537: Loss=1.1390 (C:1.1390, R:0.0018)
Batch 100/537: Loss=1.1234 (C:1.1234, R:0.0017)
Batch 125/537: Loss=1.1198 (C:1.1198, R:0.0018)
Batch 150/537: Loss=1.1090 (C:1.1090, R:0.0017)
Batch 175/537: Loss=1.1323 (C:1.1323, R:0.0018)
Batch 200/537: Loss=1.1180 (C:1.1180, R:0.0018)
Batch 225/537: Loss=1.1315 (C:1.1315, R:0.0018)
Batch 250/537: Loss=1.1040 (C:1.1040, R:0.0017)
Batch 275/537: Loss=1.1302 (C:1.1302, R:0.0017)
Batch 300/537: Loss=1.1322 (C:1.1322, R:0.0017)
Batch 325/537: Loss=1.1602 (C:1.1602, R:0.0018)
Batch 350/537: Loss=1.1537 (C:1.1537, R:0.0018)
Batch 375/537: Loss=1.1512 (C:1.1512, R:0.0018)
Batch 400/537: Loss=1.1274 (C:1.1274, R:0.0018)
Batch 425/537: Loss=1.0893 (C:1.0893, R:0.0018)
Batch 450/537: Loss=1.1311 (C:1.1311, R:0.0018)
Batch 475/537: Loss=1.1373 (C:1.1373, R:0.0018)
Batch 500/537: Loss=1.1452 (C:1.1452, R:0.0018)
Batch 525/537: Loss=1.1395 (C:1.1395, R:0.0018)

============================================================
Epoch 25/100 completed in 34.3s
Train: Loss=1.1306 (C:1.1306, R:0.0018) Ratio=3.00x
Val:   Loss=1.0872 (C:1.0872, R:0.0018) Ratio=3.71x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 25
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=1.1497 (C:1.1497, R:0.0018)
Batch  25/537: Loss=1.1014 (C:1.1014, R:0.0018)
Batch  50/537: Loss=1.1156 (C:1.1156, R:0.0018)
Batch  75/537: Loss=1.1318 (C:1.1318, R:0.0018)
Batch 100/537: Loss=1.1120 (C:1.1120, R:0.0017)
Batch 125/537: Loss=1.1278 (C:1.1278, R:0.0018)
Batch 150/537: Loss=1.1537 (C:1.1537, R:0.0018)
Batch 175/537: Loss=1.1455 (C:1.1455, R:0.0018)
Batch 200/537: Loss=1.1275 (C:1.1275, R:0.0018)
Batch 225/537: Loss=1.1241 (C:1.1241, R:0.0018)
Batch 250/537: Loss=1.1227 (C:1.1227, R:0.0017)
Batch 275/537: Loss=1.1121 (C:1.1121, R:0.0018)
Batch 300/537: Loss=1.1203 (C:1.1203, R:0.0018)
Batch 325/537: Loss=1.1470 (C:1.1470, R:0.0018)
Batch 350/537: Loss=1.1386 (C:1.1386, R:0.0018)
Batch 375/537: Loss=1.1477 (C:1.1477, R:0.0018)
Batch 400/537: Loss=1.1766 (C:1.1766, R:0.0018)
Batch 425/537: Loss=1.1275 (C:1.1275, R:0.0018)
Batch 450/537: Loss=1.1260 (C:1.1260, R:0.0018)
Batch 475/537: Loss=1.1306 (C:1.1306, R:0.0018)
Batch 500/537: Loss=1.1261 (C:1.1261, R:0.0018)
Batch 525/537: Loss=1.1484 (C:1.1484, R:0.0018)

============================================================
Epoch 26/100 completed in 28.9s
Train: Loss=1.1276 (C:1.1276, R:0.0018) Ratio=3.00x
Val:   Loss=1.0877 (C:1.0877, R:0.0018) Ratio=3.66x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=1.0871 (C:1.0871, R:0.0018)
Batch  25/537: Loss=1.1267 (C:1.1267, R:0.0017)
Batch  50/537: Loss=1.0925 (C:1.0925, R:0.0017)
Batch  75/537: Loss=1.1234 (C:1.1234, R:0.0018)
Batch 100/537: Loss=1.1413 (C:1.1413, R:0.0018)
Batch 125/537: Loss=1.1213 (C:1.1213, R:0.0018)
Batch 150/537: Loss=1.1387 (C:1.1387, R:0.0017)
Batch 175/537: Loss=1.1350 (C:1.1350, R:0.0018)
Batch 200/537: Loss=1.0840 (C:1.0840, R:0.0017)
Batch 225/537: Loss=1.1128 (C:1.1128, R:0.0018)
Batch 250/537: Loss=1.1303 (C:1.1303, R:0.0018)
Batch 275/537: Loss=1.1224 (C:1.1224, R:0.0018)
Batch 300/537: Loss=1.1649 (C:1.1649, R:0.0017)
Batch 325/537: Loss=1.1434 (C:1.1434, R:0.0018)
Batch 350/537: Loss=1.1078 (C:1.1078, R:0.0018)
Batch 375/537: Loss=1.1239 (C:1.1239, R:0.0018)
Batch 400/537: Loss=1.1470 (C:1.1470, R:0.0018)
Batch 425/537: Loss=1.1573 (C:1.1573, R:0.0018)
Batch 450/537: Loss=1.1363 (C:1.1363, R:0.0018)
Batch 475/537: Loss=1.1261 (C:1.1261, R:0.0017)
Batch 500/537: Loss=1.1168 (C:1.1168, R:0.0018)
Batch 525/537: Loss=1.1440 (C:1.1440, R:0.0018)

============================================================
Epoch 27/100 completed in 29.5s
Train: Loss=1.1265 (C:1.1265, R:0.0018) Ratio=3.07x
Val:   Loss=1.0824 (C:1.0824, R:0.0018) Ratio=3.82x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0824)
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.383 ± 0.590
    Neg distances: 1.437 ± 0.903
    Separation ratio: 3.75x
    Gap: -2.538
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=1.0835 (C:1.0835, R:0.0018)
Batch  25/537: Loss=1.1141 (C:1.1141, R:0.0017)
Batch  50/537: Loss=1.0756 (C:1.0756, R:0.0018)
Batch  75/537: Loss=1.0893 (C:1.0893, R:0.0018)
Batch 100/537: Loss=1.1010 (C:1.1010, R:0.0018)
Batch 125/537: Loss=1.1221 (C:1.1221, R:0.0018)
Batch 150/537: Loss=1.0929 (C:1.0929, R:0.0018)
Batch 175/537: Loss=1.0667 (C:1.0667, R:0.0018)
Batch 200/537: Loss=1.0693 (C:1.0693, R:0.0018)
Batch 225/537: Loss=1.0949 (C:1.0949, R:0.0018)
Batch 250/537: Loss=1.0735 (C:1.0735, R:0.0018)
Batch 275/537: Loss=1.0969 (C:1.0969, R:0.0018)
Batch 300/537: Loss=1.1065 (C:1.1065, R:0.0018)
Batch 325/537: Loss=1.0987 (C:1.0987, R:0.0018)
Batch 350/537: Loss=1.0986 (C:1.0986, R:0.0018)
Batch 375/537: Loss=1.0866 (C:1.0866, R:0.0018)
Batch 400/537: Loss=1.1148 (C:1.1148, R:0.0018)
Batch 425/537: Loss=1.1321 (C:1.1321, R:0.0018)
Batch 450/537: Loss=1.1092 (C:1.1092, R:0.0018)
Batch 475/537: Loss=1.0997 (C:1.0997, R:0.0018)
Batch 500/537: Loss=1.1535 (C:1.1535, R:0.0018)
Batch 525/537: Loss=1.0920 (C:1.0920, R:0.0018)

============================================================
Epoch 28/100 completed in 34.5s
Train: Loss=1.1032 (C:1.1032, R:0.0018) Ratio=3.14x
Val:   Loss=1.0576 (C:1.0576, R:0.0018) Ratio=3.90x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0576)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=1.0502 (C:1.0502, R:0.0018)
Batch  25/537: Loss=1.1001 (C:1.1001, R:0.0018)
Batch  50/537: Loss=1.1253 (C:1.1253, R:0.0018)
Batch  75/537: Loss=1.0916 (C:1.0916, R:0.0018)
Batch 100/537: Loss=1.1078 (C:1.1078, R:0.0017)
Batch 125/537: Loss=1.0832 (C:1.0832, R:0.0018)
Batch 150/537: Loss=1.0655 (C:1.0655, R:0.0018)
Batch 175/537: Loss=1.1527 (C:1.1527, R:0.0018)
Batch 200/537: Loss=1.1081 (C:1.1081, R:0.0018)
Batch 225/537: Loss=1.0995 (C:1.0995, R:0.0018)
Batch 250/537: Loss=1.1288 (C:1.1288, R:0.0018)
Batch 275/537: Loss=1.1108 (C:1.1108, R:0.0018)
Batch 300/537: Loss=1.0939 (C:1.0939, R:0.0018)
Batch 325/537: Loss=1.1075 (C:1.1075, R:0.0017)
Batch 350/537: Loss=1.0811 (C:1.0811, R:0.0018)
Batch 375/537: Loss=1.1397 (C:1.1397, R:0.0018)
Batch 400/537: Loss=1.0911 (C:1.0911, R:0.0018)
Batch 425/537: Loss=1.1473 (C:1.1473, R:0.0017)
Batch 450/537: Loss=1.1225 (C:1.1225, R:0.0018)
Batch 475/537: Loss=1.1388 (C:1.1388, R:0.0018)
Batch 500/537: Loss=1.1159 (C:1.1159, R:0.0018)
Batch 525/537: Loss=1.1358 (C:1.1358, R:0.0018)

============================================================
Epoch 29/100 completed in 29.8s
Train: Loss=1.1002 (C:1.1002, R:0.0018) Ratio=3.09x
Val:   Loss=1.0547 (C:1.0547, R:0.0018) Ratio=3.94x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0547)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=1.0918 (C:1.0918, R:0.0018)
Batch  25/537: Loss=1.1067 (C:1.1067, R:0.0018)
Batch  50/537: Loss=1.1208 (C:1.1208, R:0.0017)
Batch  75/537: Loss=1.1186 (C:1.1186, R:0.0017)
Batch 100/537: Loss=1.0832 (C:1.0832, R:0.0018)
Batch 125/537: Loss=1.0821 (C:1.0821, R:0.0018)
Batch 150/537: Loss=1.1156 (C:1.1156, R:0.0018)
Batch 175/537: Loss=1.1074 (C:1.1074, R:0.0018)
Batch 200/537: Loss=1.0937 (C:1.0937, R:0.0018)
Batch 225/537: Loss=1.0820 (C:1.0820, R:0.0018)
Batch 250/537: Loss=1.1160 (C:1.1160, R:0.0018)
Batch 275/537: Loss=1.1364 (C:1.1364, R:0.0018)
Batch 300/537: Loss=1.0975 (C:1.0975, R:0.0018)
Batch 325/537: Loss=1.1035 (C:1.1035, R:0.0018)
Batch 350/537: Loss=1.0779 (C:1.0779, R:0.0018)
Batch 375/537: Loss=1.1091 (C:1.1091, R:0.0018)
Batch 400/537: Loss=1.1058 (C:1.1058, R:0.0018)
Batch 425/537: Loss=1.1252 (C:1.1252, R:0.0018)
Batch 450/537: Loss=1.0854 (C:1.0854, R:0.0018)
Batch 475/537: Loss=1.0716 (C:1.0716, R:0.0017)
Batch 500/537: Loss=1.1297 (C:1.1297, R:0.0018)
Batch 525/537: Loss=1.1019 (C:1.1019, R:0.0017)

============================================================
Epoch 30/100 completed in 29.4s
Train: Loss=1.0981 (C:1.0981, R:0.0018) Ratio=3.09x
Val:   Loss=1.0528 (C:1.0528, R:0.0018) Ratio=3.95x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0528)
Checkpoint saved at epoch 30
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.368 ± 0.577
    Neg distances: 1.458 ± 0.901
    Separation ratio: 3.97x
    Gap: -2.730
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=1.0721 (C:1.0721, R:0.0018)
Batch  25/537: Loss=1.0732 (C:1.0732, R:0.0017)
Batch  50/537: Loss=1.0814 (C:1.0814, R:0.0018)
Batch  75/537: Loss=1.0614 (C:1.0614, R:0.0018)
Batch 100/537: Loss=1.1174 (C:1.1174, R:0.0018)
Batch 125/537: Loss=1.0693 (C:1.0693, R:0.0018)
Batch 150/537: Loss=1.0509 (C:1.0509, R:0.0018)
Batch 175/537: Loss=1.0800 (C:1.0800, R:0.0018)
Batch 200/537: Loss=1.0919 (C:1.0919, R:0.0018)
Batch 225/537: Loss=1.1041 (C:1.1041, R:0.0018)
Batch 250/537: Loss=1.0595 (C:1.0595, R:0.0018)
Batch 275/537: Loss=1.1109 (C:1.1109, R:0.0017)
Batch 300/537: Loss=1.0799 (C:1.0799, R:0.0017)
Batch 325/537: Loss=1.1118 (C:1.1118, R:0.0018)
Batch 350/537: Loss=1.0490 (C:1.0490, R:0.0018)
Batch 375/537: Loss=1.0667 (C:1.0667, R:0.0018)
Batch 400/537: Loss=1.0672 (C:1.0672, R:0.0017)
Batch 425/537: Loss=1.0799 (C:1.0799, R:0.0018)
Batch 450/537: Loss=1.0660 (C:1.0660, R:0.0018)
Batch 475/537: Loss=1.0794 (C:1.0794, R:0.0018)
Batch 500/537: Loss=1.1075 (C:1.1075, R:0.0017)
Batch 525/537: Loss=1.0738 (C:1.0738, R:0.0018)

============================================================
Epoch 31/100 completed in 35.2s
Train: Loss=1.0788 (C:1.0788, R:0.0018) Ratio=3.16x
Val:   Loss=1.0326 (C:1.0326, R:0.0018) Ratio=4.02x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 1.0326)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=1.0919 (C:1.0919, R:0.0018)
Batch  25/537: Loss=1.0738 (C:1.0738, R:0.0018)
Batch  50/537: Loss=1.0654 (C:1.0654, R:0.0018)
Batch  75/537: Loss=1.0928 (C:1.0928, R:0.0017)
Batch 100/537: Loss=1.0598 (C:1.0598, R:0.0018)
Batch 125/537: Loss=1.0707 (C:1.0707, R:0.0018)
Batch 150/537: Loss=1.0633 (C:1.0633, R:0.0018)
Batch 175/537: Loss=1.0588 (C:1.0588, R:0.0018)
Batch 200/537: Loss=1.0509 (C:1.0509, R:0.0018)
Batch 225/537: Loss=1.0616 (C:1.0616, R:0.0018)
Batch 250/537: Loss=1.0616 (C:1.0616, R:0.0018)
Batch 275/537: Loss=1.0766 (C:1.0766, R:0.0018)
Batch 300/537: Loss=1.0697 (C:1.0697, R:0.0018)
Batch 325/537: Loss=1.0873 (C:1.0873, R:0.0017)
Batch 350/537: Loss=1.0414 (C:1.0414, R:0.0018)
Batch 375/537: Loss=1.0574 (C:1.0574, R:0.0017)
Batch 400/537: Loss=1.0904 (C:1.0904, R:0.0018)
Batch 425/537: Loss=1.0522 (C:1.0522, R:0.0018)
Batch 450/537: Loss=1.1057 (C:1.1057, R:0.0018)
Batch 475/537: Loss=1.0789 (C:1.0789, R:0.0018)
Batch 500/537: Loss=1.0910 (C:1.0910, R:0.0018)
Batch 525/537: Loss=1.0756 (C:1.0756, R:0.0018)

============================================================
Epoch 32/100 completed in 29.4s
Train: Loss=1.0771 (C:1.0771, R:0.0018) Ratio=3.19x
Val:   Loss=1.0299 (C:1.0299, R:0.0018) Ratio=4.16x
Reconstruction weight: 0.030
✅ New best model saved (Val Loss: 1.0299)
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=1.0616 (C:1.0616, R:0.0018)
Batch  25/537: Loss=1.0456 (C:1.0456, R:0.0018)
Batch  50/537: Loss=1.0627 (C:1.0627, R:0.0018)
Batch  75/537: Loss=1.0725 (C:1.0725, R:0.0018)
Batch 100/537: Loss=1.1134 (C:1.1134, R:0.0018)
Batch 125/537: Loss=1.0570 (C:1.0570, R:0.0018)
Batch 150/537: Loss=1.0535 (C:1.0535, R:0.0018)
Batch 175/537: Loss=1.0579 (C:1.0579, R:0.0017)
Batch 200/537: Loss=1.0792 (C:1.0792, R:0.0018)
Batch 225/537: Loss=1.0413 (C:1.0413, R:0.0018)
Batch 250/537: Loss=1.0654 (C:1.0654, R:0.0017)
Batch 275/537: Loss=1.0334 (C:1.0334, R:0.0017)
Batch 300/537: Loss=1.0725 (C:1.0725, R:0.0018)
Batch 325/537: Loss=1.0983 (C:1.0983, R:0.0018)
Batch 350/537: Loss=1.0902 (C:1.0902, R:0.0018)
Batch 375/537: Loss=1.0663 (C:1.0663, R:0.0018)
Batch 400/537: Loss=1.0540 (C:1.0540, R:0.0018)
Batch 425/537: Loss=1.0778 (C:1.0778, R:0.0018)
Batch 450/537: Loss=1.0973 (C:1.0973, R:0.0018)
Batch 475/537: Loss=1.0439 (C:1.0439, R:0.0018)
Batch 500/537: Loss=1.0783 (C:1.0783, R:0.0017)
Batch 525/537: Loss=1.1052 (C:1.1052, R:0.0017)

============================================================
Epoch 33/100 completed in 29.8s
Train: Loss=1.0754 (C:1.0754, R:0.0018) Ratio=3.22x
Val:   Loss=1.0287 (C:1.0287, R:0.0018) Ratio=4.15x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 1.0287)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.352 ± 0.553
    Neg distances: 1.483 ± 0.916
    Separation ratio: 4.21x
    Gap: -2.708
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=1.0803 (C:1.0803, R:0.0018)
Batch  25/537: Loss=1.0467 (C:1.0467, R:0.0017)
Batch  50/537: Loss=1.0546 (C:1.0546, R:0.0017)
Batch  75/537: Loss=1.0386 (C:1.0386, R:0.0018)
Batch 100/537: Loss=1.0240 (C:1.0240, R:0.0017)
Batch 125/537: Loss=1.0774 (C:1.0774, R:0.0018)
Batch 150/537: Loss=1.0311 (C:1.0311, R:0.0018)
Batch 175/537: Loss=1.0608 (C:1.0608, R:0.0018)
Batch 200/537: Loss=1.0685 (C:1.0685, R:0.0018)
Batch 225/537: Loss=1.0393 (C:1.0393, R:0.0018)
Batch 250/537: Loss=1.0158 (C:1.0158, R:0.0018)
Batch 275/537: Loss=1.0562 (C:1.0562, R:0.0018)
Batch 300/537: Loss=1.0154 (C:1.0154, R:0.0018)
Batch 325/537: Loss=1.0336 (C:1.0336, R:0.0018)
Batch 350/537: Loss=1.0405 (C:1.0405, R:0.0018)
Batch 375/537: Loss=1.0243 (C:1.0243, R:0.0018)
Batch 400/537: Loss=1.0405 (C:1.0405, R:0.0018)
Batch 425/537: Loss=1.0634 (C:1.0634, R:0.0018)
Batch 450/537: Loss=1.0998 (C:1.0998, R:0.0018)
Batch 475/537: Loss=1.0572 (C:1.0572, R:0.0018)
Batch 500/537: Loss=1.0802 (C:1.0802, R:0.0018)
Batch 525/537: Loss=1.0625 (C:1.0625, R:0.0017)

============================================================
Epoch 34/100 completed in 35.2s
Train: Loss=1.0604 (C:1.0604, R:0.0018) Ratio=3.27x
Val:   Loss=1.0126 (C:1.0126, R:0.0018) Ratio=4.21x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 1.0126)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=1.0832 (C:1.0832, R:0.0018)
Batch  25/537: Loss=1.0367 (C:1.0367, R:0.0018)
Batch  50/537: Loss=1.0516 (C:1.0516, R:0.0018)
Batch  75/537: Loss=1.0511 (C:1.0511, R:0.0018)
Batch 100/537: Loss=1.0528 (C:1.0528, R:0.0018)
Batch 125/537: Loss=1.0431 (C:1.0431, R:0.0018)
Batch 150/537: Loss=1.0761 (C:1.0761, R:0.0018)
Batch 175/537: Loss=1.0802 (C:1.0802, R:0.0018)
Batch 200/537: Loss=1.0725 (C:1.0725, R:0.0018)
Batch 225/537: Loss=1.0902 (C:1.0902, R:0.0018)
Batch 250/537: Loss=1.0527 (C:1.0527, R:0.0018)
Batch 275/537: Loss=1.0355 (C:1.0355, R:0.0018)
Batch 300/537: Loss=1.0677 (C:1.0677, R:0.0017)
Batch 325/537: Loss=1.0778 (C:1.0778, R:0.0018)
Batch 350/537: Loss=1.0405 (C:1.0405, R:0.0018)
Batch 375/537: Loss=1.0692 (C:1.0692, R:0.0018)
Batch 400/537: Loss=1.0182 (C:1.0182, R:0.0018)
Batch 425/537: Loss=1.0503 (C:1.0503, R:0.0018)
Batch 450/537: Loss=1.0972 (C:1.0972, R:0.0017)
Batch 475/537: Loss=1.0697 (C:1.0697, R:0.0018)
Batch 500/537: Loss=1.0578 (C:1.0578, R:0.0018)
Batch 525/537: Loss=1.0837 (C:1.0837, R:0.0018)

============================================================
Epoch 35/100 completed in 29.9s
Train: Loss=1.0589 (C:1.0589, R:0.0018) Ratio=3.26x
Val:   Loss=1.0101 (C:1.0101, R:0.0018) Ratio=4.25x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 1.0101)
Checkpoint saved at epoch 35
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=1.0946 (C:1.0946, R:0.0018)
Batch  25/537: Loss=1.1136 (C:1.1136, R:0.0018)
Batch  50/537: Loss=1.0762 (C:1.0762, R:0.0018)
Batch  75/537: Loss=1.0290 (C:1.0290, R:0.0018)
Batch 100/537: Loss=1.0768 (C:1.0768, R:0.0018)
Batch 125/537: Loss=1.0401 (C:1.0401, R:0.0018)
Batch 150/537: Loss=1.0481 (C:1.0481, R:0.0018)
Batch 175/537: Loss=1.0543 (C:1.0543, R:0.0018)
Batch 200/537: Loss=1.0911 (C:1.0911, R:0.0017)
Batch 225/537: Loss=1.0680 (C:1.0680, R:0.0018)
Batch 250/537: Loss=1.0088 (C:1.0088, R:0.0018)
Batch 275/537: Loss=1.0484 (C:1.0484, R:0.0018)
Batch 300/537: Loss=1.0772 (C:1.0772, R:0.0018)
Batch 325/537: Loss=1.0493 (C:1.0493, R:0.0018)
Batch 350/537: Loss=1.0783 (C:1.0783, R:0.0017)
Batch 375/537: Loss=1.0494 (C:1.0494, R:0.0018)
Batch 400/537: Loss=1.1081 (C:1.1081, R:0.0017)
Batch 425/537: Loss=1.1057 (C:1.1057, R:0.0018)
Batch 450/537: Loss=1.0595 (C:1.0595, R:0.0018)
Batch 475/537: Loss=1.0690 (C:1.0690, R:0.0018)
Batch 500/537: Loss=1.0713 (C:1.0713, R:0.0018)
Batch 525/537: Loss=1.0549 (C:1.0549, R:0.0018)

============================================================
Epoch 36/100 completed in 29.7s
Train: Loss=1.0583 (C:1.0583, R:0.0018) Ratio=3.23x
Val:   Loss=1.0067 (C:1.0067, R:0.0018) Ratio=4.36x
Reconstruction weight: 0.090
✅ New best model saved (Val Loss: 1.0067)
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.349 ± 0.560
    Neg distances: 1.486 ± 0.920
    Separation ratio: 4.25x
    Gap: -2.663
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=1.0274 (C:1.0274, R:0.0017)
Batch  25/537: Loss=1.0426 (C:1.0426, R:0.0018)
Batch  50/537: Loss=1.0525 (C:1.0525, R:0.0018)
Batch  75/537: Loss=1.0611 (C:1.0611, R:0.0018)
Batch 100/537: Loss=1.0869 (C:1.0869, R:0.0018)
Batch 125/537: Loss=1.0815 (C:1.0815, R:0.0018)
Batch 150/537: Loss=1.0599 (C:1.0599, R:0.0018)
Batch 175/537: Loss=1.0681 (C:1.0681, R:0.0018)
Batch 200/537: Loss=1.0294 (C:1.0294, R:0.0018)
Batch 225/537: Loss=1.0560 (C:1.0560, R:0.0018)
Batch 250/537: Loss=1.0430 (C:1.0430, R:0.0017)
Batch 275/537: Loss=1.0710 (C:1.0710, R:0.0018)
Batch 300/537: Loss=1.0487 (C:1.0487, R:0.0018)
Batch 325/537: Loss=1.0253 (C:1.0253, R:0.0018)
Batch 350/537: Loss=1.1034 (C:1.1034, R:0.0018)
Batch 375/537: Loss=1.0508 (C:1.0508, R:0.0018)
Batch 400/537: Loss=1.0422 (C:1.0422, R:0.0018)
Batch 425/537: Loss=1.0636 (C:1.0636, R:0.0018)
Batch 450/537: Loss=1.0930 (C:1.0930, R:0.0018)
Batch 475/537: Loss=1.0554 (C:1.0554, R:0.0018)
Batch 500/537: Loss=1.0424 (C:1.0424, R:0.0017)
Batch 525/537: Loss=1.0435 (C:1.0435, R:0.0017)

============================================================
Epoch 37/100 completed in 35.4s
Train: Loss=1.0548 (C:1.0548, R:0.0018) Ratio=3.28x
Val:   Loss=1.0016 (C:1.0016, R:0.0018) Ratio=4.40x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 1.0016)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=1.0542 (C:1.0542, R:0.0018)
Batch  25/537: Loss=1.0458 (C:1.0458, R:0.0018)
Batch  50/537: Loss=1.0443 (C:1.0443, R:0.0018)
Batch  75/537: Loss=1.0590 (C:1.0590, R:0.0018)
Batch 100/537: Loss=1.0671 (C:1.0671, R:0.0018)
Batch 125/537: Loss=1.0457 (C:1.0457, R:0.0018)
Batch 150/537: Loss=1.0603 (C:1.0603, R:0.0018)
Batch 175/537: Loss=1.0575 (C:1.0575, R:0.0018)
Batch 200/537: Loss=1.0400 (C:1.0400, R:0.0018)
Batch 225/537: Loss=1.0417 (C:1.0417, R:0.0017)
Batch 250/537: Loss=1.0424 (C:1.0424, R:0.0017)
Batch 275/537: Loss=1.0340 (C:1.0340, R:0.0018)
Batch 300/537: Loss=1.0681 (C:1.0681, R:0.0017)
Batch 325/537: Loss=1.0421 (C:1.0421, R:0.0018)
Batch 350/537: Loss=1.0715 (C:1.0715, R:0.0018)
Batch 375/537: Loss=1.0522 (C:1.0522, R:0.0018)
Batch 400/537: Loss=1.0562 (C:1.0562, R:0.0018)
Batch 425/537: Loss=1.0881 (C:1.0881, R:0.0018)
Batch 450/537: Loss=1.0623 (C:1.0623, R:0.0018)
Batch 475/537: Loss=1.0615 (C:1.0615, R:0.0018)
Batch 500/537: Loss=1.0699 (C:1.0699, R:0.0018)
Batch 525/537: Loss=1.0645 (C:1.0645, R:0.0018)

============================================================
Epoch 38/100 completed in 29.6s
Train: Loss=1.0512 (C:1.0512, R:0.0018) Ratio=3.33x
Val:   Loss=0.9992 (C:0.9992, R:0.0018) Ratio=4.42x
Reconstruction weight: 0.120
✅ New best model saved (Val Loss: 0.9992)
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=1.0609 (C:1.0609, R:0.0018)
Batch  25/537: Loss=1.0423 (C:1.0423, R:0.0018)
Batch  50/537: Loss=1.0642 (C:1.0642, R:0.0018)
Batch  75/537: Loss=1.0576 (C:1.0576, R:0.0018)
Batch 100/537: Loss=1.0492 (C:1.0492, R:0.0018)
Batch 125/537: Loss=1.0969 (C:1.0969, R:0.0018)
Batch 150/537: Loss=1.0294 (C:1.0294, R:0.0018)
Batch 175/537: Loss=1.0461 (C:1.0461, R:0.0018)
Batch 200/537: Loss=1.0538 (C:1.0538, R:0.0018)
Batch 225/537: Loss=1.0626 (C:1.0626, R:0.0018)
Batch 250/537: Loss=1.0595 (C:1.0595, R:0.0018)
Batch 275/537: Loss=1.0362 (C:1.0362, R:0.0018)
Batch 300/537: Loss=1.0769 (C:1.0769, R:0.0018)
Batch 325/537: Loss=1.0417 (C:1.0417, R:0.0018)
Batch 350/537: Loss=1.0930 (C:1.0930, R:0.0018)
Batch 375/537: Loss=1.0516 (C:1.0516, R:0.0018)
Batch 400/537: Loss=1.0287 (C:1.0287, R:0.0018)
Batch 425/537: Loss=1.0613 (C:1.0613, R:0.0017)
Batch 450/537: Loss=1.0623 (C:1.0623, R:0.0018)
Batch 475/537: Loss=1.0534 (C:1.0534, R:0.0018)
Batch 500/537: Loss=1.0282 (C:1.0282, R:0.0018)
Batch 525/537: Loss=1.0715 (C:1.0715, R:0.0018)

============================================================
Epoch 39/100 completed in 29.5s
Train: Loss=1.0496 (C:1.0496, R:0.0018) Ratio=3.32x
Val:   Loss=0.9977 (C:0.9977, R:0.0018) Ratio=4.57x
Reconstruction weight: 0.135
✅ New best model saved (Val Loss: 0.9977)
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.326 ± 0.541
    Neg distances: 1.505 ± 0.935
    Separation ratio: 4.61x
    Gap: -2.640
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=1.0175 (C:1.0175, R:0.0018)
Batch  25/537: Loss=1.0173 (C:1.0173, R:0.0018)
Batch  50/537: Loss=1.0282 (C:1.0282, R:0.0018)
Batch  75/537: Loss=1.0394 (C:1.0394, R:0.0018)
Batch 100/537: Loss=0.9901 (C:0.9901, R:0.0018)
Batch 125/537: Loss=1.0790 (C:1.0790, R:0.0018)
Batch 150/537: Loss=1.0304 (C:1.0304, R:0.0017)
Batch 175/537: Loss=1.0364 (C:1.0364, R:0.0018)
Batch 200/537: Loss=1.0334 (C:1.0334, R:0.0018)
Batch 225/537: Loss=1.0623 (C:1.0623, R:0.0018)
Batch 250/537: Loss=1.0502 (C:1.0502, R:0.0018)
Batch 275/537: Loss=1.0286 (C:1.0286, R:0.0018)
Batch 300/537: Loss=1.0262 (C:1.0262, R:0.0017)
Batch 325/537: Loss=1.0085 (C:1.0085, R:0.0017)
Batch 350/537: Loss=1.0192 (C:1.0192, R:0.0018)
Batch 375/537: Loss=1.0419 (C:1.0419, R:0.0018)
Batch 400/537: Loss=1.0237 (C:1.0237, R:0.0018)
Batch 425/537: Loss=1.0279 (C:1.0279, R:0.0018)
Batch 450/537: Loss=1.0304 (C:1.0304, R:0.0018)
Batch 475/537: Loss=1.0686 (C:1.0686, R:0.0018)
Batch 500/537: Loss=1.0400 (C:1.0400, R:0.0017)
Batch 525/537: Loss=1.0129 (C:1.0129, R:0.0018)

============================================================
Epoch 40/100 completed in 34.3s
Train: Loss=1.0307 (C:1.0307, R:0.0018) Ratio=3.34x
Val:   Loss=0.9765 (C:0.9765, R:0.0018) Ratio=4.58x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.9765)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=1.0316 (C:1.0316, R:0.0018)
Batch  25/537: Loss=1.0425 (C:1.0425, R:0.0018)
Batch  50/537: Loss=1.0184 (C:1.0184, R:0.0018)
Batch  75/537: Loss=0.9978 (C:0.9978, R:0.0018)
Batch 100/537: Loss=1.0049 (C:1.0049, R:0.0018)
Batch 125/537: Loss=1.0262 (C:1.0262, R:0.0018)
Batch 150/537: Loss=1.0464 (C:1.0464, R:0.0017)
Batch 175/537: Loss=1.0162 (C:1.0162, R:0.0018)
Batch 200/537: Loss=1.0345 (C:1.0345, R:0.0018)
Batch 225/537: Loss=1.0251 (C:1.0251, R:0.0018)
Batch 250/537: Loss=1.0157 (C:1.0157, R:0.0018)
Batch 275/537: Loss=1.0405 (C:1.0405, R:0.0018)
Batch 300/537: Loss=1.0574 (C:1.0574, R:0.0018)
Batch 325/537: Loss=0.9956 (C:0.9956, R:0.0018)
Batch 350/537: Loss=1.0739 (C:1.0739, R:0.0018)
Batch 375/537: Loss=1.0422 (C:1.0422, R:0.0018)
Batch 400/537: Loss=1.0392 (C:1.0392, R:0.0018)
Batch 425/537: Loss=1.0098 (C:1.0098, R:0.0018)
Batch 450/537: Loss=1.0338 (C:1.0338, R:0.0018)
Batch 475/537: Loss=1.0243 (C:1.0243, R:0.0018)
Batch 500/537: Loss=1.0356 (C:1.0356, R:0.0018)
Batch 525/537: Loss=1.0151 (C:1.0151, R:0.0018)

============================================================
Epoch 41/100 completed in 29.6s
Train: Loss=1.0302 (C:1.0302, R:0.0018) Ratio=3.42x
Val:   Loss=0.9745 (C:0.9745, R:0.0018) Ratio=4.68x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.9745)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=1.0480 (C:1.0480, R:0.0017)
Batch  25/537: Loss=1.0147 (C:1.0147, R:0.0018)
Batch  50/537: Loss=1.0060 (C:1.0060, R:0.0017)
Batch  75/537: Loss=1.0275 (C:1.0275, R:0.0018)
Batch 100/537: Loss=1.0151 (C:1.0151, R:0.0018)
Batch 125/537: Loss=1.0676 (C:1.0676, R:0.0018)
Batch 150/537: Loss=1.0352 (C:1.0352, R:0.0018)
Batch 175/537: Loss=1.0333 (C:1.0333, R:0.0018)
Batch 200/537: Loss=1.0234 (C:1.0234, R:0.0018)
Batch 225/537: Loss=1.0403 (C:1.0403, R:0.0017)
Batch 250/537: Loss=1.0158 (C:1.0158, R:0.0018)
Batch 275/537: Loss=1.0514 (C:1.0514, R:0.0017)
Batch 300/537: Loss=1.0164 (C:1.0164, R:0.0018)
Batch 325/537: Loss=1.0229 (C:1.0229, R:0.0018)
Batch 350/537: Loss=1.0446 (C:1.0446, R:0.0018)
Batch 375/537: Loss=1.0343 (C:1.0343, R:0.0018)
Batch 400/537: Loss=1.0659 (C:1.0659, R:0.0018)
Batch 425/537: Loss=0.9976 (C:0.9976, R:0.0018)
Batch 450/537: Loss=1.0407 (C:1.0407, R:0.0018)
Batch 475/537: Loss=1.0463 (C:1.0463, R:0.0018)
Batch 500/537: Loss=1.0120 (C:1.0120, R:0.0018)
Batch 525/537: Loss=1.0136 (C:1.0136, R:0.0018)

============================================================
Epoch 42/100 completed in 29.5s
Train: Loss=1.0285 (C:1.0285, R:0.0018) Ratio=3.39x
Val:   Loss=0.9735 (C:0.9735, R:0.0018) Ratio=4.71x
Reconstruction weight: 0.180
✅ New best model saved (Val Loss: 0.9735)
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.320 ± 0.535
    Neg distances: 1.542 ± 0.945
    Separation ratio: 4.81x
    Gap: -2.731
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=1.0172 (C:1.0172, R:0.0018)
Batch  25/537: Loss=1.0264 (C:1.0264, R:0.0018)
Batch  50/537: Loss=1.0043 (C:1.0043, R:0.0018)
Batch  75/537: Loss=1.0105 (C:1.0105, R:0.0018)
Batch 100/537: Loss=1.0127 (C:1.0127, R:0.0018)
Batch 125/537: Loss=1.0004 (C:1.0004, R:0.0018)
Batch 150/537: Loss=1.0199 (C:1.0199, R:0.0018)
Batch 175/537: Loss=1.0349 (C:1.0349, R:0.0017)
Batch 200/537: Loss=1.0103 (C:1.0103, R:0.0017)
Batch 225/537: Loss=0.9943 (C:0.9943, R:0.0018)
Batch 250/537: Loss=1.0150 (C:1.0150, R:0.0018)
Batch 275/537: Loss=1.0228 (C:1.0228, R:0.0018)
Batch 300/537: Loss=1.0169 (C:1.0169, R:0.0017)
Batch 325/537: Loss=1.0184 (C:1.0184, R:0.0017)
Batch 350/537: Loss=1.0062 (C:1.0062, R:0.0018)
Batch 375/537: Loss=1.0087 (C:1.0087, R:0.0018)
Batch 400/537: Loss=1.0410 (C:1.0410, R:0.0018)
Batch 425/537: Loss=1.0268 (C:1.0268, R:0.0017)
Batch 450/537: Loss=1.0036 (C:1.0036, R:0.0017)
Batch 475/537: Loss=1.0029 (C:1.0029, R:0.0018)
Batch 500/537: Loss=1.0044 (C:1.0044, R:0.0018)
Batch 525/537: Loss=0.9896 (C:0.9896, R:0.0018)

============================================================
Epoch 43/100 completed in 35.8s
Train: Loss=1.0121 (C:1.0121, R:0.0018) Ratio=3.49x
Val:   Loss=0.9560 (C:0.9560, R:0.0018) Ratio=4.88x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.9560)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=1.0215 (C:1.0215, R:0.0018)
Batch  25/537: Loss=0.9846 (C:0.9846, R:0.0018)
Batch  50/537: Loss=1.0042 (C:1.0042, R:0.0018)
Batch  75/537: Loss=1.0046 (C:1.0046, R:0.0018)
Batch 100/537: Loss=0.9897 (C:0.9897, R:0.0018)
Batch 125/537: Loss=0.9963 (C:0.9963, R:0.0018)
Batch 150/537: Loss=1.0172 (C:1.0172, R:0.0018)
Batch 175/537: Loss=0.9848 (C:0.9848, R:0.0018)
Batch 200/537: Loss=1.0111 (C:1.0111, R:0.0018)
Batch 225/537: Loss=0.9825 (C:0.9825, R:0.0017)
Batch 250/537: Loss=1.0302 (C:1.0302, R:0.0017)
Batch 275/537: Loss=1.0143 (C:1.0143, R:0.0018)
Batch 300/537: Loss=1.0188 (C:1.0188, R:0.0018)
Batch 325/537: Loss=0.9998 (C:0.9998, R:0.0018)
Batch 350/537: Loss=1.0010 (C:1.0010, R:0.0017)
Batch 375/537: Loss=0.9581 (C:0.9581, R:0.0018)
Batch 400/537: Loss=0.9860 (C:0.9860, R:0.0018)
Batch 425/537: Loss=0.9900 (C:0.9900, R:0.0018)
Batch 450/537: Loss=1.0092 (C:1.0092, R:0.0018)
Batch 475/537: Loss=1.0145 (C:1.0145, R:0.0018)
Batch 500/537: Loss=0.9989 (C:0.9989, R:0.0018)
Batch 525/537: Loss=0.9874 (C:0.9874, R:0.0018)

============================================================
Epoch 44/100 completed in 29.6s
Train: Loss=1.0082 (C:1.0082, R:0.0018) Ratio=3.55x
Val:   Loss=0.9525 (C:0.9525, R:0.0018) Ratio=4.98x
Reconstruction weight: 0.210
✅ New best model saved (Val Loss: 0.9525)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=1.0148 (C:1.0148, R:0.0018)
Batch  25/537: Loss=0.9660 (C:0.9660, R:0.0017)
Batch  50/537: Loss=1.0348 (C:1.0348, R:0.0018)
Batch  75/537: Loss=1.0090 (C:1.0090, R:0.0018)
Batch 100/537: Loss=1.0281 (C:1.0281, R:0.0018)
Batch 125/537: Loss=1.0067 (C:1.0067, R:0.0018)
Batch 150/537: Loss=1.0045 (C:1.0045, R:0.0017)
Batch 175/537: Loss=0.9911 (C:0.9911, R:0.0018)
Batch 200/537: Loss=0.9558 (C:0.9558, R:0.0018)
Batch 225/537: Loss=1.0037 (C:1.0037, R:0.0018)
Batch 250/537: Loss=1.0356 (C:1.0356, R:0.0017)
Batch 275/537: Loss=0.9965 (C:0.9965, R:0.0018)
Batch 300/537: Loss=1.0092 (C:1.0092, R:0.0018)
Batch 325/537: Loss=0.9894 (C:0.9894, R:0.0018)
Batch 350/537: Loss=0.9841 (C:0.9841, R:0.0018)
Batch 375/537: Loss=1.0301 (C:1.0301, R:0.0018)
Batch 400/537: Loss=1.0346 (C:1.0346, R:0.0018)
Batch 425/537: Loss=1.0342 (C:1.0342, R:0.0018)
Batch 450/537: Loss=1.0016 (C:1.0016, R:0.0017)
Batch 475/537: Loss=1.0159 (C:1.0159, R:0.0017)
Batch 500/537: Loss=0.9990 (C:0.9990, R:0.0018)
Batch 525/537: Loss=1.0125 (C:1.0125, R:0.0017)

============================================================
Epoch 45/100 completed in 30.0s
Train: Loss=1.0082 (C:1.0082, R:0.0018) Ratio=3.51x
Val:   Loss=0.9531 (C:0.9531, R:0.0018) Ratio=4.92x
Reconstruction weight: 0.225
No improvement for 1 epochs
Checkpoint saved at epoch 45
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.311 ± 0.528
    Neg distances: 1.509 ± 0.980
    Separation ratio: 4.85x
    Gap: -2.751
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=1.0305 (C:1.0305, R:0.0018)
Batch  25/537: Loss=1.0068 (C:1.0068, R:0.0017)
Batch  50/537: Loss=1.0361 (C:1.0361, R:0.0018)
Batch  75/537: Loss=0.9723 (C:0.9723, R:0.0018)
Batch 100/537: Loss=1.0236 (C:1.0236, R:0.0018)
Batch 125/537: Loss=1.0125 (C:1.0125, R:0.0018)
Batch 150/537: Loss=1.0070 (C:1.0070, R:0.0017)
Batch 175/537: Loss=1.0178 (C:1.0178, R:0.0018)
Batch 200/537: Loss=1.0064 (C:1.0064, R:0.0018)
Batch 225/537: Loss=1.0262 (C:1.0262, R:0.0017)
Batch 250/537: Loss=1.0482 (C:1.0482, R:0.0018)
Batch 275/537: Loss=1.0037 (C:1.0037, R:0.0018)
Batch 300/537: Loss=1.0348 (C:1.0348, R:0.0018)
Batch 325/537: Loss=1.0281 (C:1.0281, R:0.0018)
Batch 350/537: Loss=1.0628 (C:1.0628, R:0.0018)
Batch 375/537: Loss=1.0355 (C:1.0355, R:0.0018)
Batch 400/537: Loss=1.0425 (C:1.0425, R:0.0018)
Batch 425/537: Loss=1.0322 (C:1.0322, R:0.0018)
Batch 450/537: Loss=1.0297 (C:1.0297, R:0.0018)
Batch 475/537: Loss=1.0143 (C:1.0143, R:0.0018)
Batch 500/537: Loss=1.0176 (C:1.0176, R:0.0018)
Batch 525/537: Loss=1.0087 (C:1.0087, R:0.0017)

============================================================
Epoch 46/100 completed in 35.6s
Train: Loss=1.0253 (C:1.0253, R:0.0018) Ratio=3.58x
Val:   Loss=0.9718 (C:0.9718, R:0.0018) Ratio=4.97x
Reconstruction weight: 0.240
No improvement for 2 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=1.0288 (C:1.0288, R:0.0018)
Batch  25/537: Loss=1.0346 (C:1.0346, R:0.0018)
Batch  50/537: Loss=1.0195 (C:1.0195, R:0.0018)
Batch  75/537: Loss=1.0504 (C:1.0504, R:0.0018)
Batch 100/537: Loss=1.0149 (C:1.0149, R:0.0017)
Batch 125/537: Loss=1.0171 (C:1.0171, R:0.0018)
Batch 150/537: Loss=1.0413 (C:1.0413, R:0.0018)
Batch 175/537: Loss=1.0670 (C:1.0670, R:0.0018)
Batch 200/537: Loss=1.0080 (C:1.0080, R:0.0018)
Batch 225/537: Loss=1.0341 (C:1.0341, R:0.0018)
Batch 250/537: Loss=1.0193 (C:1.0193, R:0.0018)
Batch 275/537: Loss=1.0345 (C:1.0345, R:0.0018)
Batch 300/537: Loss=1.0137 (C:1.0137, R:0.0018)
Batch 325/537: Loss=1.0057 (C:1.0057, R:0.0018)
Batch 350/537: Loss=1.0533 (C:1.0533, R:0.0018)
Batch 375/537: Loss=1.0126 (C:1.0126, R:0.0018)
Batch 400/537: Loss=1.0612 (C:1.0612, R:0.0018)
Batch 425/537: Loss=1.0400 (C:1.0400, R:0.0018)
Batch 450/537: Loss=1.0480 (C:1.0480, R:0.0018)
Batch 475/537: Loss=1.0312 (C:1.0312, R:0.0018)
Batch 500/537: Loss=1.0260 (C:1.0260, R:0.0017)
Batch 525/537: Loss=1.0289 (C:1.0289, R:0.0018)

============================================================
Epoch 47/100 completed in 30.0s
Train: Loss=1.0255 (C:1.0255, R:0.0018) Ratio=3.48x
Val:   Loss=0.9695 (C:0.9695, R:0.0018) Ratio=5.07x
Reconstruction weight: 0.255
No improvement for 3 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=1.0126 (C:1.0126, R:0.0018)
Batch  25/537: Loss=1.0282 (C:1.0282, R:0.0018)
Batch  50/537: Loss=1.0278 (C:1.0278, R:0.0018)
Batch  75/537: Loss=1.0091 (C:1.0091, R:0.0018)
Batch 100/537: Loss=1.0283 (C:1.0283, R:0.0018)
Batch 125/537: Loss=1.0143 (C:1.0143, R:0.0018)
Batch 150/537: Loss=1.0359 (C:1.0359, R:0.0018)
Batch 175/537: Loss=1.0500 (C:1.0500, R:0.0018)
Batch 200/537: Loss=1.0141 (C:1.0141, R:0.0018)
Batch 225/537: Loss=1.0402 (C:1.0402, R:0.0018)
Batch 250/537: Loss=0.9938 (C:0.9938, R:0.0018)
Batch 275/537: Loss=1.0060 (C:1.0060, R:0.0018)
Batch 300/537: Loss=1.0034 (C:1.0034, R:0.0018)
Batch 325/537: Loss=1.0197 (C:1.0197, R:0.0018)
Batch 350/537: Loss=1.0438 (C:1.0438, R:0.0018)
Batch 375/537: Loss=1.0462 (C:1.0462, R:0.0018)
Batch 400/537: Loss=0.9976 (C:0.9976, R:0.0018)
Batch 425/537: Loss=0.9952 (C:0.9952, R:0.0018)
Batch 450/537: Loss=1.0662 (C:1.0662, R:0.0018)
Batch 475/537: Loss=1.0348 (C:1.0348, R:0.0018)
Batch 500/537: Loss=1.0376 (C:1.0376, R:0.0018)
Batch 525/537: Loss=1.0108 (C:1.0108, R:0.0018)

============================================================
Epoch 48/100 completed in 29.6s
Train: Loss=1.0239 (C:1.0239, R:0.0018) Ratio=3.61x
Val:   Loss=0.9678 (C:0.9678, R:0.0018) Ratio=5.03x
Reconstruction weight: 0.270
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.299 ± 0.524
    Neg distances: 1.574 ± 0.948
    Separation ratio: 5.26x
    Gap: -2.673
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.9641 (C:0.9641, R:0.0018)
Batch  25/537: Loss=0.9988 (C:0.9988, R:0.0018)
Batch  50/537: Loss=0.9710 (C:0.9710, R:0.0018)
Batch  75/537: Loss=0.9662 (C:0.9662, R:0.0018)
Batch 100/537: Loss=0.9885 (C:0.9885, R:0.0018)
Batch 125/537: Loss=0.9945 (C:0.9945, R:0.0018)
Batch 150/537: Loss=0.9811 (C:0.9811, R:0.0018)
Batch 175/537: Loss=1.0325 (C:1.0325, R:0.0018)
Batch 200/537: Loss=1.0212 (C:1.0212, R:0.0018)
Batch 225/537: Loss=0.9353 (C:0.9353, R:0.0018)
Batch 250/537: Loss=1.0115 (C:1.0115, R:0.0018)
Batch 275/537: Loss=0.9858 (C:0.9858, R:0.0017)
Batch 300/537: Loss=0.9882 (C:0.9882, R:0.0018)
Batch 325/537: Loss=0.9955 (C:0.9955, R:0.0018)
Batch 350/537: Loss=0.9687 (C:0.9687, R:0.0018)
Batch 375/537: Loss=0.9710 (C:0.9710, R:0.0018)
Batch 400/537: Loss=1.0120 (C:1.0120, R:0.0018)
Batch 425/537: Loss=1.0015 (C:1.0015, R:0.0018)
Batch 450/537: Loss=0.9823 (C:0.9823, R:0.0018)
Batch 475/537: Loss=1.0355 (C:1.0355, R:0.0017)
Batch 500/537: Loss=1.0085 (C:1.0085, R:0.0018)
Batch 525/537: Loss=0.9651 (C:0.9651, R:0.0018)

============================================================
Epoch 49/100 completed in 35.8s
Train: Loss=0.9834 (C:0.9834, R:0.0018) Ratio=3.59x
Val:   Loss=0.9266 (C:0.9266, R:0.0018) Ratio=5.20x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.9266)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.9742 (C:0.9742, R:0.0018)
Batch  25/537: Loss=0.9796 (C:0.9796, R:0.0018)
Batch  50/537: Loss=0.9701 (C:0.9701, R:0.0018)
Batch  75/537: Loss=0.9724 (C:0.9724, R:0.0017)
Batch 100/537: Loss=0.9505 (C:0.9505, R:0.0018)
Batch 125/537: Loss=0.9690 (C:0.9690, R:0.0018)
Batch 150/537: Loss=0.9790 (C:0.9790, R:0.0018)
Batch 175/537: Loss=1.0046 (C:1.0046, R:0.0018)
Batch 200/537: Loss=1.0188 (C:1.0188, R:0.0018)
Batch 225/537: Loss=0.9992 (C:0.9992, R:0.0018)
Batch 250/537: Loss=0.9892 (C:0.9892, R:0.0018)
Batch 275/537: Loss=0.9988 (C:0.9988, R:0.0017)
Batch 300/537: Loss=0.9602 (C:0.9602, R:0.0018)
Batch 325/537: Loss=0.9480 (C:0.9480, R:0.0017)
Batch 350/537: Loss=0.9629 (C:0.9629, R:0.0018)
Batch 375/537: Loss=0.9354 (C:0.9354, R:0.0018)
Batch 400/537: Loss=0.9930 (C:0.9930, R:0.0018)
Batch 425/537: Loss=1.0159 (C:1.0159, R:0.0018)
Batch 450/537: Loss=0.9618 (C:0.9618, R:0.0018)
Batch 475/537: Loss=0.9876 (C:0.9876, R:0.0018)
Batch 500/537: Loss=0.9602 (C:0.9602, R:0.0018)
Batch 525/537: Loss=0.9390 (C:0.9390, R:0.0018)

============================================================
Epoch 50/100 completed in 29.9s
Train: Loss=0.9813 (C:0.9813, R:0.0018) Ratio=3.64x
Val:   Loss=0.9237 (C:0.9237, R:0.0018) Ratio=5.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.9237)
Checkpoint saved at epoch 50
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.9773 (C:0.9773, R:0.0018)
Batch  25/537: Loss=1.0091 (C:1.0091, R:0.0018)
Batch  50/537: Loss=0.9871 (C:0.9871, R:0.0018)
Batch  75/537: Loss=0.9606 (C:0.9606, R:0.0017)
Batch 100/537: Loss=0.9766 (C:0.9766, R:0.0018)
Batch 125/537: Loss=0.9271 (C:0.9271, R:0.0018)
Batch 150/537: Loss=0.9457 (C:0.9457, R:0.0018)
Batch 175/537: Loss=0.9540 (C:0.9540, R:0.0018)
Batch 200/537: Loss=0.9926 (C:0.9926, R:0.0017)
Batch 225/537: Loss=0.9787 (C:0.9787, R:0.0018)
Batch 250/537: Loss=0.9790 (C:0.9790, R:0.0018)
Batch 275/537: Loss=0.9765 (C:0.9765, R:0.0018)
Batch 300/537: Loss=0.9901 (C:0.9901, R:0.0018)
Batch 325/537: Loss=0.9643 (C:0.9643, R:0.0018)
Batch 350/537: Loss=0.9910 (C:0.9910, R:0.0018)
Batch 375/537: Loss=0.9994 (C:0.9994, R:0.0018)
Batch 400/537: Loss=0.9792 (C:0.9792, R:0.0018)
Batch 425/537: Loss=1.0266 (C:1.0266, R:0.0018)
Batch 450/537: Loss=0.9817 (C:0.9817, R:0.0018)
Batch 475/537: Loss=0.9669 (C:0.9669, R:0.0018)
Batch 500/537: Loss=0.9939 (C:0.9939, R:0.0017)
Batch 525/537: Loss=0.9893 (C:0.9893, R:0.0018)

============================================================
Epoch 51/100 completed in 29.7s
Train: Loss=0.9790 (C:0.9790, R:0.0018) Ratio=3.61x
Val:   Loss=0.9227 (C:0.9227, R:0.0018) Ratio=5.14x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.9227)
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.297 ± 0.510
    Neg distances: 1.538 ± 0.960
    Separation ratio: 5.18x
    Gap: -2.731
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.9675 (C:0.9675, R:0.0018)
Batch  25/537: Loss=0.9913 (C:0.9913, R:0.0018)
Batch  50/537: Loss=0.9822 (C:0.9822, R:0.0018)
Batch  75/537: Loss=0.9931 (C:0.9931, R:0.0018)
Batch 100/537: Loss=1.0065 (C:1.0065, R:0.0018)
Batch 125/537: Loss=0.9901 (C:0.9901, R:0.0018)
Batch 150/537: Loss=0.9890 (C:0.9890, R:0.0018)
Batch 175/537: Loss=1.0127 (C:1.0127, R:0.0018)
Batch 200/537: Loss=0.9665 (C:0.9665, R:0.0018)
Batch 225/537: Loss=1.0066 (C:1.0066, R:0.0018)
Batch 250/537: Loss=1.0022 (C:1.0022, R:0.0018)
Batch 275/537: Loss=0.9833 (C:0.9833, R:0.0018)
Batch 300/537: Loss=1.0024 (C:1.0024, R:0.0018)
Batch 325/537: Loss=1.0006 (C:1.0006, R:0.0018)
Batch 350/537: Loss=1.0036 (C:1.0036, R:0.0018)
Batch 375/537: Loss=1.0097 (C:1.0097, R:0.0018)
Batch 400/537: Loss=1.0012 (C:1.0012, R:0.0018)
Batch 425/537: Loss=0.9891 (C:0.9891, R:0.0017)
Batch 450/537: Loss=1.0194 (C:1.0194, R:0.0018)
Batch 475/537: Loss=1.0533 (C:1.0533, R:0.0018)
Batch 500/537: Loss=1.0169 (C:1.0169, R:0.0018)
Batch 525/537: Loss=1.0034 (C:1.0034, R:0.0018)

============================================================
Epoch 52/100 completed in 35.4s
Train: Loss=0.9952 (C:0.9952, R:0.0018) Ratio=3.60x
Val:   Loss=0.9355 (C:0.9355, R:0.0018) Ratio=5.29x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.9902 (C:0.9902, R:0.0018)
Batch  25/537: Loss=0.9783 (C:0.9783, R:0.0018)
Batch  50/537: Loss=0.9973 (C:0.9973, R:0.0018)
Batch  75/537: Loss=0.9634 (C:0.9634, R:0.0018)
Batch 100/537: Loss=0.9839 (C:0.9839, R:0.0018)
Batch 125/537: Loss=0.9861 (C:0.9861, R:0.0018)
Batch 150/537: Loss=0.9781 (C:0.9781, R:0.0018)
Batch 175/537: Loss=1.0174 (C:1.0174, R:0.0018)
Batch 200/537: Loss=1.0083 (C:1.0083, R:0.0018)
Batch 225/537: Loss=1.0009 (C:1.0009, R:0.0018)
Batch 250/537: Loss=1.0010 (C:1.0010, R:0.0018)
Batch 275/537: Loss=1.0177 (C:1.0177, R:0.0018)
Batch 300/537: Loss=0.9810 (C:0.9810, R:0.0018)
Batch 325/537: Loss=1.0057 (C:1.0057, R:0.0018)
Batch 350/537: Loss=1.0228 (C:1.0228, R:0.0018)
Batch 375/537: Loss=0.9803 (C:0.9803, R:0.0018)
Batch 400/537: Loss=0.9591 (C:0.9591, R:0.0018)
Batch 425/537: Loss=0.9926 (C:0.9926, R:0.0018)
Batch 450/537: Loss=1.0111 (C:1.0111, R:0.0018)
Batch 475/537: Loss=0.9730 (C:0.9730, R:0.0018)
Batch 500/537: Loss=0.9959 (C:0.9959, R:0.0018)
Batch 525/537: Loss=1.0216 (C:1.0216, R:0.0018)

============================================================
Epoch 53/100 completed in 29.6s
Train: Loss=0.9932 (C:0.9932, R:0.0018) Ratio=3.69x
Val:   Loss=0.9351 (C:0.9351, R:0.0018) Ratio=5.27x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.9783 (C:0.9783, R:0.0018)
Batch  25/537: Loss=1.0009 (C:1.0009, R:0.0018)
Batch  50/537: Loss=0.9934 (C:0.9934, R:0.0018)
Batch  75/537: Loss=1.0081 (C:1.0081, R:0.0018)
Batch 100/537: Loss=0.9717 (C:0.9717, R:0.0018)
Batch 125/537: Loss=1.0004 (C:1.0004, R:0.0018)
Batch 150/537: Loss=0.9892 (C:0.9892, R:0.0018)
Batch 175/537: Loss=0.9770 (C:0.9770, R:0.0018)
Batch 200/537: Loss=0.9870 (C:0.9870, R:0.0018)
Batch 225/537: Loss=1.0162 (C:1.0162, R:0.0018)
Batch 250/537: Loss=0.9731 (C:0.9731, R:0.0018)
Batch 275/537: Loss=0.9983 (C:0.9983, R:0.0018)
Batch 300/537: Loss=0.9838 (C:0.9838, R:0.0018)
Batch 325/537: Loss=1.0350 (C:1.0350, R:0.0018)
Batch 350/537: Loss=0.9784 (C:0.9784, R:0.0018)
Batch 375/537: Loss=0.9760 (C:0.9760, R:0.0018)
Batch 400/537: Loss=0.9927 (C:0.9927, R:0.0018)
Batch 425/537: Loss=0.9857 (C:0.9857, R:0.0017)
Batch 450/537: Loss=1.0061 (C:1.0061, R:0.0018)
Batch 475/537: Loss=0.9927 (C:0.9927, R:0.0018)
Batch 500/537: Loss=0.9995 (C:0.9995, R:0.0018)
Batch 525/537: Loss=0.9881 (C:0.9881, R:0.0018)

============================================================
Epoch 54/100 completed in 30.2s
Train: Loss=0.9920 (C:0.9920, R:0.0018) Ratio=3.67x
Val:   Loss=0.9340 (C:0.9340, R:0.0018) Ratio=5.37x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.297 ± 0.519
    Neg distances: 1.531 ± 0.967
    Separation ratio: 5.16x
    Gap: -2.775
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.9819 (C:0.9819, R:0.0018)
Batch  25/537: Loss=1.0206 (C:1.0206, R:0.0018)
Batch  50/537: Loss=1.0045 (C:1.0045, R:0.0018)
Batch  75/537: Loss=0.9909 (C:0.9909, R:0.0017)
Batch 100/537: Loss=0.9939 (C:0.9939, R:0.0018)
Batch 125/537: Loss=0.9870 (C:0.9870, R:0.0018)
Batch 150/537: Loss=0.9982 (C:0.9982, R:0.0018)
Batch 175/537: Loss=0.9950 (C:0.9950, R:0.0018)
Batch 200/537: Loss=0.9897 (C:0.9897, R:0.0018)
Batch 225/537: Loss=0.9839 (C:0.9839, R:0.0018)
Batch 250/537: Loss=1.0172 (C:1.0172, R:0.0018)
Batch 275/537: Loss=0.9888 (C:0.9888, R:0.0018)
Batch 300/537: Loss=0.9906 (C:0.9906, R:0.0018)
Batch 325/537: Loss=1.0173 (C:1.0173, R:0.0018)
Batch 350/537: Loss=0.9863 (C:0.9863, R:0.0018)
Batch 375/537: Loss=0.9744 (C:0.9744, R:0.0018)
Batch 400/537: Loss=0.9982 (C:0.9982, R:0.0018)
Batch 425/537: Loss=1.0260 (C:1.0260, R:0.0018)
Batch 450/537: Loss=0.9911 (C:0.9911, R:0.0017)
Batch 475/537: Loss=0.9979 (C:0.9979, R:0.0017)
Batch 500/537: Loss=0.9916 (C:0.9916, R:0.0018)
Batch 525/537: Loss=1.0090 (C:1.0090, R:0.0018)

============================================================
Epoch 55/100 completed in 34.0s
Train: Loss=0.9931 (C:0.9931, R:0.0018) Ratio=3.72x
Val:   Loss=0.9349 (C:0.9349, R:0.0018) Ratio=5.48x
Reconstruction weight: 0.300
No improvement for 4 epochs
Checkpoint saved at epoch 55
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=1.0123 (C:1.0123, R:0.0018)
Batch  25/537: Loss=0.9932 (C:0.9932, R:0.0018)
Batch  50/537: Loss=0.9540 (C:0.9540, R:0.0018)
Batch  75/537: Loss=1.0200 (C:1.0200, R:0.0018)
Batch 100/537: Loss=1.0157 (C:1.0157, R:0.0018)
Batch 125/537: Loss=0.9582 (C:0.9582, R:0.0018)
Batch 150/537: Loss=1.0035 (C:1.0035, R:0.0018)
Batch 175/537: Loss=0.9848 (C:0.9848, R:0.0018)
Batch 200/537: Loss=0.9718 (C:0.9718, R:0.0018)
Batch 225/537: Loss=0.9748 (C:0.9748, R:0.0018)
Batch 250/537: Loss=1.0108 (C:1.0108, R:0.0018)
Batch 275/537: Loss=0.9974 (C:0.9974, R:0.0018)
Batch 300/537: Loss=1.0037 (C:1.0037, R:0.0018)
Batch 325/537: Loss=0.9739 (C:0.9739, R:0.0018)
Batch 350/537: Loss=0.9975 (C:0.9975, R:0.0018)
Batch 375/537: Loss=0.9713 (C:0.9713, R:0.0018)
Batch 400/537: Loss=1.0151 (C:1.0151, R:0.0018)
Batch 425/537: Loss=0.9868 (C:0.9868, R:0.0018)
Batch 450/537: Loss=1.0099 (C:1.0099, R:0.0018)
Batch 475/537: Loss=1.0241 (C:1.0241, R:0.0018)
Batch 500/537: Loss=1.0289 (C:1.0289, R:0.0018)
Batch 525/537: Loss=0.9901 (C:0.9901, R:0.0017)

============================================================
Epoch 56/100 completed in 28.4s
Train: Loss=0.9920 (C:0.9920, R:0.0018) Ratio=3.74x
Val:   Loss=0.9318 (C:0.9318, R:0.0018) Ratio=5.63x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=1.0169 (C:1.0169, R:0.0018)
Batch  25/537: Loss=0.9497 (C:0.9497, R:0.0018)
Batch  50/537: Loss=0.9856 (C:0.9856, R:0.0018)
Batch  75/537: Loss=0.9796 (C:0.9796, R:0.0017)
Batch 100/537: Loss=1.0422 (C:1.0422, R:0.0018)
Batch 125/537: Loss=1.0285 (C:1.0285, R:0.0018)
Batch 150/537: Loss=0.9835 (C:0.9835, R:0.0018)
Batch 175/537: Loss=1.0015 (C:1.0015, R:0.0018)
Batch 200/537: Loss=0.9570 (C:0.9570, R:0.0018)
Batch 225/537: Loss=0.9697 (C:0.9697, R:0.0018)
Batch 250/537: Loss=1.0132 (C:1.0132, R:0.0018)
Batch 275/537: Loss=1.0005 (C:1.0005, R:0.0018)
Batch 300/537: Loss=1.0055 (C:1.0055, R:0.0018)
Batch 325/537: Loss=1.0048 (C:1.0048, R:0.0018)
Batch 350/537: Loss=0.9980 (C:0.9980, R:0.0018)
Batch 375/537: Loss=0.9927 (C:0.9927, R:0.0018)
Batch 400/537: Loss=1.0098 (C:1.0098, R:0.0018)
Batch 425/537: Loss=0.9849 (C:0.9849, R:0.0017)
Batch 450/537: Loss=1.0024 (C:1.0024, R:0.0018)
Batch 475/537: Loss=1.0033 (C:1.0033, R:0.0018)
Batch 500/537: Loss=1.0215 (C:1.0215, R:0.0017)
Batch 525/537: Loss=0.9909 (C:0.9909, R:0.0018)

============================================================
Epoch 57/100 completed in 28.6s
Train: Loss=0.9903 (C:0.9903, R:0.0018) Ratio=3.75x
Val:   Loss=0.9295 (C:0.9295, R:0.0018) Ratio=5.67x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.265 ± 0.489
    Neg distances: 1.531 ± 0.975
    Separation ratio: 5.78x
    Gap: -2.650
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.9703 (C:0.9703, R:0.0018)
Batch  25/537: Loss=0.9215 (C:0.9215, R:0.0018)
Batch  50/537: Loss=0.9926 (C:0.9926, R:0.0018)
Batch  75/537: Loss=0.9603 (C:0.9603, R:0.0018)
Batch 100/537: Loss=0.9639 (C:0.9639, R:0.0018)
Batch 125/537: Loss=0.9587 (C:0.9587, R:0.0018)
Batch 150/537: Loss=0.9727 (C:0.9727, R:0.0018)
Batch 175/537: Loss=0.9536 (C:0.9536, R:0.0018)
Batch 200/537: Loss=0.9682 (C:0.9682, R:0.0018)
Batch 225/537: Loss=0.9640 (C:0.9640, R:0.0018)
Batch 250/537: Loss=0.9880 (C:0.9880, R:0.0018)
Batch 275/537: Loss=0.9613 (C:0.9613, R:0.0018)
Batch 300/537: Loss=0.9769 (C:0.9769, R:0.0018)
Batch 325/537: Loss=0.9787 (C:0.9787, R:0.0018)
Batch 350/537: Loss=0.9659 (C:0.9659, R:0.0018)
Batch 375/537: Loss=0.9822 (C:0.9822, R:0.0017)
Batch 400/537: Loss=0.9493 (C:0.9493, R:0.0018)
Batch 425/537: Loss=0.9796 (C:0.9796, R:0.0018)
Batch 450/537: Loss=0.9692 (C:0.9692, R:0.0018)
Batch 475/537: Loss=0.9649 (C:0.9649, R:0.0018)
Batch 500/537: Loss=0.9680 (C:0.9680, R:0.0018)
Batch 525/537: Loss=0.9716 (C:0.9716, R:0.0018)

============================================================
Epoch 58/100 completed in 33.6s
Train: Loss=0.9702 (C:0.9702, R:0.0018) Ratio=3.82x
Val:   Loss=0.9096 (C:0.9096, R:0.0018) Ratio=5.82x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.9096)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.9370 (C:0.9370, R:0.0018)
Batch  25/537: Loss=0.9615 (C:0.9615, R:0.0018)
Batch  50/537: Loss=0.9999 (C:0.9999, R:0.0018)
Batch  75/537: Loss=0.9748 (C:0.9748, R:0.0018)
Batch 100/537: Loss=0.9754 (C:0.9754, R:0.0018)
Batch 125/537: Loss=0.9590 (C:0.9590, R:0.0018)
Batch 150/537: Loss=0.9644 (C:0.9644, R:0.0018)
Batch 175/537: Loss=0.9627 (C:0.9627, R:0.0017)
Batch 200/537: Loss=0.9536 (C:0.9536, R:0.0018)
Batch 225/537: Loss=0.9708 (C:0.9708, R:0.0018)
Batch 250/537: Loss=0.9551 (C:0.9551, R:0.0018)
Batch 275/537: Loss=0.9972 (C:0.9972, R:0.0018)
Batch 300/537: Loss=0.9576 (C:0.9576, R:0.0018)
Batch 325/537: Loss=0.9506 (C:0.9506, R:0.0018)
Batch 350/537: Loss=0.9816 (C:0.9816, R:0.0018)
Batch 375/537: Loss=0.9642 (C:0.9642, R:0.0018)
Batch 400/537: Loss=0.9991 (C:0.9991, R:0.0018)
Batch 425/537: Loss=0.9531 (C:0.9531, R:0.0017)
Batch 450/537: Loss=0.9427 (C:0.9427, R:0.0018)
Batch 475/537: Loss=0.9625 (C:0.9625, R:0.0017)
Batch 500/537: Loss=0.9611 (C:0.9611, R:0.0018)
Batch 525/537: Loss=0.9928 (C:0.9928, R:0.0018)

============================================================
Epoch 59/100 completed in 28.1s
Train: Loss=0.9695 (C:0.9695, R:0.0018) Ratio=3.86x
Val:   Loss=0.9099 (C:0.9099, R:0.0018) Ratio=5.77x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.9875 (C:0.9875, R:0.0017)
Batch  25/537: Loss=1.0148 (C:1.0148, R:0.0018)
Batch  50/537: Loss=0.9713 (C:0.9713, R:0.0018)
Batch  75/537: Loss=0.9599 (C:0.9599, R:0.0018)
Batch 100/537: Loss=0.9469 (C:0.9469, R:0.0018)
Batch 125/537: Loss=0.9375 (C:0.9375, R:0.0018)
Batch 150/537: Loss=1.0032 (C:1.0032, R:0.0018)
Batch 175/537: Loss=0.9610 (C:0.9610, R:0.0018)
Batch 200/537: Loss=0.9830 (C:0.9830, R:0.0018)
Batch 225/537: Loss=0.9613 (C:0.9613, R:0.0017)
Batch 250/537: Loss=0.9365 (C:0.9365, R:0.0018)
Batch 275/537: Loss=0.9624 (C:0.9624, R:0.0018)
Batch 300/537: Loss=0.9754 (C:0.9754, R:0.0018)
Batch 325/537: Loss=0.9815 (C:0.9815, R:0.0018)
Batch 350/537: Loss=0.9639 (C:0.9639, R:0.0018)
Batch 375/537: Loss=0.9994 (C:0.9994, R:0.0018)
Batch 400/537: Loss=0.9664 (C:0.9664, R:0.0017)
Batch 425/537: Loss=0.9616 (C:0.9616, R:0.0018)
Batch 450/537: Loss=0.9791 (C:0.9791, R:0.0018)
Batch 475/537: Loss=0.9879 (C:0.9879, R:0.0018)
Batch 500/537: Loss=0.9652 (C:0.9652, R:0.0018)
Batch 525/537: Loss=0.9301 (C:0.9301, R:0.0018)

============================================================
Epoch 60/100 completed in 28.4s
Train: Loss=0.9682 (C:0.9682, R:0.0018) Ratio=3.81x
Val:   Loss=0.9078 (C:0.9078, R:0.0018) Ratio=5.90x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.9078)
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.257 ± 0.482
    Neg distances: 1.548 ± 0.989
    Separation ratio: 6.02x
    Gap: -2.680
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.9939 (C:0.9939, R:0.0018)
Batch  25/537: Loss=0.9506 (C:0.9506, R:0.0018)
Batch  50/537: Loss=0.9371 (C:0.9371, R:0.0018)
Batch  75/537: Loss=0.9633 (C:0.9633, R:0.0018)
Batch 100/537: Loss=0.9595 (C:0.9595, R:0.0018)
Batch 125/537: Loss=0.9956 (C:0.9956, R:0.0018)
Batch 150/537: Loss=0.9573 (C:0.9573, R:0.0018)
Batch 175/537: Loss=0.9546 (C:0.9546, R:0.0017)
Batch 200/537: Loss=0.9579 (C:0.9579, R:0.0018)
Batch 225/537: Loss=0.9744 (C:0.9744, R:0.0018)
Batch 250/537: Loss=0.9414 (C:0.9414, R:0.0018)
Batch 275/537: Loss=0.9547 (C:0.9547, R:0.0018)
Batch 300/537: Loss=0.9470 (C:0.9470, R:0.0018)
Batch 325/537: Loss=0.9614 (C:0.9614, R:0.0018)
Batch 350/537: Loss=0.9445 (C:0.9445, R:0.0018)
Batch 375/537: Loss=0.9659 (C:0.9659, R:0.0018)
Batch 400/537: Loss=0.9913 (C:0.9913, R:0.0018)
Batch 425/537: Loss=0.9960 (C:0.9960, R:0.0018)
Batch 450/537: Loss=0.9632 (C:0.9632, R:0.0018)
Batch 475/537: Loss=0.9560 (C:0.9560, R:0.0018)
Batch 500/537: Loss=0.9696 (C:0.9696, R:0.0018)
Batch 525/537: Loss=0.9804 (C:0.9804, R:0.0018)

============================================================
Epoch 61/100 completed in 34.6s
Train: Loss=0.9630 (C:0.9630, R:0.0018) Ratio=3.82x
Val:   Loss=0.9024 (C:0.9024, R:0.0018) Ratio=5.88x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.9024)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.9532 (C:0.9532, R:0.0018)
Batch  25/537: Loss=0.9513 (C:0.9513, R:0.0018)
Batch  50/537: Loss=0.9925 (C:0.9925, R:0.0018)
Batch  75/537: Loss=0.9812 (C:0.9812, R:0.0018)
Batch 100/537: Loss=0.9724 (C:0.9724, R:0.0018)
Batch 125/537: Loss=0.9721 (C:0.9721, R:0.0018)
Batch 150/537: Loss=1.0022 (C:1.0022, R:0.0018)
Batch 175/537: Loss=0.9554 (C:0.9554, R:0.0018)
Batch 200/537: Loss=0.9834 (C:0.9834, R:0.0018)
Batch 225/537: Loss=0.9497 (C:0.9497, R:0.0018)
Batch 250/537: Loss=0.9718 (C:0.9718, R:0.0018)
Batch 275/537: Loss=0.9455 (C:0.9455, R:0.0018)
Batch 300/537: Loss=0.9799 (C:0.9799, R:0.0018)
Batch 325/537: Loss=0.9614 (C:0.9614, R:0.0018)
Batch 350/537: Loss=0.9832 (C:0.9832, R:0.0018)
Batch 375/537: Loss=0.9374 (C:0.9374, R:0.0017)
Batch 400/537: Loss=0.9843 (C:0.9843, R:0.0018)
Batch 425/537: Loss=0.9742 (C:0.9742, R:0.0018)
Batch 450/537: Loss=0.9550 (C:0.9550, R:0.0018)
Batch 475/537: Loss=0.9484 (C:0.9484, R:0.0018)
Batch 500/537: Loss=0.9459 (C:0.9459, R:0.0018)
Batch 525/537: Loss=0.9844 (C:0.9844, R:0.0018)

============================================================
Epoch 62/100 completed in 29.7s
Train: Loss=0.9616 (C:0.9616, R:0.0018) Ratio=3.79x
Val:   Loss=0.9006 (C:0.9006, R:0.0018) Ratio=5.94x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.9006)
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.9401 (C:0.9401, R:0.0017)
Batch  25/537: Loss=0.9829 (C:0.9829, R:0.0018)
Batch  50/537: Loss=1.0137 (C:1.0137, R:0.0018)
Batch  75/537: Loss=0.9829 (C:0.9829, R:0.0018)
Batch 100/537: Loss=0.9619 (C:0.9619, R:0.0018)
Batch 125/537: Loss=0.9726 (C:0.9726, R:0.0018)
Batch 150/537: Loss=0.9921 (C:0.9921, R:0.0018)
Batch 175/537: Loss=0.9349 (C:0.9349, R:0.0018)
Batch 200/537: Loss=0.9532 (C:0.9532, R:0.0018)
Batch 225/537: Loss=0.9453 (C:0.9453, R:0.0018)
Batch 250/537: Loss=0.9758 (C:0.9758, R:0.0018)
Batch 275/537: Loss=0.9804 (C:0.9804, R:0.0018)
Batch 300/537: Loss=0.9635 (C:0.9635, R:0.0018)
Batch 325/537: Loss=0.9574 (C:0.9574, R:0.0018)
Batch 350/537: Loss=0.9562 (C:0.9562, R:0.0018)
Batch 375/537: Loss=0.9429 (C:0.9429, R:0.0018)
Batch 400/537: Loss=0.9816 (C:0.9816, R:0.0018)
Batch 425/537: Loss=0.9939 (C:0.9939, R:0.0018)
Batch 450/537: Loss=0.9870 (C:0.9870, R:0.0018)
Batch 475/537: Loss=0.9710 (C:0.9710, R:0.0018)
Batch 500/537: Loss=0.9372 (C:0.9372, R:0.0018)
Batch 525/537: Loss=0.9552 (C:0.9552, R:0.0018)

============================================================
Epoch 63/100 completed in 29.9s
Train: Loss=0.9618 (C:0.9618, R:0.0018) Ratio=3.86x
Val:   Loss=0.9014 (C:0.9014, R:0.0018) Ratio=6.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.254 ± 0.473
    Neg distances: 1.550 ± 0.988
    Separation ratio: 6.11x
    Gap: -2.777
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.9369 (C:0.9369, R:0.0018)
Batch  25/537: Loss=0.9623 (C:0.9623, R:0.0018)
Batch  50/537: Loss=0.9546 (C:0.9546, R:0.0018)
Batch  75/537: Loss=0.9860 (C:0.9860, R:0.0018)
Batch 100/537: Loss=0.9581 (C:0.9581, R:0.0018)
Batch 125/537: Loss=0.9589 (C:0.9589, R:0.0018)
Batch 150/537: Loss=0.9539 (C:0.9539, R:0.0018)
Batch 175/537: Loss=0.9390 (C:0.9390, R:0.0018)
Batch 200/537: Loss=0.9839 (C:0.9839, R:0.0018)
Batch 225/537: Loss=0.9269 (C:0.9269, R:0.0018)
Batch 250/537: Loss=0.9697 (C:0.9697, R:0.0018)
Batch 275/537: Loss=0.9666 (C:0.9666, R:0.0018)
Batch 300/537: Loss=0.9824 (C:0.9824, R:0.0018)
Batch 325/537: Loss=0.9287 (C:0.9287, R:0.0018)
Batch 350/537: Loss=0.9270 (C:0.9270, R:0.0018)
Batch 375/537: Loss=0.9591 (C:0.9591, R:0.0018)
Batch 400/537: Loss=0.9581 (C:0.9581, R:0.0018)
Batch 425/537: Loss=0.9629 (C:0.9629, R:0.0018)
Batch 450/537: Loss=0.9634 (C:0.9634, R:0.0018)
Batch 475/537: Loss=0.9584 (C:0.9584, R:0.0018)
Batch 500/537: Loss=0.9746 (C:0.9746, R:0.0018)
Batch 525/537: Loss=0.9381 (C:0.9381, R:0.0018)

============================================================
Epoch 64/100 completed in 35.3s
Train: Loss=0.9565 (C:0.9565, R:0.0018) Ratio=3.96x
Val:   Loss=0.8963 (C:0.8963, R:0.0018) Ratio=6.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8963)
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.9507 (C:0.9507, R:0.0018)
Batch  25/537: Loss=0.9609 (C:0.9609, R:0.0018)
Batch  50/537: Loss=0.9524 (C:0.9524, R:0.0018)
Batch  75/537: Loss=0.9438 (C:0.9438, R:0.0018)
Batch 100/537: Loss=0.9241 (C:0.9241, R:0.0018)
Batch 125/537: Loss=0.9661 (C:0.9661, R:0.0018)
Batch 150/537: Loss=0.9452 (C:0.9452, R:0.0018)
Batch 175/537: Loss=0.9428 (C:0.9428, R:0.0018)
Batch 200/537: Loss=0.9508 (C:0.9508, R:0.0018)
Batch 225/537: Loss=0.9848 (C:0.9848, R:0.0018)
Batch 250/537: Loss=0.9382 (C:0.9382, R:0.0018)
Batch 275/537: Loss=0.9108 (C:0.9108, R:0.0018)
Batch 300/537: Loss=0.9352 (C:0.9352, R:0.0018)
Batch 325/537: Loss=0.9454 (C:0.9454, R:0.0018)
Batch 350/537: Loss=0.9598 (C:0.9598, R:0.0017)
Batch 375/537: Loss=0.9556 (C:0.9556, R:0.0018)
Batch 400/537: Loss=0.9538 (C:0.9538, R:0.0018)
Batch 425/537: Loss=0.9846 (C:0.9846, R:0.0017)
Batch 450/537: Loss=0.9405 (C:0.9405, R:0.0018)
Batch 475/537: Loss=0.9548 (C:0.9548, R:0.0018)
Batch 500/537: Loss=0.9656 (C:0.9656, R:0.0018)
Batch 525/537: Loss=0.9666 (C:0.9666, R:0.0018)

============================================================
Epoch 65/100 completed in 29.7s
Train: Loss=0.9567 (C:0.9567, R:0.0018) Ratio=3.94x
Val:   Loss=0.8944 (C:0.8944, R:0.0018) Ratio=6.14x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8944)
Checkpoint saved at epoch 65
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.9378 (C:0.9378, R:0.0018)
Batch  25/537: Loss=0.9601 (C:0.9601, R:0.0018)
Batch  50/537: Loss=0.9740 (C:0.9740, R:0.0018)
Batch  75/537: Loss=0.9613 (C:0.9613, R:0.0018)
Batch 100/537: Loss=0.9784 (C:0.9784, R:0.0018)
Batch 125/537: Loss=0.9412 (C:0.9412, R:0.0018)
Batch 150/537: Loss=0.9163 (C:0.9163, R:0.0018)
Batch 175/537: Loss=0.9496 (C:0.9496, R:0.0018)
Batch 200/537: Loss=0.9503 (C:0.9503, R:0.0018)
Batch 225/537: Loss=0.9575 (C:0.9575, R:0.0018)
Batch 250/537: Loss=0.9807 (C:0.9807, R:0.0018)
Batch 275/537: Loss=0.9784 (C:0.9784, R:0.0017)
Batch 300/537: Loss=0.9497 (C:0.9497, R:0.0018)
Batch 325/537: Loss=0.9664 (C:0.9664, R:0.0018)
Batch 350/537: Loss=0.9699 (C:0.9699, R:0.0018)
Batch 375/537: Loss=0.9508 (C:0.9508, R:0.0018)
Batch 400/537: Loss=0.9334 (C:0.9334, R:0.0018)
Batch 425/537: Loss=0.9522 (C:0.9522, R:0.0018)
Batch 450/537: Loss=0.9455 (C:0.9455, R:0.0018)
Batch 475/537: Loss=0.9951 (C:0.9951, R:0.0018)
Batch 500/537: Loss=0.9432 (C:0.9432, R:0.0018)
Batch 525/537: Loss=0.9734 (C:0.9734, R:0.0018)

============================================================
Epoch 66/100 completed in 29.2s
Train: Loss=0.9559 (C:0.9559, R:0.0018) Ratio=3.91x
Val:   Loss=0.8924 (C:0.8924, R:0.0018) Ratio=6.26x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8924)
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.246 ± 0.470
    Neg distances: 1.565 ± 0.994
    Separation ratio: 6.35x
    Gap: -2.755
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.9144 (C:0.9144, R:0.0018)
Batch  25/537: Loss=0.9450 (C:0.9450, R:0.0018)
Batch  50/537: Loss=0.9390 (C:0.9390, R:0.0018)
Batch  75/537: Loss=0.9534 (C:0.9534, R:0.0018)
Batch 100/537: Loss=0.9465 (C:0.9465, R:0.0017)
Batch 125/537: Loss=0.9506 (C:0.9506, R:0.0018)
Batch 150/537: Loss=0.9526 (C:0.9526, R:0.0018)
Batch 175/537: Loss=0.9473 (C:0.9473, R:0.0018)
Batch 200/537: Loss=0.9544 (C:0.9544, R:0.0018)
Batch 225/537: Loss=0.9452 (C:0.9452, R:0.0018)
Batch 250/537: Loss=0.9497 (C:0.9497, R:0.0018)
Batch 275/537: Loss=0.9410 (C:0.9410, R:0.0018)
Batch 300/537: Loss=0.9594 (C:0.9594, R:0.0017)
Batch 325/537: Loss=0.8987 (C:0.8987, R:0.0017)
Batch 350/537: Loss=0.9397 (C:0.9397, R:0.0018)
Batch 375/537: Loss=0.9244 (C:0.9244, R:0.0017)
Batch 400/537: Loss=0.9313 (C:0.9313, R:0.0018)
Batch 425/537: Loss=0.9623 (C:0.9623, R:0.0018)
Batch 450/537: Loss=0.9560 (C:0.9560, R:0.0018)
Batch 475/537: Loss=0.9823 (C:0.9823, R:0.0018)
Batch 500/537: Loss=0.9533 (C:0.9533, R:0.0017)
Batch 525/537: Loss=0.9722 (C:0.9722, R:0.0018)

============================================================
Epoch 67/100 completed in 34.9s
Train: Loss=0.9466 (C:0.9466, R:0.0018) Ratio=3.96x
Val:   Loss=0.8830 (C:0.8830, R:0.0018) Ratio=6.29x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8830)
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.9308 (C:0.9308, R:0.0018)
Batch  25/537: Loss=0.9395 (C:0.9395, R:0.0018)
Batch  50/537: Loss=0.9420 (C:0.9420, R:0.0018)
Batch  75/537: Loss=0.9651 (C:0.9651, R:0.0018)
Batch 100/537: Loss=0.9357 (C:0.9357, R:0.0018)
Batch 125/537: Loss=0.9462 (C:0.9462, R:0.0018)
Batch 150/537: Loss=0.9227 (C:0.9227, R:0.0018)
Batch 175/537: Loss=0.9636 (C:0.9636, R:0.0018)
Batch 200/537: Loss=0.9646 (C:0.9646, R:0.0018)
Batch 225/537: Loss=0.9353 (C:0.9353, R:0.0018)
Batch 250/537: Loss=0.9526 (C:0.9526, R:0.0018)
Batch 275/537: Loss=0.9777 (C:0.9777, R:0.0017)
Batch 300/537: Loss=0.9324 (C:0.9324, R:0.0018)
Batch 325/537: Loss=0.9297 (C:0.9297, R:0.0018)
Batch 350/537: Loss=0.9252 (C:0.9252, R:0.0018)
Batch 375/537: Loss=0.9631 (C:0.9631, R:0.0018)
Batch 400/537: Loss=0.9206 (C:0.9206, R:0.0018)
Batch 425/537: Loss=0.9298 (C:0.9298, R:0.0018)
Batch 450/537: Loss=0.9298 (C:0.9298, R:0.0018)
Batch 475/537: Loss=0.9576 (C:0.9576, R:0.0018)
Batch 500/537: Loss=0.9490 (C:0.9490, R:0.0018)
Batch 525/537: Loss=0.9326 (C:0.9326, R:0.0018)

============================================================
Epoch 68/100 completed in 29.3s
Train: Loss=0.9449 (C:0.9449, R:0.0018) Ratio=4.03x
Val:   Loss=0.8822 (C:0.8822, R:0.0018) Ratio=6.43x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8822)
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.9282 (C:0.9282, R:0.0018)
Batch  25/537: Loss=0.9373 (C:0.9373, R:0.0018)
Batch  50/537: Loss=0.9050 (C:0.9050, R:0.0018)
Batch  75/537: Loss=0.9631 (C:0.9631, R:0.0018)
Batch 100/537: Loss=0.9427 (C:0.9427, R:0.0018)
Batch 125/537: Loss=0.9689 (C:0.9689, R:0.0018)
Batch 150/537: Loss=0.9420 (C:0.9420, R:0.0018)
Batch 175/537: Loss=0.9414 (C:0.9414, R:0.0018)
Batch 200/537: Loss=0.9181 (C:0.9181, R:0.0018)
Batch 225/537: Loss=0.9466 (C:0.9466, R:0.0018)
Batch 250/537: Loss=0.9687 (C:0.9687, R:0.0018)
Batch 275/537: Loss=0.9268 (C:0.9268, R:0.0018)
Batch 300/537: Loss=0.9236 (C:0.9236, R:0.0018)
Batch 325/537: Loss=0.9627 (C:0.9627, R:0.0018)
Batch 350/537: Loss=0.9409 (C:0.9409, R:0.0018)
Batch 375/537: Loss=0.9356 (C:0.9356, R:0.0018)
Batch 400/537: Loss=0.9478 (C:0.9478, R:0.0018)
Batch 425/537: Loss=0.9624 (C:0.9624, R:0.0018)
Batch 450/537: Loss=0.9374 (C:0.9374, R:0.0018)
Batch 475/537: Loss=0.9448 (C:0.9448, R:0.0018)
Batch 500/537: Loss=0.9778 (C:0.9778, R:0.0018)
Batch 525/537: Loss=0.9187 (C:0.9187, R:0.0018)

============================================================
Epoch 69/100 completed in 29.9s
Train: Loss=0.9438 (C:0.9438, R:0.0018) Ratio=4.01x
Val:   Loss=0.8803 (C:0.8803, R:0.0018) Ratio=6.42x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8803)
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.234 ± 0.447
    Neg distances: 1.549 ± 1.000
    Separation ratio: 6.62x
    Gap: -2.788
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.9303 (C:0.9303, R:0.0017)
Batch  25/537: Loss=0.9534 (C:0.9534, R:0.0018)
Batch  50/537: Loss=0.9009 (C:0.9009, R:0.0017)
Batch  75/537: Loss=0.9588 (C:0.9588, R:0.0018)
Batch 100/537: Loss=0.9250 (C:0.9250, R:0.0018)
Batch 125/537: Loss=0.9379 (C:0.9379, R:0.0018)
Batch 150/537: Loss=0.9391 (C:0.9391, R:0.0018)
Batch 175/537: Loss=0.9469 (C:0.9469, R:0.0018)
Batch 200/537: Loss=0.9552 (C:0.9552, R:0.0018)
Batch 225/537: Loss=0.9151 (C:0.9151, R:0.0017)
Batch 250/537: Loss=0.9363 (C:0.9363, R:0.0018)
Batch 275/537: Loss=0.9510 (C:0.9510, R:0.0018)
Batch 300/537: Loss=0.9564 (C:0.9564, R:0.0017)
Batch 325/537: Loss=0.9320 (C:0.9320, R:0.0018)
Batch 350/537: Loss=0.9329 (C:0.9329, R:0.0018)
Batch 375/537: Loss=0.9232 (C:0.9232, R:0.0018)
Batch 400/537: Loss=0.9454 (C:0.9454, R:0.0018)
Batch 425/537: Loss=0.9358 (C:0.9358, R:0.0018)
Batch 450/537: Loss=0.9406 (C:0.9406, R:0.0018)
Batch 475/537: Loss=0.9743 (C:0.9743, R:0.0018)
Batch 500/537: Loss=0.9448 (C:0.9448, R:0.0018)
Batch 525/537: Loss=0.9299 (C:0.9299, R:0.0018)

============================================================
Epoch 70/100 completed in 34.8s
Train: Loss=0.9432 (C:0.9432, R:0.0018) Ratio=4.06x
Val:   Loss=0.8808 (C:0.8808, R:0.0018) Ratio=6.54x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 70
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.9601 (C:0.9601, R:0.0018)
Batch  25/537: Loss=0.9212 (C:0.9212, R:0.0017)
Batch  50/537: Loss=0.9077 (C:0.9077, R:0.0018)
Batch  75/537: Loss=0.9053 (C:0.9053, R:0.0017)
Batch 100/537: Loss=0.9320 (C:0.9320, R:0.0018)
Batch 125/537: Loss=0.9359 (C:0.9359, R:0.0018)
Batch 150/537: Loss=0.9415 (C:0.9415, R:0.0018)
Batch 175/537: Loss=0.9301 (C:0.9301, R:0.0017)
Batch 200/537: Loss=0.9524 (C:0.9524, R:0.0018)
Batch 225/537: Loss=0.9574 (C:0.9574, R:0.0018)
Batch 250/537: Loss=0.9537 (C:0.9537, R:0.0018)
Batch 275/537: Loss=0.9463 (C:0.9463, R:0.0017)
Batch 300/537: Loss=0.9619 (C:0.9619, R:0.0018)
Batch 325/537: Loss=0.9531 (C:0.9531, R:0.0018)
Batch 350/537: Loss=0.9826 (C:0.9826, R:0.0018)
Batch 375/537: Loss=0.9399 (C:0.9399, R:0.0018)
Batch 400/537: Loss=0.9591 (C:0.9591, R:0.0018)
Batch 425/537: Loss=0.9695 (C:0.9695, R:0.0018)
Batch 450/537: Loss=0.9606 (C:0.9606, R:0.0018)
Batch 475/537: Loss=0.9340 (C:0.9340, R:0.0018)
Batch 500/537: Loss=0.9511 (C:0.9511, R:0.0018)
Batch 525/537: Loss=0.9480 (C:0.9480, R:0.0018)

============================================================
Epoch 71/100 completed in 29.4s
Train: Loss=0.9425 (C:0.9425, R:0.0018) Ratio=3.99x
Val:   Loss=0.8820 (C:0.8820, R:0.0018) Ratio=6.33x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.9629 (C:0.9629, R:0.0018)
Batch  25/537: Loss=0.9315 (C:0.9315, R:0.0018)
Batch  50/537: Loss=0.9027 (C:0.9027, R:0.0018)
Batch  75/537: Loss=0.9065 (C:0.9065, R:0.0018)
Batch 100/537: Loss=0.9407 (C:0.9407, R:0.0018)
Batch 125/537: Loss=0.9560 (C:0.9560, R:0.0018)
Batch 150/537: Loss=0.9422 (C:0.9422, R:0.0018)
Batch 175/537: Loss=0.9342 (C:0.9342, R:0.0018)
Batch 200/537: Loss=0.9300 (C:0.9300, R:0.0018)
Batch 225/537: Loss=0.9567 (C:0.9567, R:0.0018)
Batch 250/537: Loss=0.9056 (C:0.9056, R:0.0018)
Batch 275/537: Loss=0.9600 (C:0.9600, R:0.0018)
Batch 300/537: Loss=0.9594 (C:0.9594, R:0.0018)
Batch 325/537: Loss=0.9444 (C:0.9444, R:0.0018)
Batch 350/537: Loss=0.9700 (C:0.9700, R:0.0018)
Batch 375/537: Loss=0.9427 (C:0.9427, R:0.0018)
Batch 400/537: Loss=0.9390 (C:0.9390, R:0.0018)
Batch 425/537: Loss=0.9279 (C:0.9279, R:0.0018)
Batch 450/537: Loss=0.9375 (C:0.9375, R:0.0017)
Batch 475/537: Loss=0.9277 (C:0.9277, R:0.0018)
Batch 500/537: Loss=0.9602 (C:0.9602, R:0.0018)
Batch 525/537: Loss=0.9893 (C:0.9893, R:0.0018)

============================================================
Epoch 72/100 completed in 29.5s
Train: Loss=0.9426 (C:0.9426, R:0.0018) Ratio=4.08x
Val:   Loss=0.8785 (C:0.8785, R:0.0018) Ratio=6.70x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8785)
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.234 ± 0.450
    Neg distances: 1.546 ± 1.000
    Separation ratio: 6.59x
    Gap: -2.673
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=0.9293 (C:0.9293, R:0.0018)
Batch  25/537: Loss=0.9396 (C:0.9396, R:0.0018)
Batch  50/537: Loss=0.9422 (C:0.9422, R:0.0018)
Batch  75/537: Loss=0.9523 (C:0.9523, R:0.0018)
Batch 100/537: Loss=0.9412 (C:0.9412, R:0.0018)
Batch 125/537: Loss=0.9474 (C:0.9474, R:0.0018)
Batch 150/537: Loss=0.9451 (C:0.9451, R:0.0018)
Batch 175/537: Loss=0.9473 (C:0.9473, R:0.0018)
Batch 200/537: Loss=0.9393 (C:0.9393, R:0.0017)
Batch 225/537: Loss=0.9326 (C:0.9326, R:0.0017)
Batch 250/537: Loss=0.9554 (C:0.9554, R:0.0017)
Batch 275/537: Loss=0.9230 (C:0.9230, R:0.0017)
Batch 300/537: Loss=0.9629 (C:0.9629, R:0.0018)
Batch 325/537: Loss=0.9796 (C:0.9796, R:0.0018)
Batch 350/537: Loss=0.9440 (C:0.9440, R:0.0018)
Batch 375/537: Loss=0.9561 (C:0.9561, R:0.0018)
Batch 400/537: Loss=0.9586 (C:0.9586, R:0.0018)
Batch 425/537: Loss=0.9803 (C:0.9803, R:0.0018)
Batch 450/537: Loss=0.9436 (C:0.9436, R:0.0018)
Batch 475/537: Loss=0.9424 (C:0.9424, R:0.0018)
Batch 500/537: Loss=0.9507 (C:0.9507, R:0.0018)
Batch 525/537: Loss=0.9328 (C:0.9328, R:0.0018)

============================================================
Epoch 73/100 completed in 34.2s
Train: Loss=0.9417 (C:0.9417, R:0.0018) Ratio=4.01x
Val:   Loss=0.8783 (C:0.8783, R:0.0018) Ratio=6.73x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8783)
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=0.9339 (C:0.9339, R:0.0018)
Batch  25/537: Loss=0.9234 (C:0.9234, R:0.0018)
Batch  50/537: Loss=0.9313 (C:0.9313, R:0.0018)
Batch  75/537: Loss=0.9831 (C:0.9831, R:0.0018)
Batch 100/537: Loss=0.9607 (C:0.9607, R:0.0018)
Batch 125/537: Loss=0.9444 (C:0.9444, R:0.0017)
Batch 150/537: Loss=0.9439 (C:0.9439, R:0.0018)
Batch 175/537: Loss=0.9042 (C:0.9042, R:0.0018)
Batch 200/537: Loss=0.9391 (C:0.9391, R:0.0018)
Batch 225/537: Loss=0.9668 (C:0.9668, R:0.0018)
Batch 250/537: Loss=0.9430 (C:0.9430, R:0.0018)
Batch 275/537: Loss=0.9549 (C:0.9549, R:0.0018)
Batch 300/537: Loss=0.9538 (C:0.9538, R:0.0018)
Batch 325/537: Loss=0.9567 (C:0.9567, R:0.0018)
Batch 350/537: Loss=0.9421 (C:0.9421, R:0.0018)
Batch 375/537: Loss=0.9340 (C:0.9340, R:0.0018)
Batch 400/537: Loss=0.9519 (C:0.9519, R:0.0018)
Batch 425/537: Loss=0.9324 (C:0.9324, R:0.0018)
Batch 450/537: Loss=0.9378 (C:0.9378, R:0.0018)
Batch 475/537: Loss=0.9645 (C:0.9645, R:0.0018)
Batch 500/537: Loss=0.9533 (C:0.9533, R:0.0018)
Batch 525/537: Loss=0.9518 (C:0.9518, R:0.0018)

============================================================
Epoch 74/100 completed in 28.9s
Train: Loss=0.9411 (C:0.9411, R:0.0018) Ratio=4.03x
Val:   Loss=0.8763 (C:0.8763, R:0.0018) Ratio=6.84x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8763)
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=0.9385 (C:0.9385, R:0.0018)
Batch  25/537: Loss=0.9348 (C:0.9348, R:0.0018)
Batch  50/537: Loss=0.9402 (C:0.9402, R:0.0018)
Batch  75/537: Loss=0.9351 (C:0.9351, R:0.0018)
Batch 100/537: Loss=0.9235 (C:0.9235, R:0.0018)
Batch 125/537: Loss=0.9572 (C:0.9572, R:0.0017)
Batch 150/537: Loss=0.9276 (C:0.9276, R:0.0018)
Batch 175/537: Loss=0.9153 (C:0.9153, R:0.0018)
Batch 200/537: Loss=0.9290 (C:0.9290, R:0.0018)
Batch 225/537: Loss=0.9445 (C:0.9445, R:0.0017)
Batch 250/537: Loss=0.9631 (C:0.9631, R:0.0017)
Batch 275/537: Loss=0.9553 (C:0.9553, R:0.0018)
Batch 300/537: Loss=0.9360 (C:0.9360, R:0.0018)
Batch 325/537: Loss=0.9358 (C:0.9358, R:0.0018)
Batch 350/537: Loss=0.9597 (C:0.9597, R:0.0018)
Batch 375/537: Loss=0.9273 (C:0.9273, R:0.0017)
Batch 400/537: Loss=0.9478 (C:0.9478, R:0.0018)
Batch 425/537: Loss=0.9774 (C:0.9774, R:0.0018)
Batch 450/537: Loss=0.9506 (C:0.9506, R:0.0018)
Batch 475/537: Loss=0.9362 (C:0.9362, R:0.0017)
Batch 500/537: Loss=0.9578 (C:0.9578, R:0.0018)
Batch 525/537: Loss=0.9319 (C:0.9319, R:0.0018)

============================================================
Epoch 75/100 completed in 29.0s
Train: Loss=0.9399 (C:0.9399, R:0.0018) Ratio=4.03x
Val:   Loss=0.8761 (C:0.8761, R:0.0018) Ratio=6.85x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8761)
Checkpoint saved at epoch 75
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.224 ± 0.446
    Neg distances: 1.572 ± 1.003
    Separation ratio: 7.01x
    Gap: -2.701
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=0.9275 (C:0.9275, R:0.0018)
Batch  25/537: Loss=0.9302 (C:0.9302, R:0.0018)
Batch  50/537: Loss=0.9598 (C:0.9598, R:0.0018)
Batch  75/537: Loss=0.8828 (C:0.8828, R:0.0018)
Batch 100/537: Loss=0.9345 (C:0.9345, R:0.0018)
Batch 125/537: Loss=0.9162 (C:0.9162, R:0.0018)
Batch 150/537: Loss=0.9144 (C:0.9144, R:0.0018)
Batch 175/537: Loss=0.9313 (C:0.9313, R:0.0018)
Batch 200/537: Loss=0.9548 (C:0.9548, R:0.0018)
Batch 225/537: Loss=0.9510 (C:0.9510, R:0.0018)
Batch 250/537: Loss=0.9279 (C:0.9279, R:0.0018)
Batch 275/537: Loss=0.9390 (C:0.9390, R:0.0018)
Batch 300/537: Loss=0.8888 (C:0.8888, R:0.0018)
Batch 325/537: Loss=0.9095 (C:0.9095, R:0.0018)
Batch 350/537: Loss=0.9241 (C:0.9241, R:0.0018)
Batch 375/537: Loss=0.9207 (C:0.9207, R:0.0017)
Batch 400/537: Loss=0.9553 (C:0.9553, R:0.0017)
Batch 425/537: Loss=0.9405 (C:0.9405, R:0.0018)
Batch 450/537: Loss=0.9293 (C:0.9293, R:0.0018)
Batch 475/537: Loss=0.9308 (C:0.9308, R:0.0018)
Batch 500/537: Loss=0.9169 (C:0.9169, R:0.0018)
Batch 525/537: Loss=0.9451 (C:0.9451, R:0.0018)

============================================================
Epoch 76/100 completed in 35.3s
Train: Loss=0.9248 (C:0.9248, R:0.0018) Ratio=4.16x
Val:   Loss=0.8613 (C:0.8613, R:0.0018) Ratio=6.92x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8613)
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=0.9630 (C:0.9630, R:0.0018)
Batch  25/537: Loss=0.8865 (C:0.8865, R:0.0018)
Batch  50/537: Loss=0.8981 (C:0.8981, R:0.0018)
Batch  75/537: Loss=0.9217 (C:0.9217, R:0.0018)
Batch 100/537: Loss=0.9640 (C:0.9640, R:0.0018)
Batch 125/537: Loss=0.9081 (C:0.9081, R:0.0018)
Batch 150/537: Loss=0.9423 (C:0.9423, R:0.0018)
Batch 175/537: Loss=0.8989 (C:0.8989, R:0.0018)
Batch 200/537: Loss=0.9419 (C:0.9419, R:0.0018)
Batch 225/537: Loss=0.9089 (C:0.9089, R:0.0017)
Batch 250/537: Loss=0.9365 (C:0.9365, R:0.0018)
Batch 275/537: Loss=0.9310 (C:0.9310, R:0.0017)
Batch 300/537: Loss=0.9135 (C:0.9135, R:0.0018)
Batch 325/537: Loss=0.9224 (C:0.9224, R:0.0018)
Batch 350/537: Loss=0.9605 (C:0.9605, R:0.0018)
Batch 375/537: Loss=0.9146 (C:0.9146, R:0.0018)
Batch 400/537: Loss=0.9587 (C:0.9587, R:0.0018)
Batch 425/537: Loss=0.9055 (C:0.9055, R:0.0018)
Batch 450/537: Loss=0.9403 (C:0.9403, R:0.0018)
Batch 475/537: Loss=0.9473 (C:0.9473, R:0.0018)
Batch 500/537: Loss=0.9397 (C:0.9397, R:0.0018)
Batch 525/537: Loss=0.9127 (C:0.9127, R:0.0018)

============================================================
Epoch 77/100 completed in 29.4s
Train: Loss=0.9244 (C:0.9244, R:0.0018) Ratio=4.06x
Val:   Loss=0.8595 (C:0.8595, R:0.0018) Ratio=6.99x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8595)
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=0.9061 (C:0.9061, R:0.0018)
Batch  25/537: Loss=0.9149 (C:0.9149, R:0.0017)
Batch  50/537: Loss=0.9031 (C:0.9031, R:0.0018)
Batch  75/537: Loss=0.8790 (C:0.8790, R:0.0018)
Batch 100/537: Loss=0.9123 (C:0.9123, R:0.0018)
Batch 125/537: Loss=0.9226 (C:0.9226, R:0.0018)
Batch 150/537: Loss=0.9244 (C:0.9244, R:0.0018)
Batch 175/537: Loss=0.9549 (C:0.9549, R:0.0018)
Batch 200/537: Loss=0.9143 (C:0.9143, R:0.0018)
Batch 225/537: Loss=0.9487 (C:0.9487, R:0.0018)
Batch 250/537: Loss=0.9283 (C:0.9283, R:0.0018)
Batch 275/537: Loss=0.9485 (C:0.9485, R:0.0018)
Batch 300/537: Loss=0.8957 (C:0.8957, R:0.0018)
Batch 325/537: Loss=0.9337 (C:0.9337, R:0.0018)
Batch 350/537: Loss=0.9327 (C:0.9327, R:0.0018)
Batch 375/537: Loss=0.9133 (C:0.9133, R:0.0018)
Batch 400/537: Loss=0.9299 (C:0.9299, R:0.0018)
Batch 425/537: Loss=0.9311 (C:0.9311, R:0.0018)
Batch 450/537: Loss=0.9107 (C:0.9107, R:0.0018)
Batch 475/537: Loss=0.9294 (C:0.9294, R:0.0018)
Batch 500/537: Loss=0.9200 (C:0.9200, R:0.0018)
Batch 525/537: Loss=0.9309 (C:0.9309, R:0.0018)

============================================================
Epoch 78/100 completed in 29.4s
Train: Loss=0.9232 (C:0.9232, R:0.0018) Ratio=4.16x
Val:   Loss=0.8586 (C:0.8586, R:0.0018) Ratio=7.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8586)
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.241 ± 0.472
    Neg distances: 1.550 ± 1.011
    Separation ratio: 6.42x
    Gap: -2.655
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=0.9310 (C:0.9310, R:0.0018)
Batch  25/537: Loss=0.9151 (C:0.9151, R:0.0018)
Batch  50/537: Loss=0.9283 (C:0.9283, R:0.0018)
Batch  75/537: Loss=0.9553 (C:0.9553, R:0.0018)
Batch 100/537: Loss=0.9203 (C:0.9203, R:0.0018)
Batch 125/537: Loss=0.9549 (C:0.9549, R:0.0018)
Batch 150/537: Loss=0.9544 (C:0.9544, R:0.0018)
Batch 175/537: Loss=0.9546 (C:0.9546, R:0.0018)
Batch 200/537: Loss=0.9337 (C:0.9337, R:0.0018)
Batch 225/537: Loss=0.9406 (C:0.9406, R:0.0018)
Batch 250/537: Loss=0.9176 (C:0.9176, R:0.0017)
Batch 275/537: Loss=0.9897 (C:0.9897, R:0.0018)
Batch 300/537: Loss=0.9402 (C:0.9402, R:0.0018)
Batch 325/537: Loss=0.9591 (C:0.9591, R:0.0018)
Batch 350/537: Loss=0.9342 (C:0.9342, R:0.0018)
Batch 375/537: Loss=0.9402 (C:0.9402, R:0.0018)
Batch 400/537: Loss=0.9598 (C:0.9598, R:0.0018)
Batch 425/537: Loss=0.9328 (C:0.9328, R:0.0018)
Batch 450/537: Loss=0.9291 (C:0.9291, R:0.0018)
Batch 475/537: Loss=0.9791 (C:0.9791, R:0.0018)
Batch 500/537: Loss=0.9227 (C:0.9227, R:0.0018)
Batch 525/537: Loss=0.9409 (C:0.9409, R:0.0018)

============================================================
Epoch 79/100 completed in 35.1s
Train: Loss=0.9431 (C:0.9431, R:0.0018) Ratio=4.15x
Val:   Loss=0.8795 (C:0.8795, R:0.0018) Ratio=7.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=0.9556 (C:0.9556, R:0.0018)
Batch  25/537: Loss=0.9491 (C:0.9491, R:0.0018)
Batch  50/537: Loss=0.9620 (C:0.9620, R:0.0017)
Batch  75/537: Loss=0.9765 (C:0.9765, R:0.0017)
Batch 100/537: Loss=0.9648 (C:0.9648, R:0.0018)
Batch 125/537: Loss=0.9152 (C:0.9152, R:0.0018)
Batch 150/537: Loss=0.9364 (C:0.9364, R:0.0018)
Batch 175/537: Loss=0.9229 (C:0.9229, R:0.0018)
Batch 200/537: Loss=0.9538 (C:0.9538, R:0.0018)
Batch 225/537: Loss=0.9379 (C:0.9379, R:0.0018)
Batch 250/537: Loss=0.9620 (C:0.9620, R:0.0018)
Batch 275/537: Loss=0.9792 (C:0.9792, R:0.0018)
Batch 300/537: Loss=0.9481 (C:0.9481, R:0.0018)
Batch 325/537: Loss=0.9491 (C:0.9491, R:0.0018)
Batch 350/537: Loss=0.9451 (C:0.9451, R:0.0018)
Batch 375/537: Loss=0.9405 (C:0.9405, R:0.0018)
Batch 400/537: Loss=0.9485 (C:0.9485, R:0.0017)
Batch 425/537: Loss=0.9722 (C:0.9722, R:0.0018)
Batch 450/537: Loss=0.9599 (C:0.9599, R:0.0018)
Batch 475/537: Loss=0.9524 (C:0.9524, R:0.0018)
Batch 500/537: Loss=0.9481 (C:0.9481, R:0.0018)
Batch 525/537: Loss=0.9630 (C:0.9630, R:0.0017)

============================================================
Epoch 80/100 completed in 29.0s
Train: Loss=0.9411 (C:0.9411, R:0.0018) Ratio=4.07x
Val:   Loss=0.8770 (C:0.8770, R:0.0018) Ratio=7.21x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=0.9542 (C:0.9542, R:0.0018)
Batch  25/537: Loss=0.9355 (C:0.9355, R:0.0017)
Batch  50/537: Loss=0.9125 (C:0.9125, R:0.0018)
Batch  75/537: Loss=0.9110 (C:0.9110, R:0.0018)
Batch 100/537: Loss=0.9154 (C:0.9154, R:0.0017)
Batch 125/537: Loss=0.9276 (C:0.9276, R:0.0018)
Batch 150/537: Loss=0.9396 (C:0.9396, R:0.0017)
Batch 175/537: Loss=0.9425 (C:0.9425, R:0.0018)
Batch 200/537: Loss=0.8978 (C:0.8978, R:0.0018)
Batch 225/537: Loss=0.9544 (C:0.9544, R:0.0018)
Batch 250/537: Loss=0.9110 (C:0.9110, R:0.0018)
Batch 275/537: Loss=0.9458 (C:0.9458, R:0.0018)
Batch 300/537: Loss=0.9239 (C:0.9239, R:0.0017)
Batch 325/537: Loss=0.9163 (C:0.9163, R:0.0017)
Batch 350/537: Loss=0.9473 (C:0.9473, R:0.0018)
Batch 375/537: Loss=0.9548 (C:0.9548, R:0.0018)
Batch 400/537: Loss=0.9094 (C:0.9094, R:0.0018)
Batch 425/537: Loss=0.9418 (C:0.9418, R:0.0018)
Batch 450/537: Loss=0.9488 (C:0.9488, R:0.0018)
Batch 475/537: Loss=0.9476 (C:0.9476, R:0.0018)
Batch 500/537: Loss=0.8976 (C:0.8976, R:0.0018)
Batch 525/537: Loss=0.9423 (C:0.9423, R:0.0018)

============================================================
Epoch 81/100 completed in 29.2s
Train: Loss=0.9401 (C:0.9401, R:0.0018) Ratio=4.28x
Val:   Loss=0.8760 (C:0.8760, R:0.0018) Ratio=7.39x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.225 ± 0.479
    Neg distances: 1.571 ± 1.022
    Separation ratio: 6.97x
    Gap: -2.688
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=0.9404 (C:0.9404, R:0.0018)
Batch  25/537: Loss=0.9318 (C:0.9318, R:0.0018)
Batch  50/537: Loss=0.9090 (C:0.9090, R:0.0018)
Batch  75/537: Loss=0.9212 (C:0.9212, R:0.0018)
Batch 100/537: Loss=0.9204 (C:0.9204, R:0.0017)
Batch 125/537: Loss=0.9110 (C:0.9110, R:0.0018)
Batch 150/537: Loss=0.9195 (C:0.9195, R:0.0018)
Batch 175/537: Loss=0.9348 (C:0.9348, R:0.0018)
Batch 200/537: Loss=0.9323 (C:0.9323, R:0.0018)
Batch 225/537: Loss=0.9269 (C:0.9269, R:0.0018)
Batch 250/537: Loss=0.9154 (C:0.9154, R:0.0018)
Batch 275/537: Loss=0.9125 (C:0.9125, R:0.0018)
Batch 300/537: Loss=0.9619 (C:0.9619, R:0.0018)
Batch 325/537: Loss=0.9050 (C:0.9050, R:0.0018)
Batch 350/537: Loss=0.9596 (C:0.9596, R:0.0018)
Batch 375/537: Loss=0.9185 (C:0.9185, R:0.0018)
Batch 400/537: Loss=0.9220 (C:0.9220, R:0.0018)
Batch 425/537: Loss=0.9245 (C:0.9245, R:0.0018)
Batch 450/537: Loss=0.9446 (C:0.9446, R:0.0018)
Batch 475/537: Loss=0.9265 (C:0.9265, R:0.0018)
Batch 500/537: Loss=0.9317 (C:0.9317, R:0.0017)
Batch 525/537: Loss=0.9170 (C:0.9170, R:0.0018)

============================================================
Epoch 82/100 completed in 34.2s
Train: Loss=0.9268 (C:0.9268, R:0.0018) Ratio=4.26x
Val:   Loss=0.8626 (C:0.8626, R:0.0018) Ratio=7.21x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=0.8990 (C:0.8990, R:0.0018)
Batch  25/537: Loss=0.9253 (C:0.9253, R:0.0018)
Batch  50/537: Loss=0.9411 (C:0.9411, R:0.0018)
Batch  75/537: Loss=0.9127 (C:0.9127, R:0.0018)
Batch 100/537: Loss=0.9038 (C:0.9038, R:0.0018)
Batch 125/537: Loss=0.9528 (C:0.9528, R:0.0018)
Batch 150/537: Loss=0.8978 (C:0.8978, R:0.0018)
Batch 175/537: Loss=0.9229 (C:0.9229, R:0.0018)
Batch 200/537: Loss=0.9366 (C:0.9366, R:0.0018)
Batch 225/537: Loss=0.9300 (C:0.9300, R:0.0018)
Batch 250/537: Loss=0.9078 (C:0.9078, R:0.0017)
Batch 275/537: Loss=0.9271 (C:0.9271, R:0.0018)
Batch 300/537: Loss=0.8985 (C:0.8985, R:0.0018)
Batch 325/537: Loss=0.9359 (C:0.9359, R:0.0018)
Batch 350/537: Loss=0.9008 (C:0.9008, R:0.0018)
Batch 375/537: Loss=0.8887 (C:0.8887, R:0.0018)
Batch 400/537: Loss=0.9159 (C:0.9159, R:0.0018)
Batch 425/537: Loss=0.8778 (C:0.8778, R:0.0018)
Batch 450/537: Loss=0.9569 (C:0.9569, R:0.0017)
Batch 475/537: Loss=0.9170 (C:0.9170, R:0.0017)
Batch 500/537: Loss=0.9609 (C:0.9609, R:0.0017)
Batch 525/537: Loss=0.9283 (C:0.9283, R:0.0018)

============================================================
Epoch 83/100 completed in 28.8s
Train: Loss=0.9252 (C:0.9252, R:0.0018) Ratio=4.15x
Val:   Loss=0.8623 (C:0.8623, R:0.0018) Ratio=7.31x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=0.9063 (C:0.9063, R:0.0018)
Batch  25/537: Loss=0.9167 (C:0.9167, R:0.0018)
Batch  50/537: Loss=0.8964 (C:0.8964, R:0.0018)
Batch  75/537: Loss=0.9158 (C:0.9158, R:0.0017)
Batch 100/537: Loss=0.9434 (C:0.9434, R:0.0017)
Batch 125/537: Loss=0.9083 (C:0.9083, R:0.0018)
Batch 150/537: Loss=0.9144 (C:0.9144, R:0.0018)
Batch 175/537: Loss=0.9267 (C:0.9267, R:0.0017)
Batch 200/537: Loss=0.9509 (C:0.9509, R:0.0018)
Batch 225/537: Loss=0.9423 (C:0.9423, R:0.0018)
Batch 250/537: Loss=0.9004 (C:0.9004, R:0.0018)
Batch 275/537: Loss=0.9153 (C:0.9153, R:0.0017)
Batch 300/537: Loss=0.9248 (C:0.9248, R:0.0018)
Batch 325/537: Loss=0.9353 (C:0.9353, R:0.0018)
Batch 350/537: Loss=0.9308 (C:0.9308, R:0.0018)
Batch 375/537: Loss=0.9832 (C:0.9832, R:0.0018)
Batch 400/537: Loss=0.9585 (C:0.9585, R:0.0018)
Batch 425/537: Loss=0.9292 (C:0.9292, R:0.0018)
Batch 450/537: Loss=0.9224 (C:0.9224, R:0.0018)
Batch 475/537: Loss=0.9142 (C:0.9142, R:0.0018)
Batch 500/537: Loss=0.9233 (C:0.9233, R:0.0018)
Batch 525/537: Loss=0.9217 (C:0.9217, R:0.0018)

============================================================
Epoch 84/100 completed in 29.2s
Train: Loss=0.9233 (C:0.9233, R:0.0018) Ratio=4.17x
Val:   Loss=0.8602 (C:0.8602, R:0.0018) Ratio=7.32x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 85
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.228 ± 0.466
    Neg distances: 1.564 ± 1.010
    Separation ratio: 6.87x
    Gap: -2.711
    ✅ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=0.8976 (C:0.8976, R:0.0018)
Batch  25/537: Loss=0.9250 (C:0.9250, R:0.0018)
Batch  50/537: Loss=0.9068 (C:0.9068, R:0.0017)
Batch  75/537: Loss=0.8978 (C:0.8978, R:0.0018)
Batch 100/537: Loss=0.9416 (C:0.9416, R:0.0018)
Batch 125/537: Loss=0.9106 (C:0.9106, R:0.0018)
Batch 150/537: Loss=0.9046 (C:0.9046, R:0.0018)
Batch 175/537: Loss=0.9015 (C:0.9015, R:0.0018)
Batch 200/537: Loss=0.9521 (C:0.9521, R:0.0018)
Batch 225/537: Loss=0.9205 (C:0.9205, R:0.0018)
Batch 250/537: Loss=0.9456 (C:0.9456, R:0.0018)
Batch 275/537: Loss=0.9125 (C:0.9125, R:0.0017)
Batch 300/537: Loss=0.9287 (C:0.9287, R:0.0018)
Batch 325/537: Loss=0.8940 (C:0.8940, R:0.0018)
Batch 350/537: Loss=0.9391 (C:0.9391, R:0.0018)
Batch 375/537: Loss=0.9133 (C:0.9133, R:0.0018)
Batch 400/537: Loss=0.9455 (C:0.9455, R:0.0018)
Batch 425/537: Loss=0.9372 (C:0.9372, R:0.0018)
Batch 450/537: Loss=0.8952 (C:0.8952, R:0.0018)
Batch 475/537: Loss=0.9184 (C:0.9184, R:0.0018)
Batch 500/537: Loss=0.9418 (C:0.9418, R:0.0017)
Batch 525/537: Loss=0.9267 (C:0.9267, R:0.0018)

============================================================
Epoch 85/100 completed in 35.7s
Train: Loss=0.9247 (C:0.9247, R:0.0018) Ratio=4.31x
Val:   Loss=0.8584 (C:0.8584, R:0.0018) Ratio=7.50x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8584)
Checkpoint saved at epoch 85
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=0.9517 (C:0.9517, R:0.0018)
Batch  25/537: Loss=0.8777 (C:0.8777, R:0.0018)
Batch  50/537: Loss=0.9004 (C:0.9004, R:0.0018)
Batch  75/537: Loss=0.9117 (C:0.9117, R:0.0018)
Batch 100/537: Loss=0.9158 (C:0.9158, R:0.0018)
Batch 125/537: Loss=0.9202 (C:0.9202, R:0.0018)
Batch 150/537: Loss=0.9604 (C:0.9604, R:0.0018)
Batch 175/537: Loss=0.9117 (C:0.9117, R:0.0018)
Batch 200/537: Loss=0.9173 (C:0.9173, R:0.0018)
Batch 225/537: Loss=0.9124 (C:0.9124, R:0.0018)
Batch 250/537: Loss=0.9374 (C:0.9374, R:0.0018)
Batch 275/537: Loss=0.8736 (C:0.8736, R:0.0018)
Batch 300/537: Loss=0.9179 (C:0.9179, R:0.0018)
Batch 325/537: Loss=0.9306 (C:0.9306, R:0.0018)
Batch 350/537: Loss=0.9354 (C:0.9354, R:0.0018)
Batch 375/537: Loss=0.8977 (C:0.8977, R:0.0018)
Batch 400/537: Loss=0.8962 (C:0.8962, R:0.0018)
Batch 425/537: Loss=0.9208 (C:0.9208, R:0.0018)
Batch 450/537: Loss=0.9388 (C:0.9388, R:0.0018)
Batch 475/537: Loss=0.9476 (C:0.9476, R:0.0018)
Batch 500/537: Loss=0.9211 (C:0.9211, R:0.0017)
Batch 525/537: Loss=0.9186 (C:0.9186, R:0.0018)

============================================================
Epoch 86/100 completed in 30.0s
Train: Loss=0.9237 (C:0.9237, R:0.0018) Ratio=4.32x
Val:   Loss=0.8606 (C:0.8606, R:0.0018) Ratio=7.49x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=0.9172 (C:0.9172, R:0.0018)
Batch  25/537: Loss=0.9210 (C:0.9210, R:0.0018)
Batch  50/537: Loss=0.9231 (C:0.9231, R:0.0017)
Batch  75/537: Loss=0.9220 (C:0.9220, R:0.0018)
Batch 100/537: Loss=0.9106 (C:0.9106, R:0.0017)
Batch 125/537: Loss=0.9446 (C:0.9446, R:0.0018)
Batch 150/537: Loss=0.9312 (C:0.9312, R:0.0018)
Batch 175/537: Loss=0.9263 (C:0.9263, R:0.0018)
Batch 200/537: Loss=0.8967 (C:0.8967, R:0.0018)
Batch 225/537: Loss=0.9349 (C:0.9349, R:0.0018)
Batch 250/537: Loss=0.9220 (C:0.9220, R:0.0018)
Batch 275/537: Loss=0.9008 (C:0.9008, R:0.0018)
Batch 300/537: Loss=0.9502 (C:0.9502, R:0.0018)
Batch 325/537: Loss=0.9309 (C:0.9309, R:0.0018)
Batch 350/537: Loss=0.9548 (C:0.9548, R:0.0018)
Batch 375/537: Loss=0.9081 (C:0.9081, R:0.0018)
Batch 400/537: Loss=0.8986 (C:0.8986, R:0.0018)
Batch 425/537: Loss=0.9066 (C:0.9066, R:0.0018)
Batch 450/537: Loss=0.9066 (C:0.9066, R:0.0018)
Batch 475/537: Loss=0.9121 (C:0.9121, R:0.0018)
Batch 500/537: Loss=0.9280 (C:0.9280, R:0.0018)
Batch 525/537: Loss=0.9337 (C:0.9337, R:0.0018)

============================================================
Epoch 87/100 completed in 29.9s
Train: Loss=0.9227 (C:0.9227, R:0.0018) Ratio=4.31x
Val:   Loss=0.8590 (C:0.8590, R:0.0018) Ratio=7.64x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 88
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.200 ± 0.424
    Neg distances: 1.558 ± 1.002
    Separation ratio: 7.78x
    Gap: -2.664
    ✅ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=0.8943 (C:0.8943, R:0.0018)
Batch  25/537: Loss=0.8933 (C:0.8933, R:0.0018)
Batch  50/537: Loss=0.8907 (C:0.8907, R:0.0018)
Batch  75/537: Loss=0.8982 (C:0.8982, R:0.0017)
Batch 100/537: Loss=0.9084 (C:0.9084, R:0.0018)
Batch 125/537: Loss=0.9287 (C:0.9287, R:0.0017)
Batch 150/537: Loss=0.9042 (C:0.9042, R:0.0018)
Batch 175/537: Loss=0.9178 (C:0.9178, R:0.0018)
Batch 200/537: Loss=0.9155 (C:0.9155, R:0.0018)
Batch 225/537: Loss=0.8822 (C:0.8822, R:0.0018)
Batch 250/537: Loss=0.9142 (C:0.9142, R:0.0018)
Batch 275/537: Loss=0.9001 (C:0.9001, R:0.0018)
Batch 300/537: Loss=0.9076 (C:0.9076, R:0.0018)
Batch 325/537: Loss=0.9347 (C:0.9347, R:0.0018)
Batch 350/537: Loss=0.9192 (C:0.9192, R:0.0018)
Batch 375/537: Loss=0.8938 (C:0.8938, R:0.0018)
Batch 400/537: Loss=0.8879 (C:0.8879, R:0.0018)
Batch 425/537: Loss=0.8833 (C:0.8833, R:0.0018)
Batch 450/537: Loss=0.9019 (C:0.9019, R:0.0018)
Batch 475/537: Loss=0.8881 (C:0.8881, R:0.0018)
Batch 500/537: Loss=0.8973 (C:0.8973, R:0.0018)
Batch 525/537: Loss=0.9239 (C:0.9239, R:0.0017)

============================================================
Epoch 88/100 completed in 35.8s
Train: Loss=0.9025 (C:0.9025, R:0.0018) Ratio=4.27x
Val:   Loss=0.8405 (C:0.8405, R:0.0018) Ratio=7.41x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8405)
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=0.9195 (C:0.9195, R:0.0018)
Batch  25/537: Loss=0.9179 (C:0.9179, R:0.0018)
Batch  50/537: Loss=0.9065 (C:0.9065, R:0.0018)
Batch  75/537: Loss=0.9223 (C:0.9223, R:0.0018)
Batch 100/537: Loss=0.9056 (C:0.9056, R:0.0017)
Batch 125/537: Loss=0.8891 (C:0.8891, R:0.0017)
Batch 150/537: Loss=0.9357 (C:0.9357, R:0.0018)
Batch 175/537: Loss=0.9076 (C:0.9076, R:0.0018)
Batch 200/537: Loss=0.8771 (C:0.8771, R:0.0018)
Batch 225/537: Loss=0.9031 (C:0.9031, R:0.0018)
Batch 250/537: Loss=0.9201 (C:0.9201, R:0.0018)
Batch 275/537: Loss=0.8616 (C:0.8616, R:0.0018)
Batch 300/537: Loss=0.9273 (C:0.9273, R:0.0018)
Batch 325/537: Loss=0.9032 (C:0.9032, R:0.0018)
Batch 350/537: Loss=0.8704 (C:0.8704, R:0.0018)
Batch 375/537: Loss=0.8867 (C:0.8867, R:0.0018)
Batch 400/537: Loss=0.9020 (C:0.9020, R:0.0018)
Batch 425/537: Loss=0.9360 (C:0.9360, R:0.0018)
Batch 450/537: Loss=0.9105 (C:0.9105, R:0.0018)
Batch 475/537: Loss=0.9268 (C:0.9268, R:0.0018)
Batch 500/537: Loss=0.9399 (C:0.9399, R:0.0018)
Batch 525/537: Loss=0.8996 (C:0.8996, R:0.0018)

============================================================
Epoch 89/100 completed in 29.9s
Train: Loss=0.9033 (C:0.9033, R:0.0018) Ratio=4.31x
Val:   Loss=0.8382 (C:0.8382, R:0.0018) Ratio=7.69x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8382)
============================================================

Epoch 90 Training
----------------------------------------
Batch   0/537: Loss=0.8952 (C:0.8952, R:0.0018)
Batch  25/537: Loss=0.8820 (C:0.8820, R:0.0018)
Batch  50/537: Loss=0.9071 (C:0.9071, R:0.0018)
Batch  75/537: Loss=0.9235 (C:0.9235, R:0.0018)
Batch 100/537: Loss=0.8809 (C:0.8809, R:0.0018)
Batch 125/537: Loss=0.8945 (C:0.8945, R:0.0018)
Batch 150/537: Loss=0.9094 (C:0.9094, R:0.0018)
Batch 175/537: Loss=0.9178 (C:0.9178, R:0.0018)
Batch 200/537: Loss=0.8812 (C:0.8812, R:0.0018)
Batch 225/537: Loss=0.8964 (C:0.8964, R:0.0018)
Batch 250/537: Loss=0.9088 (C:0.9088, R:0.0017)
Batch 275/537: Loss=0.9057 (C:0.9057, R:0.0018)
Batch 300/537: Loss=0.9084 (C:0.9084, R:0.0017)
Batch 325/537: Loss=0.9120 (C:0.9120, R:0.0018)
Batch 350/537: Loss=0.8871 (C:0.8871, R:0.0018)
Batch 375/537: Loss=0.9116 (C:0.9116, R:0.0018)
Batch 400/537: Loss=0.9006 (C:0.9006, R:0.0018)
Batch 425/537: Loss=0.8830 (C:0.8830, R:0.0018)
Batch 450/537: Loss=0.9112 (C:0.9112, R:0.0018)
Batch 475/537: Loss=0.9195 (C:0.9195, R:0.0018)
Batch 500/537: Loss=0.9139 (C:0.9139, R:0.0018)
Batch 525/537: Loss=0.9174 (C:0.9174, R:0.0018)

============================================================
Epoch 90/100 completed in 30.2s
Train: Loss=0.9017 (C:0.9017, R:0.0018) Ratio=4.28x
Val:   Loss=0.8367 (C:0.8367, R:0.0018) Ratio=7.71x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8367)
Checkpoint saved at epoch 90
============================================================

🌍 Updating global dataset at epoch 91
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.209 ± 0.443
    Neg distances: 1.557 ± 0.995
    Separation ratio: 7.43x
    Gap: -2.749
    ✅ Excellent global separation!

Epoch 91 Training
----------------------------------------
Batch   0/537: Loss=0.9013 (C:0.9013, R:0.0018)
Batch  25/537: Loss=0.9076 (C:0.9076, R:0.0018)
Batch  50/537: Loss=0.9023 (C:0.9023, R:0.0018)
Batch  75/537: Loss=0.9143 (C:0.9143, R:0.0018)
Batch 100/537: Loss=0.8864 (C:0.8864, R:0.0018)
Batch 125/537: Loss=0.9139 (C:0.9139, R:0.0018)
Batch 150/537: Loss=0.9081 (C:0.9081, R:0.0018)
Batch 175/537: Loss=0.9173 (C:0.9173, R:0.0018)
Batch 200/537: Loss=0.9129 (C:0.9129, R:0.0018)
Batch 225/537: Loss=0.9050 (C:0.9050, R:0.0018)
Batch 250/537: Loss=0.9253 (C:0.9253, R:0.0018)
Batch 275/537: Loss=0.8807 (C:0.8807, R:0.0018)
Batch 300/537: Loss=0.9079 (C:0.9079, R:0.0018)
Batch 325/537: Loss=0.9127 (C:0.9127, R:0.0017)
Batch 350/537: Loss=0.9106 (C:0.9106, R:0.0018)
Batch 375/537: Loss=0.9115 (C:0.9115, R:0.0018)
Batch 400/537: Loss=0.9300 (C:0.9300, R:0.0018)
Batch 425/537: Loss=0.8963 (C:0.8963, R:0.0018)
Batch 450/537: Loss=0.8959 (C:0.8959, R:0.0018)
Batch 475/537: Loss=0.8862 (C:0.8862, R:0.0018)
Batch 500/537: Loss=0.9249 (C:0.9249, R:0.0018)
Batch 525/537: Loss=0.9232 (C:0.9232, R:0.0018)

============================================================
Epoch 91/100 completed in 35.7s
Train: Loss=0.9047 (C:0.9047, R:0.0018) Ratio=4.35x
Val:   Loss=0.8406 (C:0.8406, R:0.0018) Ratio=7.97x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 92 Training
----------------------------------------
Batch   0/537: Loss=0.8752 (C:0.8752, R:0.0017)
Batch  25/537: Loss=0.9079 (C:0.9079, R:0.0018)
Batch  50/537: Loss=0.8761 (C:0.8761, R:0.0017)
Batch  75/537: Loss=0.9110 (C:0.9110, R:0.0018)
Batch 100/537: Loss=0.8958 (C:0.8958, R:0.0018)
Batch 125/537: Loss=0.9046 (C:0.9046, R:0.0018)
Batch 150/537: Loss=0.8899 (C:0.8899, R:0.0018)
Batch 175/537: Loss=0.8745 (C:0.8745, R:0.0018)
Batch 200/537: Loss=0.8921 (C:0.8921, R:0.0018)
Batch 225/537: Loss=0.8680 (C:0.8680, R:0.0018)
Batch 250/537: Loss=0.9024 (C:0.9024, R:0.0018)
Batch 275/537: Loss=0.8980 (C:0.8980, R:0.0017)
Batch 300/537: Loss=0.8805 (C:0.8805, R:0.0018)
Batch 325/537: Loss=0.9062 (C:0.9062, R:0.0018)
Batch 350/537: Loss=0.9270 (C:0.9270, R:0.0018)
Batch 375/537: Loss=0.8893 (C:0.8893, R:0.0018)
Batch 400/537: Loss=0.8976 (C:0.8976, R:0.0018)
Batch 425/537: Loss=0.8799 (C:0.8799, R:0.0018)
Batch 450/537: Loss=0.8717 (C:0.8717, R:0.0018)
Batch 475/537: Loss=0.9037 (C:0.9037, R:0.0018)
Batch 500/537: Loss=0.9186 (C:0.9186, R:0.0018)
Batch 525/537: Loss=0.9221 (C:0.9221, R:0.0018)

============================================================
Epoch 92/100 completed in 29.7s
Train: Loss=0.9048 (C:0.9048, R:0.0018) Ratio=4.48x
Val:   Loss=0.8403 (C:0.8403, R:0.0018) Ratio=8.10x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 93 Training
----------------------------------------
Batch   0/537: Loss=0.9106 (C:0.9106, R:0.0018)
Batch  25/537: Loss=0.8858 (C:0.8858, R:0.0017)
Batch  50/537: Loss=0.9141 (C:0.9141, R:0.0018)
Batch  75/537: Loss=0.8971 (C:0.8971, R:0.0018)
Batch 100/537: Loss=0.9283 (C:0.9283, R:0.0018)
Batch 125/537: Loss=0.9202 (C:0.9202, R:0.0018)
Batch 150/537: Loss=0.8863 (C:0.8863, R:0.0018)
Batch 175/537: Loss=0.8997 (C:0.8997, R:0.0018)
Batch 200/537: Loss=0.9144 (C:0.9144, R:0.0018)
Batch 225/537: Loss=0.9028 (C:0.9028, R:0.0018)
Batch 250/537: Loss=0.9211 (C:0.9211, R:0.0017)
Batch 275/537: Loss=0.9089 (C:0.9089, R:0.0018)
Batch 300/537: Loss=0.9168 (C:0.9168, R:0.0018)
Batch 325/537: Loss=0.9038 (C:0.9038, R:0.0018)
Batch 350/537: Loss=0.9223 (C:0.9223, R:0.0018)
Batch 375/537: Loss=0.9228 (C:0.9228, R:0.0018)
Batch 400/537: Loss=0.9338 (C:0.9338, R:0.0018)
Batch 425/537: Loss=0.9049 (C:0.9049, R:0.0018)
Batch 450/537: Loss=0.8855 (C:0.8855, R:0.0018)
Batch 475/537: Loss=0.9203 (C:0.9203, R:0.0018)
Batch 500/537: Loss=0.8886 (C:0.8886, R:0.0018)
Batch 525/537: Loss=0.9217 (C:0.9217, R:0.0018)

============================================================
Epoch 93/100 completed in 29.8s
Train: Loss=0.9050 (C:0.9050, R:0.0018) Ratio=4.43x
Val:   Loss=0.8395 (C:0.8395, R:0.0018) Ratio=7.94x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 94
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.216 ± 0.457
    Neg distances: 1.544 ± 1.005
    Separation ratio: 7.16x
    Gap: -2.648
    ✅ Excellent global separation!

Epoch 94 Training
----------------------------------------
Batch   0/537: Loss=0.9031 (C:0.9031, R:0.0018)
Batch  25/537: Loss=0.9166 (C:0.9166, R:0.0018)
Batch  50/537: Loss=0.9151 (C:0.9151, R:0.0018)
Batch  75/537: Loss=0.9055 (C:0.9055, R:0.0018)
Batch 100/537: Loss=0.9003 (C:0.9003, R:0.0018)
Batch 125/537: Loss=0.9163 (C:0.9163, R:0.0018)
Batch 150/537: Loss=0.8937 (C:0.8937, R:0.0018)
Batch 175/537: Loss=0.9076 (C:0.9076, R:0.0017)
Batch 200/537: Loss=0.9258 (C:0.9258, R:0.0018)
Batch 225/537: Loss=0.9017 (C:0.9017, R:0.0018)
Batch 250/537: Loss=0.9234 (C:0.9234, R:0.0018)
Batch 275/537: Loss=0.9146 (C:0.9146, R:0.0018)
Batch 300/537: Loss=0.9156 (C:0.9156, R:0.0018)
Batch 325/537: Loss=0.9055 (C:0.9055, R:0.0018)
Batch 350/537: Loss=0.9197 (C:0.9197, R:0.0018)
Batch 375/537: Loss=0.9024 (C:0.9024, R:0.0017)
Batch 400/537: Loss=0.9118 (C:0.9118, R:0.0018)
Batch 425/537: Loss=0.9167 (C:0.9167, R:0.0018)
Batch 450/537: Loss=0.9197 (C:0.9197, R:0.0017)
Batch 475/537: Loss=0.9185 (C:0.9185, R:0.0018)
Batch 500/537: Loss=0.9161 (C:0.9161, R:0.0018)
Batch 525/537: Loss=0.9175 (C:0.9175, R:0.0018)

============================================================
Epoch 94/100 completed in 37.0s
Train: Loss=0.9129 (C:0.9129, R:0.0018) Ratio=4.43x
Val:   Loss=0.8485 (C:0.8485, R:0.0018) Ratio=7.95x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 95 Training
----------------------------------------
Batch   0/537: Loss=0.9181 (C:0.9181, R:0.0018)
Batch  25/537: Loss=0.9204 (C:0.9204, R:0.0018)
Batch  50/537: Loss=0.9139 (C:0.9139, R:0.0018)
Batch  75/537: Loss=0.8921 (C:0.8921, R:0.0018)
Batch 100/537: Loss=0.9209 (C:0.9209, R:0.0017)
Batch 125/537: Loss=0.9073 (C:0.9073, R:0.0018)
Batch 150/537: Loss=0.9129 (C:0.9129, R:0.0017)
Batch 175/537: Loss=0.8823 (C:0.8823, R:0.0018)
Batch 200/537: Loss=0.9208 (C:0.9208, R:0.0018)
Batch 225/537: Loss=0.9416 (C:0.9416, R:0.0018)
Batch 250/537: Loss=0.8787 (C:0.8787, R:0.0018)
Batch 275/537: Loss=0.8941 (C:0.8941, R:0.0018)
Batch 300/537: Loss=0.8859 (C:0.8859, R:0.0017)
Batch 325/537: Loss=0.9486 (C:0.9486, R:0.0018)
Batch 350/537: Loss=0.9060 (C:0.9060, R:0.0018)
Batch 375/537: Loss=0.9332 (C:0.9332, R:0.0018)
Batch 400/537: Loss=0.9295 (C:0.9295, R:0.0018)
Batch 425/537: Loss=0.9149 (C:0.9149, R:0.0018)
Batch 450/537: Loss=0.9130 (C:0.9130, R:0.0018)
Batch 475/537: Loss=0.9358 (C:0.9358, R:0.0018)
Batch 500/537: Loss=0.9050 (C:0.9050, R:0.0017)
Batch 525/537: Loss=0.8948 (C:0.8948, R:0.0017)

============================================================
Epoch 95/100 completed in 32.6s
Train: Loss=0.9124 (C:0.9124, R:0.0018) Ratio=4.48x
Val:   Loss=0.8484 (C:0.8484, R:0.0018) Ratio=8.28x
Reconstruction weight: 0.300
No improvement for 5 epochs
Checkpoint saved at epoch 95
============================================================

Epoch 96 Training
----------------------------------------
Batch   0/537: Loss=0.9036 (C:0.9036, R:0.0018)
Batch  25/537: Loss=0.9155 (C:0.9155, R:0.0018)
Batch  50/537: Loss=0.8909 (C:0.8909, R:0.0018)
Batch  75/537: Loss=0.9012 (C:0.9012, R:0.0017)
Batch 100/537: Loss=0.9190 (C:0.9190, R:0.0018)
Batch 125/537: Loss=0.9308 (C:0.9308, R:0.0018)
Batch 150/537: Loss=0.8982 (C:0.8982, R:0.0018)
Batch 175/537: Loss=0.9110 (C:0.9110, R:0.0018)
Batch 200/537: Loss=0.9086 (C:0.9086, R:0.0018)
Batch 225/537: Loss=0.8895 (C:0.8895, R:0.0017)
Batch 250/537: Loss=0.9460 (C:0.9460, R:0.0018)
Batch 275/537: Loss=0.8938 (C:0.8938, R:0.0018)
Batch 300/537: Loss=0.9550 (C:0.9550, R:0.0018)
Batch 325/537: Loss=0.8839 (C:0.8839, R:0.0017)
Batch 350/537: Loss=0.9146 (C:0.9146, R:0.0018)
Batch 375/537: Loss=0.9188 (C:0.9188, R:0.0018)
Batch 400/537: Loss=0.9080 (C:0.9080, R:0.0018)
Batch 425/537: Loss=0.8862 (C:0.8862, R:0.0018)
Batch 450/537: Loss=0.9217 (C:0.9217, R:0.0018)
Batch 475/537: Loss=0.9224 (C:0.9224, R:0.0018)
Batch 500/537: Loss=0.8969 (C:0.8969, R:0.0018)
Batch 525/537: Loss=0.9083 (C:0.9083, R:0.0018)

============================================================
Epoch 96/100 completed in 33.3s
Train: Loss=0.9108 (C:0.9108, R:0.0018) Ratio=4.49x
Val:   Loss=0.8475 (C:0.8475, R:0.0018) Ratio=8.23x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 97
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.186 ± 0.397
    Neg distances: 1.562 ± 1.006
    Separation ratio: 8.38x
    Gap: -2.685
    ✅ Excellent global separation!

Epoch 97 Training
----------------------------------------
Batch   0/537: Loss=0.9010 (C:0.9010, R:0.0018)
Batch  25/537: Loss=0.8929 (C:0.8929, R:0.0018)
Batch  50/537: Loss=0.8966 (C:0.8966, R:0.0018)
Batch  75/537: Loss=0.9109 (C:0.9109, R:0.0018)
Batch 100/537: Loss=0.8862 (C:0.8862, R:0.0018)
Batch 125/537: Loss=0.8537 (C:0.8537, R:0.0018)
Batch 150/537: Loss=0.8857 (C:0.8857, R:0.0018)
Batch 175/537: Loss=0.9305 (C:0.9305, R:0.0018)
Batch 200/537: Loss=0.8803 (C:0.8803, R:0.0018)
Batch 225/537: Loss=0.8844 (C:0.8844, R:0.0017)
Batch 250/537: Loss=0.8993 (C:0.8993, R:0.0018)
Batch 275/537: Loss=0.8757 (C:0.8757, R:0.0018)
Batch 300/537: Loss=0.9012 (C:0.9012, R:0.0018)
Batch 325/537: Loss=0.8853 (C:0.8853, R:0.0017)
Batch 350/537: Loss=0.9018 (C:0.9018, R:0.0018)
Batch 375/537: Loss=0.9218 (C:0.9218, R:0.0017)
Batch 400/537: Loss=0.8772 (C:0.8772, R:0.0018)
Batch 425/537: Loss=0.9177 (C:0.9177, R:0.0018)
Batch 450/537: Loss=0.8746 (C:0.8746, R:0.0018)
Batch 475/537: Loss=0.9009 (C:0.9009, R:0.0018)
Batch 500/537: Loss=0.8746 (C:0.8746, R:0.0018)
Batch 525/537: Loss=0.9091 (C:0.9091, R:0.0017)

============================================================
Epoch 97/100 completed in 38.2s
Train: Loss=0.8891 (C:0.8891, R:0.0018) Ratio=4.45x
Val:   Loss=0.8237 (C:0.8237, R:0.0018) Ratio=8.55x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8237)
============================================================

Epoch 98 Training
----------------------------------------
Batch   0/537: Loss=0.9161 (C:0.9161, R:0.0018)
Batch  25/537: Loss=0.8713 (C:0.8713, R:0.0018)
Batch  50/537: Loss=0.8660 (C:0.8660, R:0.0018)
Batch  75/537: Loss=0.8819 (C:0.8819, R:0.0018)
Batch 100/537: Loss=0.8592 (C:0.8592, R:0.0018)
Batch 125/537: Loss=0.9102 (C:0.9102, R:0.0018)
Batch 150/537: Loss=0.8777 (C:0.8777, R:0.0018)
Batch 175/537: Loss=0.9140 (C:0.9140, R:0.0018)
Batch 200/537: Loss=0.8856 (C:0.8856, R:0.0018)
Batch 225/537: Loss=0.9041 (C:0.9041, R:0.0018)
Batch 250/537: Loss=0.9072 (C:0.9072, R:0.0018)
Batch 275/537: Loss=0.8856 (C:0.8856, R:0.0018)
Batch 300/537: Loss=0.8838 (C:0.8838, R:0.0018)
Batch 325/537: Loss=0.8863 (C:0.8863, R:0.0017)
Batch 350/537: Loss=0.8857 (C:0.8857, R:0.0018)
Batch 375/537: Loss=0.8829 (C:0.8829, R:0.0017)
Batch 400/537: Loss=0.8822 (C:0.8822, R:0.0017)
Batch 425/537: Loss=0.9111 (C:0.9111, R:0.0018)
Batch 450/537: Loss=0.8728 (C:0.8728, R:0.0018)
Batch 475/537: Loss=0.9152 (C:0.9152, R:0.0017)
Batch 500/537: Loss=0.8660 (C:0.8660, R:0.0018)
Batch 525/537: Loss=0.9215 (C:0.9215, R:0.0018)

============================================================
Epoch 98/100 completed in 30.9s
Train: Loss=0.8875 (C:0.8875, R:0.0018) Ratio=4.39x
Val:   Loss=0.8229 (C:0.8229, R:0.0018) Ratio=8.62x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8229)
============================================================

Epoch 99 Training
----------------------------------------
Batch   0/537: Loss=0.8680 (C:0.8680, R:0.0018)
Batch  25/537: Loss=0.8853 (C:0.8853, R:0.0018)
Batch  50/537: Loss=0.9008 (C:0.9008, R:0.0017)
Batch  75/537: Loss=0.8964 (C:0.8964, R:0.0018)
Batch 100/537: Loss=0.8958 (C:0.8958, R:0.0018)
Batch 125/537: Loss=0.8857 (C:0.8857, R:0.0018)
Batch 150/537: Loss=0.8996 (C:0.8996, R:0.0018)
Batch 175/537: Loss=0.8723 (C:0.8723, R:0.0018)
Batch 200/537: Loss=0.9091 (C:0.9091, R:0.0018)
Batch 225/537: Loss=0.9144 (C:0.9144, R:0.0018)
Batch 250/537: Loss=0.8905 (C:0.8905, R:0.0018)
Batch 275/537: Loss=0.8780 (C:0.8780, R:0.0018)
Batch 300/537: Loss=0.8830 (C:0.8830, R:0.0018)
Batch 325/537: Loss=0.8742 (C:0.8742, R:0.0017)
Batch 350/537: Loss=0.9048 (C:0.9048, R:0.0018)
Batch 375/537: Loss=0.8698 (C:0.8698, R:0.0018)
Batch 400/537: Loss=0.8688 (C:0.8688, R:0.0018)
Batch 425/537: Loss=0.8533 (C:0.8533, R:0.0018)
Batch 450/537: Loss=0.8753 (C:0.8753, R:0.0018)
Batch 475/537: Loss=0.8994 (C:0.8994, R:0.0017)
Batch 500/537: Loss=0.9045 (C:0.9045, R:0.0018)
Batch 525/537: Loss=0.8557 (C:0.8557, R:0.0018)

============================================================
Epoch 99/100 completed in 29.5s
Train: Loss=0.8874 (C:0.8874, R:0.0018) Ratio=4.51x
Val:   Loss=0.8232 (C:0.8232, R:0.0018) Ratio=8.47x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 100
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.189 ± 0.418
    Neg distances: 1.519 ± 1.030
    Separation ratio: 8.03x
    Gap: -2.795
    ✅ Excellent global separation!

Epoch 100 Training
----------------------------------------
Batch   0/537: Loss=0.9076 (C:0.9076, R:0.0018)
Batch  25/537: Loss=0.9123 (C:0.9123, R:0.0018)
Batch  50/537: Loss=0.9281 (C:0.9281, R:0.0018)
Batch  75/537: Loss=0.8937 (C:0.8937, R:0.0018)
Batch 100/537: Loss=0.9548 (C:0.9548, R:0.0017)
Batch 125/537: Loss=0.9140 (C:0.9140, R:0.0018)
Batch 150/537: Loss=0.9170 (C:0.9170, R:0.0018)
Batch 175/537: Loss=0.9149 (C:0.9149, R:0.0018)
Batch 200/537: Loss=0.9397 (C:0.9397, R:0.0017)
Batch 225/537: Loss=0.9204 (C:0.9204, R:0.0018)
Batch 250/537: Loss=0.9094 (C:0.9094, R:0.0018)
Batch 275/537: Loss=0.9311 (C:0.9311, R:0.0018)
Batch 300/537: Loss=0.9020 (C:0.9020, R:0.0018)
Batch 325/537: Loss=0.9424 (C:0.9424, R:0.0018)
Batch 350/537: Loss=0.9161 (C:0.9161, R:0.0018)
Batch 375/537: Loss=0.9407 (C:0.9407, R:0.0017)
Batch 400/537: Loss=0.8877 (C:0.8877, R:0.0018)
Batch 425/537: Loss=0.8640 (C:0.8640, R:0.0018)
Batch 450/537: Loss=0.8940 (C:0.8940, R:0.0018)
Batch 475/537: Loss=0.8958 (C:0.8958, R:0.0018)
Batch 500/537: Loss=0.9076 (C:0.9076, R:0.0018)
Batch 525/537: Loss=0.9342 (C:0.9342, R:0.0018)

============================================================
Epoch 100/100 completed in 34.5s
Train: Loss=0.9123 (C:0.9123, R:0.0018) Ratio=4.40x
Val:   Loss=0.8495 (C:0.8495, R:0.0018) Ratio=8.43x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 100
============================================================

Global Dataset Training Completed!
Best epoch: 98
Best validation loss: 0.8229
Final separation ratios: Train=4.40x, Val=8.43x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_162355/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/539 batches
  Processed 51/539 batches
  Processed 101/539 batches
  Processed 151/539 batches
  Processed 201/539 batches
  Processed 251/539 batches
  Processed 301/539 batches
  Processed 351/539 batches
  Processed 401/539 batches
  Processed 451/539 batches
  Processed 501/539 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: 0.4035
  Adjusted Rand Score: 0.5238
  Clustering Accuracy: 0.6979
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.7850
  Per-class F1: [0.7087880528431937, 0.6675187729669275, 0.9718164379518962]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.001759
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 0.192 ± 0.426
  Negative distances: 1.557 ± 1.025
  Separation ratio: 8.11x
  Gap: -2.758
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4035
  Clustering Accuracy: 0.6979
  Adjusted Rand Score: 0.5238

Classification Performance:
  Accuracy: 0.7850

Separation Quality:
  Separation Ratio: 8.11x
  Gap: -2.758
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.001759
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_162355/results/evaluation_results_20250712_171756.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_162355/results/evaluation_results_20250712_171756.json
Saving final experiment results...

PIPELINE FAILED: Object of type float32 is not JSON serializable

Analysis completed with exit code: 0
Time: Sat 12 Jul 17:17:58 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
