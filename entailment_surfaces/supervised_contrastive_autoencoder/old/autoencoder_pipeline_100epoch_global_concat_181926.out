Starting Surface Distance Metric Analysis job...
Job ID: 181926
Node: gpuvm17
Time: Sat 12 Jul 16:24:05 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 12 16:24:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   31C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-12 16:24:20.318534
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 100
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_162420
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_162420/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 537
  Test batches: 539
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 549367 samples, 537 batches
  Test: 549367 samples, 539 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,876,555
Model created with 1,876,555 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,876,555
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.088 ± 0.010
    Neg distances: 0.089 ± 0.010
    Separation ratio: 1.00x
    Gap: -0.113
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9999 (C:1.9999, R:0.0117)
Batch  25/537: Loss=1.9958 (C:1.9958, R:0.0115)
Batch  50/537: Loss=1.9782 (C:1.9782, R:0.0114)
Batch  75/537: Loss=1.9735 (C:1.9735, R:0.0112)
Batch 100/537: Loss=1.9656 (C:1.9656, R:0.0111)
Batch 125/537: Loss=1.9624 (C:1.9624, R:0.0109)
Batch 150/537: Loss=1.9498 (C:1.9498, R:0.0109)
Batch 175/537: Loss=1.9441 (C:1.9441, R:0.0108)
Batch 200/537: Loss=1.9406 (C:1.9406, R:0.0107)
Batch 225/537: Loss=1.9307 (C:1.9307, R:0.0107)
Batch 250/537: Loss=1.9230 (C:1.9230, R:0.0107)
Batch 275/537: Loss=1.9174 (C:1.9174, R:0.0106)
Batch 300/537: Loss=1.9155 (C:1.9155, R:0.0106)
Batch 325/537: Loss=1.9183 (C:1.9183, R:0.0106)
Batch 350/537: Loss=1.9233 (C:1.9233, R:0.0106)
Batch 375/537: Loss=1.9137 (C:1.9137, R:0.0106)
Batch 400/537: Loss=1.9192 (C:1.9192, R:0.0105)
Batch 425/537: Loss=1.9088 (C:1.9088, R:0.0105)
Batch 450/537: Loss=1.9142 (C:1.9142, R:0.0105)
Batch 475/537: Loss=1.9037 (C:1.9037, R:0.0105)
Batch 500/537: Loss=1.9084 (C:1.9084, R:0.0106)
Batch 525/537: Loss=1.9113 (C:1.9113, R:0.0105)

============================================================
Epoch 1/100 completed in 38.5s
Train: Loss=1.9350 (C:1.9350, R:0.0108) Ratio=1.62x
Val:   Loss=1.8937 (C:1.8937, R:0.0105) Ratio=2.19x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8937)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.9046 (C:1.9046, R:0.0105)
Batch  25/537: Loss=1.8927 (C:1.8927, R:0.0105)
Batch  50/537: Loss=1.9005 (C:1.9005, R:0.0105)
Batch  75/537: Loss=1.8958 (C:1.8958, R:0.0106)
Batch 100/537: Loss=1.9062 (C:1.9062, R:0.0105)
Batch 125/537: Loss=1.8917 (C:1.8917, R:0.0105)
Batch 150/537: Loss=1.9020 (C:1.9020, R:0.0105)
Batch 175/537: Loss=1.8826 (C:1.8826, R:0.0105)
Batch 200/537: Loss=1.9010 (C:1.9010, R:0.0105)
Batch 225/537: Loss=1.9011 (C:1.9011, R:0.0105)
Batch 250/537: Loss=1.8864 (C:1.8864, R:0.0106)
Batch 275/537: Loss=1.8793 (C:1.8793, R:0.0105)
Batch 300/537: Loss=1.8929 (C:1.8929, R:0.0105)
Batch 325/537: Loss=1.9003 (C:1.9003, R:0.0105)
Batch 350/537: Loss=1.8941 (C:1.8941, R:0.0105)
Batch 375/537: Loss=1.9020 (C:1.9020, R:0.0105)
Batch 400/537: Loss=1.8942 (C:1.8942, R:0.0105)
Batch 425/537: Loss=1.8994 (C:1.8994, R:0.0105)
Batch 450/537: Loss=1.8971 (C:1.8971, R:0.0105)
Batch 475/537: Loss=1.8919 (C:1.8919, R:0.0105)
Batch 500/537: Loss=1.8841 (C:1.8841, R:0.0105)
Batch 525/537: Loss=1.8931 (C:1.8931, R:0.0105)

============================================================
Epoch 2/100 completed in 31.7s
Train: Loss=1.8949 (C:1.8949, R:0.0105) Ratio=2.16x
Val:   Loss=1.8783 (C:1.8783, R:0.0105) Ratio=2.47x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8783)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8879 (C:1.8879, R:0.0105)
Batch  25/537: Loss=1.8908 (C:1.8908, R:0.0105)
Batch  50/537: Loss=1.8746 (C:1.8746, R:0.0105)
Batch  75/537: Loss=1.8733 (C:1.8733, R:0.0105)
Batch 100/537: Loss=1.8893 (C:1.8893, R:0.0105)
Batch 125/537: Loss=1.8752 (C:1.8752, R:0.0105)
Batch 150/537: Loss=1.8845 (C:1.8845, R:0.0105)
Batch 175/537: Loss=1.8859 (C:1.8859, R:0.0105)
Batch 200/537: Loss=1.8861 (C:1.8861, R:0.0105)
Batch 225/537: Loss=1.8890 (C:1.8890, R:0.0105)
Batch 250/537: Loss=1.8807 (C:1.8807, R:0.0105)
Batch 275/537: Loss=1.8881 (C:1.8881, R:0.0105)
Batch 300/537: Loss=1.8775 (C:1.8775, R:0.0105)
Batch 325/537: Loss=1.8814 (C:1.8814, R:0.0105)
Batch 350/537: Loss=1.8856 (C:1.8856, R:0.0105)
Batch 375/537: Loss=1.8782 (C:1.8782, R:0.0106)
Batch 400/537: Loss=1.8731 (C:1.8731, R:0.0105)
Batch 425/537: Loss=1.8854 (C:1.8854, R:0.0105)
Batch 450/537: Loss=1.8816 (C:1.8816, R:0.0105)
Batch 475/537: Loss=1.8761 (C:1.8761, R:0.0105)
Batch 500/537: Loss=1.8691 (C:1.8691, R:0.0105)
Batch 525/537: Loss=1.8819 (C:1.8819, R:0.0105)

============================================================
Epoch 3/100 completed in 31.4s
Train: Loss=1.8833 (C:1.8833, R:0.0105) Ratio=2.38x
Val:   Loss=1.8680 (C:1.8680, R:0.0105) Ratio=2.68x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8680)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.573 ± 0.553
    Neg distances: 1.484 ± 0.818
    Separation ratio: 2.59x
    Gap: -3.342
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.2415 (C:1.2415, R:0.0105)
Batch  25/537: Loss=1.2243 (C:1.2243, R:0.0105)
Batch  50/537: Loss=1.2471 (C:1.2471, R:0.0105)
Batch  75/537: Loss=1.2494 (C:1.2494, R:0.0105)
Batch 100/537: Loss=1.2665 (C:1.2665, R:0.0105)
Batch 125/537: Loss=1.2697 (C:1.2697, R:0.0105)
Batch 150/537: Loss=1.2680 (C:1.2680, R:0.0105)
Batch 175/537: Loss=1.2417 (C:1.2417, R:0.0105)
Batch 200/537: Loss=1.2186 (C:1.2186, R:0.0105)
Batch 225/537: Loss=1.2232 (C:1.2232, R:0.0106)
Batch 250/537: Loss=1.2249 (C:1.2249, R:0.0105)
Batch 275/537: Loss=1.1848 (C:1.1848, R:0.0105)
Batch 300/537: Loss=1.2395 (C:1.2395, R:0.0105)
Batch 325/537: Loss=1.2546 (C:1.2546, R:0.0105)
Batch 350/537: Loss=1.2506 (C:1.2506, R:0.0105)
Batch 375/537: Loss=1.2630 (C:1.2630, R:0.0105)
Batch 400/537: Loss=1.2665 (C:1.2665, R:0.0105)
Batch 425/537: Loss=1.2571 (C:1.2571, R:0.0105)
Batch 450/537: Loss=1.2319 (C:1.2319, R:0.0105)
Batch 475/537: Loss=1.2265 (C:1.2265, R:0.0105)
Batch 500/537: Loss=1.2442 (C:1.2442, R:0.0105)
Batch 525/537: Loss=1.2094 (C:1.2094, R:0.0105)

============================================================
Epoch 4/100 completed in 37.1s
Train: Loss=1.2425 (C:1.2425, R:0.0105) Ratio=2.50x
Val:   Loss=1.2008 (C:1.2008, R:0.0105) Ratio=2.89x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2008)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.2223 (C:1.2223, R:0.0105)
Batch  25/537: Loss=1.2494 (C:1.2494, R:0.0105)
Batch  50/537: Loss=1.2374 (C:1.2374, R:0.0105)
Batch  75/537: Loss=1.2189 (C:1.2189, R:0.0105)
Batch 100/537: Loss=1.2263 (C:1.2263, R:0.0105)
Batch 125/537: Loss=1.2338 (C:1.2338, R:0.0105)
Batch 150/537: Loss=1.2335 (C:1.2335, R:0.0105)
Batch 175/537: Loss=1.2515 (C:1.2515, R:0.0105)
Batch 200/537: Loss=1.2387 (C:1.2387, R:0.0106)
Batch 225/537: Loss=1.1958 (C:1.1958, R:0.0105)
Batch 250/537: Loss=1.2165 (C:1.2165, R:0.0105)
Batch 275/537: Loss=1.2149 (C:1.2149, R:0.0105)
Batch 300/537: Loss=1.1980 (C:1.1980, R:0.0105)
Batch 325/537: Loss=1.2183 (C:1.2183, R:0.0105)
Batch 350/537: Loss=1.2208 (C:1.2208, R:0.0105)
Batch 375/537: Loss=1.2177 (C:1.2177, R:0.0105)
Batch 400/537: Loss=1.2270 (C:1.2270, R:0.0105)
Batch 425/537: Loss=1.2219 (C:1.2219, R:0.0105)
Batch 450/537: Loss=1.2388 (C:1.2388, R:0.0105)
Batch 475/537: Loss=1.2614 (C:1.2614, R:0.0105)
Batch 500/537: Loss=1.2198 (C:1.2198, R:0.0105)
Batch 525/537: Loss=1.2279 (C:1.2279, R:0.0105)

============================================================
Epoch 5/100 completed in 30.7s
Train: Loss=1.2211 (C:1.2211, R:0.0105) Ratio=2.63x
Val:   Loss=1.1852 (C:1.1852, R:0.0105) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1852)
Checkpoint saved at epoch 5
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.2434 (C:1.2434, R:0.0105)
Batch  25/537: Loss=1.2007 (C:1.2007, R:0.0105)
Batch  50/537: Loss=1.2258 (C:1.2258, R:0.0105)
Batch  75/537: Loss=1.2270 (C:1.2270, R:0.0105)
Batch 100/537: Loss=1.1963 (C:1.1963, R:0.0105)
Batch 125/537: Loss=1.2090 (C:1.2090, R:0.0105)
Batch 150/537: Loss=1.1850 (C:1.1850, R:0.0105)
Batch 175/537: Loss=1.2076 (C:1.2076, R:0.0106)
Batch 200/537: Loss=1.2167 (C:1.2167, R:0.0105)
Batch 225/537: Loss=1.2101 (C:1.2101, R:0.0105)
Batch 250/537: Loss=1.1882 (C:1.1882, R:0.0105)
Batch 275/537: Loss=1.2162 (C:1.2162, R:0.0105)
Batch 300/537: Loss=1.1980 (C:1.1980, R:0.0105)
Batch 325/537: Loss=1.1859 (C:1.1859, R:0.0105)
Batch 350/537: Loss=1.2114 (C:1.2114, R:0.0105)
Batch 375/537: Loss=1.1876 (C:1.1876, R:0.0105)
Batch 400/537: Loss=1.2290 (C:1.2290, R:0.0105)
Batch 425/537: Loss=1.2132 (C:1.2132, R:0.0105)
Batch 450/537: Loss=1.2133 (C:1.2133, R:0.0105)
Batch 475/537: Loss=1.2162 (C:1.2162, R:0.0105)
Batch 500/537: Loss=1.1775 (C:1.1775, R:0.0105)
Batch 525/537: Loss=1.2154 (C:1.2154, R:0.0105)

============================================================
Epoch 6/100 completed in 31.3s
Train: Loss=1.2076 (C:1.2076, R:0.0105) Ratio=2.79x
Val:   Loss=1.1731 (C:1.1731, R:0.0105) Ratio=3.24x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1731)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.489 ± 0.551
    Neg distances: 1.613 ± 0.834
    Separation ratio: 3.30x
    Gap: -3.170
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.0974 (C:1.0974, R:0.0105)
Batch  25/537: Loss=1.0646 (C:1.0646, R:0.0105)
Batch  50/537: Loss=1.0867 (C:1.0867, R:0.0104)
Batch  75/537: Loss=1.0849 (C:1.0849, R:0.0105)
Batch 100/537: Loss=1.0845 (C:1.0845, R:0.0105)
Batch 125/537: Loss=1.0943 (C:1.0943, R:0.0105)
Batch 150/537: Loss=1.1250 (C:1.1250, R:0.0105)
Batch 175/537: Loss=1.0883 (C:1.0883, R:0.0105)
Batch 200/537: Loss=1.0931 (C:1.0931, R:0.0105)
Batch 225/537: Loss=1.1163 (C:1.1163, R:0.0105)
Batch 250/537: Loss=1.0933 (C:1.0933, R:0.0105)
Batch 275/537: Loss=1.1315 (C:1.1315, R:0.0105)
Batch 300/537: Loss=1.1134 (C:1.1134, R:0.0105)
Batch 325/537: Loss=1.1024 (C:1.1024, R:0.0105)
Batch 350/537: Loss=1.1137 (C:1.1137, R:0.0105)
Batch 375/537: Loss=1.1292 (C:1.1292, R:0.0105)
Batch 400/537: Loss=1.1290 (C:1.1290, R:0.0105)
Batch 425/537: Loss=1.1274 (C:1.1274, R:0.0105)
Batch 450/537: Loss=1.0938 (C:1.0938, R:0.0105)
Batch 475/537: Loss=1.1126 (C:1.1126, R:0.0105)
Batch 500/537: Loss=1.1070 (C:1.1070, R:0.0105)
Batch 525/537: Loss=1.1179 (C:1.1179, R:0.0106)

============================================================
Epoch 7/100 completed in 38.4s
Train: Loss=1.1026 (C:1.1026, R:0.0105) Ratio=2.90x
Val:   Loss=1.0618 (C:1.0618, R:0.0105) Ratio=3.39x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0618)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.0977 (C:1.0977, R:0.0106)
Batch  25/537: Loss=1.1163 (C:1.1163, R:0.0105)
Batch  50/537: Loss=1.0611 (C:1.0611, R:0.0105)
Batch  75/537: Loss=1.0853 (C:1.0853, R:0.0105)
Batch 100/537: Loss=1.0975 (C:1.0975, R:0.0105)
Batch 125/537: Loss=1.0999 (C:1.0999, R:0.0105)
Batch 150/537: Loss=1.0868 (C:1.0868, R:0.0105)
Batch 175/537: Loss=1.1002 (C:1.1002, R:0.0105)
Batch 200/537: Loss=1.0951 (C:1.0951, R:0.0105)
Batch 225/537: Loss=1.1166 (C:1.1166, R:0.0105)
Batch 250/537: Loss=1.0677 (C:1.0677, R:0.0105)
Batch 275/537: Loss=1.1103 (C:1.1103, R:0.0105)
Batch 300/537: Loss=1.1009 (C:1.1009, R:0.0105)
Batch 325/537: Loss=1.0963 (C:1.0963, R:0.0105)
Batch 350/537: Loss=1.0947 (C:1.0947, R:0.0105)
Batch 375/537: Loss=1.1023 (C:1.1023, R:0.0105)
Batch 400/537: Loss=1.1114 (C:1.1114, R:0.0105)
Batch 425/537: Loss=1.0964 (C:1.0964, R:0.0106)
Batch 450/537: Loss=1.0930 (C:1.0930, R:0.0105)
Batch 475/537: Loss=1.0573 (C:1.0573, R:0.0105)
Batch 500/537: Loss=1.1001 (C:1.1001, R:0.0106)
Batch 525/537: Loss=1.1101 (C:1.1101, R:0.0105)

============================================================
Epoch 8/100 completed in 33.1s
Train: Loss=1.0930 (C:1.0930, R:0.0105) Ratio=3.00x
Val:   Loss=1.0524 (C:1.0524, R:0.0105) Ratio=3.54x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0524)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.1084 (C:1.1084, R:0.0105)
Batch  25/537: Loss=1.0799 (C:1.0799, R:0.0105)
Batch  50/537: Loss=1.0920 (C:1.0920, R:0.0105)
Batch  75/537: Loss=1.0731 (C:1.0731, R:0.0105)
Batch 100/537: Loss=1.0555 (C:1.0555, R:0.0105)
Batch 125/537: Loss=1.1047 (C:1.1047, R:0.0105)
Batch 150/537: Loss=1.0987 (C:1.0987, R:0.0105)
Batch 175/537: Loss=1.0663 (C:1.0663, R:0.0105)
Batch 200/537: Loss=1.0813 (C:1.0813, R:0.0105)
Batch 225/537: Loss=1.0434 (C:1.0434, R:0.0105)
Batch 250/537: Loss=1.1065 (C:1.1065, R:0.0105)
Batch 275/537: Loss=1.0878 (C:1.0878, R:0.0105)
Batch 300/537: Loss=1.0437 (C:1.0437, R:0.0105)
Batch 325/537: Loss=1.0993 (C:1.0993, R:0.0105)
Batch 350/537: Loss=1.0838 (C:1.0838, R:0.0105)
Batch 375/537: Loss=1.1120 (C:1.1120, R:0.0105)
Batch 400/537: Loss=1.0685 (C:1.0685, R:0.0105)
Batch 425/537: Loss=1.0598 (C:1.0598, R:0.0105)
Batch 450/537: Loss=1.0958 (C:1.0958, R:0.0105)
Batch 475/537: Loss=1.0712 (C:1.0712, R:0.0105)
Batch 500/537: Loss=1.0835 (C:1.0835, R:0.0105)
Batch 525/537: Loss=1.1092 (C:1.1092, R:0.0105)

============================================================
Epoch 9/100 completed in 32.0s
Train: Loss=1.0863 (C:1.0863, R:0.0105) Ratio=3.10x
Val:   Loss=1.0458 (C:1.0458, R:0.0105) Ratio=3.66x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0458)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.483 ± 0.547
    Neg distances: 1.669 ± 0.843
    Separation ratio: 3.46x
    Gap: -3.268
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.0272 (C:1.0272, R:0.0106)
Batch  25/537: Loss=1.0466 (C:1.0466, R:0.0105)
Batch  50/537: Loss=1.0652 (C:1.0652, R:0.0105)
Batch  75/537: Loss=1.0495 (C:1.0495, R:0.0105)
Batch 100/537: Loss=1.0859 (C:1.0859, R:0.0105)
Batch 125/537: Loss=1.0294 (C:1.0294, R:0.0105)
Batch 150/537: Loss=1.0096 (C:1.0096, R:0.0106)
Batch 175/537: Loss=1.0962 (C:1.0962, R:0.0105)
Batch 200/537: Loss=1.0626 (C:1.0626, R:0.0105)
Batch 225/537: Loss=1.0794 (C:1.0794, R:0.0105)
Batch 250/537: Loss=1.0485 (C:1.0485, R:0.0105)
Batch 275/537: Loss=1.0594 (C:1.0594, R:0.0105)
Batch 300/537: Loss=1.0850 (C:1.0850, R:0.0105)
Batch 325/537: Loss=1.0453 (C:1.0453, R:0.0105)
Batch 350/537: Loss=1.0777 (C:1.0777, R:0.0105)
Batch 375/537: Loss=1.0630 (C:1.0630, R:0.0105)
Batch 400/537: Loss=1.0655 (C:1.0655, R:0.0105)
Batch 425/537: Loss=1.0576 (C:1.0576, R:0.0105)
Batch 450/537: Loss=1.0359 (C:1.0359, R:0.0105)
Batch 475/537: Loss=1.0746 (C:1.0746, R:0.0105)
Batch 500/537: Loss=1.0424 (C:1.0424, R:0.0105)
Batch 525/537: Loss=1.0471 (C:1.0471, R:0.0105)

============================================================
Epoch 10/100 completed in 39.1s
Train: Loss=1.0559 (C:1.0559, R:0.0105) Ratio=3.13x
Val:   Loss=1.0137 (C:1.0137, R:0.0105) Ratio=3.79x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0137)
Checkpoint saved at epoch 10
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.0414 (C:1.0414, R:0.0105)
Batch  25/537: Loss=1.0627 (C:1.0627, R:0.0105)
Batch  50/537: Loss=1.0717 (C:1.0717, R:0.0105)
Batch  75/537: Loss=1.0825 (C:1.0825, R:0.0105)
Batch 100/537: Loss=1.0276 (C:1.0276, R:0.0105)
Batch 125/537: Loss=1.0582 (C:1.0582, R:0.0105)
Batch 150/537: Loss=1.0227 (C:1.0227, R:0.0105)
Batch 175/537: Loss=1.0369 (C:1.0369, R:0.0105)
Batch 200/537: Loss=1.0535 (C:1.0535, R:0.0105)
Batch 225/537: Loss=1.0299 (C:1.0299, R:0.0105)
Batch 250/537: Loss=1.0799 (C:1.0799, R:0.0105)
Batch 275/537: Loss=1.0626 (C:1.0626, R:0.0105)
Batch 300/537: Loss=1.0314 (C:1.0314, R:0.0105)
Batch 325/537: Loss=1.0328 (C:1.0328, R:0.0105)
Batch 350/537: Loss=1.0516 (C:1.0516, R:0.0105)
Batch 375/537: Loss=1.0831 (C:1.0831, R:0.0105)
Batch 400/537: Loss=1.0358 (C:1.0358, R:0.0105)
Batch 425/537: Loss=1.0467 (C:1.0467, R:0.0105)
Batch 450/537: Loss=1.0296 (C:1.0296, R:0.0105)
Batch 475/537: Loss=1.0497 (C:1.0497, R:0.0105)
Batch 500/537: Loss=1.0921 (C:1.0921, R:0.0105)
Batch 525/537: Loss=1.0307 (C:1.0307, R:0.0105)

============================================================
Epoch 11/100 completed in 32.0s
Train: Loss=1.0497 (C:1.0497, R:0.0105) Ratio=3.24x
Val:   Loss=1.0070 (C:1.0070, R:0.0105) Ratio=3.88x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0070)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.0357 (C:1.0357, R:0.0105)
Batch  25/537: Loss=1.0509 (C:1.0509, R:0.0106)
Batch  50/537: Loss=1.0002 (C:1.0002, R:0.0106)
Batch  75/537: Loss=1.0382 (C:1.0382, R:0.0105)
Batch 100/537: Loss=1.0431 (C:1.0431, R:0.0105)
Batch 125/537: Loss=1.0283 (C:1.0283, R:0.0105)
Batch 150/537: Loss=1.0504 (C:1.0504, R:0.0105)
Batch 175/537: Loss=1.0478 (C:1.0478, R:0.0105)
Batch 200/537: Loss=1.0377 (C:1.0377, R:0.0105)
Batch 225/537: Loss=1.0371 (C:1.0371, R:0.0105)
Batch 250/537: Loss=1.0514 (C:1.0514, R:0.0105)
Batch 275/537: Loss=1.0133 (C:1.0133, R:0.0105)
Batch 300/537: Loss=1.0390 (C:1.0390, R:0.0105)
Batch 325/537: Loss=1.0533 (C:1.0533, R:0.0105)
Batch 350/537: Loss=1.0574 (C:1.0574, R:0.0105)
Batch 375/537: Loss=1.0379 (C:1.0379, R:0.0105)
Batch 400/537: Loss=1.0636 (C:1.0636, R:0.0105)
Batch 425/537: Loss=1.0425 (C:1.0425, R:0.0105)
Batch 450/537: Loss=1.0790 (C:1.0790, R:0.0105)
Batch 475/537: Loss=1.0400 (C:1.0400, R:0.0105)
Batch 500/537: Loss=1.0584 (C:1.0584, R:0.0105)
Batch 525/537: Loss=1.0627 (C:1.0627, R:0.0105)

============================================================
Epoch 12/100 completed in 30.9s
Train: Loss=1.0441 (C:1.0441, R:0.0105) Ratio=3.30x
Val:   Loss=1.0013 (C:1.0013, R:0.0105) Ratio=3.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0013)
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.459 ± 0.551
    Neg distances: 1.771 ± 0.871
    Separation ratio: 3.86x
    Gap: -3.254
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9747 (C:0.9747, R:0.0105)
Batch  25/537: Loss=1.0235 (C:1.0235, R:0.0105)
Batch  50/537: Loss=0.9783 (C:0.9783, R:0.0105)
Batch  75/537: Loss=0.9559 (C:0.9559, R:0.0105)
Batch 100/537: Loss=1.0085 (C:1.0085, R:0.0105)
Batch 125/537: Loss=0.9861 (C:0.9861, R:0.0105)
Batch 150/537: Loss=1.0191 (C:1.0191, R:0.0105)
Batch 175/537: Loss=0.9603 (C:0.9603, R:0.0105)
Batch 200/537: Loss=0.9594 (C:0.9594, R:0.0105)
Batch 225/537: Loss=1.0300 (C:1.0300, R:0.0105)
Batch 250/537: Loss=0.9771 (C:0.9771, R:0.0105)
Batch 275/537: Loss=0.9537 (C:0.9537, R:0.0105)
Batch 300/537: Loss=0.9952 (C:0.9952, R:0.0105)
Batch 325/537: Loss=0.9920 (C:0.9920, R:0.0106)
Batch 350/537: Loss=1.0050 (C:1.0050, R:0.0105)
Batch 375/537: Loss=1.0253 (C:1.0253, R:0.0105)
Batch 400/537: Loss=1.0074 (C:1.0074, R:0.0105)
Batch 425/537: Loss=1.0142 (C:1.0142, R:0.0105)
Batch 450/537: Loss=0.9699 (C:0.9699, R:0.0105)
Batch 475/537: Loss=0.9774 (C:0.9774, R:0.0105)
Batch 500/537: Loss=0.9943 (C:0.9943, R:0.0105)
Batch 525/537: Loss=0.9952 (C:0.9952, R:0.0105)

============================================================
Epoch 13/100 completed in 36.9s
Train: Loss=0.9949 (C:0.9949, R:0.0105) Ratio=3.35x
Val:   Loss=0.9468 (C:0.9468, R:0.0105) Ratio=4.10x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9468)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9896 (C:0.9896, R:0.0105)
Batch  25/537: Loss=0.9811 (C:0.9811, R:0.0105)
Batch  50/537: Loss=0.9798 (C:0.9798, R:0.0105)
Batch  75/537: Loss=0.9740 (C:0.9740, R:0.0105)
Batch 100/537: Loss=1.0062 (C:1.0062, R:0.0105)
Batch 125/537: Loss=1.0224 (C:1.0224, R:0.0105)
Batch 150/537: Loss=0.9726 (C:0.9726, R:0.0105)
Batch 175/537: Loss=0.9940 (C:0.9940, R:0.0105)
Batch 200/537: Loss=1.0086 (C:1.0086, R:0.0105)
Batch 225/537: Loss=0.9902 (C:0.9902, R:0.0105)
Batch 250/537: Loss=0.9788 (C:0.9788, R:0.0105)
Batch 275/537: Loss=1.0061 (C:1.0061, R:0.0105)
Batch 300/537: Loss=0.9558 (C:0.9558, R:0.0106)
Batch 325/537: Loss=0.9878 (C:0.9878, R:0.0105)
Batch 350/537: Loss=0.9911 (C:0.9911, R:0.0105)
Batch 375/537: Loss=1.0020 (C:1.0020, R:0.0105)
Batch 400/537: Loss=1.0090 (C:1.0090, R:0.0105)
Batch 425/537: Loss=0.9792 (C:0.9792, R:0.0106)
Batch 450/537: Loss=1.0120 (C:1.0120, R:0.0105)
Batch 475/537: Loss=0.9893 (C:0.9893, R:0.0105)
Batch 500/537: Loss=0.9990 (C:0.9990, R:0.0105)
Batch 525/537: Loss=0.9975 (C:0.9975, R:0.0105)

============================================================
Epoch 14/100 completed in 30.9s
Train: Loss=0.9894 (C:0.9894, R:0.0105) Ratio=3.36x
Val:   Loss=0.9406 (C:0.9406, R:0.0105) Ratio=4.19x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9406)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.9941 (C:0.9941, R:0.0105)
Batch  25/537: Loss=0.9517 (C:0.9517, R:0.0105)
Batch  50/537: Loss=0.9613 (C:0.9613, R:0.0105)
Batch  75/537: Loss=1.0025 (C:1.0025, R:0.0105)
Batch 100/537: Loss=0.9547 (C:0.9547, R:0.0105)
Batch 125/537: Loss=1.0167 (C:1.0167, R:0.0105)
Batch 150/537: Loss=0.9940 (C:0.9940, R:0.0105)
Batch 175/537: Loss=0.9559 (C:0.9559, R:0.0105)
Batch 200/537: Loss=0.9793 (C:0.9793, R:0.0105)
Batch 225/537: Loss=0.9618 (C:0.9618, R:0.0105)
Batch 250/537: Loss=0.9750 (C:0.9750, R:0.0105)
Batch 275/537: Loss=0.9669 (C:0.9669, R:0.0105)
Batch 300/537: Loss=0.9593 (C:0.9593, R:0.0105)
Batch 325/537: Loss=0.9902 (C:0.9902, R:0.0105)
Batch 350/537: Loss=1.0056 (C:1.0056, R:0.0105)
Batch 375/537: Loss=0.9835 (C:0.9835, R:0.0105)
Batch 400/537: Loss=0.9844 (C:0.9844, R:0.0105)
Batch 425/537: Loss=0.9885 (C:0.9885, R:0.0106)
Batch 450/537: Loss=0.9741 (C:0.9741, R:0.0105)
Batch 475/537: Loss=1.0127 (C:1.0127, R:0.0105)
Batch 500/537: Loss=0.9856 (C:0.9856, R:0.0105)
Batch 525/537: Loss=0.9811 (C:0.9811, R:0.0105)

============================================================
Epoch 15/100 completed in 32.2s
Train: Loss=0.9850 (C:0.9850, R:0.0105) Ratio=3.48x
Val:   Loss=0.9351 (C:0.9351, R:0.0105) Ratio=4.31x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9351)
Checkpoint saved at epoch 15
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.435 ± 0.541
    Neg distances: 1.852 ± 0.882
    Separation ratio: 4.26x
    Gap: -3.335
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.9378 (C:0.9378, R:0.0105)
Batch  25/537: Loss=0.9277 (C:0.9277, R:0.0105)
Batch  50/537: Loss=0.9351 (C:0.9351, R:0.0106)
Batch  75/537: Loss=0.9623 (C:0.9623, R:0.0105)
Batch 100/537: Loss=0.9584 (C:0.9584, R:0.0105)
Batch 125/537: Loss=0.9623 (C:0.9623, R:0.0105)
Batch 150/537: Loss=0.9381 (C:0.9381, R:0.0105)
Batch 175/537: Loss=0.9135 (C:0.9135, R:0.0105)
Batch 200/537: Loss=0.9144 (C:0.9144, R:0.0105)
Batch 225/537: Loss=0.9282 (C:0.9282, R:0.0105)
Batch 250/537: Loss=0.9472 (C:0.9472, R:0.0105)
Batch 275/537: Loss=0.9334 (C:0.9334, R:0.0105)
Batch 300/537: Loss=0.9295 (C:0.9295, R:0.0105)
Batch 325/537: Loss=0.9729 (C:0.9729, R:0.0105)
Batch 350/537: Loss=0.9309 (C:0.9309, R:0.0105)
Batch 375/537: Loss=0.9464 (C:0.9464, R:0.0105)
Batch 400/537: Loss=0.9399 (C:0.9399, R:0.0105)
Batch 425/537: Loss=0.9294 (C:0.9294, R:0.0105)
Batch 450/537: Loss=0.9720 (C:0.9720, R:0.0105)
Batch 475/537: Loss=0.9506 (C:0.9506, R:0.0105)
Batch 500/537: Loss=0.9520 (C:0.9520, R:0.0105)
Batch 525/537: Loss=0.9354 (C:0.9354, R:0.0105)

============================================================
Epoch 16/100 completed in 38.6s
Train: Loss=0.9413 (C:0.9413, R:0.0105) Ratio=3.53x
Val:   Loss=0.8885 (C:0.8885, R:0.0105) Ratio=4.39x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8885)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.9161 (C:0.9161, R:0.0105)
Batch  25/537: Loss=0.9438 (C:0.9438, R:0.0105)
Batch  50/537: Loss=0.9610 (C:0.9610, R:0.0105)
Batch  75/537: Loss=0.9248 (C:0.9248, R:0.0105)
Batch 100/537: Loss=0.9390 (C:0.9390, R:0.0105)
Batch 125/537: Loss=0.9471 (C:0.9471, R:0.0105)
Batch 150/537: Loss=0.9802 (C:0.9802, R:0.0105)
Batch 175/537: Loss=0.9110 (C:0.9110, R:0.0105)
Batch 200/537: Loss=0.9368 (C:0.9368, R:0.0105)
Batch 225/537: Loss=0.9184 (C:0.9184, R:0.0105)
Batch 250/537: Loss=0.9419 (C:0.9419, R:0.0105)
Batch 275/537: Loss=0.9822 (C:0.9822, R:0.0105)
Batch 300/537: Loss=0.8863 (C:0.8863, R:0.0105)
Batch 325/537: Loss=0.9017 (C:0.9017, R:0.0105)
Batch 350/537: Loss=0.8883 (C:0.8883, R:0.0105)
Batch 375/537: Loss=0.9503 (C:0.9503, R:0.0105)
Batch 400/537: Loss=0.9290 (C:0.9290, R:0.0105)
Batch 425/537: Loss=0.9331 (C:0.9331, R:0.0105)
Batch 450/537: Loss=0.9335 (C:0.9335, R:0.0105)
Batch 475/537: Loss=0.9065 (C:0.9065, R:0.0106)
Batch 500/537: Loss=0.9337 (C:0.9337, R:0.0105)
Batch 525/537: Loss=0.9571 (C:0.9571, R:0.0105)

============================================================
Epoch 17/100 completed in 30.8s
Train: Loss=0.9357 (C:0.9357, R:0.0105) Ratio=3.58x
Val:   Loss=0.8843 (C:0.8843, R:0.0105) Ratio=4.47x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8843)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.9380 (C:0.9380, R:0.0105)
Batch  25/537: Loss=0.9233 (C:0.9233, R:0.0105)
Batch  50/537: Loss=0.9638 (C:0.9638, R:0.0105)
Batch  75/537: Loss=0.9248 (C:0.9248, R:0.0105)
Batch 100/537: Loss=0.9145 (C:0.9145, R:0.0105)
Batch 125/537: Loss=0.9551 (C:0.9551, R:0.0105)
Batch 150/537: Loss=0.9102 (C:0.9102, R:0.0105)
Batch 175/537: Loss=0.8689 (C:0.8689, R:0.0106)
Batch 200/537: Loss=0.9390 (C:0.9390, R:0.0105)
Batch 225/537: Loss=0.9528 (C:0.9528, R:0.0105)
Batch 250/537: Loss=0.9127 (C:0.9127, R:0.0105)
Batch 275/537: Loss=0.9402 (C:0.9402, R:0.0105)
Batch 300/537: Loss=0.9384 (C:0.9384, R:0.0105)
Batch 325/537: Loss=0.9398 (C:0.9398, R:0.0105)
Batch 350/537: Loss=0.9131 (C:0.9131, R:0.0105)
Batch 375/537: Loss=0.9224 (C:0.9224, R:0.0105)
Batch 400/537: Loss=0.9534 (C:0.9534, R:0.0105)
Batch 425/537: Loss=0.9227 (C:0.9227, R:0.0105)
Batch 450/537: Loss=0.9499 (C:0.9499, R:0.0105)
Batch 475/537: Loss=0.9351 (C:0.9351, R:0.0105)
Batch 500/537: Loss=0.9244 (C:0.9244, R:0.0105)
Batch 525/537: Loss=0.9378 (C:0.9378, R:0.0105)

============================================================
Epoch 18/100 completed in 30.7s
Train: Loss=0.9328 (C:0.9328, R:0.0105) Ratio=3.61x
Val:   Loss=0.8775 (C:0.8775, R:0.0105) Ratio=4.58x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8775)
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.419 ± 0.524
    Neg distances: 1.926 ± 0.893
    Separation ratio: 4.60x
    Gap: -3.409
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.8784 (C:0.8784, R:0.0105)
Batch  25/537: Loss=0.8921 (C:0.8921, R:0.0105)
Batch  50/537: Loss=0.8828 (C:0.8828, R:0.0105)
Batch  75/537: Loss=0.9036 (C:0.9036, R:0.0105)
Batch 100/537: Loss=0.8746 (C:0.8746, R:0.0105)
Batch 125/537: Loss=0.8858 (C:0.8858, R:0.0105)
Batch 150/537: Loss=0.9079 (C:0.9079, R:0.0105)
Batch 175/537: Loss=0.9044 (C:0.9044, R:0.0105)
Batch 200/537: Loss=0.9130 (C:0.9130, R:0.0105)
Batch 225/537: Loss=0.9095 (C:0.9095, R:0.0105)
Batch 250/537: Loss=0.8768 (C:0.8768, R:0.0105)
Batch 275/537: Loss=0.8884 (C:0.8884, R:0.0105)
Batch 300/537: Loss=0.9125 (C:0.9125, R:0.0106)
Batch 325/537: Loss=0.9195 (C:0.9195, R:0.0105)
Batch 350/537: Loss=0.9207 (C:0.9207, R:0.0105)
Batch 375/537: Loss=0.8884 (C:0.8884, R:0.0105)
Batch 400/537: Loss=0.9067 (C:0.9067, R:0.0106)
Batch 425/537: Loss=0.8731 (C:0.8731, R:0.0105)
Batch 450/537: Loss=0.9163 (C:0.9163, R:0.0105)
Batch 475/537: Loss=0.9250 (C:0.9250, R:0.0105)
Batch 500/537: Loss=0.9146 (C:0.9146, R:0.0105)
Batch 525/537: Loss=0.8898 (C:0.8898, R:0.0105)

============================================================
Epoch 19/100 completed in 37.2s
Train: Loss=0.8957 (C:0.8957, R:0.0105) Ratio=3.64x
Val:   Loss=0.8360 (C:0.8360, R:0.0105) Ratio=4.66x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8360)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.8635 (C:0.8635, R:0.0105)
Batch  25/537: Loss=0.8711 (C:0.8711, R:0.0105)
Batch  50/537: Loss=0.9096 (C:0.9096, R:0.0105)
Batch  75/537: Loss=0.8966 (C:0.8966, R:0.0105)
Batch 100/537: Loss=0.8952 (C:0.8952, R:0.0105)
Batch 125/537: Loss=0.8941 (C:0.8941, R:0.0105)
Batch 150/537: Loss=0.8886 (C:0.8886, R:0.0105)
Batch 175/537: Loss=0.8677 (C:0.8677, R:0.0105)
Batch 200/537: Loss=0.8868 (C:0.8868, R:0.0105)
Batch 225/537: Loss=0.9070 (C:0.9070, R:0.0105)
Batch 250/537: Loss=0.8893 (C:0.8893, R:0.0105)
Batch 275/537: Loss=0.9058 (C:0.9058, R:0.0105)
Batch 300/537: Loss=0.9160 (C:0.9160, R:0.0105)
Batch 325/537: Loss=0.9121 (C:0.9121, R:0.0105)
Batch 350/537: Loss=0.9085 (C:0.9085, R:0.0105)
Batch 375/537: Loss=0.9092 (C:0.9092, R:0.0105)
Batch 400/537: Loss=0.9183 (C:0.9183, R:0.0105)
Batch 425/537: Loss=0.8965 (C:0.8965, R:0.0105)
Batch 450/537: Loss=0.9268 (C:0.9268, R:0.0105)
Batch 475/537: Loss=0.9097 (C:0.9097, R:0.0105)
Batch 500/537: Loss=0.9295 (C:0.9295, R:0.0105)
Batch 525/537: Loss=0.8953 (C:0.8953, R:0.0105)

============================================================
Epoch 20/100 completed in 31.0s
Train: Loss=0.8895 (C:0.8895, R:0.0105) Ratio=3.61x
Val:   Loss=0.8335 (C:0.8335, R:0.0105) Ratio=4.74x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8335)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8987 (C:0.8987, R:0.0105)
Batch  25/537: Loss=0.9036 (C:0.9036, R:0.0106)
Batch  50/537: Loss=0.8518 (C:0.8518, R:0.0105)
Batch  75/537: Loss=0.8982 (C:0.8982, R:0.0105)
Batch 100/537: Loss=0.8778 (C:0.8778, R:0.0105)
Batch 125/537: Loss=0.8964 (C:0.8964, R:0.0105)
Batch 150/537: Loss=0.8671 (C:0.8671, R:0.0105)
Batch 175/537: Loss=0.8926 (C:0.8926, R:0.0105)
Batch 200/537: Loss=0.8856 (C:0.8856, R:0.0106)
Batch 225/537: Loss=0.8511 (C:0.8511, R:0.0105)
Batch 250/537: Loss=0.8906 (C:0.8906, R:0.0105)
Batch 275/537: Loss=0.8891 (C:0.8891, R:0.0105)
Batch 300/537: Loss=0.9068 (C:0.9068, R:0.0105)
Batch 325/537: Loss=0.9091 (C:0.9091, R:0.0105)
Batch 350/537: Loss=0.9031 (C:0.9031, R:0.0105)
Batch 375/537: Loss=0.9225 (C:0.9225, R:0.0105)
Batch 400/537: Loss=0.8988 (C:0.8988, R:0.0105)
Batch 425/537: Loss=0.8910 (C:0.8910, R:0.0105)
Batch 450/537: Loss=0.8794 (C:0.8794, R:0.0105)
Batch 475/537: Loss=0.8913 (C:0.8913, R:0.0105)
Batch 500/537: Loss=0.8686 (C:0.8686, R:0.0106)
Batch 525/537: Loss=0.8757 (C:0.8757, R:0.0105)

============================================================
Epoch 21/100 completed in 30.7s
Train: Loss=0.8871 (C:0.8871, R:0.0105) Ratio=3.73x
Val:   Loss=0.8291 (C:0.8291, R:0.0105) Ratio=4.85x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8291)
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.404 ± 0.524
    Neg distances: 2.020 ± 0.918
    Separation ratio: 5.00x
    Gap: -3.559
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8478 (C:0.8478, R:0.0105)
Batch  25/537: Loss=0.8637 (C:0.8637, R:0.0105)
Batch  50/537: Loss=0.8660 (C:0.8660, R:0.0105)
Batch  75/537: Loss=0.8612 (C:0.8612, R:0.0105)
Batch 100/537: Loss=0.8445 (C:0.8445, R:0.0105)
Batch 125/537: Loss=0.8498 (C:0.8498, R:0.0105)
Batch 150/537: Loss=0.8267 (C:0.8267, R:0.0105)
Batch 175/537: Loss=0.8367 (C:0.8367, R:0.0105)
Batch 200/537: Loss=0.8465 (C:0.8465, R:0.0105)
Batch 225/537: Loss=0.8300 (C:0.8300, R:0.0105)
Batch 250/537: Loss=0.8470 (C:0.8470, R:0.0105)
Batch 275/537: Loss=0.8318 (C:0.8318, R:0.0105)
Batch 300/537: Loss=0.8693 (C:0.8693, R:0.0105)
Batch 325/537: Loss=0.8731 (C:0.8731, R:0.0105)
Batch 350/537: Loss=0.8526 (C:0.8526, R:0.0105)
Batch 375/537: Loss=0.8812 (C:0.8812, R:0.0105)
Batch 400/537: Loss=0.8408 (C:0.8408, R:0.0105)
Batch 425/537: Loss=0.8603 (C:0.8603, R:0.0105)
Batch 450/537: Loss=0.8519 (C:0.8519, R:0.0105)
Batch 475/537: Loss=0.8435 (C:0.8435, R:0.0105)
Batch 500/537: Loss=0.8379 (C:0.8379, R:0.0105)
Batch 525/537: Loss=0.8500 (C:0.8500, R:0.0105)

============================================================
Epoch 22/100 completed in 36.9s
Train: Loss=0.8491 (C:0.8491, R:0.0105) Ratio=3.73x
Val:   Loss=0.7842 (C:0.7842, R:0.0105) Ratio=4.92x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7842)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.8142 (C:0.8142, R:0.0105)
Batch  25/537: Loss=0.8338 (C:0.8338, R:0.0105)
Batch  50/537: Loss=0.8391 (C:0.8391, R:0.0105)
Batch  75/537: Loss=0.8364 (C:0.8364, R:0.0105)
Batch 100/537: Loss=0.8263 (C:0.8263, R:0.0105)
Batch 125/537: Loss=0.8449 (C:0.8449, R:0.0105)
Batch 150/537: Loss=0.8035 (C:0.8035, R:0.0105)
Batch 175/537: Loss=0.8508 (C:0.8508, R:0.0105)
Batch 200/537: Loss=0.8393 (C:0.8393, R:0.0105)
Batch 225/537: Loss=0.8432 (C:0.8432, R:0.0105)
Batch 250/537: Loss=0.8162 (C:0.8162, R:0.0105)
Batch 275/537: Loss=0.8646 (C:0.8646, R:0.0105)
Batch 300/537: Loss=0.8416 (C:0.8416, R:0.0105)
Batch 325/537: Loss=0.8538 (C:0.8538, R:0.0106)
Batch 350/537: Loss=0.8242 (C:0.8242, R:0.0105)
Batch 375/537: Loss=0.8125 (C:0.8125, R:0.0105)
Batch 400/537: Loss=0.8414 (C:0.8414, R:0.0105)
Batch 425/537: Loss=0.8834 (C:0.8834, R:0.0105)
Batch 450/537: Loss=0.8545 (C:0.8545, R:0.0105)
Batch 475/537: Loss=0.8449 (C:0.8449, R:0.0106)
Batch 500/537: Loss=0.8419 (C:0.8419, R:0.0105)
Batch 525/537: Loss=0.8386 (C:0.8386, R:0.0105)

============================================================
Epoch 23/100 completed in 30.7s
Train: Loss=0.8440 (C:0.8440, R:0.0105) Ratio=3.90x
Val:   Loss=0.7809 (C:0.7809, R:0.0105) Ratio=4.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7809)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8815 (C:0.8815, R:0.0105)
Batch  25/537: Loss=0.8225 (C:0.8225, R:0.0105)
Batch  50/537: Loss=0.8386 (C:0.8386, R:0.0105)
Batch  75/537: Loss=0.8003 (C:0.8003, R:0.0105)
Batch 100/537: Loss=0.8151 (C:0.8151, R:0.0105)
Batch 125/537: Loss=0.8537 (C:0.8537, R:0.0105)
Batch 150/537: Loss=0.8280 (C:0.8280, R:0.0105)
Batch 175/537: Loss=0.8620 (C:0.8620, R:0.0105)
Batch 200/537: Loss=0.8242 (C:0.8242, R:0.0105)
Batch 225/537: Loss=0.8315 (C:0.8315, R:0.0105)
Batch 250/537: Loss=0.8300 (C:0.8300, R:0.0105)
Batch 275/537: Loss=0.8181 (C:0.8181, R:0.0105)
Batch 300/537: Loss=0.8461 (C:0.8461, R:0.0105)
Batch 325/537: Loss=0.8309 (C:0.8309, R:0.0105)
Batch 350/537: Loss=0.8382 (C:0.8382, R:0.0105)
Batch 375/537: Loss=0.8042 (C:0.8042, R:0.0105)
Batch 400/537: Loss=0.8370 (C:0.8370, R:0.0105)
Batch 425/537: Loss=0.8445 (C:0.8445, R:0.0105)
Batch 450/537: Loss=0.8516 (C:0.8516, R:0.0105)
Batch 475/537: Loss=0.8355 (C:0.8355, R:0.0105)
Batch 500/537: Loss=0.8227 (C:0.8227, R:0.0105)
Batch 525/537: Loss=0.8529 (C:0.8529, R:0.0105)

============================================================
Epoch 24/100 completed in 30.7s
Train: Loss=0.8417 (C:0.8417, R:0.0105) Ratio=3.92x
Val:   Loss=0.7764 (C:0.7764, R:0.0105) Ratio=5.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7764)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.416 ± 0.551
    Neg distances: 2.083 ± 0.945
    Separation ratio: 5.00x
    Gap: -3.814
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.8331 (C:0.8331, R:0.0105)
Batch  25/537: Loss=0.8289 (C:0.8289, R:0.0105)
Batch  50/537: Loss=0.8296 (C:0.8296, R:0.0105)
Batch  75/537: Loss=0.7717 (C:0.7717, R:0.0106)
Batch 100/537: Loss=0.8322 (C:0.8322, R:0.0105)
Batch 125/537: Loss=0.8089 (C:0.8089, R:0.0105)
Batch 150/537: Loss=0.8368 (C:0.8368, R:0.0105)
Batch 175/537: Loss=0.8621 (C:0.8621, R:0.0105)
Batch 200/537: Loss=0.8029 (C:0.8029, R:0.0105)
Batch 225/537: Loss=0.8089 (C:0.8089, R:0.0105)
Batch 250/537: Loss=0.8485 (C:0.8485, R:0.0105)
Batch 275/537: Loss=0.8396 (C:0.8396, R:0.0105)
Batch 300/537: Loss=0.8639 (C:0.8639, R:0.0105)
Batch 325/537: Loss=0.8496 (C:0.8496, R:0.0105)
Batch 350/537: Loss=0.8260 (C:0.8260, R:0.0105)
Batch 375/537: Loss=0.8151 (C:0.8151, R:0.0105)
Batch 400/537: Loss=0.8368 (C:0.8368, R:0.0105)
Batch 425/537: Loss=0.8354 (C:0.8354, R:0.0105)
Batch 450/537: Loss=0.8304 (C:0.8304, R:0.0105)
Batch 475/537: Loss=0.8334 (C:0.8334, R:0.0106)
Batch 500/537: Loss=0.8571 (C:0.8571, R:0.0105)
Batch 525/537: Loss=0.8093 (C:0.8093, R:0.0105)

============================================================
Epoch 25/100 completed in 36.8s
Train: Loss=0.8292 (C:0.8292, R:0.0105) Ratio=3.91x
Val:   Loss=0.7598 (C:0.7598, R:0.0105) Ratio=5.16x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7598)
Checkpoint saved at epoch 25
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.8497 (C:0.8497, R:0.0105)
Batch  25/537: Loss=0.8225 (C:0.8225, R:0.0105)
Batch  50/537: Loss=0.8218 (C:0.8218, R:0.0105)
Batch  75/537: Loss=0.8311 (C:0.8311, R:0.0105)
Batch 100/537: Loss=0.8201 (C:0.8201, R:0.0105)
Batch 125/537: Loss=0.8320 (C:0.8320, R:0.0105)
Batch 150/537: Loss=0.8509 (C:0.8509, R:0.0105)
Batch 175/537: Loss=0.8077 (C:0.8077, R:0.0105)
Batch 200/537: Loss=0.8190 (C:0.8190, R:0.0105)
Batch 225/537: Loss=0.8383 (C:0.8383, R:0.0105)
Batch 250/537: Loss=0.8069 (C:0.8069, R:0.0105)
Batch 275/537: Loss=0.8148 (C:0.8148, R:0.0105)
Batch 300/537: Loss=0.8027 (C:0.8027, R:0.0105)
Batch 325/537: Loss=0.8045 (C:0.8045, R:0.0105)
Batch 350/537: Loss=0.8567 (C:0.8567, R:0.0105)
Batch 375/537: Loss=0.8315 (C:0.8315, R:0.0105)
Batch 400/537: Loss=0.8165 (C:0.8165, R:0.0105)
Batch 425/537: Loss=0.8159 (C:0.8159, R:0.0105)
Batch 450/537: Loss=0.8274 (C:0.8274, R:0.0106)
Batch 475/537: Loss=0.8497 (C:0.8497, R:0.0105)
Batch 500/537: Loss=0.8278 (C:0.8278, R:0.0105)
Batch 525/537: Loss=0.8235 (C:0.8235, R:0.0105)

============================================================
Epoch 26/100 completed in 30.9s
Train: Loss=0.8256 (C:0.8256, R:0.0105) Ratio=3.91x
Val:   Loss=0.7564 (C:0.7564, R:0.0105) Ratio=5.24x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7564)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.8683 (C:0.8683, R:0.0105)
Batch  25/537: Loss=0.8104 (C:0.8104, R:0.0105)
Batch  50/537: Loss=0.8149 (C:0.8149, R:0.0105)
Batch  75/537: Loss=0.7834 (C:0.7834, R:0.0105)
Batch 100/537: Loss=0.8396 (C:0.8396, R:0.0105)
Batch 125/537: Loss=0.7823 (C:0.7823, R:0.0105)
Batch 150/537: Loss=0.8160 (C:0.8160, R:0.0105)
Batch 175/537: Loss=0.8118 (C:0.8118, R:0.0105)
Batch 200/537: Loss=0.8198 (C:0.8198, R:0.0105)
Batch 225/537: Loss=0.8052 (C:0.8052, R:0.0105)
Batch 250/537: Loss=0.8255 (C:0.8255, R:0.0105)
Batch 275/537: Loss=0.8404 (C:0.8404, R:0.0105)
Batch 300/537: Loss=0.8050 (C:0.8050, R:0.0105)
Batch 325/537: Loss=0.8367 (C:0.8367, R:0.0105)
Batch 350/537: Loss=0.8180 (C:0.8180, R:0.0106)
Batch 375/537: Loss=0.8011 (C:0.8011, R:0.0105)
Batch 400/537: Loss=0.8311 (C:0.8311, R:0.0106)
Batch 425/537: Loss=0.8273 (C:0.8273, R:0.0105)
Batch 450/537: Loss=0.8382 (C:0.8382, R:0.0105)
Batch 475/537: Loss=0.8018 (C:0.8018, R:0.0105)
Batch 500/537: Loss=0.8239 (C:0.8239, R:0.0105)
Batch 525/537: Loss=0.7994 (C:0.7994, R:0.0106)

============================================================
Epoch 27/100 completed in 30.8s
Train: Loss=0.8223 (C:0.8223, R:0.0105) Ratio=3.98x
Val:   Loss=0.7527 (C:0.7527, R:0.0105) Ratio=5.36x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7527)
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.401 ± 0.553
    Neg distances: 2.167 ± 0.966
    Separation ratio: 5.41x
    Gap: -3.765
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.7958 (C:0.7958, R:0.0105)
Batch  25/537: Loss=0.7930 (C:0.7930, R:0.0105)
Batch  50/537: Loss=0.7674 (C:0.7674, R:0.0105)
Batch  75/537: Loss=0.8216 (C:0.8216, R:0.0105)
Batch 100/537: Loss=0.7842 (C:0.7842, R:0.0105)
Batch 125/537: Loss=0.7873 (C:0.7873, R:0.0105)
Batch 150/537: Loss=0.7675 (C:0.7675, R:0.0105)
Batch 175/537: Loss=0.7597 (C:0.7597, R:0.0105)
Batch 200/537: Loss=0.7928 (C:0.7928, R:0.0105)
Batch 225/537: Loss=0.7618 (C:0.7618, R:0.0105)
Batch 250/537: Loss=0.8162 (C:0.8162, R:0.0105)
Batch 275/537: Loss=0.7364 (C:0.7364, R:0.0105)
Batch 300/537: Loss=0.7698 (C:0.7698, R:0.0105)
Batch 325/537: Loss=0.7987 (C:0.7987, R:0.0105)
Batch 350/537: Loss=0.8085 (C:0.8085, R:0.0105)
Batch 375/537: Loss=0.7683 (C:0.7683, R:0.0105)
Batch 400/537: Loss=0.7680 (C:0.7680, R:0.0105)
Batch 425/537: Loss=0.8072 (C:0.8072, R:0.0105)
Batch 450/537: Loss=0.7882 (C:0.7882, R:0.0105)
Batch 475/537: Loss=0.7706 (C:0.7706, R:0.0105)
Batch 500/537: Loss=0.8113 (C:0.8113, R:0.0105)
Batch 525/537: Loss=0.7979 (C:0.7979, R:0.0105)

============================================================
Epoch 28/100 completed in 37.5s
Train: Loss=0.7855 (C:0.7855, R:0.0105) Ratio=3.97x
Val:   Loss=0.7129 (C:0.7129, R:0.0105) Ratio=5.38x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7129)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.7803 (C:0.7803, R:0.0105)
Batch  25/537: Loss=0.7504 (C:0.7504, R:0.0105)
Batch  50/537: Loss=0.7935 (C:0.7935, R:0.0105)
Batch  75/537: Loss=0.7958 (C:0.7958, R:0.0105)
Batch 100/537: Loss=0.7546 (C:0.7546, R:0.0105)
Batch 125/537: Loss=0.8088 (C:0.8088, R:0.0105)
Batch 150/537: Loss=0.8074 (C:0.8074, R:0.0105)
Batch 175/537: Loss=0.7841 (C:0.7841, R:0.0105)
Batch 200/537: Loss=0.7790 (C:0.7790, R:0.0105)
Batch 225/537: Loss=0.7770 (C:0.7770, R:0.0105)
Batch 250/537: Loss=0.7819 (C:0.7819, R:0.0105)
Batch 275/537: Loss=0.7823 (C:0.7823, R:0.0105)
Batch 300/537: Loss=0.8014 (C:0.8014, R:0.0105)
Batch 325/537: Loss=0.7872 (C:0.7872, R:0.0105)
Batch 350/537: Loss=0.8021 (C:0.8021, R:0.0105)
Batch 375/537: Loss=0.7970 (C:0.7970, R:0.0106)
Batch 400/537: Loss=0.7780 (C:0.7780, R:0.0105)
Batch 425/537: Loss=0.8037 (C:0.8037, R:0.0105)
Batch 450/537: Loss=0.7984 (C:0.7984, R:0.0105)
Batch 475/537: Loss=0.8099 (C:0.8099, R:0.0105)
Batch 500/537: Loss=0.7804 (C:0.7804, R:0.0106)
Batch 525/537: Loss=0.7688 (C:0.7688, R:0.0105)

============================================================
Epoch 29/100 completed in 30.7s
Train: Loss=0.7832 (C:0.7832, R:0.0105) Ratio=4.01x
Val:   Loss=0.7092 (C:0.7092, R:0.0105) Ratio=5.50x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7092)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7829 (C:0.7829, R:0.0105)
Batch  25/537: Loss=0.7852 (C:0.7852, R:0.0105)
Batch  50/537: Loss=0.7274 (C:0.7274, R:0.0105)
Batch  75/537: Loss=0.8116 (C:0.8116, R:0.0105)
Batch 100/537: Loss=0.7859 (C:0.7859, R:0.0105)
Batch 125/537: Loss=0.7478 (C:0.7478, R:0.0105)
Batch 150/537: Loss=0.7906 (C:0.7906, R:0.0105)
Batch 175/537: Loss=0.7575 (C:0.7575, R:0.0105)
Batch 200/537: Loss=0.7770 (C:0.7770, R:0.0105)
Batch 225/537: Loss=0.7916 (C:0.7916, R:0.0105)
Batch 250/537: Loss=0.7875 (C:0.7875, R:0.0105)
Batch 275/537: Loss=0.7848 (C:0.7848, R:0.0105)
Batch 300/537: Loss=0.7562 (C:0.7562, R:0.0106)
Batch 325/537: Loss=0.7940 (C:0.7940, R:0.0105)
Batch 350/537: Loss=0.7752 (C:0.7752, R:0.0105)
Batch 375/537: Loss=0.7870 (C:0.7870, R:0.0105)
Batch 400/537: Loss=0.7617 (C:0.7617, R:0.0105)
Batch 425/537: Loss=0.7730 (C:0.7730, R:0.0105)
Batch 450/537: Loss=0.7932 (C:0.7932, R:0.0105)
Batch 475/537: Loss=0.7928 (C:0.7928, R:0.0105)
Batch 500/537: Loss=0.8006 (C:0.8006, R:0.0105)
Batch 525/537: Loss=0.7992 (C:0.7992, R:0.0105)

============================================================
Epoch 30/100 completed in 30.8s
Train: Loss=0.7800 (C:0.7800, R:0.0105) Ratio=4.05x
Val:   Loss=0.7047 (C:0.7047, R:0.0105) Ratio=5.61x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.7047)
Checkpoint saved at epoch 30
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.403 ± 0.571
    Neg distances: 2.239 ± 0.991
    Separation ratio: 5.55x
    Gap: -3.973
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.7625 (C:0.7625, R:0.0105)
Batch  25/537: Loss=0.7683 (C:0.7683, R:0.0105)
Batch  50/537: Loss=0.7623 (C:0.7623, R:0.0105)
Batch  75/537: Loss=0.7811 (C:0.7811, R:0.0105)
Batch 100/537: Loss=0.7219 (C:0.7219, R:0.0105)
Batch 125/537: Loss=0.7572 (C:0.7572, R:0.0105)
Batch 150/537: Loss=0.7621 (C:0.7621, R:0.0105)
Batch 175/537: Loss=0.7563 (C:0.7563, R:0.0105)
Batch 200/537: Loss=0.8214 (C:0.8214, R:0.0105)
Batch 225/537: Loss=0.7212 (C:0.7212, R:0.0105)
Batch 250/537: Loss=0.7605 (C:0.7605, R:0.0105)
Batch 275/537: Loss=0.7828 (C:0.7828, R:0.0105)
Batch 300/537: Loss=0.7789 (C:0.7789, R:0.0105)
Batch 325/537: Loss=0.7616 (C:0.7616, R:0.0105)
Batch 350/537: Loss=0.7393 (C:0.7393, R:0.0105)
Batch 375/537: Loss=0.7625 (C:0.7625, R:0.0105)
Batch 400/537: Loss=0.7670 (C:0.7670, R:0.0105)
Batch 425/537: Loss=0.7467 (C:0.7467, R:0.0105)
Batch 450/537: Loss=0.7467 (C:0.7467, R:0.0105)
Batch 475/537: Loss=0.7633 (C:0.7633, R:0.0105)
Batch 500/537: Loss=0.7833 (C:0.7833, R:0.0105)
Batch 525/537: Loss=0.8033 (C:0.8033, R:0.0105)

============================================================
Epoch 31/100 completed in 37.3s
Train: Loss=0.7609 (C:0.7609, R:0.0105) Ratio=4.08x
Val:   Loss=0.6817 (C:0.6817, R:0.0105) Ratio=5.66x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.6817)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.7535 (C:0.7535, R:0.0105)
Batch  25/537: Loss=0.7666 (C:0.7666, R:0.0105)
Batch  50/537: Loss=0.7495 (C:0.7495, R:0.0105)
Batch  75/537: Loss=0.7554 (C:0.7554, R:0.0105)
Batch 100/537: Loss=0.8051 (C:0.8051, R:0.0105)
Batch 125/537: Loss=0.7513 (C:0.7513, R:0.0105)
Batch 150/537: Loss=0.7729 (C:0.7729, R:0.0105)
Batch 175/537: Loss=0.7824 (C:0.7824, R:0.0105)
Batch 200/537: Loss=0.7546 (C:0.7546, R:0.0105)
Batch 225/537: Loss=0.7410 (C:0.7410, R:0.0105)
Batch 250/537: Loss=0.7694 (C:0.7694, R:0.0106)
Batch 275/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 300/537: Loss=0.7513 (C:0.7513, R:0.0105)
Batch 325/537: Loss=0.7523 (C:0.7523, R:0.0105)
Batch 350/537: Loss=0.7392 (C:0.7392, R:0.0105)
Batch 375/537: Loss=0.7473 (C:0.7473, R:0.0105)
Batch 400/537: Loss=0.7739 (C:0.7739, R:0.0105)
Batch 425/537: Loss=0.7735 (C:0.7735, R:0.0105)
Batch 450/537: Loss=0.7327 (C:0.7327, R:0.0105)
Batch 475/537: Loss=0.7332 (C:0.7332, R:0.0105)
Batch 500/537: Loss=0.7422 (C:0.7422, R:0.0105)
Batch 525/537: Loss=0.7896 (C:0.7896, R:0.0105)

============================================================
Epoch 32/100 completed in 31.2s
Train: Loss=0.7583 (C:0.7583, R:0.0105) Ratio=4.15x
Val:   Loss=0.6775 (C:0.6775, R:0.0105) Ratio=5.78x
Reconstruction weight: 0.030
✅ New best model saved (Val Loss: 0.6775)
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.7447 (C:0.7447, R:0.0104)
Batch  25/537: Loss=0.7088 (C:0.7088, R:0.0105)
Batch  50/537: Loss=0.7763 (C:0.7763, R:0.0105)
Batch  75/537: Loss=0.7584 (C:0.7584, R:0.0105)
Batch 100/537: Loss=0.7783 (C:0.7783, R:0.0105)
Batch 125/537: Loss=0.7390 (C:0.7390, R:0.0105)
Batch 150/537: Loss=0.7254 (C:0.7254, R:0.0105)
Batch 175/537: Loss=0.7829 (C:0.7829, R:0.0105)
Batch 200/537: Loss=0.7659 (C:0.7659, R:0.0105)
Batch 225/537: Loss=0.7570 (C:0.7570, R:0.0105)
Batch 250/537: Loss=0.7357 (C:0.7357, R:0.0105)
Batch 275/537: Loss=0.7277 (C:0.7277, R:0.0105)
Batch 300/537: Loss=0.7263 (C:0.7263, R:0.0105)
Batch 325/537: Loss=0.7496 (C:0.7496, R:0.0105)
Batch 350/537: Loss=0.7795 (C:0.7795, R:0.0105)
Batch 375/537: Loss=0.7631 (C:0.7631, R:0.0105)
Batch 400/537: Loss=0.7957 (C:0.7957, R:0.0105)
Batch 425/537: Loss=0.7687 (C:0.7687, R:0.0105)
Batch 450/537: Loss=0.7633 (C:0.7633, R:0.0105)
Batch 475/537: Loss=0.7559 (C:0.7559, R:0.0105)
Batch 500/537: Loss=0.7423 (C:0.7423, R:0.0105)
Batch 525/537: Loss=0.7571 (C:0.7571, R:0.0105)

============================================================
Epoch 33/100 completed in 30.9s
Train: Loss=0.7566 (C:0.7566, R:0.0105) Ratio=4.16x
Val:   Loss=0.6738 (C:0.6738, R:0.0105) Ratio=5.87x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.6738)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.384 ± 0.546
    Neg distances: 2.305 ± 1.007
    Separation ratio: 6.00x
    Gap: -3.922
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.6862 (C:0.6862, R:0.0105)
Batch  25/537: Loss=0.7051 (C:0.7051, R:0.0105)
Batch  50/537: Loss=0.6983 (C:0.6983, R:0.0105)
Batch  75/537: Loss=0.7060 (C:0.7060, R:0.0105)
Batch 100/537: Loss=0.7096 (C:0.7096, R:0.0105)
Batch 125/537: Loss=0.7096 (C:0.7096, R:0.0105)
Batch 150/537: Loss=0.6952 (C:0.6952, R:0.0105)
Batch 175/537: Loss=0.7171 (C:0.7171, R:0.0105)
Batch 200/537: Loss=0.7367 (C:0.7367, R:0.0105)
Batch 225/537: Loss=0.7021 (C:0.7021, R:0.0105)
Batch 250/537: Loss=0.7089 (C:0.7089, R:0.0105)
Batch 275/537: Loss=0.7334 (C:0.7334, R:0.0105)
Batch 300/537: Loss=0.7583 (C:0.7583, R:0.0105)
Batch 325/537: Loss=0.7190 (C:0.7190, R:0.0105)
Batch 350/537: Loss=0.7286 (C:0.7286, R:0.0105)
Batch 375/537: Loss=0.6908 (C:0.6908, R:0.0105)
Batch 400/537: Loss=0.7288 (C:0.7288, R:0.0105)
Batch 425/537: Loss=0.7399 (C:0.7399, R:0.0105)
Batch 450/537: Loss=0.7614 (C:0.7614, R:0.0105)
Batch 475/537: Loss=0.7561 (C:0.7561, R:0.0105)
Batch 500/537: Loss=0.7330 (C:0.7330, R:0.0105)
Batch 525/537: Loss=0.7422 (C:0.7422, R:0.0105)

============================================================
Epoch 34/100 completed in 37.1s
Train: Loss=0.7237 (C:0.7237, R:0.0105) Ratio=4.24x
Val:   Loss=0.6384 (C:0.6384, R:0.0105) Ratio=5.94x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.6384)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.7074 (C:0.7074, R:0.0105)
Batch  25/537: Loss=0.7224 (C:0.7224, R:0.0105)
Batch  50/537: Loss=0.6885 (C:0.6885, R:0.0105)
Batch  75/537: Loss=0.7160 (C:0.7160, R:0.0105)
Batch 100/537: Loss=0.7234 (C:0.7234, R:0.0105)
Batch 125/537: Loss=0.7395 (C:0.7395, R:0.0106)
Batch 150/537: Loss=0.7111 (C:0.7111, R:0.0105)
Batch 175/537: Loss=0.7010 (C:0.7010, R:0.0105)
Batch 200/537: Loss=0.7489 (C:0.7489, R:0.0105)
Batch 225/537: Loss=0.7241 (C:0.7241, R:0.0105)
Batch 250/537: Loss=0.6950 (C:0.6950, R:0.0105)
Batch 275/537: Loss=0.7331 (C:0.7331, R:0.0105)
Batch 300/537: Loss=0.7093 (C:0.7093, R:0.0105)
Batch 325/537: Loss=0.7461 (C:0.7461, R:0.0105)
Batch 350/537: Loss=0.7237 (C:0.7237, R:0.0105)
Batch 375/537: Loss=0.7509 (C:0.7509, R:0.0105)
Batch 400/537: Loss=0.7171 (C:0.7171, R:0.0105)
Batch 425/537: Loss=0.7236 (C:0.7236, R:0.0105)
Batch 450/537: Loss=0.7580 (C:0.7580, R:0.0105)
Batch 475/537: Loss=0.7158 (C:0.7158, R:0.0105)
Batch 500/537: Loss=0.7217 (C:0.7217, R:0.0105)
Batch 525/537: Loss=0.7356 (C:0.7356, R:0.0105)

============================================================
Epoch 35/100 completed in 30.6s
Train: Loss=0.7204 (C:0.7204, R:0.0105) Ratio=4.24x
Val:   Loss=0.6358 (C:0.6358, R:0.0105) Ratio=5.98x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 0.6358)
Checkpoint saved at epoch 35
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.7236 (C:0.7236, R:0.0105)
Batch  25/537: Loss=0.7109 (C:0.7109, R:0.0105)
Batch  50/537: Loss=0.7201 (C:0.7201, R:0.0105)
Batch  75/537: Loss=0.6806 (C:0.6806, R:0.0105)
Batch 100/537: Loss=0.6816 (C:0.6816, R:0.0105)
Batch 125/537: Loss=0.7551 (C:0.7551, R:0.0105)
Batch 150/537: Loss=0.7379 (C:0.7379, R:0.0105)
Batch 175/537: Loss=0.7204 (C:0.7204, R:0.0105)
Batch 200/537: Loss=0.7455 (C:0.7455, R:0.0105)
Batch 225/537: Loss=0.7415 (C:0.7415, R:0.0105)
Batch 250/537: Loss=0.7082 (C:0.7082, R:0.0105)
Batch 275/537: Loss=0.6857 (C:0.6857, R:0.0105)
Batch 300/537: Loss=0.7539 (C:0.7539, R:0.0105)
Batch 325/537: Loss=0.7104 (C:0.7104, R:0.0106)
Batch 350/537: Loss=0.7223 (C:0.7223, R:0.0105)
Batch 375/537: Loss=0.7365 (C:0.7365, R:0.0105)
Batch 400/537: Loss=0.7355 (C:0.7355, R:0.0105)
Batch 425/537: Loss=0.7262 (C:0.7262, R:0.0105)
Batch 450/537: Loss=0.7267 (C:0.7267, R:0.0105)
Batch 475/537: Loss=0.7448 (C:0.7448, R:0.0105)
Batch 500/537: Loss=0.7438 (C:0.7438, R:0.0105)
Batch 525/537: Loss=0.7138 (C:0.7138, R:0.0105)

============================================================
Epoch 36/100 completed in 30.5s
Train: Loss=0.7181 (C:0.7181, R:0.0105) Ratio=4.24x
Val:   Loss=0.6302 (C:0.6302, R:0.0105) Ratio=6.17x
Reconstruction weight: 0.090
✅ New best model saved (Val Loss: 0.6302)
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.381 ± 0.568
    Neg distances: 2.390 ± 1.036
    Separation ratio: 6.28x
    Gap: -4.077
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.7000 (C:0.7000, R:0.0105)
Batch  25/537: Loss=0.6688 (C:0.6688, R:0.0105)
Batch  50/537: Loss=0.7293 (C:0.7293, R:0.0106)
Batch  75/537: Loss=0.7022 (C:0.7022, R:0.0105)
Batch 100/537: Loss=0.6709 (C:0.6709, R:0.0105)
Batch 125/537: Loss=0.7065 (C:0.7065, R:0.0105)
Batch 150/537: Loss=0.7404 (C:0.7404, R:0.0105)
Batch 175/537: Loss=0.6866 (C:0.6866, R:0.0105)
Batch 200/537: Loss=0.6851 (C:0.6851, R:0.0105)
Batch 225/537: Loss=0.6553 (C:0.6553, R:0.0106)
Batch 250/537: Loss=0.7395 (C:0.7395, R:0.0105)
Batch 275/537: Loss=0.6921 (C:0.6921, R:0.0105)
Batch 300/537: Loss=0.6889 (C:0.6889, R:0.0105)
Batch 325/537: Loss=0.6773 (C:0.6773, R:0.0105)
Batch 350/537: Loss=0.7020 (C:0.7020, R:0.0105)
Batch 375/537: Loss=0.7101 (C:0.7101, R:0.0105)
Batch 400/537: Loss=0.7001 (C:0.7001, R:0.0105)
Batch 425/537: Loss=0.6972 (C:0.6972, R:0.0105)
Batch 450/537: Loss=0.7224 (C:0.7224, R:0.0105)
Batch 475/537: Loss=0.7283 (C:0.7283, R:0.0105)
Batch 500/537: Loss=0.7133 (C:0.7133, R:0.0105)
Batch 525/537: Loss=0.7045 (C:0.7045, R:0.0105)

============================================================
Epoch 37/100 completed in 37.4s
Train: Loss=0.6984 (C:0.6984, R:0.0105) Ratio=4.27x
Val:   Loss=0.6044 (C:0.6044, R:0.0105) Ratio=6.22x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.6044)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.6887 (C:0.6887, R:0.0105)
Batch  25/537: Loss=0.6698 (C:0.6698, R:0.0105)
Batch  50/537: Loss=0.6758 (C:0.6758, R:0.0105)
Batch  75/537: Loss=0.6727 (C:0.6727, R:0.0105)
Batch 100/537: Loss=0.7056 (C:0.7056, R:0.0105)
Batch 125/537: Loss=0.6682 (C:0.6682, R:0.0106)
Batch 150/537: Loss=0.6837 (C:0.6837, R:0.0105)
Batch 175/537: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 200/537: Loss=0.7175 (C:0.7175, R:0.0105)
Batch 225/537: Loss=0.6944 (C:0.6944, R:0.0105)
Batch 250/537: Loss=0.7131 (C:0.7131, R:0.0105)
Batch 275/537: Loss=0.6563 (C:0.6563, R:0.0105)
Batch 300/537: Loss=0.6870 (C:0.6870, R:0.0106)
Batch 325/537: Loss=0.6988 (C:0.6988, R:0.0105)
Batch 350/537: Loss=0.6760 (C:0.6760, R:0.0105)
Batch 375/537: Loss=0.6910 (C:0.6910, R:0.0106)
Batch 400/537: Loss=0.7031 (C:0.7031, R:0.0106)
Batch 425/537: Loss=0.6762 (C:0.6762, R:0.0105)
Batch 450/537: Loss=0.7007 (C:0.7007, R:0.0105)
Batch 475/537: Loss=0.6897 (C:0.6897, R:0.0105)
Batch 500/537: Loss=0.7088 (C:0.7088, R:0.0105)
Batch 525/537: Loss=0.7082 (C:0.7082, R:0.0105)

============================================================
Epoch 38/100 completed in 32.3s
Train: Loss=0.6932 (C:0.6932, R:0.0105) Ratio=4.38x
Val:   Loss=0.6005 (C:0.6005, R:0.0105) Ratio=6.35x
Reconstruction weight: 0.120
✅ New best model saved (Val Loss: 0.6005)
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.6591 (C:0.6591, R:0.0105)
Batch  25/537: Loss=0.6710 (C:0.6710, R:0.0105)
Batch  50/537: Loss=0.7005 (C:0.7005, R:0.0105)
Batch  75/537: Loss=0.6937 (C:0.6937, R:0.0105)
Batch 100/537: Loss=0.6478 (C:0.6478, R:0.0105)
Batch 125/537: Loss=0.6898 (C:0.6898, R:0.0105)
Batch 150/537: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 175/537: Loss=0.7197 (C:0.7197, R:0.0105)
Batch 200/537: Loss=0.6990 (C:0.6990, R:0.0105)
Batch 225/537: Loss=0.7201 (C:0.7201, R:0.0105)
Batch 250/537: Loss=0.6828 (C:0.6828, R:0.0105)
Batch 275/537: Loss=0.6939 (C:0.6939, R:0.0105)
Batch 300/537: Loss=0.7159 (C:0.7159, R:0.0105)
Batch 325/537: Loss=0.6812 (C:0.6812, R:0.0105)
Batch 350/537: Loss=0.6886 (C:0.6886, R:0.0105)
Batch 375/537: Loss=0.6823 (C:0.6823, R:0.0105)
Batch 400/537: Loss=0.7058 (C:0.7058, R:0.0105)
Batch 425/537: Loss=0.7108 (C:0.7108, R:0.0105)
Batch 450/537: Loss=0.6792 (C:0.6792, R:0.0105)
Batch 475/537: Loss=0.6810 (C:0.6810, R:0.0105)
Batch 500/537: Loss=0.7067 (C:0.7067, R:0.0105)
Batch 525/537: Loss=0.6833 (C:0.6833, R:0.0105)

============================================================
Epoch 39/100 completed in 32.7s
Train: Loss=0.6917 (C:0.6917, R:0.0105) Ratio=4.39x
Val:   Loss=0.5968 (C:0.5968, R:0.0105) Ratio=6.42x
Reconstruction weight: 0.135
✅ New best model saved (Val Loss: 0.5968)
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.374 ± 0.575
    Neg distances: 2.433 ± 1.044
    Separation ratio: 6.50x
    Gap: -4.097
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.6599 (C:0.6599, R:0.0106)
Batch  25/537: Loss=0.6757 (C:0.6757, R:0.0105)
Batch  50/537: Loss=0.6471 (C:0.6471, R:0.0105)
Batch  75/537: Loss=0.6239 (C:0.6239, R:0.0105)
Batch 100/537: Loss=0.6818 (C:0.6818, R:0.0105)
Batch 125/537: Loss=0.6484 (C:0.6484, R:0.0105)
Batch 150/537: Loss=0.6513 (C:0.6513, R:0.0105)
Batch 175/537: Loss=0.6717 (C:0.6717, R:0.0105)
Batch 200/537: Loss=0.6711 (C:0.6711, R:0.0105)
Batch 225/537: Loss=0.6717 (C:0.6717, R:0.0105)
Batch 250/537: Loss=0.7171 (C:0.7171, R:0.0105)
Batch 275/537: Loss=0.6491 (C:0.6491, R:0.0105)
Batch 300/537: Loss=0.7012 (C:0.7012, R:0.0105)
Batch 325/537: Loss=0.6547 (C:0.6547, R:0.0105)
Batch 350/537: Loss=0.7096 (C:0.7096, R:0.0105)
Batch 375/537: Loss=0.6717 (C:0.6717, R:0.0105)
Batch 400/537: Loss=0.7062 (C:0.7062, R:0.0105)
Batch 425/537: Loss=0.6843 (C:0.6843, R:0.0105)
Batch 450/537: Loss=0.6719 (C:0.6719, R:0.0105)
Batch 475/537: Loss=0.7062 (C:0.7062, R:0.0105)
Batch 500/537: Loss=0.6820 (C:0.6820, R:0.0105)
Batch 525/537: Loss=0.7182 (C:0.7182, R:0.0105)

============================================================
Epoch 40/100 completed in 39.1s
Train: Loss=0.6743 (C:0.6743, R:0.0105) Ratio=4.42x
Val:   Loss=0.5753 (C:0.5753, R:0.0105) Ratio=6.48x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.5753)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6430 (C:0.6430, R:0.0105)
Batch  25/537: Loss=0.6737 (C:0.6737, R:0.0105)
Batch  50/537: Loss=0.6623 (C:0.6623, R:0.0105)
Batch  75/537: Loss=0.6606 (C:0.6606, R:0.0105)
Batch 100/537: Loss=0.6823 (C:0.6823, R:0.0105)
Batch 125/537: Loss=0.7059 (C:0.7059, R:0.0105)
Batch 150/537: Loss=0.6262 (C:0.6262, R:0.0105)
Batch 175/537: Loss=0.6802 (C:0.6802, R:0.0105)
Batch 200/537: Loss=0.6598 (C:0.6598, R:0.0105)
Batch 225/537: Loss=0.6627 (C:0.6627, R:0.0105)
Batch 250/537: Loss=0.6558 (C:0.6558, R:0.0106)
Batch 275/537: Loss=0.6446 (C:0.6446, R:0.0105)
Batch 300/537: Loss=0.6709 (C:0.6709, R:0.0105)
Batch 325/537: Loss=0.7065 (C:0.7065, R:0.0105)
Batch 350/537: Loss=0.6761 (C:0.6761, R:0.0105)
Batch 375/537: Loss=0.6738 (C:0.6738, R:0.0105)
Batch 400/537: Loss=0.6871 (C:0.6871, R:0.0105)
Batch 425/537: Loss=0.6645 (C:0.6645, R:0.0105)
Batch 450/537: Loss=0.6967 (C:0.6967, R:0.0105)
Batch 475/537: Loss=0.6318 (C:0.6318, R:0.0105)
Batch 500/537: Loss=0.6788 (C:0.6788, R:0.0105)
Batch 525/537: Loss=0.6648 (C:0.6648, R:0.0105)

============================================================
Epoch 41/100 completed in 31.9s
Train: Loss=0.6694 (C:0.6694, R:0.0105) Ratio=4.52x
Val:   Loss=0.5729 (C:0.5729, R:0.0105) Ratio=6.58x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.5729)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.6074 (C:0.6074, R:0.0105)
Batch  25/537: Loss=0.6509 (C:0.6509, R:0.0105)
Batch  50/537: Loss=0.6463 (C:0.6463, R:0.0105)
Batch  75/537: Loss=0.6878 (C:0.6878, R:0.0105)
Batch 100/537: Loss=0.6776 (C:0.6776, R:0.0105)
Batch 125/537: Loss=0.6616 (C:0.6616, R:0.0105)
Batch 150/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch 175/537: Loss=0.6736 (C:0.6736, R:0.0105)
Batch 200/537: Loss=0.6731 (C:0.6731, R:0.0105)
Batch 225/537: Loss=0.7249 (C:0.7249, R:0.0105)
Batch 250/537: Loss=0.6543 (C:0.6543, R:0.0105)
Batch 275/537: Loss=0.6730 (C:0.6730, R:0.0105)
Batch 300/537: Loss=0.6443 (C:0.6443, R:0.0105)
Batch 325/537: Loss=0.6805 (C:0.6805, R:0.0105)
Batch 350/537: Loss=0.6970 (C:0.6970, R:0.0105)
Batch 375/537: Loss=0.6697 (C:0.6697, R:0.0105)
Batch 400/537: Loss=0.7155 (C:0.7155, R:0.0105)
Batch 425/537: Loss=0.6811 (C:0.6811, R:0.0105)
Batch 450/537: Loss=0.6885 (C:0.6885, R:0.0105)
Batch 475/537: Loss=0.6815 (C:0.6815, R:0.0105)
Batch 500/537: Loss=0.6935 (C:0.6935, R:0.0105)
Batch 525/537: Loss=0.6809 (C:0.6809, R:0.0105)

============================================================
Epoch 42/100 completed in 32.8s
Train: Loss=0.6683 (C:0.6683, R:0.0105) Ratio=4.51x
Val:   Loss=0.5674 (C:0.5674, R:0.0105) Ratio=6.75x
Reconstruction weight: 0.180
✅ New best model saved (Val Loss: 0.5674)
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.358 ± 0.571
    Neg distances: 2.469 ± 1.055
    Separation ratio: 6.89x
    Gap: -4.152
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.6257 (C:0.6257, R:0.0105)
Batch  25/537: Loss=0.6453 (C:0.6453, R:0.0105)
Batch  50/537: Loss=0.6492 (C:0.6492, R:0.0105)
Batch  75/537: Loss=0.6409 (C:0.6409, R:0.0106)
Batch 100/537: Loss=0.6335 (C:0.6335, R:0.0105)
Batch 125/537: Loss=0.6271 (C:0.6271, R:0.0106)
Batch 150/537: Loss=0.6560 (C:0.6560, R:0.0105)
Batch 175/537: Loss=0.6858 (C:0.6858, R:0.0105)
Batch 200/537: Loss=0.6419 (C:0.6419, R:0.0105)
Batch 225/537: Loss=0.6202 (C:0.6202, R:0.0105)
Batch 250/537: Loss=0.6425 (C:0.6425, R:0.0105)
Batch 275/537: Loss=0.6298 (C:0.6298, R:0.0105)
Batch 300/537: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 325/537: Loss=0.6412 (C:0.6412, R:0.0105)
Batch 350/537: Loss=0.6616 (C:0.6616, R:0.0105)
Batch 375/537: Loss=0.6532 (C:0.6532, R:0.0105)
Batch 400/537: Loss=0.6488 (C:0.6488, R:0.0105)
Batch 425/537: Loss=0.5777 (C:0.5777, R:0.0105)
Batch 450/537: Loss=0.6421 (C:0.6421, R:0.0105)
Batch 475/537: Loss=0.6670 (C:0.6670, R:0.0105)
Batch 500/537: Loss=0.6394 (C:0.6394, R:0.0105)
Batch 525/537: Loss=0.6287 (C:0.6287, R:0.0105)

============================================================
Epoch 43/100 completed in 38.8s
Train: Loss=0.6491 (C:0.6491, R:0.0105) Ratio=4.59x
Val:   Loss=0.5460 (C:0.5460, R:0.0105) Ratio=6.80x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.5460)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.6216 (C:0.6216, R:0.0105)
Batch  25/537: Loss=0.6565 (C:0.6565, R:0.0105)
Batch  50/537: Loss=0.6585 (C:0.6585, R:0.0105)
Batch  75/537: Loss=0.6782 (C:0.6782, R:0.0105)
Batch 100/537: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 125/537: Loss=0.6571 (C:0.6571, R:0.0105)
Batch 150/537: Loss=0.6651 (C:0.6651, R:0.0105)
Batch 175/537: Loss=0.6467 (C:0.6467, R:0.0105)
Batch 200/537: Loss=0.6358 (C:0.6358, R:0.0105)
Batch 225/537: Loss=0.6054 (C:0.6054, R:0.0105)
Batch 250/537: Loss=0.6448 (C:0.6448, R:0.0105)
Batch 275/537: Loss=0.6603 (C:0.6603, R:0.0105)
Batch 300/537: Loss=0.6387 (C:0.6387, R:0.0105)
Batch 325/537: Loss=0.6693 (C:0.6693, R:0.0105)
Batch 350/537: Loss=0.6266 (C:0.6266, R:0.0105)
Batch 375/537: Loss=0.6509 (C:0.6509, R:0.0105)
Batch 400/537: Loss=0.6590 (C:0.6590, R:0.0105)
Batch 425/537: Loss=0.6799 (C:0.6799, R:0.0105)
Batch 450/537: Loss=0.6871 (C:0.6871, R:0.0105)
Batch 475/537: Loss=0.6589 (C:0.6589, R:0.0105)
Batch 500/537: Loss=0.6396 (C:0.6396, R:0.0105)
Batch 525/537: Loss=0.6685 (C:0.6685, R:0.0105)

============================================================
Epoch 44/100 completed in 31.4s
Train: Loss=0.6452 (C:0.6452, R:0.0105) Ratio=4.50x
Val:   Loss=0.5440 (C:0.5440, R:0.0105) Ratio=6.88x
Reconstruction weight: 0.210
✅ New best model saved (Val Loss: 0.5440)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.6306 (C:0.6306, R:0.0105)
Batch  25/537: Loss=0.6392 (C:0.6392, R:0.0105)
Batch  50/537: Loss=0.6187 (C:0.6187, R:0.0106)
Batch  75/537: Loss=0.6094 (C:0.6094, R:0.0106)
Batch 100/537: Loss=0.6517 (C:0.6517, R:0.0105)
Batch 125/537: Loss=0.6336 (C:0.6336, R:0.0105)
Batch 150/537: Loss=0.6453 (C:0.6453, R:0.0105)
Batch 175/537: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 200/537: Loss=0.6282 (C:0.6282, R:0.0106)
Batch 225/537: Loss=0.6777 (C:0.6777, R:0.0105)
Batch 250/537: Loss=0.6424 (C:0.6424, R:0.0105)
Batch 275/537: Loss=0.6500 (C:0.6500, R:0.0105)
Batch 300/537: Loss=0.6847 (C:0.6847, R:0.0106)
Batch 325/537: Loss=0.6704 (C:0.6704, R:0.0105)
Batch 350/537: Loss=0.6265 (C:0.6265, R:0.0105)
Batch 375/537: Loss=0.6374 (C:0.6374, R:0.0105)
Batch 400/537: Loss=0.6609 (C:0.6609, R:0.0105)
Batch 425/537: Loss=0.6443 (C:0.6443, R:0.0105)
Batch 450/537: Loss=0.6550 (C:0.6550, R:0.0105)
Batch 475/537: Loss=0.6619 (C:0.6619, R:0.0105)
Batch 500/537: Loss=0.6537 (C:0.6537, R:0.0105)
Batch 525/537: Loss=0.6705 (C:0.6705, R:0.0105)

============================================================
Epoch 45/100 completed in 32.2s
Train: Loss=0.6440 (C:0.6440, R:0.0105) Ratio=4.53x
Val:   Loss=0.5438 (C:0.5438, R:0.0105) Ratio=6.95x
Reconstruction weight: 0.225
✅ New best model saved (Val Loss: 0.5438)
Checkpoint saved at epoch 45
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.377 ± 0.595
    Neg distances: 2.484 ± 1.068
    Separation ratio: 6.59x
    Gap: -4.178
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.6915 (C:0.6915, R:0.0105)
Batch  25/537: Loss=0.6311 (C:0.6311, R:0.0105)
Batch  50/537: Loss=0.6592 (C:0.6592, R:0.0105)
Batch  75/537: Loss=0.6702 (C:0.6702, R:0.0105)
Batch 100/537: Loss=0.6593 (C:0.6593, R:0.0105)
Batch 125/537: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 150/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 175/537: Loss=0.6627 (C:0.6627, R:0.0105)
Batch 200/537: Loss=0.6419 (C:0.6419, R:0.0105)
Batch 225/537: Loss=0.6840 (C:0.6840, R:0.0105)
Batch 250/537: Loss=0.6324 (C:0.6324, R:0.0105)
Batch 275/537: Loss=0.6214 (C:0.6214, R:0.0105)
Batch 300/537: Loss=0.6835 (C:0.6835, R:0.0105)
Batch 325/537: Loss=0.6458 (C:0.6458, R:0.0105)
Batch 350/537: Loss=0.6663 (C:0.6663, R:0.0105)
Batch 375/537: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 400/537: Loss=0.6470 (C:0.6470, R:0.0105)
Batch 425/537: Loss=0.6756 (C:0.6756, R:0.0105)
Batch 450/537: Loss=0.6760 (C:0.6760, R:0.0105)
Batch 475/537: Loss=0.6732 (C:0.6732, R:0.0105)
Batch 500/537: Loss=0.6378 (C:0.6378, R:0.0105)
Batch 525/537: Loss=0.6618 (C:0.6618, R:0.0105)

============================================================
Epoch 46/100 completed in 39.8s
Train: Loss=0.6508 (C:0.6508, R:0.0105) Ratio=4.51x
Val:   Loss=0.5457 (C:0.5457, R:0.0105) Ratio=7.02x
Reconstruction weight: 0.240
No improvement for 1 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.6393 (C:0.6393, R:0.0105)
Batch  25/537: Loss=0.6527 (C:0.6527, R:0.0105)
Batch  50/537: Loss=0.6093 (C:0.6093, R:0.0105)
Batch  75/537: Loss=0.6404 (C:0.6404, R:0.0105)
Batch 100/537: Loss=0.6361 (C:0.6361, R:0.0105)
Batch 125/537: Loss=0.6078 (C:0.6078, R:0.0105)
Batch 150/537: Loss=0.6514 (C:0.6514, R:0.0106)
Batch 175/537: Loss=0.6324 (C:0.6324, R:0.0105)
Batch 200/537: Loss=0.6628 (C:0.6628, R:0.0105)
Batch 225/537: Loss=0.6340 (C:0.6340, R:0.0105)
Batch 250/537: Loss=0.6695 (C:0.6695, R:0.0105)
Batch 275/537: Loss=0.6412 (C:0.6412, R:0.0105)
Batch 300/537: Loss=0.6352 (C:0.6352, R:0.0105)
Batch 325/537: Loss=0.5993 (C:0.5993, R:0.0106)
Batch 350/537: Loss=0.6547 (C:0.6547, R:0.0105)
Batch 375/537: Loss=0.6459 (C:0.6459, R:0.0105)
Batch 400/537: Loss=0.6496 (C:0.6496, R:0.0105)
Batch 425/537: Loss=0.6962 (C:0.6962, R:0.0105)
Batch 450/537: Loss=0.6726 (C:0.6726, R:0.0105)
Batch 475/537: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 500/537: Loss=0.6622 (C:0.6622, R:0.0105)
Batch 525/537: Loss=0.6645 (C:0.6645, R:0.0105)

============================================================
Epoch 47/100 completed in 32.1s
Train: Loss=0.6483 (C:0.6483, R:0.0105) Ratio=4.69x
Val:   Loss=0.5427 (C:0.5427, R:0.0105) Ratio=7.08x
Reconstruction weight: 0.255
✅ New best model saved (Val Loss: 0.5427)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.6306 (C:0.6306, R:0.0105)
Batch  25/537: Loss=0.6288 (C:0.6288, R:0.0105)
Batch  50/537: Loss=0.6304 (C:0.6304, R:0.0105)
Batch  75/537: Loss=0.5968 (C:0.5968, R:0.0105)
Batch 100/537: Loss=0.6478 (C:0.6478, R:0.0105)
Batch 125/537: Loss=0.6678 (C:0.6678, R:0.0105)
Batch 150/537: Loss=0.6106 (C:0.6106, R:0.0105)
Batch 175/537: Loss=0.6371 (C:0.6371, R:0.0105)
Batch 200/537: Loss=0.6832 (C:0.6832, R:0.0105)
Batch 225/537: Loss=0.6188 (C:0.6188, R:0.0105)
Batch 250/537: Loss=0.6455 (C:0.6455, R:0.0105)
Batch 275/537: Loss=0.6301 (C:0.6301, R:0.0105)
Batch 300/537: Loss=0.6489 (C:0.6489, R:0.0105)
Batch 325/537: Loss=0.6370 (C:0.6370, R:0.0105)
Batch 350/537: Loss=0.6883 (C:0.6883, R:0.0105)
Batch 375/537: Loss=0.6389 (C:0.6389, R:0.0105)
Batch 400/537: Loss=0.6327 (C:0.6327, R:0.0105)
Batch 425/537: Loss=0.6325 (C:0.6325, R:0.0105)
Batch 450/537: Loss=0.6411 (C:0.6411, R:0.0105)
Batch 475/537: Loss=0.6576 (C:0.6576, R:0.0105)
Batch 500/537: Loss=0.6276 (C:0.6276, R:0.0105)
Batch 525/537: Loss=0.6207 (C:0.6207, R:0.0105)

============================================================
Epoch 48/100 completed in 32.1s
Train: Loss=0.6472 (C:0.6472, R:0.0105) Ratio=4.70x
Val:   Loss=0.5409 (C:0.5409, R:0.0105) Ratio=7.19x
Reconstruction weight: 0.270
✅ New best model saved (Val Loss: 0.5409)
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.341 ± 0.559
    Neg distances: 2.536 ± 1.062
    Separation ratio: 7.44x
    Gap: -4.296
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.5958 (C:0.5958, R:0.0105)
Batch  25/537: Loss=0.6111 (C:0.6111, R:0.0105)
Batch  50/537: Loss=0.6202 (C:0.6202, R:0.0105)
Batch  75/537: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 100/537: Loss=0.6165 (C:0.6165, R:0.0105)
Batch 125/537: Loss=0.6255 (C:0.6255, R:0.0105)
Batch 150/537: Loss=0.5899 (C:0.5899, R:0.0105)
Batch 175/537: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 200/537: Loss=0.5930 (C:0.5930, R:0.0105)
Batch 225/537: Loss=0.6331 (C:0.6331, R:0.0105)
Batch 250/537: Loss=0.6066 (C:0.6066, R:0.0105)
Batch 275/537: Loss=0.6018 (C:0.6018, R:0.0105)
Batch 300/537: Loss=0.6152 (C:0.6152, R:0.0105)
Batch 325/537: Loss=0.6582 (C:0.6582, R:0.0105)
Batch 350/537: Loss=0.6135 (C:0.6135, R:0.0105)
Batch 375/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 400/537: Loss=0.6283 (C:0.6283, R:0.0105)
Batch 425/537: Loss=0.5882 (C:0.5882, R:0.0105)
Batch 450/537: Loss=0.6251 (C:0.6251, R:0.0105)
Batch 475/537: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 500/537: Loss=0.5986 (C:0.5986, R:0.0105)
Batch 525/537: Loss=0.5971 (C:0.5971, R:0.0105)

============================================================
Epoch 49/100 completed in 38.7s
Train: Loss=0.6108 (C:0.6108, R:0.0105) Ratio=4.78x
Val:   Loss=0.5021 (C:0.5021, R:0.0105) Ratio=7.29x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.5021)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.5822 (C:0.5822, R:0.0105)
Batch  25/537: Loss=0.6136 (C:0.6136, R:0.0105)
Batch  50/537: Loss=0.5910 (C:0.5910, R:0.0105)
Batch  75/537: Loss=0.6009 (C:0.6009, R:0.0105)
Batch 100/537: Loss=0.6137 (C:0.6137, R:0.0105)
Batch 125/537: Loss=0.6402 (C:0.6402, R:0.0105)
Batch 150/537: Loss=0.5793 (C:0.5793, R:0.0105)
Batch 175/537: Loss=0.5898 (C:0.5898, R:0.0105)
Batch 200/537: Loss=0.6180 (C:0.6180, R:0.0105)
Batch 225/537: Loss=0.6470 (C:0.6470, R:0.0105)
Batch 250/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch 275/537: Loss=0.6132 (C:0.6132, R:0.0105)
Batch 300/537: Loss=0.5924 (C:0.5924, R:0.0105)
Batch 325/537: Loss=0.6210 (C:0.6210, R:0.0106)
Batch 350/537: Loss=0.6223 (C:0.6223, R:0.0105)
Batch 375/537: Loss=0.6084 (C:0.6084, R:0.0105)
Batch 400/537: Loss=0.6161 (C:0.6161, R:0.0105)
Batch 425/537: Loss=0.6267 (C:0.6267, R:0.0105)
Batch 450/537: Loss=0.6321 (C:0.6321, R:0.0105)
Batch 475/537: Loss=0.5883 (C:0.5883, R:0.0105)
Batch 500/537: Loss=0.6128 (C:0.6128, R:0.0105)
Batch 525/537: Loss=0.6184 (C:0.6184, R:0.0105)

============================================================
Epoch 50/100 completed in 32.3s
Train: Loss=0.6098 (C:0.6098, R:0.0105) Ratio=4.75x
Val:   Loss=0.4996 (C:0.4996, R:0.0105) Ratio=7.33x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4996)
Checkpoint saved at epoch 50
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.6062 (C:0.6062, R:0.0105)
Batch  25/537: Loss=0.6245 (C:0.6245, R:0.0105)
Batch  50/537: Loss=0.6141 (C:0.6141, R:0.0105)
Batch  75/537: Loss=0.5794 (C:0.5794, R:0.0105)
Batch 100/537: Loss=0.5950 (C:0.5950, R:0.0106)
Batch 125/537: Loss=0.6140 (C:0.6140, R:0.0105)
Batch 150/537: Loss=0.6199 (C:0.6199, R:0.0105)
Batch 175/537: Loss=0.6073 (C:0.6073, R:0.0105)
Batch 200/537: Loss=0.6278 (C:0.6278, R:0.0105)
Batch 225/537: Loss=0.5492 (C:0.5492, R:0.0106)
Batch 250/537: Loss=0.5985 (C:0.5985, R:0.0105)
Batch 275/537: Loss=0.6203 (C:0.6203, R:0.0105)
Batch 300/537: Loss=0.6165 (C:0.6165, R:0.0105)
Batch 325/537: Loss=0.6732 (C:0.6732, R:0.0105)
Batch 350/537: Loss=0.6070 (C:0.6070, R:0.0105)
Batch 375/537: Loss=0.6124 (C:0.6124, R:0.0105)
Batch 400/537: Loss=0.5953 (C:0.5953, R:0.0105)
Batch 425/537: Loss=0.6404 (C:0.6404, R:0.0105)
Batch 450/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 475/537: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 500/537: Loss=0.6061 (C:0.6061, R:0.0105)
Batch 525/537: Loss=0.6288 (C:0.6288, R:0.0105)

============================================================
Epoch 51/100 completed in 31.4s
Train: Loss=0.6077 (C:0.6077, R:0.0105) Ratio=4.72x
Val:   Loss=0.4998 (C:0.4998, R:0.0105) Ratio=7.41x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.351 ± 0.570
    Neg distances: 2.554 ± 1.078
    Separation ratio: 7.29x
    Gap: -4.272
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.6011 (C:0.6011, R:0.0106)
Batch  25/537: Loss=0.6052 (C:0.6052, R:0.0106)
Batch  50/537: Loss=0.5546 (C:0.5546, R:0.0105)
Batch  75/537: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 100/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch 125/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch 150/537: Loss=0.5865 (C:0.5865, R:0.0105)
Batch 175/537: Loss=0.6355 (C:0.6355, R:0.0105)
Batch 200/537: Loss=0.6357 (C:0.6357, R:0.0105)
Batch 225/537: Loss=0.6253 (C:0.6253, R:0.0105)
Batch 250/537: Loss=0.6168 (C:0.6168, R:0.0105)
Batch 275/537: Loss=0.5703 (C:0.5703, R:0.0105)
Batch 300/537: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 325/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 350/537: Loss=0.6064 (C:0.6064, R:0.0105)
Batch 375/537: Loss=0.6344 (C:0.6344, R:0.0105)
Batch 400/537: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 425/537: Loss=0.6073 (C:0.6073, R:0.0105)
Batch 450/537: Loss=0.5971 (C:0.5971, R:0.0105)
Batch 475/537: Loss=0.6205 (C:0.6205, R:0.0105)
Batch 500/537: Loss=0.5999 (C:0.5999, R:0.0105)
Batch 525/537: Loss=0.5915 (C:0.5915, R:0.0105)

============================================================
Epoch 52/100 completed in 37.4s
Train: Loss=0.6101 (C:0.6101, R:0.0105) Ratio=4.82x
Val:   Loss=0.5010 (C:0.5010, R:0.0105) Ratio=7.45x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.6178 (C:0.6178, R:0.0105)
Batch  25/537: Loss=0.6495 (C:0.6495, R:0.0105)
Batch  50/537: Loss=0.5889 (C:0.5889, R:0.0105)
Batch  75/537: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 100/537: Loss=0.5731 (C:0.5731, R:0.0105)
Batch 125/537: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 150/537: Loss=0.6169 (C:0.6169, R:0.0105)
Batch 175/537: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 200/537: Loss=0.6024 (C:0.6024, R:0.0105)
Batch 225/537: Loss=0.6171 (C:0.6171, R:0.0105)
Batch 250/537: Loss=0.6112 (C:0.6112, R:0.0105)
Batch 275/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 300/537: Loss=0.5880 (C:0.5880, R:0.0106)
Batch 325/537: Loss=0.5976 (C:0.5976, R:0.0105)
Batch 350/537: Loss=0.6004 (C:0.6004, R:0.0105)
Batch 375/537: Loss=0.6093 (C:0.6093, R:0.0105)
Batch 400/537: Loss=0.6128 (C:0.6128, R:0.0105)
Batch 425/537: Loss=0.5844 (C:0.5844, R:0.0105)
Batch 450/537: Loss=0.6064 (C:0.6064, R:0.0105)
Batch 475/537: Loss=0.6042 (C:0.6042, R:0.0105)
Batch 500/537: Loss=0.6296 (C:0.6296, R:0.0105)
Batch 525/537: Loss=0.6408 (C:0.6408, R:0.0105)

============================================================
Epoch 53/100 completed in 30.7s
Train: Loss=0.6075 (C:0.6075, R:0.0105) Ratio=4.73x
Val:   Loss=0.5002 (C:0.5002, R:0.0105) Ratio=7.54x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.6013 (C:0.6013, R:0.0105)
Batch  25/537: Loss=0.5813 (C:0.5813, R:0.0106)
Batch  50/537: Loss=0.6088 (C:0.6088, R:0.0105)
Batch  75/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 100/537: Loss=0.6073 (C:0.6073, R:0.0105)
Batch 125/537: Loss=0.5835 (C:0.5835, R:0.0105)
Batch 150/537: Loss=0.5966 (C:0.5966, R:0.0105)
Batch 175/537: Loss=0.5878 (C:0.5878, R:0.0105)
Batch 200/537: Loss=0.6654 (C:0.6654, R:0.0105)
Batch 225/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch 250/537: Loss=0.6180 (C:0.6180, R:0.0105)
Batch 275/537: Loss=0.6112 (C:0.6112, R:0.0105)
Batch 300/537: Loss=0.6140 (C:0.6140, R:0.0105)
Batch 325/537: Loss=0.6210 (C:0.6210, R:0.0106)
Batch 350/537: Loss=0.6077 (C:0.6077, R:0.0105)
Batch 375/537: Loss=0.6207 (C:0.6207, R:0.0105)
Batch 400/537: Loss=0.6174 (C:0.6174, R:0.0105)
Batch 425/537: Loss=0.6065 (C:0.6065, R:0.0105)
Batch 450/537: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 475/537: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 500/537: Loss=0.5984 (C:0.5984, R:0.0105)
Batch 525/537: Loss=0.6080 (C:0.6080, R:0.0105)

============================================================
Epoch 54/100 completed in 29.8s
Train: Loss=0.6067 (C:0.6067, R:0.0105) Ratio=4.70x
Val:   Loss=0.4953 (C:0.4953, R:0.0105) Ratio=7.69x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4953)
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.328 ± 0.570
    Neg distances: 2.570 ± 1.066
    Separation ratio: 7.83x
    Gap: -4.312
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.6243 (C:0.6243, R:0.0105)
Batch  25/537: Loss=0.5540 (C:0.5540, R:0.0105)
Batch  50/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch  75/537: Loss=0.5575 (C:0.5575, R:0.0105)
Batch 100/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 125/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 150/537: Loss=0.5470 (C:0.5470, R:0.0105)
Batch 175/537: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 200/537: Loss=0.5801 (C:0.5801, R:0.0105)
Batch 225/537: Loss=0.5733 (C:0.5733, R:0.0105)
Batch 250/537: Loss=0.5808 (C:0.5808, R:0.0105)
Batch 275/537: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 300/537: Loss=0.5991 (C:0.5991, R:0.0105)
Batch 325/537: Loss=0.5945 (C:0.5945, R:0.0105)
Batch 350/537: Loss=0.6177 (C:0.6177, R:0.0105)
Batch 375/537: Loss=0.5923 (C:0.5923, R:0.0105)
Batch 400/537: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 425/537: Loss=0.5605 (C:0.5605, R:0.0105)
Batch 450/537: Loss=0.5926 (C:0.5926, R:0.0105)
Batch 475/537: Loss=0.5782 (C:0.5782, R:0.0105)
Batch 500/537: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 525/537: Loss=0.5645 (C:0.5645, R:0.0105)

============================================================
Epoch 55/100 completed in 36.5s
Train: Loss=0.5830 (C:0.5830, R:0.0105) Ratio=4.90x
Val:   Loss=0.4718 (C:0.4718, R:0.0105) Ratio=7.77x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4718)
Checkpoint saved at epoch 55
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch  25/537: Loss=0.6100 (C:0.6100, R:0.0105)
Batch  50/537: Loss=0.5721 (C:0.5721, R:0.0105)
Batch  75/537: Loss=0.5463 (C:0.5463, R:0.0105)
Batch 100/537: Loss=0.6070 (C:0.6070, R:0.0105)
Batch 125/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 150/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 175/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 200/537: Loss=0.5786 (C:0.5786, R:0.0105)
Batch 225/537: Loss=0.6023 (C:0.6023, R:0.0105)
Batch 250/537: Loss=0.5229 (C:0.5229, R:0.0105)
Batch 275/537: Loss=0.5716 (C:0.5716, R:0.0105)
Batch 300/537: Loss=0.5810 (C:0.5810, R:0.0105)
Batch 325/537: Loss=0.5719 (C:0.5719, R:0.0105)
Batch 350/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 375/537: Loss=0.5522 (C:0.5522, R:0.0105)
Batch 400/537: Loss=0.5936 (C:0.5936, R:0.0105)
Batch 425/537: Loss=0.5779 (C:0.5779, R:0.0105)
Batch 450/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 475/537: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 500/537: Loss=0.5760 (C:0.5760, R:0.0105)
Batch 525/537: Loss=0.5533 (C:0.5533, R:0.0105)

============================================================
Epoch 56/100 completed in 31.7s
Train: Loss=0.5801 (C:0.5801, R:0.0105) Ratio=4.97x
Val:   Loss=0.4680 (C:0.4680, R:0.0105) Ratio=7.77x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4680)
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.6079 (C:0.6079, R:0.0106)
Batch  25/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch  50/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch  75/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 100/537: Loss=0.5899 (C:0.5899, R:0.0105)
Batch 125/537: Loss=0.5206 (C:0.5206, R:0.0105)
Batch 150/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 175/537: Loss=0.5921 (C:0.5921, R:0.0105)
Batch 200/537: Loss=0.5840 (C:0.5840, R:0.0105)
Batch 225/537: Loss=0.5663 (C:0.5663, R:0.0105)
Batch 250/537: Loss=0.6043 (C:0.6043, R:0.0105)
Batch 275/537: Loss=0.5935 (C:0.5935, R:0.0105)
Batch 300/537: Loss=0.5720 (C:0.5720, R:0.0105)
Batch 325/537: Loss=0.5913 (C:0.5913, R:0.0105)
Batch 350/537: Loss=0.6064 (C:0.6064, R:0.0105)
Batch 375/537: Loss=0.5833 (C:0.5833, R:0.0105)
Batch 400/537: Loss=0.5891 (C:0.5891, R:0.0105)
Batch 425/537: Loss=0.5772 (C:0.5772, R:0.0105)
Batch 450/537: Loss=0.5834 (C:0.5834, R:0.0105)
Batch 475/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 500/537: Loss=0.5762 (C:0.5762, R:0.0105)
Batch 525/537: Loss=0.5984 (C:0.5984, R:0.0105)

============================================================
Epoch 57/100 completed in 30.8s
Train: Loss=0.5793 (C:0.5793, R:0.0105) Ratio=4.84x
Val:   Loss=0.4679 (C:0.4679, R:0.0105) Ratio=7.88x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4679)
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.351 ± 0.611
    Neg distances: 2.596 ± 1.100
    Separation ratio: 7.40x
    Gap: -4.321
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.5717 (C:0.5717, R:0.0105)
Batch  25/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch  50/537: Loss=0.5764 (C:0.5764, R:0.0105)
Batch  75/537: Loss=0.5809 (C:0.5809, R:0.0105)
Batch 100/537: Loss=0.5825 (C:0.5825, R:0.0105)
Batch 125/537: Loss=0.5906 (C:0.5906, R:0.0105)
Batch 150/537: Loss=0.6110 (C:0.6110, R:0.0105)
Batch 175/537: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 200/537: Loss=0.6061 (C:0.6061, R:0.0105)
Batch 225/537: Loss=0.5761 (C:0.5761, R:0.0106)
Batch 250/537: Loss=0.6098 (C:0.6098, R:0.0105)
Batch 275/537: Loss=0.5607 (C:0.5607, R:0.0105)
Batch 300/537: Loss=0.5893 (C:0.5893, R:0.0105)
Batch 325/537: Loss=0.6284 (C:0.6284, R:0.0105)
Batch 350/537: Loss=0.6032 (C:0.6032, R:0.0105)
Batch 375/537: Loss=0.6109 (C:0.6109, R:0.0105)
Batch 400/537: Loss=0.5783 (C:0.5783, R:0.0105)
Batch 425/537: Loss=0.6218 (C:0.6218, R:0.0105)
Batch 450/537: Loss=0.5822 (C:0.5822, R:0.0105)
Batch 475/537: Loss=0.5923 (C:0.5923, R:0.0105)
Batch 500/537: Loss=0.5767 (C:0.5767, R:0.0105)
Batch 525/537: Loss=0.6027 (C:0.6027, R:0.0105)

============================================================
Epoch 58/100 completed in 37.4s
Train: Loss=0.5958 (C:0.5958, R:0.0105) Ratio=4.95x
Val:   Loss=0.4831 (C:0.4831, R:0.0105) Ratio=7.92x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.6333 (C:0.6333, R:0.0105)
Batch  25/537: Loss=0.6298 (C:0.6298, R:0.0105)
Batch  50/537: Loss=0.5925 (C:0.5925, R:0.0106)
Batch  75/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 100/537: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 125/537: Loss=0.5947 (C:0.5947, R:0.0105)
Batch 150/537: Loss=0.6208 (C:0.6208, R:0.0105)
Batch 175/537: Loss=0.5664 (C:0.5664, R:0.0105)
Batch 200/537: Loss=0.5868 (C:0.5868, R:0.0105)
Batch 225/537: Loss=0.6031 (C:0.6031, R:0.0105)
Batch 250/537: Loss=0.5714 (C:0.5714, R:0.0105)
Batch 275/537: Loss=0.6119 (C:0.6119, R:0.0105)
Batch 300/537: Loss=0.5863 (C:0.5863, R:0.0105)
Batch 325/537: Loss=0.6070 (C:0.6070, R:0.0105)
Batch 350/537: Loss=0.6170 (C:0.6170, R:0.0105)
Batch 375/537: Loss=0.5724 (C:0.5724, R:0.0105)
Batch 400/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 425/537: Loss=0.6037 (C:0.6037, R:0.0105)
Batch 450/537: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 475/537: Loss=0.6171 (C:0.6171, R:0.0105)
Batch 500/537: Loss=0.6012 (C:0.6012, R:0.0105)
Batch 525/537: Loss=0.6221 (C:0.6221, R:0.0105)

============================================================
Epoch 59/100 completed in 31.6s
Train: Loss=0.5939 (C:0.5939, R:0.0105) Ratio=4.96x
Val:   Loss=0.4812 (C:0.4812, R:0.0105) Ratio=8.04x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.5692 (C:0.5692, R:0.0105)
Batch  25/537: Loss=0.5929 (C:0.5929, R:0.0105)
Batch  50/537: Loss=0.5794 (C:0.5794, R:0.0105)
Batch  75/537: Loss=0.6010 (C:0.6010, R:0.0105)
Batch 100/537: Loss=0.5843 (C:0.5843, R:0.0105)
Batch 125/537: Loss=0.5824 (C:0.5824, R:0.0105)
Batch 150/537: Loss=0.5831 (C:0.5831, R:0.0105)
Batch 175/537: Loss=0.6000 (C:0.6000, R:0.0105)
Batch 200/537: Loss=0.6426 (C:0.6426, R:0.0105)
Batch 225/537: Loss=0.5874 (C:0.5874, R:0.0105)
Batch 250/537: Loss=0.5802 (C:0.5802, R:0.0105)
Batch 275/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 300/537: Loss=0.5817 (C:0.5817, R:0.0105)
Batch 325/537: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 350/537: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 375/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 400/537: Loss=0.6099 (C:0.6099, R:0.0105)
Batch 425/537: Loss=0.5832 (C:0.5832, R:0.0105)
Batch 450/537: Loss=0.6056 (C:0.6056, R:0.0106)
Batch 475/537: Loss=0.5764 (C:0.5764, R:0.0105)
Batch 500/537: Loss=0.5813 (C:0.5813, R:0.0105)
Batch 525/537: Loss=0.6196 (C:0.6196, R:0.0105)

============================================================
Epoch 60/100 completed in 31.4s
Train: Loss=0.5934 (C:0.5934, R:0.0105) Ratio=5.04x
Val:   Loss=0.4821 (C:0.4821, R:0.0105) Ratio=8.03x
Reconstruction weight: 0.300
No improvement for 3 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.333 ± 0.562
    Neg distances: 2.635 ± 1.098
    Separation ratio: 7.90x
    Gap: -4.488
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.5705 (C:0.5705, R:0.0105)
Batch  25/537: Loss=0.6027 (C:0.6027, R:0.0105)
Batch  50/537: Loss=0.5326 (C:0.5326, R:0.0105)
Batch  75/537: Loss=0.5626 (C:0.5626, R:0.0105)
Batch 100/537: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 125/537: Loss=0.5608 (C:0.5608, R:0.0105)
Batch 150/537: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 175/537: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 200/537: Loss=0.5951 (C:0.5951, R:0.0105)
Batch 225/537: Loss=0.5808 (C:0.5808, R:0.0105)
Batch 250/537: Loss=0.5861 (C:0.5861, R:0.0105)
Batch 275/537: Loss=0.5903 (C:0.5903, R:0.0105)
Batch 300/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch 325/537: Loss=0.5731 (C:0.5731, R:0.0105)
Batch 350/537: Loss=0.5707 (C:0.5707, R:0.0105)
Batch 375/537: Loss=0.5745 (C:0.5745, R:0.0105)
Batch 400/537: Loss=0.6058 (C:0.6058, R:0.0105)
Batch 425/537: Loss=0.5769 (C:0.5769, R:0.0106)
Batch 450/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 475/537: Loss=0.5494 (C:0.5494, R:0.0105)
Batch 500/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 525/537: Loss=0.5759 (C:0.5759, R:0.0105)

============================================================
Epoch 61/100 completed in 37.2s
Train: Loss=0.5752 (C:0.5752, R:0.0105) Ratio=4.95x
Val:   Loss=0.4636 (C:0.4636, R:0.0105) Ratio=8.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4636)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.5513 (C:0.5513, R:0.0105)
Batch  25/537: Loss=0.5899 (C:0.5899, R:0.0105)
Batch  50/537: Loss=0.5878 (C:0.5878, R:0.0105)
Batch  75/537: Loss=0.5805 (C:0.5805, R:0.0105)
Batch 100/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 125/537: Loss=0.5561 (C:0.5561, R:0.0105)
Batch 150/537: Loss=0.6029 (C:0.6029, R:0.0105)
Batch 175/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 200/537: Loss=0.5707 (C:0.5707, R:0.0105)
Batch 225/537: Loss=0.5716 (C:0.5716, R:0.0105)
Batch 250/537: Loss=0.5796 (C:0.5796, R:0.0105)
Batch 275/537: Loss=0.6229 (C:0.6229, R:0.0105)
Batch 300/537: Loss=0.5851 (C:0.5851, R:0.0105)
Batch 325/537: Loss=0.5753 (C:0.5753, R:0.0105)
Batch 350/537: Loss=0.5851 (C:0.5851, R:0.0105)
Batch 375/537: Loss=0.5502 (C:0.5502, R:0.0105)
Batch 400/537: Loss=0.5664 (C:0.5664, R:0.0105)
Batch 425/537: Loss=0.5979 (C:0.5979, R:0.0105)
Batch 450/537: Loss=0.5700 (C:0.5700, R:0.0105)
Batch 475/537: Loss=0.5857 (C:0.5857, R:0.0105)
Batch 500/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 525/537: Loss=0.5907 (C:0.5907, R:0.0105)

============================================================
Epoch 62/100 completed in 32.0s
Train: Loss=0.5746 (C:0.5746, R:0.0105) Ratio=4.99x
Val:   Loss=0.4606 (C:0.4606, R:0.0105) Ratio=8.19x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4606)
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.5472 (C:0.5472, R:0.0105)
Batch  25/537: Loss=0.5654 (C:0.5654, R:0.0105)
Batch  50/537: Loss=0.5470 (C:0.5470, R:0.0105)
Batch  75/537: Loss=0.5484 (C:0.5484, R:0.0105)
Batch 100/537: Loss=0.5628 (C:0.5628, R:0.0105)
Batch 125/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 150/537: Loss=0.5762 (C:0.5762, R:0.0106)
Batch 175/537: Loss=0.5496 (C:0.5496, R:0.0105)
Batch 200/537: Loss=0.5979 (C:0.5979, R:0.0105)
Batch 225/537: Loss=0.5626 (C:0.5626, R:0.0105)
Batch 250/537: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 275/537: Loss=0.5873 (C:0.5873, R:0.0105)
Batch 300/537: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 325/537: Loss=0.5570 (C:0.5570, R:0.0105)
Batch 350/537: Loss=0.5898 (C:0.5898, R:0.0105)
Batch 375/537: Loss=0.5765 (C:0.5765, R:0.0105)
Batch 400/537: Loss=0.5744 (C:0.5744, R:0.0105)
Batch 425/537: Loss=0.6057 (C:0.6057, R:0.0105)
Batch 450/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 475/537: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 500/537: Loss=0.6099 (C:0.6099, R:0.0105)
Batch 525/537: Loss=0.5829 (C:0.5829, R:0.0105)

============================================================
Epoch 63/100 completed in 31.9s
Train: Loss=0.5732 (C:0.5732, R:0.0105) Ratio=4.91x
Val:   Loss=0.4579 (C:0.4579, R:0.0105) Ratio=8.31x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4579)
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.317 ± 0.561
    Neg distances: 2.651 ± 1.097
    Separation ratio: 8.35x
    Gap: -4.493
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.5145 (C:0.5145, R:0.0105)
Batch  25/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch  50/537: Loss=0.5797 (C:0.5797, R:0.0105)
Batch  75/537: Loss=0.5357 (C:0.5357, R:0.0105)
Batch 100/537: Loss=0.5469 (C:0.5469, R:0.0105)
Batch 125/537: Loss=0.5619 (C:0.5619, R:0.0105)
Batch 150/537: Loss=0.5496 (C:0.5496, R:0.0105)
Batch 175/537: Loss=0.5697 (C:0.5697, R:0.0106)
Batch 200/537: Loss=0.5722 (C:0.5722, R:0.0105)
Batch 225/537: Loss=0.5487 (C:0.5487, R:0.0105)
Batch 250/537: Loss=0.5154 (C:0.5154, R:0.0105)
Batch 275/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 300/537: Loss=0.5715 (C:0.5715, R:0.0105)
Batch 325/537: Loss=0.5424 (C:0.5424, R:0.0105)
Batch 350/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 375/537: Loss=0.5512 (C:0.5512, R:0.0105)
Batch 400/537: Loss=0.5825 (C:0.5825, R:0.0105)
Batch 425/537: Loss=0.5722 (C:0.5722, R:0.0105)
Batch 450/537: Loss=0.5552 (C:0.5552, R:0.0105)
Batch 475/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 500/537: Loss=0.5690 (C:0.5690, R:0.0105)
Batch 525/537: Loss=0.5503 (C:0.5503, R:0.0105)

============================================================
Epoch 64/100 completed in 38.9s
Train: Loss=0.5565 (C:0.5565, R:0.0105) Ratio=5.08x
Val:   Loss=0.4424 (C:0.4424, R:0.0105) Ratio=8.34x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4424)
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.5358 (C:0.5358, R:0.0105)
Batch  25/537: Loss=0.5591 (C:0.5591, R:0.0105)
Batch  50/537: Loss=0.5505 (C:0.5505, R:0.0105)
Batch  75/537: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 100/537: Loss=0.5697 (C:0.5697, R:0.0105)
Batch 125/537: Loss=0.5567 (C:0.5567, R:0.0105)
Batch 150/537: Loss=0.5591 (C:0.5591, R:0.0105)
Batch 175/537: Loss=0.5398 (C:0.5398, R:0.0106)
Batch 200/537: Loss=0.5438 (C:0.5438, R:0.0105)
Batch 225/537: Loss=0.5305 (C:0.5305, R:0.0105)
Batch 250/537: Loss=0.5439 (C:0.5439, R:0.0106)
Batch 275/537: Loss=0.5631 (C:0.5631, R:0.0105)
Batch 300/537: Loss=0.5645 (C:0.5645, R:0.0106)
Batch 325/537: Loss=0.5767 (C:0.5767, R:0.0105)
Batch 350/537: Loss=0.5877 (C:0.5877, R:0.0105)
Batch 375/537: Loss=0.5694 (C:0.5694, R:0.0105)
Batch 400/537: Loss=0.5970 (C:0.5970, R:0.0106)
Batch 425/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch 450/537: Loss=0.5976 (C:0.5976, R:0.0105)
Batch 475/537: Loss=0.5683 (C:0.5683, R:0.0105)
Batch 500/537: Loss=0.5516 (C:0.5516, R:0.0105)
Batch 525/537: Loss=0.5606 (C:0.5606, R:0.0105)

============================================================
Epoch 65/100 completed in 32.9s
Train: Loss=0.5560 (C:0.5560, R:0.0105) Ratio=5.01x
Val:   Loss=0.4391 (C:0.4391, R:0.0105) Ratio=8.46x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4391)
Checkpoint saved at epoch 65
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.5438 (C:0.5438, R:0.0105)
Batch  25/537: Loss=0.5030 (C:0.5030, R:0.0105)
Batch  50/537: Loss=0.5331 (C:0.5331, R:0.0105)
Batch  75/537: Loss=0.5332 (C:0.5332, R:0.0105)
Batch 100/537: Loss=0.5255 (C:0.5255, R:0.0105)
Batch 125/537: Loss=0.5351 (C:0.5351, R:0.0105)
Batch 150/537: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 175/537: Loss=0.5436 (C:0.5436, R:0.0105)
Batch 200/537: Loss=0.5458 (C:0.5458, R:0.0105)
Batch 225/537: Loss=0.5511 (C:0.5511, R:0.0105)
Batch 250/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 275/537: Loss=0.5360 (C:0.5360, R:0.0105)
Batch 300/537: Loss=0.5094 (C:0.5094, R:0.0105)
Batch 325/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 350/537: Loss=0.5238 (C:0.5238, R:0.0105)
Batch 375/537: Loss=0.5540 (C:0.5540, R:0.0105)
Batch 400/537: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 425/537: Loss=0.5678 (C:0.5678, R:0.0105)
Batch 450/537: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 475/537: Loss=0.5328 (C:0.5328, R:0.0105)
Batch 500/537: Loss=0.5342 (C:0.5342, R:0.0105)
Batch 525/537: Loss=0.5914 (C:0.5914, R:0.0106)

============================================================
Epoch 66/100 completed in 32.7s
Train: Loss=0.5538 (C:0.5538, R:0.0105) Ratio=5.19x
Val:   Loss=0.4393 (C:0.4393, R:0.0105) Ratio=8.48x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.327 ± 0.576
    Neg distances: 2.665 ± 1.103
    Separation ratio: 8.15x
    Gap: -4.527
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.5697 (C:0.5697, R:0.0105)
Batch  25/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch  50/537: Loss=0.5690 (C:0.5690, R:0.0106)
Batch  75/537: Loss=0.5482 (C:0.5482, R:0.0105)
Batch 100/537: Loss=0.5084 (C:0.5084, R:0.0105)
Batch 125/537: Loss=0.5517 (C:0.5517, R:0.0105)
Batch 150/537: Loss=0.5747 (C:0.5747, R:0.0105)
Batch 175/537: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 200/537: Loss=0.5917 (C:0.5917, R:0.0105)
Batch 225/537: Loss=0.5483 (C:0.5483, R:0.0105)
Batch 250/537: Loss=0.5912 (C:0.5912, R:0.0105)
Batch 275/537: Loss=0.5874 (C:0.5874, R:0.0105)
Batch 300/537: Loss=0.5550 (C:0.5550, R:0.0105)
Batch 325/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 350/537: Loss=0.5607 (C:0.5607, R:0.0105)
Batch 375/537: Loss=0.5628 (C:0.5628, R:0.0105)
Batch 400/537: Loss=0.5606 (C:0.5606, R:0.0105)
Batch 425/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 450/537: Loss=0.5807 (C:0.5807, R:0.0105)
Batch 475/537: Loss=0.5732 (C:0.5732, R:0.0105)
Batch 500/537: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 525/537: Loss=0.5522 (C:0.5522, R:0.0105)

============================================================
Epoch 67/100 completed in 38.1s
Train: Loss=0.5583 (C:0.5583, R:0.0105) Ratio=5.00x
Val:   Loss=0.4415 (C:0.4415, R:0.0105) Ratio=8.58x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.5607 (C:0.5607, R:0.0105)
Batch  25/537: Loss=0.5622 (C:0.5622, R:0.0105)
Batch  50/537: Loss=0.5394 (C:0.5394, R:0.0105)
Batch  75/537: Loss=0.5406 (C:0.5406, R:0.0105)
Batch 100/537: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 125/537: Loss=0.5988 (C:0.5988, R:0.0105)
Batch 150/537: Loss=0.5361 (C:0.5361, R:0.0105)
Batch 175/537: Loss=0.5331 (C:0.5331, R:0.0105)
Batch 200/537: Loss=0.5424 (C:0.5424, R:0.0105)
Batch 225/537: Loss=0.5897 (C:0.5897, R:0.0105)
Batch 250/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch 275/537: Loss=0.5374 (C:0.5374, R:0.0106)
Batch 300/537: Loss=0.5687 (C:0.5687, R:0.0105)
Batch 325/537: Loss=0.5551 (C:0.5551, R:0.0105)
Batch 350/537: Loss=0.5719 (C:0.5719, R:0.0105)
Batch 375/537: Loss=0.5636 (C:0.5636, R:0.0105)
Batch 400/537: Loss=0.5774 (C:0.5774, R:0.0105)
Batch 425/537: Loss=0.5664 (C:0.5664, R:0.0105)
Batch 450/537: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 475/537: Loss=0.5275 (C:0.5275, R:0.0105)
Batch 500/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 525/537: Loss=0.5847 (C:0.5847, R:0.0105)

============================================================
Epoch 68/100 completed in 31.5s
Train: Loss=0.5556 (C:0.5556, R:0.0105) Ratio=5.18x
Val:   Loss=0.4427 (C:0.4427, R:0.0105) Ratio=8.58x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.5137 (C:0.5137, R:0.0105)
Batch  25/537: Loss=0.5249 (C:0.5249, R:0.0105)
Batch  50/537: Loss=0.5179 (C:0.5179, R:0.0106)
Batch  75/537: Loss=0.5493 (C:0.5493, R:0.0105)
Batch 100/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 125/537: Loss=0.5503 (C:0.5503, R:0.0105)
Batch 150/537: Loss=0.5428 (C:0.5428, R:0.0105)
Batch 175/537: Loss=0.5724 (C:0.5724, R:0.0105)
Batch 200/537: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 225/537: Loss=0.5604 (C:0.5604, R:0.0105)
Batch 250/537: Loss=0.5510 (C:0.5510, R:0.0105)
Batch 275/537: Loss=0.5579 (C:0.5579, R:0.0105)
Batch 300/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 325/537: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 350/537: Loss=0.5913 (C:0.5913, R:0.0105)
Batch 375/537: Loss=0.5566 (C:0.5566, R:0.0105)
Batch 400/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch 425/537: Loss=0.5542 (C:0.5542, R:0.0105)
Batch 450/537: Loss=0.5525 (C:0.5525, R:0.0105)
Batch 475/537: Loss=0.5366 (C:0.5366, R:0.0105)
Batch 500/537: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 525/537: Loss=0.5704 (C:0.5704, R:0.0105)

============================================================
Epoch 69/100 completed in 31.9s
Train: Loss=0.5548 (C:0.5548, R:0.0105) Ratio=5.17x
Val:   Loss=0.4403 (C:0.4403, R:0.0105) Ratio=8.69x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.290 ± 0.519
    Neg distances: 2.693 ± 1.093
    Separation ratio: 9.27x
    Gap: -4.519
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.5052 (C:0.5052, R:0.0105)
Batch  25/537: Loss=0.5110 (C:0.5110, R:0.0105)
Batch  50/537: Loss=0.5145 (C:0.5145, R:0.0105)
Batch  75/537: Loss=0.4944 (C:0.4944, R:0.0105)
Batch 100/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 125/537: Loss=0.5415 (C:0.5415, R:0.0105)
Batch 150/537: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 175/537: Loss=0.5237 (C:0.5237, R:0.0105)
Batch 200/537: Loss=0.5264 (C:0.5264, R:0.0105)
Batch 225/537: Loss=0.5192 (C:0.5192, R:0.0105)
Batch 250/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch 275/537: Loss=0.5432 (C:0.5432, R:0.0105)
Batch 300/537: Loss=0.5539 (C:0.5539, R:0.0105)
Batch 325/537: Loss=0.5146 (C:0.5146, R:0.0105)
Batch 350/537: Loss=0.5173 (C:0.5173, R:0.0105)
Batch 375/537: Loss=0.5439 (C:0.5439, R:0.0105)
Batch 400/537: Loss=0.5117 (C:0.5117, R:0.0105)
Batch 425/537: Loss=0.5318 (C:0.5318, R:0.0105)
Batch 450/537: Loss=0.5366 (C:0.5366, R:0.0105)
Batch 475/537: Loss=0.5377 (C:0.5377, R:0.0106)
Batch 500/537: Loss=0.5095 (C:0.5095, R:0.0105)
Batch 525/537: Loss=0.5051 (C:0.5051, R:0.0105)

============================================================
Epoch 70/100 completed in 39.1s
Train: Loss=0.5240 (C:0.5240, R:0.0105) Ratio=5.17x
Val:   Loss=0.4065 (C:0.4065, R:0.0105) Ratio=8.79x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4065)
Checkpoint saved at epoch 70
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.5236 (C:0.5236, R:0.0105)
Batch  25/537: Loss=0.4766 (C:0.4766, R:0.0105)
Batch  50/537: Loss=0.5126 (C:0.5126, R:0.0105)
Batch  75/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 100/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 125/537: Loss=0.5097 (C:0.5097, R:0.0105)
Batch 150/537: Loss=0.5212 (C:0.5212, R:0.0105)
Batch 175/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 200/537: Loss=0.5250 (C:0.5250, R:0.0105)
Batch 225/537: Loss=0.5287 (C:0.5287, R:0.0106)
Batch 250/537: Loss=0.5122 (C:0.5122, R:0.0105)
Batch 275/537: Loss=0.5287 (C:0.5287, R:0.0105)
Batch 300/537: Loss=0.5549 (C:0.5549, R:0.0105)
Batch 325/537: Loss=0.5406 (C:0.5406, R:0.0105)
Batch 350/537: Loss=0.5434 (C:0.5434, R:0.0105)
Batch 375/537: Loss=0.5286 (C:0.5286, R:0.0105)
Batch 400/537: Loss=0.5134 (C:0.5134, R:0.0105)
Batch 425/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 450/537: Loss=0.5133 (C:0.5133, R:0.0105)
Batch 475/537: Loss=0.5110 (C:0.5110, R:0.0105)
Batch 500/537: Loss=0.5195 (C:0.5195, R:0.0105)
Batch 525/537: Loss=0.5173 (C:0.5173, R:0.0105)

============================================================
Epoch 71/100 completed in 32.0s
Train: Loss=0.5232 (C:0.5232, R:0.0105) Ratio=5.19x
Val:   Loss=0.4059 (C:0.4059, R:0.0105) Ratio=8.92x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4059)
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.5035 (C:0.5035, R:0.0105)
Batch  25/537: Loss=0.5304 (C:0.5304, R:0.0105)
Batch  50/537: Loss=0.4784 (C:0.4784, R:0.0105)
Batch  75/537: Loss=0.4975 (C:0.4975, R:0.0106)
Batch 100/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 125/537: Loss=0.5366 (C:0.5366, R:0.0105)
Batch 150/537: Loss=0.5130 (C:0.5130, R:0.0105)
Batch 175/537: Loss=0.5499 (C:0.5499, R:0.0105)
Batch 200/537: Loss=0.5111 (C:0.5111, R:0.0105)
Batch 225/537: Loss=0.4995 (C:0.4995, R:0.0105)
Batch 250/537: Loss=0.5102 (C:0.5102, R:0.0105)
Batch 275/537: Loss=0.5257 (C:0.5257, R:0.0105)
Batch 300/537: Loss=0.5143 (C:0.5143, R:0.0105)
Batch 325/537: Loss=0.5263 (C:0.5263, R:0.0105)
Batch 350/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 375/537: Loss=0.5177 (C:0.5177, R:0.0105)
Batch 400/537: Loss=0.5443 (C:0.5443, R:0.0105)
Batch 425/537: Loss=0.5619 (C:0.5619, R:0.0105)
Batch 450/537: Loss=0.4928 (C:0.4928, R:0.0105)
Batch 475/537: Loss=0.5223 (C:0.5223, R:0.0105)
Batch 500/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 525/537: Loss=0.5275 (C:0.5275, R:0.0105)

============================================================
Epoch 72/100 completed in 32.6s
Train: Loss=0.5234 (C:0.5234, R:0.0105) Ratio=5.25x
Val:   Loss=0.4031 (C:0.4031, R:0.0105) Ratio=9.00x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4031)
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.307 ± 0.579
    Neg distances: 2.652 ± 1.090
    Separation ratio: 8.63x
    Gap: -4.447
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=0.5209 (C:0.5209, R:0.0105)
Batch  25/537: Loss=0.5310 (C:0.5310, R:0.0105)
Batch  50/537: Loss=0.5108 (C:0.5108, R:0.0105)
Batch  75/537: Loss=0.5123 (C:0.5123, R:0.0105)
Batch 100/537: Loss=0.5554 (C:0.5554, R:0.0105)
Batch 125/537: Loss=0.5438 (C:0.5438, R:0.0105)
Batch 150/537: Loss=0.5155 (C:0.5155, R:0.0106)
Batch 175/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 200/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch 225/537: Loss=0.5714 (C:0.5714, R:0.0105)
Batch 250/537: Loss=0.5392 (C:0.5392, R:0.0106)
Batch 275/537: Loss=0.5219 (C:0.5219, R:0.0105)
Batch 300/537: Loss=0.5302 (C:0.5302, R:0.0105)
Batch 325/537: Loss=0.5322 (C:0.5322, R:0.0105)
Batch 350/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch 375/537: Loss=0.5316 (C:0.5316, R:0.0105)
Batch 400/537: Loss=0.5585 (C:0.5585, R:0.0105)
Batch 425/537: Loss=0.5280 (C:0.5280, R:0.0105)
Batch 450/537: Loss=0.4953 (C:0.4953, R:0.0105)
Batch 475/537: Loss=0.5191 (C:0.5191, R:0.0105)
Batch 500/537: Loss=0.5632 (C:0.5632, R:0.0105)
Batch 525/537: Loss=0.5481 (C:0.5481, R:0.0105)

============================================================
Epoch 73/100 completed in 40.0s
Train: Loss=0.5348 (C:0.5348, R:0.0105) Ratio=5.27x
Val:   Loss=0.4178 (C:0.4178, R:0.0105) Ratio=9.02x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=0.5243 (C:0.5243, R:0.0105)
Batch  25/537: Loss=0.5154 (C:0.5154, R:0.0106)
Batch  50/537: Loss=0.5431 (C:0.5431, R:0.0105)
Batch  75/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 100/537: Loss=0.5261 (C:0.5261, R:0.0105)
Batch 125/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 150/537: Loss=0.5640 (C:0.5640, R:0.0105)
Batch 175/537: Loss=0.5250 (C:0.5250, R:0.0105)
Batch 200/537: Loss=0.5179 (C:0.5179, R:0.0105)
Batch 225/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 250/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch 275/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 300/537: Loss=0.5449 (C:0.5449, R:0.0106)
Batch 325/537: Loss=0.5684 (C:0.5684, R:0.0105)
Batch 350/537: Loss=0.5038 (C:0.5038, R:0.0105)
Batch 375/537: Loss=0.5250 (C:0.5250, R:0.0105)
Batch 400/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 425/537: Loss=0.5507 (C:0.5507, R:0.0105)
Batch 450/537: Loss=0.5503 (C:0.5503, R:0.0105)
Batch 475/537: Loss=0.5305 (C:0.5305, R:0.0105)
Batch 500/537: Loss=0.5529 (C:0.5529, R:0.0105)
Batch 525/537: Loss=0.5136 (C:0.5136, R:0.0105)

============================================================
Epoch 74/100 completed in 32.2s
Train: Loss=0.5336 (C:0.5336, R:0.0105) Ratio=5.16x
Val:   Loss=0.4181 (C:0.4181, R:0.0105) Ratio=8.94x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=0.5171 (C:0.5171, R:0.0105)
Batch  25/537: Loss=0.4980 (C:0.4980, R:0.0105)
Batch  50/537: Loss=0.5256 (C:0.5256, R:0.0105)
Batch  75/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 100/537: Loss=0.5270 (C:0.5270, R:0.0105)
Batch 125/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 150/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch 175/537: Loss=0.5256 (C:0.5256, R:0.0105)
Batch 200/537: Loss=0.5198 (C:0.5198, R:0.0106)
Batch 225/537: Loss=0.5812 (C:0.5812, R:0.0105)
Batch 250/537: Loss=0.5460 (C:0.5460, R:0.0105)
Batch 275/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 300/537: Loss=0.4903 (C:0.4903, R:0.0105)
Batch 325/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 350/537: Loss=0.5476 (C:0.5476, R:0.0105)
Batch 375/537: Loss=0.5308 (C:0.5308, R:0.0105)
Batch 400/537: Loss=0.5865 (C:0.5865, R:0.0105)
Batch 425/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 450/537: Loss=0.5425 (C:0.5425, R:0.0105)
Batch 475/537: Loss=0.5408 (C:0.5408, R:0.0105)
Batch 500/537: Loss=0.5413 (C:0.5413, R:0.0105)
Batch 525/537: Loss=0.5342 (C:0.5342, R:0.0105)

============================================================
Epoch 75/100 completed in 31.0s
Train: Loss=0.5329 (C:0.5329, R:0.0105) Ratio=5.30x
Val:   Loss=0.4164 (C:0.4164, R:0.0105) Ratio=9.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
Checkpoint saved at epoch 75
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.291 ± 0.522
    Neg distances: 2.654 ± 1.082
    Separation ratio: 9.12x
    Gap: -4.524
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=0.5155 (C:0.5155, R:0.0105)
Batch  25/537: Loss=0.5052 (C:0.5052, R:0.0105)
Batch  50/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch  75/537: Loss=0.5061 (C:0.5061, R:0.0105)
Batch 100/537: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 125/537: Loss=0.5287 (C:0.5287, R:0.0105)
Batch 150/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 175/537: Loss=0.4733 (C:0.4733, R:0.0105)
Batch 200/537: Loss=0.5033 (C:0.5033, R:0.0105)
Batch 225/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 250/537: Loss=0.5343 (C:0.5343, R:0.0105)
Batch 275/537: Loss=0.5301 (C:0.5301, R:0.0105)
Batch 300/537: Loss=0.5032 (C:0.5032, R:0.0105)
Batch 325/537: Loss=0.5448 (C:0.5448, R:0.0105)
Batch 350/537: Loss=0.5403 (C:0.5403, R:0.0105)
Batch 375/537: Loss=0.5297 (C:0.5297, R:0.0105)
Batch 400/537: Loss=0.5198 (C:0.5198, R:0.0105)
Batch 425/537: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 450/537: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 475/537: Loss=0.4965 (C:0.4965, R:0.0105)
Batch 500/537: Loss=0.5198 (C:0.5198, R:0.0105)
Batch 525/537: Loss=0.5263 (C:0.5263, R:0.0106)

============================================================
Epoch 76/100 completed in 38.8s
Train: Loss=0.5199 (C:0.5199, R:0.0105) Ratio=5.31x
Val:   Loss=0.4028 (C:0.4028, R:0.0105) Ratio=9.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4028)
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=0.5248 (C:0.5248, R:0.0105)
Batch  25/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch  50/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch  75/537: Loss=0.5317 (C:0.5317, R:0.0106)
Batch 100/537: Loss=0.5324 (C:0.5324, R:0.0105)
Batch 125/537: Loss=0.5238 (C:0.5238, R:0.0105)
Batch 150/537: Loss=0.4877 (C:0.4877, R:0.0105)
Batch 175/537: Loss=0.5103 (C:0.5103, R:0.0105)
Batch 200/537: Loss=0.5215 (C:0.5215, R:0.0105)
Batch 225/537: Loss=0.5072 (C:0.5072, R:0.0105)
Batch 250/537: Loss=0.4919 (C:0.4919, R:0.0105)
Batch 275/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 300/537: Loss=0.5292 (C:0.5292, R:0.0106)
Batch 325/537: Loss=0.5308 (C:0.5308, R:0.0105)
Batch 350/537: Loss=0.4999 (C:0.4999, R:0.0105)
Batch 375/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 400/537: Loss=0.5234 (C:0.5234, R:0.0105)
Batch 425/537: Loss=0.5451 (C:0.5451, R:0.0105)
Batch 450/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 475/537: Loss=0.5085 (C:0.5085, R:0.0105)
Batch 500/537: Loss=0.5167 (C:0.5167, R:0.0105)
Batch 525/537: Loss=0.5342 (C:0.5342, R:0.0105)

============================================================
Epoch 77/100 completed in 33.7s
Train: Loss=0.5173 (C:0.5173, R:0.0105) Ratio=5.33x
Val:   Loss=0.4009 (C:0.4009, R:0.0105) Ratio=9.22x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4009)
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=0.5151 (C:0.5151, R:0.0105)
Batch  25/537: Loss=0.4843 (C:0.4843, R:0.0105)
Batch  50/537: Loss=0.5154 (C:0.5154, R:0.0105)
Batch  75/537: Loss=0.4974 (C:0.4974, R:0.0105)
Batch 100/537: Loss=0.5138 (C:0.5138, R:0.0105)
Batch 125/537: Loss=0.5471 (C:0.5471, R:0.0105)
Batch 150/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 175/537: Loss=0.5511 (C:0.5511, R:0.0105)
Batch 200/537: Loss=0.4842 (C:0.4842, R:0.0105)
Batch 225/537: Loss=0.5275 (C:0.5275, R:0.0106)
Batch 250/537: Loss=0.4924 (C:0.4924, R:0.0105)
Batch 275/537: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 300/537: Loss=0.5237 (C:0.5237, R:0.0105)
Batch 325/537: Loss=0.5235 (C:0.5235, R:0.0105)
Batch 350/537: Loss=0.5073 (C:0.5073, R:0.0105)
Batch 375/537: Loss=0.5255 (C:0.5255, R:0.0105)
Batch 400/537: Loss=0.4726 (C:0.4726, R:0.0105)
Batch 425/537: Loss=0.5653 (C:0.5653, R:0.0105)
Batch 450/537: Loss=0.5046 (C:0.5046, R:0.0105)
Batch 475/537: Loss=0.5212 (C:0.5212, R:0.0105)
Batch 500/537: Loss=0.5264 (C:0.5264, R:0.0105)
Batch 525/537: Loss=0.5159 (C:0.5159, R:0.0105)

============================================================
Epoch 78/100 completed in 32.6s
Train: Loss=0.5179 (C:0.5179, R:0.0105) Ratio=5.36x
Val:   Loss=0.4000 (C:0.4000, R:0.0105) Ratio=9.31x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.4000)
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.312 ± 0.573
    Neg distances: 2.669 ± 1.104
    Separation ratio: 8.54x
    Gap: -4.585
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=0.4997 (C:0.4997, R:0.0106)
Batch  25/537: Loss=0.5429 (C:0.5429, R:0.0105)
Batch  50/537: Loss=0.5204 (C:0.5204, R:0.0105)
Batch  75/537: Loss=0.5042 (C:0.5042, R:0.0105)
Batch 100/537: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 125/537: Loss=0.5370 (C:0.5370, R:0.0105)
Batch 150/537: Loss=0.5128 (C:0.5128, R:0.0105)
Batch 175/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 200/537: Loss=0.5320 (C:0.5320, R:0.0105)
Batch 225/537: Loss=0.5383 (C:0.5383, R:0.0105)
Batch 250/537: Loss=0.5686 (C:0.5686, R:0.0105)
Batch 275/537: Loss=0.5029 (C:0.5029, R:0.0105)
Batch 300/537: Loss=0.5166 (C:0.5166, R:0.0105)
Batch 325/537: Loss=0.5092 (C:0.5092, R:0.0105)
Batch 350/537: Loss=0.5018 (C:0.5018, R:0.0105)
Batch 375/537: Loss=0.5258 (C:0.5258, R:0.0105)
Batch 400/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 425/537: Loss=0.5209 (C:0.5209, R:0.0105)
Batch 450/537: Loss=0.5310 (C:0.5310, R:0.0105)
Batch 475/537: Loss=0.5630 (C:0.5630, R:0.0105)
Batch 500/537: Loss=0.5041 (C:0.5041, R:0.0105)
Batch 525/537: Loss=0.5550 (C:0.5550, R:0.0105)

============================================================
Epoch 79/100 completed in 39.2s
Train: Loss=0.5313 (C:0.5313, R:0.0105) Ratio=5.44x
Val:   Loss=0.4148 (C:0.4148, R:0.0105) Ratio=9.34x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=0.5300 (C:0.5300, R:0.0105)
Batch  25/537: Loss=0.5242 (C:0.5242, R:0.0105)
Batch  50/537: Loss=0.5320 (C:0.5320, R:0.0105)
Batch  75/537: Loss=0.5003 (C:0.5003, R:0.0105)
Batch 100/537: Loss=0.4879 (C:0.4879, R:0.0105)
Batch 125/537: Loss=0.5561 (C:0.5561, R:0.0105)
Batch 150/537: Loss=0.5225 (C:0.5225, R:0.0105)
Batch 175/537: Loss=0.5564 (C:0.5564, R:0.0106)
Batch 200/537: Loss=0.4968 (C:0.4968, R:0.0105)
Batch 225/537: Loss=0.5156 (C:0.5156, R:0.0106)
Batch 250/537: Loss=0.5399 (C:0.5399, R:0.0105)
Batch 275/537: Loss=0.5154 (C:0.5154, R:0.0105)
Batch 300/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 325/537: Loss=0.5130 (C:0.5130, R:0.0105)
Batch 350/537: Loss=0.5571 (C:0.5571, R:0.0105)
Batch 375/537: Loss=0.5103 (C:0.5103, R:0.0105)
Batch 400/537: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 425/537: Loss=0.5422 (C:0.5422, R:0.0105)
Batch 450/537: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 475/537: Loss=0.5086 (C:0.5086, R:0.0105)
Batch 500/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 525/537: Loss=0.5587 (C:0.5587, R:0.0105)

============================================================
Epoch 80/100 completed in 32.1s
Train: Loss=0.5316 (C:0.5316, R:0.0105) Ratio=5.45x
Val:   Loss=0.4145 (C:0.4145, R:0.0105) Ratio=9.43x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=0.4885 (C:0.4885, R:0.0105)
Batch  25/537: Loss=0.5497 (C:0.5497, R:0.0105)
Batch  50/537: Loss=0.5140 (C:0.5140, R:0.0105)
Batch  75/537: Loss=0.5073 (C:0.5073, R:0.0105)
Batch 100/537: Loss=0.4899 (C:0.4899, R:0.0105)
Batch 125/537: Loss=0.5059 (C:0.5059, R:0.0105)
Batch 150/537: Loss=0.5248 (C:0.5248, R:0.0105)
Batch 175/537: Loss=0.5476 (C:0.5476, R:0.0105)
Batch 200/537: Loss=0.5006 (C:0.5006, R:0.0105)
Batch 225/537: Loss=0.5088 (C:0.5088, R:0.0105)
Batch 250/537: Loss=0.5317 (C:0.5317, R:0.0105)
Batch 275/537: Loss=0.5391 (C:0.5391, R:0.0105)
Batch 300/537: Loss=0.5571 (C:0.5571, R:0.0105)
Batch 325/537: Loss=0.5280 (C:0.5280, R:0.0105)
Batch 350/537: Loss=0.5727 (C:0.5727, R:0.0105)
Batch 375/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 400/537: Loss=0.5661 (C:0.5661, R:0.0105)
Batch 425/537: Loss=0.5635 (C:0.5635, R:0.0105)
Batch 450/537: Loss=0.5535 (C:0.5535, R:0.0105)
Batch 475/537: Loss=0.5361 (C:0.5361, R:0.0105)
Batch 500/537: Loss=0.5538 (C:0.5538, R:0.0105)
Batch 525/537: Loss=0.5288 (C:0.5288, R:0.0105)

============================================================
Epoch 81/100 completed in 31.7s
Train: Loss=0.5300 (C:0.5300, R:0.0105) Ratio=5.32x
Val:   Loss=0.4133 (C:0.4133, R:0.0105) Ratio=9.51x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.279 ± 0.531
    Neg distances: 2.697 ± 1.094
    Separation ratio: 9.66x
    Gap: -4.543
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=0.4945 (C:0.4945, R:0.0105)
Batch  25/537: Loss=0.5182 (C:0.5182, R:0.0105)
Batch  50/537: Loss=0.4931 (C:0.4931, R:0.0105)
Batch  75/537: Loss=0.4798 (C:0.4798, R:0.0105)
Batch 100/537: Loss=0.4850 (C:0.4850, R:0.0105)
Batch 125/537: Loss=0.5177 (C:0.5177, R:0.0105)
Batch 150/537: Loss=0.4921 (C:0.4921, R:0.0105)
Batch 175/537: Loss=0.5023 (C:0.5023, R:0.0105)
Batch 200/537: Loss=0.4859 (C:0.4859, R:0.0105)
Batch 225/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 250/537: Loss=0.4921 (C:0.4921, R:0.0105)
Batch 275/537: Loss=0.5046 (C:0.5046, R:0.0105)
Batch 300/537: Loss=0.4886 (C:0.4886, R:0.0105)
Batch 325/537: Loss=0.5293 (C:0.5293, R:0.0105)
Batch 350/537: Loss=0.4709 (C:0.4709, R:0.0104)
Batch 375/537: Loss=0.5262 (C:0.5262, R:0.0105)
Batch 400/537: Loss=0.5006 (C:0.5006, R:0.0105)
Batch 425/537: Loss=0.4989 (C:0.4989, R:0.0106)
Batch 450/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 475/537: Loss=0.4848 (C:0.4848, R:0.0105)
Batch 500/537: Loss=0.4442 (C:0.4442, R:0.0105)
Batch 525/537: Loss=0.5312 (C:0.5312, R:0.0105)

============================================================
Epoch 82/100 completed in 37.6s
Train: Loss=0.5016 (C:0.5016, R:0.0105) Ratio=5.43x
Val:   Loss=0.3841 (C:0.3841, R:0.0105) Ratio=9.50x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3841)
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch  25/537: Loss=0.4764 (C:0.4764, R:0.0105)
Batch  50/537: Loss=0.4974 (C:0.4974, R:0.0104)
Batch  75/537: Loss=0.4655 (C:0.4655, R:0.0105)
Batch 100/537: Loss=0.5092 (C:0.5092, R:0.0104)
Batch 125/537: Loss=0.4967 (C:0.4967, R:0.0105)
Batch 150/537: Loss=0.5176 (C:0.5176, R:0.0105)
Batch 175/537: Loss=0.4701 (C:0.4701, R:0.0105)
Batch 200/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch 225/537: Loss=0.4843 (C:0.4843, R:0.0105)
Batch 250/537: Loss=0.4836 (C:0.4836, R:0.0105)
Batch 275/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch 300/537: Loss=0.4960 (C:0.4960, R:0.0105)
Batch 325/537: Loss=0.4897 (C:0.4897, R:0.0105)
Batch 350/537: Loss=0.5037 (C:0.5037, R:0.0105)
Batch 375/537: Loss=0.5283 (C:0.5283, R:0.0105)
Batch 400/537: Loss=0.4593 (C:0.4593, R:0.0105)
Batch 425/537: Loss=0.5072 (C:0.5072, R:0.0105)
Batch 450/537: Loss=0.4984 (C:0.4984, R:0.0105)
Batch 475/537: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 500/537: Loss=0.5040 (C:0.5040, R:0.0105)
Batch 525/537: Loss=0.5294 (C:0.5294, R:0.0105)

============================================================
Epoch 83/100 completed in 31.3s
Train: Loss=0.4986 (C:0.4986, R:0.0105) Ratio=5.38x
Val:   Loss=0.3815 (C:0.3815, R:0.0105) Ratio=9.65x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3815)
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=0.4714 (C:0.4714, R:0.0105)
Batch  25/537: Loss=0.4749 (C:0.4749, R:0.0105)
Batch  50/537: Loss=0.4958 (C:0.4958, R:0.0105)
Batch  75/537: Loss=0.4858 (C:0.4858, R:0.0105)
Batch 100/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 125/537: Loss=0.5157 (C:0.5157, R:0.0105)
Batch 150/537: Loss=0.4706 (C:0.4706, R:0.0105)
Batch 175/537: Loss=0.5058 (C:0.5058, R:0.0105)
Batch 200/537: Loss=0.5194 (C:0.5194, R:0.0105)
Batch 225/537: Loss=0.4725 (C:0.4725, R:0.0105)
Batch 250/537: Loss=0.4972 (C:0.4972, R:0.0105)
Batch 275/537: Loss=0.4982 (C:0.4982, R:0.0106)
Batch 300/537: Loss=0.5166 (C:0.5166, R:0.0105)
Batch 325/537: Loss=0.4977 (C:0.4977, R:0.0106)
Batch 350/537: Loss=0.5043 (C:0.5043, R:0.0105)
Batch 375/537: Loss=0.4596 (C:0.4596, R:0.0105)
Batch 400/537: Loss=0.4960 (C:0.4960, R:0.0105)
Batch 425/537: Loss=0.4757 (C:0.4757, R:0.0105)
Batch 450/537: Loss=0.4768 (C:0.4768, R:0.0105)
Batch 475/537: Loss=0.5041 (C:0.5041, R:0.0105)
Batch 500/537: Loss=0.4847 (C:0.4847, R:0.0105)
Batch 525/537: Loss=0.5324 (C:0.5324, R:0.0105)

============================================================
Epoch 84/100 completed in 31.7s
Train: Loss=0.4976 (C:0.4976, R:0.0105) Ratio=5.53x
Val:   Loss=0.3802 (C:0.3802, R:0.0105) Ratio=9.71x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3802)
============================================================

🌍 Updating global dataset at epoch 85
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.294 ± 0.576
    Neg distances: 2.732 ± 1.113
    Separation ratio: 9.29x
    Gap: -4.559
    ✅ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch  25/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch  50/537: Loss=0.5037 (C:0.5037, R:0.0105)
Batch  75/537: Loss=0.4979 (C:0.4979, R:0.0105)
Batch 100/537: Loss=0.5163 (C:0.5163, R:0.0105)
Batch 125/537: Loss=0.4703 (C:0.4703, R:0.0105)
Batch 150/537: Loss=0.4993 (C:0.4993, R:0.0105)
Batch 175/537: Loss=0.5327 (C:0.5327, R:0.0106)
Batch 200/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch 225/537: Loss=0.5323 (C:0.5323, R:0.0105)
Batch 250/537: Loss=0.4721 (C:0.4721, R:0.0105)
Batch 275/537: Loss=0.5502 (C:0.5502, R:0.0105)
Batch 300/537: Loss=0.5110 (C:0.5110, R:0.0105)
Batch 325/537: Loss=0.5044 (C:0.5044, R:0.0105)
Batch 350/537: Loss=0.5367 (C:0.5367, R:0.0105)
Batch 375/537: Loss=0.4682 (C:0.4682, R:0.0105)
Batch 400/537: Loss=0.5141 (C:0.5141, R:0.0105)
Batch 425/537: Loss=0.5281 (C:0.5281, R:0.0105)
Batch 450/537: Loss=0.5170 (C:0.5170, R:0.0105)
Batch 475/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 500/537: Loss=0.4931 (C:0.4931, R:0.0105)
Batch 525/537: Loss=0.5410 (C:0.5410, R:0.0105)

============================================================
Epoch 85/100 completed in 38.1s
Train: Loss=0.5079 (C:0.5079, R:0.0105) Ratio=5.38x
Val:   Loss=0.3901 (C:0.3901, R:0.0105) Ratio=9.69x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 85
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=0.4746 (C:0.4746, R:0.0105)
Batch  25/537: Loss=0.4944 (C:0.4944, R:0.0105)
Batch  50/537: Loss=0.5108 (C:0.5108, R:0.0105)
Batch  75/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 100/537: Loss=0.5117 (C:0.5117, R:0.0105)
Batch 125/537: Loss=0.4863 (C:0.4863, R:0.0106)
Batch 150/537: Loss=0.5264 (C:0.5264, R:0.0105)
Batch 175/537: Loss=0.5101 (C:0.5101, R:0.0106)
Batch 200/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 225/537: Loss=0.5032 (C:0.5032, R:0.0105)
Batch 250/537: Loss=0.4946 (C:0.4946, R:0.0105)
Batch 275/537: Loss=0.4940 (C:0.4940, R:0.0106)
Batch 300/537: Loss=0.4951 (C:0.4951, R:0.0106)
Batch 325/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 350/537: Loss=0.5194 (C:0.5194, R:0.0105)
Batch 375/537: Loss=0.4791 (C:0.4791, R:0.0105)
Batch 400/537: Loss=0.4747 (C:0.4747, R:0.0105)
Batch 425/537: Loss=0.4777 (C:0.4777, R:0.0105)
Batch 450/537: Loss=0.5256 (C:0.5256, R:0.0105)
Batch 475/537: Loss=0.5055 (C:0.5055, R:0.0106)
Batch 500/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 525/537: Loss=0.5037 (C:0.5037, R:0.0105)

============================================================
Epoch 86/100 completed in 31.8s
Train: Loss=0.5064 (C:0.5064, R:0.0105) Ratio=5.60x
Val:   Loss=0.3878 (C:0.3878, R:0.0105) Ratio=9.87x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=0.4789 (C:0.4789, R:0.0105)
Batch  25/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch  50/537: Loss=0.4650 (C:0.4650, R:0.0105)
Batch  75/537: Loss=0.4939 (C:0.4939, R:0.0105)
Batch 100/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch 125/537: Loss=0.5219 (C:0.5219, R:0.0105)
Batch 150/537: Loss=0.5292 (C:0.5292, R:0.0106)
Batch 175/537: Loss=0.5360 (C:0.5360, R:0.0105)
Batch 200/537: Loss=0.5224 (C:0.5224, R:0.0105)
Batch 225/537: Loss=0.4989 (C:0.4989, R:0.0105)
Batch 250/537: Loss=0.5075 (C:0.5075, R:0.0105)
Batch 275/537: Loss=0.4961 (C:0.4961, R:0.0105)
Batch 300/537: Loss=0.4934 (C:0.4934, R:0.0105)
Batch 325/537: Loss=0.4743 (C:0.4743, R:0.0105)
Batch 350/537: Loss=0.5291 (C:0.5291, R:0.0105)
Batch 375/537: Loss=0.4833 (C:0.4833, R:0.0105)
Batch 400/537: Loss=0.5133 (C:0.5133, R:0.0105)
Batch 425/537: Loss=0.4533 (C:0.4533, R:0.0105)
Batch 450/537: Loss=0.5175 (C:0.5175, R:0.0105)
Batch 475/537: Loss=0.5381 (C:0.5381, R:0.0105)
Batch 500/537: Loss=0.5004 (C:0.5004, R:0.0105)
Batch 525/537: Loss=0.4746 (C:0.4746, R:0.0105)

============================================================
Epoch 87/100 completed in 31.1s
Train: Loss=0.5053 (C:0.5053, R:0.0105) Ratio=5.55x
Val:   Loss=0.3846 (C:0.3846, R:0.0105) Ratio=9.98x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 88
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.279 ± 0.549
    Neg distances: 2.742 ± 1.111
    Separation ratio: 9.83x
    Gap: -4.486
    ✅ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=0.4839 (C:0.4839, R:0.0105)
Batch  25/537: Loss=0.5281 (C:0.5281, R:0.0105)
Batch  50/537: Loss=0.4801 (C:0.4801, R:0.0105)
Batch  75/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 100/537: Loss=0.4838 (C:0.4838, R:0.0105)
Batch 125/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 150/537: Loss=0.4792 (C:0.4792, R:0.0105)
Batch 175/537: Loss=0.4743 (C:0.4743, R:0.0105)
Batch 200/537: Loss=0.5097 (C:0.5097, R:0.0105)
Batch 225/537: Loss=0.4560 (C:0.4560, R:0.0105)
Batch 250/537: Loss=0.5245 (C:0.5245, R:0.0106)
Batch 275/537: Loss=0.4949 (C:0.4949, R:0.0105)
Batch 300/537: Loss=0.4849 (C:0.4849, R:0.0105)
Batch 325/537: Loss=0.4896 (C:0.4896, R:0.0105)
Batch 350/537: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 375/537: Loss=0.4753 (C:0.4753, R:0.0105)
Batch 400/537: Loss=0.5211 (C:0.5211, R:0.0106)
Batch 425/537: Loss=0.5051 (C:0.5051, R:0.0105)
Batch 450/537: Loss=0.5074 (C:0.5074, R:0.0105)
Batch 475/537: Loss=0.4774 (C:0.4774, R:0.0105)
Batch 500/537: Loss=0.4994 (C:0.4994, R:0.0105)
Batch 525/537: Loss=0.5260 (C:0.5260, R:0.0105)

============================================================
Epoch 88/100 completed in 37.1s
Train: Loss=0.4949 (C:0.4949, R:0.0105) Ratio=5.57x
Val:   Loss=0.3749 (C:0.3749, R:0.0105) Ratio=9.92x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3749)
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=0.4798 (C:0.4798, R:0.0105)
Batch  25/537: Loss=0.5067 (C:0.5067, R:0.0105)
Batch  50/537: Loss=0.4657 (C:0.4657, R:0.0105)
Batch  75/537: Loss=0.5245 (C:0.5245, R:0.0105)
Batch 100/537: Loss=0.4912 (C:0.4912, R:0.0105)
Batch 125/537: Loss=0.4891 (C:0.4891, R:0.0105)
Batch 150/537: Loss=0.4863 (C:0.4863, R:0.0105)
Batch 175/537: Loss=0.4933 (C:0.4933, R:0.0105)
Batch 200/537: Loss=0.5186 (C:0.5186, R:0.0105)
Batch 225/537: Loss=0.4677 (C:0.4677, R:0.0105)
Batch 250/537: Loss=0.4772 (C:0.4772, R:0.0105)
Batch 275/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch 300/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch 325/537: Loss=0.4979 (C:0.4979, R:0.0105)
Batch 350/537: Loss=0.4735 (C:0.4735, R:0.0105)
Batch 375/537: Loss=0.5218 (C:0.5218, R:0.0105)
Batch 400/537: Loss=0.4862 (C:0.4862, R:0.0105)
Batch 425/537: Loss=0.4940 (C:0.4940, R:0.0105)
Batch 450/537: Loss=0.5256 (C:0.5256, R:0.0105)
Batch 475/537: Loss=0.4896 (C:0.4896, R:0.0106)
Batch 500/537: Loss=0.5528 (C:0.5528, R:0.0105)
Batch 525/537: Loss=0.4832 (C:0.4832, R:0.0105)

============================================================
Epoch 89/100 completed in 31.1s
Train: Loss=0.4931 (C:0.4931, R:0.0105) Ratio=5.49x
Val:   Loss=0.3731 (C:0.3731, R:0.0105) Ratio=10.01x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3731)
============================================================

Epoch 90 Training
----------------------------------------
Batch   0/537: Loss=0.4582 (C:0.4582, R:0.0105)
Batch  25/537: Loss=0.4774 (C:0.4774, R:0.0105)
Batch  50/537: Loss=0.4875 (C:0.4875, R:0.0105)
Batch  75/537: Loss=0.4986 (C:0.4986, R:0.0105)
Batch 100/537: Loss=0.5047 (C:0.5047, R:0.0105)
Batch 125/537: Loss=0.4731 (C:0.4731, R:0.0105)
Batch 150/537: Loss=0.4870 (C:0.4870, R:0.0105)
Batch 175/537: Loss=0.4837 (C:0.4837, R:0.0105)
Batch 200/537: Loss=0.4836 (C:0.4836, R:0.0105)
Batch 225/537: Loss=0.4872 (C:0.4872, R:0.0105)
Batch 250/537: Loss=0.4919 (C:0.4919, R:0.0105)
Batch 275/537: Loss=0.4980 (C:0.4980, R:0.0105)
Batch 300/537: Loss=0.4909 (C:0.4909, R:0.0105)
Batch 325/537: Loss=0.4745 (C:0.4745, R:0.0105)
Batch 350/537: Loss=0.4813 (C:0.4813, R:0.0105)
Batch 375/537: Loss=0.4759 (C:0.4759, R:0.0105)
Batch 400/537: Loss=0.4699 (C:0.4699, R:0.0105)
Batch 425/537: Loss=0.5094 (C:0.5094, R:0.0105)
Batch 450/537: Loss=0.4908 (C:0.4908, R:0.0105)
Batch 475/537: Loss=0.4938 (C:0.4938, R:0.0105)
Batch 500/537: Loss=0.5391 (C:0.5391, R:0.0105)
Batch 525/537: Loss=0.4921 (C:0.4921, R:0.0105)

============================================================
Epoch 90/100 completed in 30.7s
Train: Loss=0.4942 (C:0.4942, R:0.0105) Ratio=5.57x
Val:   Loss=0.3727 (C:0.3727, R:0.0105) Ratio=10.03x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3727)
Checkpoint saved at epoch 90
============================================================

🌍 Updating global dataset at epoch 91
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.279 ± 0.535
    Neg distances: 2.731 ± 1.111
    Separation ratio: 9.80x
    Gap: -4.546
    ✅ Excellent global separation!

Epoch 91 Training
----------------------------------------
Batch   0/537: Loss=0.4610 (C:0.4610, R:0.0105)
Batch  25/537: Loss=0.5160 (C:0.5160, R:0.0105)
Batch  50/537: Loss=0.4758 (C:0.4758, R:0.0105)
Batch  75/537: Loss=0.4956 (C:0.4956, R:0.0105)
Batch 100/537: Loss=0.4833 (C:0.4833, R:0.0105)
Batch 125/537: Loss=0.5074 (C:0.5074, R:0.0105)
Batch 150/537: Loss=0.4990 (C:0.4990, R:0.0105)
Batch 175/537: Loss=0.5052 (C:0.5052, R:0.0106)
Batch 200/537: Loss=0.5053 (C:0.5053, R:0.0105)
Batch 225/537: Loss=0.4996 (C:0.4996, R:0.0105)
Batch 250/537: Loss=0.5051 (C:0.5051, R:0.0105)
Batch 275/537: Loss=0.4686 (C:0.4686, R:0.0105)
Batch 300/537: Loss=0.4549 (C:0.4549, R:0.0105)
Batch 325/537: Loss=0.4863 (C:0.4863, R:0.0105)
Batch 350/537: Loss=0.4969 (C:0.4969, R:0.0105)
Batch 375/537: Loss=0.5010 (C:0.5010, R:0.0105)
Batch 400/537: Loss=0.4948 (C:0.4948, R:0.0105)
Batch 425/537: Loss=0.5028 (C:0.5028, R:0.0105)
Batch 450/537: Loss=0.4686 (C:0.4686, R:0.0105)
Batch 475/537: Loss=0.4748 (C:0.4748, R:0.0105)
Batch 500/537: Loss=0.4766 (C:0.4766, R:0.0105)
Batch 525/537: Loss=0.4739 (C:0.4739, R:0.0105)

============================================================
Epoch 91/100 completed in 37.4s
Train: Loss=0.4922 (C:0.4922, R:0.0105) Ratio=5.55x
Val:   Loss=0.3709 (C:0.3709, R:0.0105) Ratio=10.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3709)
============================================================

Epoch 92 Training
----------------------------------------
Batch   0/537: Loss=0.5057 (C:0.5057, R:0.0105)
Batch  25/537: Loss=0.4977 (C:0.4977, R:0.0105)
Batch  50/537: Loss=0.4759 (C:0.4759, R:0.0105)
Batch  75/537: Loss=0.5143 (C:0.5143, R:0.0105)
Batch 100/537: Loss=0.5307 (C:0.5307, R:0.0105)
Batch 125/537: Loss=0.4904 (C:0.4904, R:0.0105)
Batch 150/537: Loss=0.5081 (C:0.5081, R:0.0105)
Batch 175/537: Loss=0.5248 (C:0.5248, R:0.0106)
Batch 200/537: Loss=0.4788 (C:0.4788, R:0.0105)
Batch 225/537: Loss=0.4866 (C:0.4866, R:0.0105)
Batch 250/537: Loss=0.5037 (C:0.5037, R:0.0105)
Batch 275/537: Loss=0.4805 (C:0.4805, R:0.0105)
Batch 300/537: Loss=0.4718 (C:0.4718, R:0.0105)
Batch 325/537: Loss=0.4902 (C:0.4902, R:0.0105)
Batch 350/537: Loss=0.4919 (C:0.4919, R:0.0105)
Batch 375/537: Loss=0.5049 (C:0.5049, R:0.0105)
Batch 400/537: Loss=0.4829 (C:0.4829, R:0.0105)
Batch 425/537: Loss=0.4233 (C:0.4233, R:0.0105)
Batch 450/537: Loss=0.4900 (C:0.4900, R:0.0105)
Batch 475/537: Loss=0.4840 (C:0.4840, R:0.0105)
Batch 500/537: Loss=0.4857 (C:0.4857, R:0.0105)
Batch 525/537: Loss=0.4846 (C:0.4846, R:0.0105)

============================================================
Epoch 92/100 completed in 31.0s
Train: Loss=0.4909 (C:0.4909, R:0.0105) Ratio=5.52x
Val:   Loss=0.3683 (C:0.3683, R:0.0105) Ratio=10.33x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3683)
============================================================

Epoch 93 Training
----------------------------------------
Batch   0/537: Loss=0.4899 (C:0.4899, R:0.0105)
Batch  25/537: Loss=0.4838 (C:0.4838, R:0.0105)
Batch  50/537: Loss=0.4713 (C:0.4713, R:0.0105)
Batch  75/537: Loss=0.4876 (C:0.4876, R:0.0105)
Batch 100/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 125/537: Loss=0.4883 (C:0.4883, R:0.0105)
Batch 150/537: Loss=0.4656 (C:0.4656, R:0.0105)
Batch 175/537: Loss=0.4571 (C:0.4571, R:0.0105)
Batch 200/537: Loss=0.5171 (C:0.5171, R:0.0105)
Batch 225/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 250/537: Loss=0.4935 (C:0.4935, R:0.0105)
Batch 275/537: Loss=0.4765 (C:0.4765, R:0.0105)
Batch 300/537: Loss=0.4889 (C:0.4889, R:0.0105)
Batch 325/537: Loss=0.4925 (C:0.4925, R:0.0106)
Batch 350/537: Loss=0.4873 (C:0.4873, R:0.0105)
Batch 375/537: Loss=0.5018 (C:0.5018, R:0.0105)
Batch 400/537: Loss=0.4780 (C:0.4780, R:0.0105)
Batch 425/537: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 450/537: Loss=0.5474 (C:0.5474, R:0.0105)
Batch 475/537: Loss=0.4775 (C:0.4775, R:0.0105)
Batch 500/537: Loss=0.4917 (C:0.4917, R:0.0105)
Batch 525/537: Loss=0.4980 (C:0.4980, R:0.0105)

============================================================
Epoch 93/100 completed in 31.5s
Train: Loss=0.4895 (C:0.4895, R:0.0105) Ratio=5.55x
Val:   Loss=0.3668 (C:0.3668, R:0.0105) Ratio=10.40x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3668)
============================================================

🌍 Updating global dataset at epoch 94
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.267 ± 0.545
    Neg distances: 2.738 ± 1.102
    Separation ratio: 10.24x
    Gap: -4.660
    ✅ Excellent global separation!

Epoch 94 Training
----------------------------------------
Batch   0/537: Loss=0.4966 (C:0.4966, R:0.0105)
Batch  25/537: Loss=0.4905 (C:0.4905, R:0.0105)
Batch  50/537: Loss=0.4831 (C:0.4831, R:0.0105)
Batch  75/537: Loss=0.4691 (C:0.4691, R:0.0105)
Batch 100/537: Loss=0.4674 (C:0.4674, R:0.0105)
Batch 125/537: Loss=0.4803 (C:0.4803, R:0.0105)
Batch 150/537: Loss=0.4784 (C:0.4784, R:0.0105)
Batch 175/537: Loss=0.4749 (C:0.4749, R:0.0105)
Batch 200/537: Loss=0.4670 (C:0.4670, R:0.0105)
Batch 225/537: Loss=0.4837 (C:0.4837, R:0.0105)
Batch 250/537: Loss=0.4633 (C:0.4633, R:0.0105)
Batch 275/537: Loss=0.4856 (C:0.4856, R:0.0105)
Batch 300/537: Loss=0.4618 (C:0.4618, R:0.0105)
Batch 325/537: Loss=0.4618 (C:0.4618, R:0.0105)
Batch 350/537: Loss=0.4747 (C:0.4747, R:0.0105)
Batch 375/537: Loss=0.4620 (C:0.4620, R:0.0105)
Batch 400/537: Loss=0.4215 (C:0.4215, R:0.0105)
Batch 425/537: Loss=0.4575 (C:0.4575, R:0.0105)
Batch 450/537: Loss=0.4791 (C:0.4791, R:0.0105)
Batch 475/537: Loss=0.4908 (C:0.4908, R:0.0105)
Batch 500/537: Loss=0.4930 (C:0.4930, R:0.0105)
Batch 525/537: Loss=0.4775 (C:0.4775, R:0.0105)

============================================================
Epoch 94/100 completed in 37.7s
Train: Loss=0.4787 (C:0.4787, R:0.0105) Ratio=5.65x
Val:   Loss=0.3571 (C:0.3571, R:0.0105) Ratio=10.41x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3571)
============================================================

Epoch 95 Training
----------------------------------------
Batch   0/537: Loss=0.4715 (C:0.4715, R:0.0105)
Batch  25/537: Loss=0.4657 (C:0.4657, R:0.0105)
Batch  50/537: Loss=0.4801 (C:0.4801, R:0.0105)
Batch  75/537: Loss=0.4734 (C:0.4734, R:0.0105)
Batch 100/537: Loss=0.4831 (C:0.4831, R:0.0105)
Batch 125/537: Loss=0.4938 (C:0.4938, R:0.0106)
Batch 150/537: Loss=0.4480 (C:0.4480, R:0.0105)
Batch 175/537: Loss=0.4947 (C:0.4947, R:0.0105)
Batch 200/537: Loss=0.4798 (C:0.4798, R:0.0105)
Batch 225/537: Loss=0.4887 (C:0.4887, R:0.0105)
Batch 250/537: Loss=0.4598 (C:0.4598, R:0.0105)
Batch 275/537: Loss=0.4968 (C:0.4968, R:0.0105)
Batch 300/537: Loss=0.4786 (C:0.4786, R:0.0105)
Batch 325/537: Loss=0.4967 (C:0.4967, R:0.0105)
Batch 350/537: Loss=0.4989 (C:0.4989, R:0.0106)
Batch 375/537: Loss=0.4744 (C:0.4744, R:0.0105)
Batch 400/537: Loss=0.4523 (C:0.4523, R:0.0105)
Batch 425/537: Loss=0.4910 (C:0.4910, R:0.0105)
Batch 450/537: Loss=0.4897 (C:0.4897, R:0.0105)
Batch 475/537: Loss=0.4761 (C:0.4761, R:0.0105)
Batch 500/537: Loss=0.5340 (C:0.5340, R:0.0105)
Batch 525/537: Loss=0.4739 (C:0.4739, R:0.0105)

============================================================
Epoch 95/100 completed in 31.0s
Train: Loss=0.4790 (C:0.4790, R:0.0105) Ratio=5.65x
Val:   Loss=0.3581 (C:0.3581, R:0.0105) Ratio=10.39x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 95
============================================================

Epoch 96 Training
----------------------------------------
Batch   0/537: Loss=0.4853 (C:0.4853, R:0.0105)
Batch  25/537: Loss=0.4512 (C:0.4512, R:0.0105)
Batch  50/537: Loss=0.4608 (C:0.4608, R:0.0105)
Batch  75/537: Loss=0.4227 (C:0.4227, R:0.0105)
Batch 100/537: Loss=0.4677 (C:0.4677, R:0.0105)
Batch 125/537: Loss=0.4689 (C:0.4689, R:0.0105)
Batch 150/537: Loss=0.4777 (C:0.4777, R:0.0105)
Batch 175/537: Loss=0.4527 (C:0.4527, R:0.0105)
Batch 200/537: Loss=0.4856 (C:0.4856, R:0.0105)
Batch 225/537: Loss=0.5218 (C:0.5218, R:0.0105)
Batch 250/537: Loss=0.4837 (C:0.4837, R:0.0105)
Batch 275/537: Loss=0.4645 (C:0.4645, R:0.0105)
Batch 300/537: Loss=0.5003 (C:0.5003, R:0.0105)
Batch 325/537: Loss=0.4549 (C:0.4549, R:0.0105)
Batch 350/537: Loss=0.4827 (C:0.4827, R:0.0105)
Batch 375/537: Loss=0.4929 (C:0.4929, R:0.0105)
Batch 400/537: Loss=0.4629 (C:0.4629, R:0.0105)
Batch 425/537: Loss=0.4716 (C:0.4716, R:0.0105)
Batch 450/537: Loss=0.4824 (C:0.4824, R:0.0105)
Batch 475/537: Loss=0.4996 (C:0.4996, R:0.0105)
Batch 500/537: Loss=0.4969 (C:0.4969, R:0.0105)
Batch 525/537: Loss=0.5111 (C:0.5111, R:0.0105)

============================================================
Epoch 96/100 completed in 30.6s
Train: Loss=0.4774 (C:0.4774, R:0.0105) Ratio=5.63x
Val:   Loss=0.3558 (C:0.3558, R:0.0105) Ratio=10.46x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3558)
============================================================

🌍 Updating global dataset at epoch 97
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.264 ± 0.542
    Neg distances: 2.749 ± 1.106
    Separation ratio: 10.43x
    Gap: -4.644
    ✅ Excellent global separation!

Epoch 97 Training
----------------------------------------
Batch   0/537: Loss=0.4889 (C:0.4889, R:0.0105)
Batch  25/537: Loss=0.4731 (C:0.4731, R:0.0105)
Batch  50/537: Loss=0.4454 (C:0.4454, R:0.0105)
Batch  75/537: Loss=0.4336 (C:0.4336, R:0.0105)
Batch 100/537: Loss=0.4681 (C:0.4681, R:0.0105)
Batch 125/537: Loss=0.4201 (C:0.4201, R:0.0105)
Batch 150/537: Loss=0.4537 (C:0.4537, R:0.0105)
Batch 175/537: Loss=0.4752 (C:0.4752, R:0.0105)
Batch 200/537: Loss=0.4838 (C:0.4838, R:0.0105)
Batch 225/537: Loss=0.4514 (C:0.4514, R:0.0105)
Batch 250/537: Loss=0.4601 (C:0.4601, R:0.0105)
Batch 275/537: Loss=0.4913 (C:0.4913, R:0.0105)
Batch 300/537: Loss=0.4855 (C:0.4855, R:0.0105)
Batch 325/537: Loss=0.4623 (C:0.4623, R:0.0105)
Batch 350/537: Loss=0.5010 (C:0.5010, R:0.0105)
Batch 375/537: Loss=0.4839 (C:0.4839, R:0.0105)
Batch 400/537: Loss=0.4850 (C:0.4850, R:0.0105)
Batch 425/537: Loss=0.4999 (C:0.4999, R:0.0105)
Batch 450/537: Loss=0.4580 (C:0.4580, R:0.0105)
Batch 475/537: Loss=0.4936 (C:0.4936, R:0.0105)
Batch 500/537: Loss=0.4914 (C:0.4914, R:0.0105)
Batch 525/537: Loss=0.4613 (C:0.4613, R:0.0105)

============================================================
Epoch 97/100 completed in 36.4s
Train: Loss=0.4729 (C:0.4729, R:0.0105) Ratio=5.65x
Val:   Loss=0.3551 (C:0.3551, R:0.0105) Ratio=10.35x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3551)
============================================================

Epoch 98 Training
----------------------------------------
Batch   0/537: Loss=0.4522 (C:0.4522, R:0.0105)
Batch  25/537: Loss=0.4708 (C:0.4708, R:0.0105)
Batch  50/537: Loss=0.4763 (C:0.4763, R:0.0105)
Batch  75/537: Loss=0.4240 (C:0.4240, R:0.0105)
Batch 100/537: Loss=0.4557 (C:0.4557, R:0.0105)
Batch 125/537: Loss=0.4974 (C:0.4974, R:0.0105)
Batch 150/537: Loss=0.4408 (C:0.4408, R:0.0105)
Batch 175/537: Loss=0.4745 (C:0.4745, R:0.0105)
Batch 200/537: Loss=0.4530 (C:0.4530, R:0.0105)
Batch 225/537: Loss=0.4585 (C:0.4585, R:0.0105)
Batch 250/537: Loss=0.4799 (C:0.4799, R:0.0105)
Batch 275/537: Loss=0.4676 (C:0.4676, R:0.0105)
Batch 300/537: Loss=0.4589 (C:0.4589, R:0.0105)
Batch 325/537: Loss=0.4853 (C:0.4853, R:0.0105)
Batch 350/537: Loss=0.5034 (C:0.5034, R:0.0105)
Batch 375/537: Loss=0.4687 (C:0.4687, R:0.0105)
Batch 400/537: Loss=0.4934 (C:0.4934, R:0.0105)
Batch 425/537: Loss=0.5121 (C:0.5121, R:0.0105)
Batch 450/537: Loss=0.4912 (C:0.4912, R:0.0105)
Batch 475/537: Loss=0.4585 (C:0.4585, R:0.0105)
Batch 500/537: Loss=0.4843 (C:0.4843, R:0.0105)
Batch 525/537: Loss=0.4785 (C:0.4785, R:0.0105)

============================================================
Epoch 98/100 completed in 30.5s
Train: Loss=0.4732 (C:0.4732, R:0.0105) Ratio=5.74x
Val:   Loss=0.3510 (C:0.3510, R:0.0105) Ratio=10.64x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3510)
============================================================

Epoch 99 Training
----------------------------------------
Batch   0/537: Loss=0.4565 (C:0.4565, R:0.0105)
Batch  25/537: Loss=0.4809 (C:0.4809, R:0.0105)
Batch  50/537: Loss=0.4759 (C:0.4759, R:0.0105)
Batch  75/537: Loss=0.4545 (C:0.4545, R:0.0105)
Batch 100/537: Loss=0.4244 (C:0.4244, R:0.0106)
Batch 125/537: Loss=0.4412 (C:0.4412, R:0.0105)
Batch 150/537: Loss=0.5106 (C:0.5106, R:0.0105)
Batch 175/537: Loss=0.4342 (C:0.4342, R:0.0105)
Batch 200/537: Loss=0.4636 (C:0.4636, R:0.0105)
Batch 225/537: Loss=0.4619 (C:0.4619, R:0.0105)
Batch 250/537: Loss=0.4642 (C:0.4642, R:0.0105)
Batch 275/537: Loss=0.4405 (C:0.4405, R:0.0105)
Batch 300/537: Loss=0.4575 (C:0.4575, R:0.0105)
Batch 325/537: Loss=0.4762 (C:0.4762, R:0.0105)
Batch 350/537: Loss=0.4773 (C:0.4773, R:0.0105)
Batch 375/537: Loss=0.5102 (C:0.5102, R:0.0105)
Batch 400/537: Loss=0.4697 (C:0.4697, R:0.0105)
Batch 425/537: Loss=0.4714 (C:0.4714, R:0.0105)
Batch 450/537: Loss=0.4549 (C:0.4549, R:0.0105)
Batch 475/537: Loss=0.4585 (C:0.4585, R:0.0105)
Batch 500/537: Loss=0.4884 (C:0.4884, R:0.0105)
Batch 525/537: Loss=0.4731 (C:0.4731, R:0.0106)

============================================================
Epoch 99/100 completed in 30.6s
Train: Loss=0.4720 (C:0.4720, R:0.0105) Ratio=5.73x
Val:   Loss=0.3512 (C:0.3512, R:0.0105) Ratio=10.72x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 100
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.261 ± 0.527
    Neg distances: 2.741 ± 1.098
    Separation ratio: 10.50x
    Gap: -4.539
    ✅ Excellent global separation!

Epoch 100 Training
----------------------------------------
Batch   0/537: Loss=0.4817 (C:0.4817, R:0.0105)
Batch  25/537: Loss=0.4438 (C:0.4438, R:0.0105)
Batch  50/537: Loss=0.4790 (C:0.4790, R:0.0105)
Batch  75/537: Loss=0.4896 (C:0.4896, R:0.0105)
Batch 100/537: Loss=0.4613 (C:0.4613, R:0.0105)
Batch 125/537: Loss=0.4652 (C:0.4652, R:0.0105)
Batch 150/537: Loss=0.4513 (C:0.4513, R:0.0105)
Batch 175/537: Loss=0.4680 (C:0.4680, R:0.0105)
Batch 200/537: Loss=0.4550 (C:0.4550, R:0.0105)
Batch 225/537: Loss=0.4650 (C:0.4650, R:0.0105)
Batch 250/537: Loss=0.4653 (C:0.4653, R:0.0105)
Batch 275/537: Loss=0.4835 (C:0.4835, R:0.0105)
Batch 300/537: Loss=0.4594 (C:0.4594, R:0.0105)
Batch 325/537: Loss=0.4505 (C:0.4505, R:0.0105)
Batch 350/537: Loss=0.4616 (C:0.4616, R:0.0105)
Batch 375/537: Loss=0.4539 (C:0.4539, R:0.0105)
Batch 400/537: Loss=0.4626 (C:0.4626, R:0.0105)
Batch 425/537: Loss=0.4806 (C:0.4806, R:0.0105)
Batch 450/537: Loss=0.4806 (C:0.4806, R:0.0105)
Batch 475/537: Loss=0.4970 (C:0.4970, R:0.0105)
Batch 500/537: Loss=0.4846 (C:0.4846, R:0.0105)
Batch 525/537: Loss=0.4671 (C:0.4671, R:0.0105)

============================================================
Epoch 100/100 completed in 36.6s
Train: Loss=0.4690 (C:0.4690, R:0.0105) Ratio=5.68x
Val:   Loss=0.3498 (C:0.3498, R:0.0105) Ratio=10.59x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.3498)
Checkpoint saved at epoch 100
============================================================

Global Dataset Training Completed!
Best epoch: 100
Best validation loss: 0.3498
Final separation ratios: Train=5.68x, Val=10.59x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_162420/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/539 batches
  Processed 51/539 batches
  Processed 101/539 batches
  Processed 151/539 batches
  Processed 201/539 batches
  Processed 251/539 batches
  Processed 301/539 batches
  Processed 351/539 batches
  Processed 401/539 batches
  Processed 451/539 batches
  Processed 501/539 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: 0.8371
  Adjusted Rand Score: 0.9022
  Clustering Accuracy: 0.9664
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.9669
  Per-class F1: [0.9671884406983744, 0.9532379406992181, 0.9806902843241598]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010513
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 0.260 ± 0.521
  Negative distances: 2.769 ± 1.105
  Separation ratio: 10.67x
  Gap: -4.710
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.8371
  Clustering Accuracy: 0.9664
  Adjusted Rand Score: 0.9022

Classification Performance:
  Accuracy: 0.9669

Separation Quality:
  Separation Ratio: 10.67x
  Gap: -4.710
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010513
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_162420/results/evaluation_results_20250712_172204.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_162420/results/evaluation_results_20250712_172204.json
Saving final experiment results...

PIPELINE FAILED: Object of type float32 is not JSON serializable

Analysis completed with exit code: 0
Time: Sat 12 Jul 17:22:06 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
