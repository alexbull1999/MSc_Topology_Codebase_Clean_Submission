Starting Surface Distance Metric Analysis job...
Job ID: 185635
Node: gpuvm13
Time: Thu 24 Jul 11:33:27 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Thu Jul 24 11:33:29 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-24 11:33:36.127231
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 200
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
AttentionAutoencoder initialized:
  Input dim: 1536
  Latent dim: 100
  Hidden dims: [1024, 768, 512, 256, 128]
  Attention Heads: 5
  Total parameters: 5,905,916
Model created with 5,905,916 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 0.0
  Base reconstruction weight: 1.0
Optimizer created: Adam (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 5,905,916
Starting training...
========================================
Starting Global Dataset Training...
============================================================

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=0.0111 (C:3.2069, R:0.0111)
Batch  25/537: Loss=0.0101 (C:3.0433, R:0.0101)
Batch  50/537: Loss=0.0100 (C:2.9069, R:0.0100)
Batch  75/537: Loss=0.0100 (C:2.7790, R:0.0100)
Batch 100/537: Loss=0.0100 (C:2.6644, R:0.0100)
Batch 125/537: Loss=0.0100 (C:2.5771, R:0.0100)
Batch 150/537: Loss=0.0100 (C:2.4611, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.3683, R:0.0100)
Batch 200/537: Loss=0.0099 (C:2.2887, R:0.0099)
Batch 225/537: Loss=0.0100 (C:2.1986, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.1399, R:0.0099)
Batch 275/537: Loss=0.0100 (C:2.0928, R:0.0100)
Batch 300/537: Loss=0.0100 (C:2.0452, R:0.0100)
Batch 325/537: Loss=0.0099 (C:2.0246, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0130, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0032, R:0.0100)
Batch 400/537: Loss=0.0100 (C:2.0017, R:0.0100)
Batch 425/537: Loss=0.0100 (C:2.0005, R:0.0100)
Batch 450/537: Loss=0.0099 (C:1.9997, R:0.0099)
Batch 475/537: Loss=0.0100 (C:1.9997, R:0.0100)
Batch 500/537: Loss=0.0099 (C:2.0001, R:0.0099)
Batch 525/537: Loss=0.0100 (C:2.0003, R:0.0100)

============================================================
Epoch 1/200 completed in 13.9s
Train: Loss=0.0100 (C:2.2904, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0005, R:0.0100)
Batch  25/537: Loss=0.0100 (C:2.0002, R:0.0100)
Batch  50/537: Loss=0.0099 (C:2.0002, R:0.0099)
Batch  75/537: Loss=0.0100 (C:2.0003, R:0.0100)
Batch 100/537: Loss=0.0100 (C:1.9999, R:0.0100)
Batch 125/537: Loss=0.0099 (C:1.9997, R:0.0099)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0001, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0004, R:0.0100)
Batch 225/537: Loss=0.0100 (C:1.9998, R:0.0100)
Batch 250/537: Loss=0.0100 (C:2.0002, R:0.0100)
Batch 275/537: Loss=0.0099 (C:1.9999, R:0.0099)
Batch 300/537: Loss=0.0100 (C:2.0001, R:0.0100)
Batch 325/537: Loss=0.0100 (C:2.0003, R:0.0100)
Batch 350/537: Loss=0.0099 (C:2.0002, R:0.0099)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0100 (C:2.0003, R:0.0100)
Batch 425/537: Loss=0.0100 (C:1.9999, R:0.0100)
Batch 450/537: Loss=0.0100 (C:1.9997, R:0.0100)
Batch 475/537: Loss=0.0100 (C:2.0002, R:0.0100)
Batch 500/537: Loss=0.0100 (C:2.0001, R:0.0100)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 2/200 completed in 13.7s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 1 epochs
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0001, R:0.0100)
Batch  25/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  50/537: Loss=0.0100 (C:2.0001, R:0.0100)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0099 (C:2.0001, R:0.0099)
Batch 125/537: Loss=0.0100 (C:1.9999, R:0.0100)
Batch 150/537: Loss=0.0100 (C:2.0001, R:0.0100)
Batch 175/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 250/537: Loss=0.0100 (C:1.9999, R:0.0100)
Batch 275/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 300/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 325/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 350/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 375/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0100 (C:1.9999, R:0.0100)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 3/200 completed in 15.4s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 300/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 325/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 350/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 450/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 4/200 completed in 13.7s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 250/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 5/200 completed in 14.1s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 375/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 6/200 completed in 13.5s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
No improvement for 1 epochs
============================================================

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 175/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 200/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 325/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 7/200 completed in 13.7s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=0.99x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 350/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 375/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 8/200 completed in 13.9s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 9/200 completed in 13.4s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
No improvement for 1 epochs
============================================================

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 250/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 10/200 completed in 13.4s
Train: Loss=0.0100 (C:2.0000, R:0.0100) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 2 epochs
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 11/200 completed in 13.8s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  25/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 175/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 200/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 12/200 completed in 13.8s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
No improvement for 1 epochs
============================================================

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 13/200 completed in 13.1s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 2 epochs
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 300/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 14/200 completed in 13.2s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 3 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  50/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 175/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 15/200 completed in 13.2s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
No improvement for 4 epochs
============================================================

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  25/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  50/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 400/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 16/200 completed in 13.2s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
No improvement for 5 epochs
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 175/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 325/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 350/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 17/200 completed in 13.4s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 6 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  50/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 125/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 150/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 225/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 525/537: Loss=0.0099 (C:2.0000, R:0.0099)

============================================================
Epoch 18/200 completed in 13.7s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=0.00x
Reconstruction weight: 1.000
No improvement for 7 epochs
============================================================

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  25/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  50/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch  75/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 100/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 125/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 150/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 175/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 200/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 250/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 275/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 300/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 325/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 350/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 375/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 400/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 425/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 450/537: Loss=0.0100 (C:2.0000, R:0.0100)
Batch 475/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 500/537: Loss=0.0099 (C:2.0000, R:0.0099)
Batch 525/537: Loss=0.0100 (C:2.0000, R:0.0100)

============================================================
Epoch 19/200 completed in 13.6s
Train: Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Val:   Loss=0.0099 (C:2.0000, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 8 epochs

Early stopping triggered after 19 epochs
Best model was at epoch 11 with Val Loss: 0.0099

Global Dataset Training Completed!
Best epoch: 11
Best validation loss: 0.0099
Final separation ratios: Train=1.00x, Val=1.00x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 100])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.0000
  Adjusted Rand Score: 0.0000
  Clustering Accuracy: 0.3428
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 100])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 100])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.3333
  Per-class F1: [0.0, 0.5, 0.0]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009858
Evaluating separation quality...
Separation Results:
  Positive distances: 0.000 ± 0.000
  Negative distances: 0.000 ± 0.000
  Separation ratio: 1.00x
  Gap: 0.000
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.0000
  Clustering Accuracy: 0.3428
  Adjusted Rand Score: 0.0000

Classification Performance:
  Accuracy: 0.3333

Separation Quality:
  Separation Ratio: 1.00x
  Gap: 0.000
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009858
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336/results/evaluation_results_20250724_113822.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336/results/evaluation_results_20250724_113822.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_113336/final_results.json

Key Results:
  Separation ratio: 1.00x
  Perfect separation: False
  Classification accuracy: 0.3333

Analysis completed with exit code: 0
Time: Thu 24 Jul 11:38:23 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
