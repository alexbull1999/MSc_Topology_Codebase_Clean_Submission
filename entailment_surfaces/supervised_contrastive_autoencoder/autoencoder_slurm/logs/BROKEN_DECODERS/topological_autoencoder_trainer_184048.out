Starting Surface Distance Metric Analysis job...
Job ID: 184048
Node: gpuvm13
Time: Sat 19 Jul 16:45:20 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 16:45:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164528
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164528/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,852,466
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,852,466
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 5.0
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=2.0011 (C:2.0000, R:0.0110, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.7288 (C:1.7278, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.5894 (C:1.5884, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.5104 (C:1.5094, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.5022 (C:1.5012, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.4299 (C:1.4289, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.4322 (C:1.4312, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.4552 (C:1.4542, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.4513 (C:1.4503, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.4279 (C:1.4269, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.4261 (C:1.4251, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.4530 (C:1.4520, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3783 (C:1.3773, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.4430 (C:1.4420, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3729 (C:1.3719, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5015
  Contrastive: 1.5005
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3501
  Contrastive: 1.3491
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (20.9s)
Train Loss: 1.5015 (C:1.5005, R:0.0100, T:0.0000)
Val Loss:   1.3501 (C:1.3491, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3715 (C:1.3705, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.4129 (C:1.4119, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3928 (C:1.3918, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.4100 (C:1.4090, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3504 (C:1.3494, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3956 (C:1.3946, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.4060 (C:1.4050, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3756 (C:1.3746, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3995 (C:1.3985, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3544 (C:1.3534, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3355 (C:1.3345, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3622 (C:1.3612, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.4104 (C:1.4094, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3954 (C:1.3944, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3698 (C:1.3688, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.3807
  Contrastive: 1.3797
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2944
  Contrastive: 1.2934
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (20.8s)
Train Loss: 1.3807 (C:1.3797, R:0.0100, T:0.0000)
Val Loss:   1.2944 (C:1.2934, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3120 (C:1.3110, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3722 (C:1.3712, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3666 (C:1.3656, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.4062 (C:1.4052, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3797 (C:1.3787, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3564 (C:1.3554, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3703 (C:1.3694, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3258 (C:1.3248, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3695 (C:1.3685, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3296 (C:1.3286, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3367 (C:1.3357, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3397 (C:1.3387, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3480 (C:1.3470, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3293 (C:1.3283, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3643 (C:1.3633, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.3504
  Contrastive: 1.3494
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2727
  Contrastive: 1.2717
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (23.2s)
Train Loss: 1.3504 (C:1.3494, R:0.0100, T:0.0000)
Val Loss:   1.2727 (C:1.2717, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3556 (C:1.3546, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3268 (C:1.3258, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3540 (C:1.3530, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3264 (C:1.3254, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2966 (C:1.2957, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3006 (C:1.2996, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3637 (C:1.3627, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3686 (C:1.3677, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3764 (C:1.3754, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3462 (C:1.3453, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2829 (C:1.2819, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3564 (C:1.3554, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3622 (C:1.3612, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3198 (C:1.3188, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3008 (C:1.2998, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.3302
  Contrastive: 1.3292
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2686
  Contrastive: 1.2676
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (22.0s)
Train Loss: 1.3302 (C:1.3292, R:0.0100, T:0.0000)
Val Loss:   1.2686 (C:1.2676, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3169 (C:1.3159, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3194 (C:1.3184, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3203 (C:1.3193, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3564 (C:1.3554, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3304 (C:1.3294, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3289 (C:1.3279, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3280 (C:1.3270, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2942 (C:1.2932, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3423 (C:1.3413, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3048 (C:1.3038, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3747 (C:1.3737, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2981 (C:1.2971, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3439 (C:1.3429, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3089 (C:1.3079, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3432 (C:1.3422, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.3196
  Contrastive: 1.3186
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2491
  Contrastive: 1.2481
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (22.7s)
Train Loss: 1.3196 (C:1.3186, R:0.0100, T:0.0000)
Val Loss:   1.2491 (C:1.2481, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3017 (C:1.3007, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3000 (C:1.2990, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2742 (C:1.2732, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2518 (C:1.2508, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2956 (C:1.2946, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3054 (C:1.3044, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2606 (C:1.2596, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2894 (C:1.2884, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3170 (C:1.3160, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2705 (C:1.2695, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3673 (C:1.3663, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2964 (C:1.2955, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2732 (C:1.2722, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3136 (C:1.3126, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3355 (C:1.3345, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.3041
  Contrastive: 1.3031
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2322
  Contrastive: 1.2312
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (21.9s)
Train Loss: 1.3041 (C:1.3031, R:0.0100, T:0.0000)
Val Loss:   1.2322 (C:1.2312, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3045 (C:1.3035, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3031 (C:1.3021, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3412 (C:1.3402, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2947 (C:1.2937, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3032 (C:1.3022, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3362 (C:1.3352, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3099 (C:1.3089, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3296 (C:1.3286, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2992 (C:1.2982, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2744 (C:1.2734, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3771 (C:1.3761, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2605 (C:1.2595, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3375 (C:1.3365, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2889 (C:1.2879, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2966 (C:1.2956, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.2943
  Contrastive: 1.2933
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2396
  Contrastive: 1.2386
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 7/50 COMPLETE (20.9s)
Train Loss: 1.2943 (C:1.2933, R:0.0100, T:0.0000)
Val Loss:   1.2396 (C:1.2386, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2687 (C:1.2677, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2951 (C:1.2941, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3360 (C:1.3350, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3008 (C:1.2998, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3142 (C:1.3132, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2913 (C:1.2903, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2767 (C:1.2757, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3364 (C:1.3354, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2780 (C:1.2770, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2889 (C:1.2879, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3192 (C:1.3182, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2782 (C:1.2772, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3009 (C:1.3000, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2975 (C:1.2965, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2946 (C:1.2936, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.2930
  Contrastive: 1.2920
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2361
  Contrastive: 1.2351
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 8/50 COMPLETE (21.7s)
Train Loss: 1.2930 (C:1.2920, R:0.0100, T:0.0000)
Val Loss:   1.2361 (C:1.2351, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3338 (C:1.3328, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2722 (C:1.2712, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2981 (C:1.2971, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2430 (C:1.2420, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3000 (C:1.2990, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2686 (C:1.2676, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2057 (C:1.2047, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3049 (C:1.3039, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3162 (C:1.3152, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2819 (C:1.2809, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2908 (C:1.2898, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2747 (C:1.2737, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2017 (C:1.2007, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2549 (C:1.2539, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2607 (C:1.2597, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.2807
  Contrastive: 1.2797
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2101
  Contrastive: 1.2091
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 9/50 COMPLETE (22.5s)
Train Loss: 1.2807 (C:1.2797, R:0.0100, T:0.0000)
Val Loss:   1.2101 (C:1.2091, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2818 (C:1.2808, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2738 (C:1.2728, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2754 (C:1.2744, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3208 (C:1.3198, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2675 (C:1.2665, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2205 (C:1.2195, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2901 (C:1.2891, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2473 (C:1.2463, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3214 (C:1.3204, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2258 (C:1.2248, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3146 (C:1.3136, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2634 (C:1.2624, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2385 (C:1.2375, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3124 (C:1.3114, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2309 (C:1.2299, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.2708
  Contrastive: 1.2698
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1996
  Contrastive: 1.1986
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 10/50 COMPLETE (21.7s)
Train Loss: 1.2708 (C:1.2698, R:0.0100, T:0.0000)
Val Loss:   1.1996 (C:1.1986, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=35789.4492 (C:1.2283, R:0.0099, T:35788.2188(w:5.000)⚠️)
Batch  25/365: Loss=843.8937 (C:34.1799, R:0.0099, T:809.7128(w:5.000)⚠️)
Batch  50/365: Loss=864.6746 (C:79.0171, R:0.0099, T:785.6565(w:5.000)⚠️)
Batch  75/365: Loss=2168.1323 (C:131.9328, R:0.0100, T:2036.1986(w:5.000)⚠️)
Batch 100/365: Loss=497.6400 (C:108.9846, R:0.0099, T:388.6544(w:5.000)⚠️)
Batch 125/365: Loss=717.6249 (C:155.4333, R:0.0100, T:562.1907(w:5.000)⚠️)
Batch 150/365: Loss=302.4150 (C:134.8177, R:0.0100, T:167.5964(w:5.000)⚠️)
Batch 175/365: Loss=496.2510 (C:150.0752, R:0.0099, T:346.1749(w:5.000)⚠️)
Batch 200/365: Loss=745.0693 (C:132.3446, R:0.0099, T:612.7238(w:5.000)⚠️)
Batch 225/365: Loss=8674.2148 (C:145.2866, R:0.0100, T:8528.9268(w:5.000)⚠️)
Batch 250/365: Loss=396.8003 (C:168.8822, R:0.0099, T:227.9171(w:5.000)⚠️)
Batch 275/365: Loss=246.1289 (C:165.9290, R:0.0100, T:80.1989(w:5.000)⚠️)
Batch 300/365: Loss=1769.8058 (C:179.2640, R:0.0100, T:1590.5408(w:5.000)⚠️)
Batch 325/365: Loss=747.1382 (C:201.5758, R:0.0100, T:545.5613(w:5.000)⚠️)
Batch 350/365: Loss=1589.7312 (C:210.2961, R:0.0100, T:1379.4341(w:5.000)⚠️)
🎉 MILESTONE: First topological learning detected at epoch 11!
   Initial topological loss: 3642.0700
📈 New best topological loss: 3642.0700

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 3778.7672
  Contrastive: 136.6962
  Reconstruction: 0.0100
  Topological: 3642.0700 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 30750.1392
  Contrastive: 212.1832
  Reconstruction: 0.0100
  Topological: 30537.9549 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 11/50 COMPLETE (117.1s)
Train Loss: 3778.7672 (C:136.6962, R:0.0100, T:3642.0700)
Val Loss:   30750.1392 (C:212.1832, R:0.0100, T:30537.9549)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1272.0913 (C:239.4487, R:0.0099, T:1032.6416(w:5.000)⚠️)
Batch  25/365: Loss=3097.3247 (C:227.9599, R:0.0099, T:2869.3638(w:5.000)⚠️)
Batch  50/365: Loss=7807.2783 (C:227.3604, R:0.0099, T:7579.9170(w:5.000)⚠️)
Batch  75/365: Loss=1271.3999 (C:226.8745, R:0.0100, T:1044.5244(w:5.000)⚠️)
Batch 100/365: Loss=1237.4772 (C:232.2192, R:0.0099, T:1005.2570(w:5.000)⚠️)
Batch 125/365: Loss=311.2018 (C:210.8271, R:0.0099, T:100.3737(w:5.000)⚠️)
Batch 150/365: Loss=858.4274 (C:221.7516, R:0.0099, T:636.6747(w:5.000)⚠️)
Batch 175/365: Loss=1151.9473 (C:218.7608, R:0.0100, T:933.1855(w:5.000)⚠️)
Batch 200/365: Loss=1256.5402 (C:193.7395, R:0.0100, T:1062.7997(w:5.000)⚠️)
Batch 225/365: Loss=889.7274 (C:203.5624, R:0.0099, T:686.1639(w:5.000)⚠️)
Batch 250/365: Loss=4157.2837 (C:196.9053, R:0.0100, T:3960.3772(w:5.000)⚠️)
Batch 275/365: Loss=1428.1416 (C:171.1351, R:0.0099, T:1257.0055(w:5.000)⚠️)
Batch 300/365: Loss=2943.4551 (C:169.9729, R:0.0100, T:2773.4812(w:5.000)⚠️)
Batch 325/365: Loss=1916.0873 (C:160.1625, R:0.0100, T:1755.9238(w:5.000)⚠️)
Batch 350/365: Loss=5497.2910 (C:131.0304, R:0.0099, T:5366.2598(w:5.000)⚠️)
📈 New best topological loss: 2106.5369

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 2309.4752
  Contrastive: 202.9374
  Reconstruction: 0.0100
  Topological: 2106.5369 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 8284.4166
  Contrastive: 110.8689
  Reconstruction: 0.0100
  Topological: 8173.5467 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (120.9s)
Train Loss: 2309.4752 (C:202.9374, R:0.0100, T:2106.5369)
Val Loss:   8284.4166 (C:110.8689, R:0.0100, T:8173.5467)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1693.0342 (C:125.4446, R:0.0099, T:1567.5886(w:5.000)⚠️)
Batch  25/365: Loss=2338.1814 (C:82.3799, R:0.0100, T:2255.8005(w:5.000)⚠️)
Batch  50/365: Loss=1019.4974 (C:49.0180, R:0.0099, T:970.4784(w:5.000)⚠️)
Batch  75/365: Loss=726.3113 (C:51.7250, R:0.0099, T:674.5853(w:5.000)⚠️)
Batch 100/365: Loss=271.6573 (C:44.4949, R:0.0100, T:227.1614(w:5.000)⚠️)
Batch 125/365: Loss=297.9982 (C:44.0528, R:0.0100, T:253.9445(w:5.000)⚠️)
Batch 150/365: Loss=3646.2290 (C:52.7160, R:0.0099, T:3593.5120(w:5.000)⚠️)
Batch 175/365: Loss=829.5368 (C:50.3938, R:0.0099, T:779.1420(w:5.000)⚠️)
Batch 200/365: Loss=2124.2178 (C:53.9332, R:0.0100, T:2070.2837(w:5.000)⚠️)
Batch 225/365: Loss=630.5031 (C:52.4269, R:0.0100, T:578.0752(w:5.000)⚠️)
Batch 250/365: Loss=4764.1396 (C:48.8751, R:0.0100, T:4715.2637(w:5.000)⚠️)
Batch 275/365: Loss=1553.7083 (C:55.2399, R:0.0100, T:1498.4674(w:5.000)⚠️)
Batch 300/365: Loss=829.9104 (C:59.1954, R:0.0100, T:770.7141(w:5.000)⚠️)
Batch 325/365: Loss=1942.8652 (C:64.6735, R:0.0099, T:1878.1908(w:5.000)⚠️)
Batch 350/365: Loss=910.4337 (C:74.9090, R:0.0100, T:835.5236(w:5.000)⚠️)
📈 New best topological loss: 1636.6017

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 1694.9422
  Contrastive: 58.3395
  Reconstruction: 0.0100
  Topological: 1636.6017 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 9189.0831
  Contrastive: 76.4395
  Reconstruction: 0.0100
  Topological: 9112.6427 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 13/50 COMPLETE (121.0s)
Train Loss: 1694.9422 (C:58.3395, R:0.0100, T:1636.6017)
Val Loss:   9189.0831 (C:76.4395, R:0.0100, T:9112.6427)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1659.2932 (C:86.4565, R:0.0099, T:1572.8357(w:5.000)⚠️)
Batch  25/365: Loss=283.1835 (C:106.4910, R:0.0099, T:176.6915(w:5.000)⚠️)
Batch  50/365: Loss=1358.9569 (C:111.2594, R:0.0099, T:1247.6965(w:5.000)⚠️)
Batch  75/365: Loss=1155.5540 (C:83.9544, R:0.0100, T:1071.5985(w:5.000)⚠️)
Batch 100/365: Loss=1278.6083 (C:117.6683, R:0.0099, T:1160.9390(w:5.000)⚠️)
Batch 125/365: Loss=2802.8706 (C:168.0477, R:0.0100, T:2634.8220(w:5.000)⚠️)
Batch 150/365: Loss=3150.6731 (C:153.1449, R:0.0100, T:2997.5273(w:5.000)⚠️)
Batch 175/365: Loss=1031.5269 (C:155.7865, R:0.0099, T:875.7394(w:5.000)⚠️)
Batch 200/365: Loss=6272.3931 (C:192.3040, R:0.0100, T:6080.0879(w:5.000)⚠️)
Batch 225/365: Loss=642.2341 (C:159.5307, R:0.0099, T:482.7025(w:5.000)⚠️)
Batch 250/365: Loss=2110.2229 (C:205.7098, R:0.0100, T:1904.5122(w:5.000)⚠️)
Batch 275/365: Loss=735.2164 (C:201.0596, R:0.0099, T:534.1558(w:5.000)⚠️)
Batch 300/365: Loss=636.3906 (C:190.1518, R:0.0100, T:446.2379(w:5.000)⚠️)
Batch 325/365: Loss=3363.3179 (C:203.4482, R:0.0099, T:3159.8687(w:5.000)⚠️)
Batch 350/365: Loss=868.4895 (C:263.6582, R:0.0100, T:604.8303(w:5.000)⚠️)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 1902.9453
  Contrastive: 162.6452
  Reconstruction: 0.0100
  Topological: 1740.2992 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 18470.2441
  Contrastive: 300.5314
  Reconstruction: 0.0100
  Topological: 18169.7118 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 14/50 COMPLETE (116.5s)
Train Loss: 1902.9453 (C:162.6452, R:0.0100, T:1740.2992)
Val Loss:   18470.2441 (C:300.5314, R:0.0100, T:18169.7118)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=944.8921 (C:331.2974, R:0.0100, T:613.5936(w:5.000)⚠️)
Batch  25/365: Loss=6495.9736 (C:350.0049, R:0.0100, T:6145.9678(w:5.000)⚠️)
Batch  50/365: Loss=2572.8196 (C:376.8691, R:0.0099, T:2195.9495(w:5.000)⚠️)
Batch  75/365: Loss=3037.4177 (C:401.4499, R:0.0099, T:2635.9668(w:5.000)⚠️)
Batch 100/365: Loss=746.8389 (C:392.6816, R:0.0099, T:354.1562(w:5.000)⚠️)
Batch 125/365: Loss=405.5269 (C:309.7136, R:0.0100, T:95.8123(w:5.000)⚠️)
Batch 150/365: Loss=1151.9047 (C:399.1846, R:0.0100, T:752.7190(w:5.000)⚠️)
Batch 175/365: Loss=1273.1871 (C:410.3853, R:0.0099, T:862.8008(w:5.000)⚠️)
Batch 200/365: Loss=1578.6747 (C:472.1508, R:0.0100, T:1106.5228(w:5.000)⚠️)
Batch 225/365: Loss=3289.3047 (C:497.7229, R:0.0100, T:2791.5808(w:5.000)⚠️)
Batch 250/365: Loss=3864.7788 (C:512.3217, R:0.0099, T:3352.4561(w:5.000)⚠️)
Batch 275/365: Loss=2386.9888 (C:526.3623, R:0.0100, T:1860.6256(w:5.000)⚠️)
Batch 300/365: Loss=3119.6462 (C:443.8152, R:0.0099, T:2675.8301(w:5.000)⚠️)
Batch 325/365: Loss=2435.7668 (C:403.6212, R:0.0100, T:2032.1445(w:5.000)⚠️)
Batch 350/365: Loss=975.8766 (C:420.7835, R:0.0099, T:555.0921(w:5.000)⚠️)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 2685.4640
  Contrastive: 427.4651
  Reconstruction: 0.0100
  Topological: 2257.9979 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 19091.5389
  Contrastive: 377.5385
  Reconstruction: 0.0100
  Topological: 18713.9994 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 15/50 COMPLETE (125.1s)
Train Loss: 2685.4640 (C:427.4651, R:0.0100, T:2257.9979)
Val Loss:   19091.5389 (C:377.5385, R:0.0100, T:18713.9994)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1780.8271 (C:399.7302, R:0.0100, T:1381.0959(w:5.000)⚠️)
Batch  25/365: Loss=1932.8555 (C:389.3582, R:0.0099, T:1543.4962(w:5.000)⚠️)
Batch  50/365: Loss=2928.7197 (C:360.9930, R:0.0100, T:2567.7258(w:5.000)⚠️)
Batch  75/365: Loss=1164.2767 (C:425.4067, R:0.0099, T:738.8690(w:5.000)⚠️)
Batch 100/365: Loss=1107.0593 (C:386.1062, R:0.0099, T:720.9521(w:5.000)⚠️)
Batch 125/365: Loss=1570.9091 (C:339.1179, R:0.0099, T:1231.7902(w:5.000)⚠️)
Batch 150/365: Loss=1025.0455 (C:302.0454, R:0.0100, T:722.9991(w:5.000)⚠️)
Batch 175/365: Loss=2233.9150 (C:346.4095, R:0.0100, T:1887.5046(w:5.000)⚠️)
Batch 200/365: Loss=2326.7749 (C:347.3026, R:0.0100, T:1979.4712(w:5.000)⚠️)
Batch 225/365: Loss=3704.3582 (C:378.5492, R:0.0100, T:3325.8079(w:5.000)⚠️)
Batch 250/365: Loss=2062.0955 (C:407.6081, R:0.0099, T:1654.4863(w:5.000)⚠️)
Batch 275/365: Loss=1627.1176 (C:404.8143, R:0.0099, T:1222.3022(w:5.000)⚠️)
Batch 300/365: Loss=2100.9648 (C:465.7301, R:0.0100, T:1635.2336(w:5.000)⚠️)
Batch 325/365: Loss=2476.6973 (C:496.2418, R:0.0100, T:1980.4543(w:5.000)⚠️)
Batch 350/365: Loss=903.9445 (C:558.6644, R:0.0099, T:345.2792(w:5.000)⚠️)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 2295.0729
  Contrastive: 396.8474
  Reconstruction: 0.0100
  Topological: 1898.2245 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 27674.7824
  Contrastive: 503.2470
  Reconstruction: 0.0100
  Topological: 27171.5343 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 16/50 COMPLETE (128.4s)
Train Loss: 2295.0729 (C:396.8474, R:0.0100, T:1898.2245)
Val Loss:   27674.7824 (C:503.2470, R:0.0100, T:27171.5343)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2834.9783 (C:531.6639, R:0.0100, T:2303.3135(w:5.000)⚠️)
Batch  25/365: Loss=2737.2178 (C:539.4376, R:0.0099, T:2197.7793(w:5.000)⚠️)
Batch  50/365: Loss=981.0406 (C:503.9811, R:0.0100, T:477.0586(w:5.000)⚠️)
Batch  75/365: Loss=3074.2358 (C:514.8358, R:0.0099, T:2559.3992(w:5.000)⚠️)
Batch 100/365: Loss=3135.3105 (C:527.1564, R:0.0099, T:2608.1531(w:5.000)⚠️)
Batch 125/365: Loss=914.1714 (C:593.5331, R:0.0100, T:320.6373(w:5.000)⚠️)
Batch 150/365: Loss=3174.9377 (C:570.4905, R:0.0100, T:2604.4463(w:5.000)⚠️)
Batch 175/365: Loss=1150.9673 (C:631.9556, R:0.0100, T:519.0107(w:5.000)⚠️)
Batch 200/365: Loss=1539.9202 (C:694.4886, R:0.0100, T:845.4306(w:5.000)⚠️)
Batch 225/365: Loss=5143.9741 (C:737.1030, R:0.0099, T:4406.8701(w:5.000)⚠️)
Batch 250/365: Loss=5965.8140 (C:813.7848, R:0.0100, T:5152.0283(w:5.000)⚠️)
Batch 275/365: Loss=1735.7808 (C:728.5781, R:0.0099, T:1007.2018(w:5.000)⚠️)
Batch 300/365: Loss=2482.3020 (C:624.4441, R:0.0100, T:1857.8569(w:5.000)⚠️)
Batch 325/365: Loss=7172.9746 (C:612.1703, R:0.0100, T:6560.8032(w:5.000)⚠️)
Batch 350/365: Loss=2268.7671 (C:685.7603, R:0.0100, T:1583.0059(w:5.000)⚠️)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 3803.5165
  Contrastive: 641.1211
  Reconstruction: 0.0100
  Topological: 3162.3944 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 26318.3122
  Contrastive: 698.8247
  Reconstruction: 0.0100
  Topological: 25619.4866 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 17/50 COMPLETE (127.1s)
Train Loss: 3803.5165 (C:641.1211, R:0.0100, T:3162.3944)
Val Loss:   26318.3122 (C:698.8247, R:0.0100, T:25619.4866)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2146.6372 (C:779.9930, R:0.0100, T:1366.6432(w:5.000)⚠️)
Batch  25/365: Loss=1490.9275 (C:721.1588, R:0.0099, T:769.7678(w:5.000)⚠️)
Batch  50/365: Loss=1797.6846 (C:718.3306, R:0.0100, T:1079.3529(w:5.000)⚠️)
Batch  75/365: Loss=7274.1738 (C:690.8635, R:0.0099, T:6583.3096(w:5.000)⚠️)
Batch 100/365: Loss=2939.9685 (C:734.5674, R:0.0099, T:2205.4001(w:5.000)⚠️)
Batch 125/365: Loss=3483.1753 (C:769.8287, R:0.0100, T:2713.3457(w:5.000)⚠️)
Batch 150/365: Loss=5207.9287 (C:686.4557, R:0.0100, T:4521.4722(w:5.000)⚠️)
Batch 175/365: Loss=2177.0200 (C:701.8228, R:0.0100, T:1475.1963(w:5.000)⚠️)
Batch 200/365: Loss=2496.9126 (C:670.8636, R:0.0100, T:1826.0479(w:5.000)⚠️)
Batch 225/365: Loss=1776.8376 (C:693.8454, R:0.0100, T:1082.9913(w:5.000)⚠️)
Batch 250/365: Loss=4175.4160 (C:709.3715, R:0.0100, T:3466.0437(w:5.000)⚠️)
Batch 275/365: Loss=2331.0264 (C:761.8390, R:0.0100, T:1569.1864(w:5.000)⚠️)
Batch 300/365: Loss=5869.2271 (C:725.3784, R:0.0100, T:5143.8477(w:5.000)⚠️)
Batch 325/365: Loss=3288.3804 (C:703.1893, R:0.0099, T:2585.1902(w:5.000)⚠️)
Batch 350/365: Loss=1746.2549 (C:635.8201, R:0.0099, T:1110.4338(w:5.000)⚠️)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 3613.5462
  Contrastive: 704.4153
  Reconstruction: 0.0100
  Topological: 2909.1299 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 25580.7419
  Contrastive: 536.4127
  Reconstruction: 0.0100
  Topological: 25044.3283 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 18/50 COMPLETE (123.2s)
Train Loss: 3613.5462 (C:704.4153, R:0.0100, T:2909.1299)
Val Loss:   25580.7419 (C:536.4127, R:0.0100, T:25044.3283)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2891.4473 (C:531.7408, R:0.0099, T:2359.7056(w:5.000)⚠️)
Batch  25/365: Loss=1191.9489 (C:629.8599, R:0.0100, T:562.0880(w:5.000)⚠️)
Batch  50/365: Loss=6134.3477 (C:612.7526, R:0.0100, T:5521.5942(w:5.000)⚠️)
Batch  75/365: Loss=1562.6584 (C:663.5073, R:0.0100, T:899.1501(w:5.000)⚠️)
Batch 100/365: Loss=1493.7953 (C:692.5019, R:0.0099, T:801.2924(w:5.000)⚠️)
Batch 125/365: Loss=1273.3158 (C:692.0009, R:0.0100, T:581.3140(w:5.000)⚠️)
Batch 150/365: Loss=837.9149 (C:754.8828, R:0.0100, T:83.0311(w:5.000)⚠️)
Batch 175/365: Loss=2241.3174 (C:772.5066, R:0.0099, T:1468.8099(w:5.000)⚠️)
Batch 200/365: Loss=1262.8865 (C:775.2146, R:0.0100, T:487.6709(w:5.000)⚠️)
Batch 225/365: Loss=2938.3052 (C:626.4697, R:0.0100, T:2311.8345(w:5.000)⚠️)
Batch 250/365: Loss=1065.1821 (C:842.6876, R:0.0099, T:222.4936(w:5.000)⚠️)
Batch 275/365: Loss=1067.0918 (C:706.0129, R:0.0099, T:361.0779(w:5.000)⚠️)
Batch 300/365: Loss=2600.0969 (C:774.1398, R:0.0100, T:1825.9561(w:5.000)⚠️)
Batch 325/365: Loss=1261.0664 (C:905.6993, R:0.0100, T:355.3661(w:5.000)⚠️)
Batch 350/365: Loss=2016.1532 (C:853.1338, R:0.0099, T:1163.0184(w:5.000)⚠️)
📈 New best topological loss: 1426.7704

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 2143.9861
  Contrastive: 717.2147
  Reconstruction: 0.0100
  Topological: 1426.7704 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 24724.1068
  Contrastive: 777.2920
  Reconstruction: 0.0100
  Topological: 23946.8139 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 19/50 COMPLETE (122.5s)
Train Loss: 2143.9861 (C:717.2147, R:0.0100, T:1426.7704)
Val Loss:   24724.1068 (C:777.2920, R:0.0100, T:23946.8139)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1962.3862 (C:846.8105, R:0.0099, T:1115.5747(w:5.000)⚠️)
Batch  25/365: Loss=1138.4192 (C:683.0473, R:0.0100, T:455.3709(w:5.000)⚠️)
Batch  50/365: Loss=1773.8545 (C:689.0758, R:0.0099, T:1084.7777(w:5.000)⚠️)
Batch  75/365: Loss=2140.9592 (C:826.2825, R:0.0099, T:1314.6758(w:5.000)⚠️)
Batch 100/365: Loss=4081.5635 (C:822.0704, R:0.0099, T:3259.4919(w:5.000)⚠️)
Batch 125/365: Loss=2056.2061 (C:755.2183, R:0.0100, T:1300.9869(w:5.000)⚠️)
Batch 150/365: Loss=3138.6270 (C:773.9421, R:0.0099, T:2364.6838(w:5.000)⚠️)
Batch 175/365: Loss=1902.7073 (C:737.2505, R:0.0100, T:1165.4558(w:5.000)⚠️)
Batch 200/365: Loss=2009.1211 (C:730.0073, R:0.0100, T:1279.1128(w:5.000)⚠️)
Batch 225/365: Loss=906.8902 (C:664.1365, R:0.0100, T:242.7527(w:5.000)⚠️)
Batch 250/365: Loss=1596.8682 (C:695.5469, R:0.0099, T:901.3203(w:5.000)⚠️)
Batch 275/365: Loss=1734.7573 (C:689.7038, R:0.0100, T:1045.0525(w:5.000)⚠️)
Batch 300/365: Loss=1599.1689 (C:731.8907, R:0.0099, T:867.2772(w:5.000)⚠️)
Batch 325/365: Loss=2584.7610 (C:668.9785, R:0.0100, T:1915.7815(w:5.000)⚠️)
Batch 350/365: Loss=1948.7554 (C:739.2153, R:0.0100, T:1209.5391(w:5.000)⚠️)
📈 New best topological loss: 946.0921

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 1676.2716
  Contrastive: 730.1785
  Reconstruction: 0.0100
  Topological: 946.0921 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 20152.7785
  Contrastive: 676.2234
  Reconstruction: 0.0100
  Topological: 19476.5541 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 20/50 COMPLETE (127.3s)
Train Loss: 1676.2716 (C:730.1785, R:0.0100, T:946.0921)
Val Loss:   20152.7785 (C:676.2234, R:0.0100, T:19476.5541)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1420.8403 (C:717.1339, R:0.0099, T:703.7055(w:5.000)⚠️)
Batch  25/365: Loss=3683.8894 (C:724.0193, R:0.0099, T:2959.8691(w:5.000)⚠️)
Batch  50/365: Loss=2243.7927 (C:685.3121, R:0.0100, T:1558.4796(w:5.000)⚠️)
Batch  75/365: Loss=1894.7842 (C:666.6207, R:0.0100, T:1228.1625(w:5.000)⚠️)
Batch 100/365: Loss=1081.9730 (C:633.5409, R:0.0099, T:448.4312(w:5.000)⚠️)
Batch 125/365: Loss=1089.5061 (C:528.8519, R:0.0100, T:560.6533(w:5.000)⚠️)
Batch 150/365: Loss=1211.9670 (C:674.6188, R:0.0100, T:537.3474(w:5.000)⚠️)
Batch 175/365: Loss=1077.9919 (C:634.8425, R:0.0099, T:443.1484(w:5.000)⚠️)
Batch 200/365: Loss=2559.1433 (C:710.5742, R:0.0099, T:1848.5681(w:5.000)⚠️)
Batch 225/365: Loss=1425.5886 (C:642.5403, R:0.0099, T:783.0474(w:5.000)⚠️)
Batch 250/365: Loss=830.7991 (C:547.3943, R:0.0100, T:283.4038(w:5.000)⚠️)
Batch 275/365: Loss=1838.6426 (C:534.3149, R:0.0100, T:1304.3267(w:5.000)⚠️)
Batch 300/365: Loss=1308.2911 (C:446.4444, R:0.0100, T:861.8457(w:5.000)⚠️)
Batch 325/365: Loss=1023.4739 (C:436.0849, R:0.0100, T:587.3879(w:5.000)⚠️)
Batch 350/365: Loss=840.8180 (C:475.5321, R:0.0100, T:365.2849(w:5.000)⚠️)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 1582.3832
  Contrastive: 596.6498
  Reconstruction: 0.0100
  Topological: 985.7324 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 19251.6422
  Contrastive: 495.1486
  Reconstruction: 0.0100
  Topological: 18756.4927 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 21/50 COMPLETE (127.5s)
Train Loss: 1582.3832 (C:596.6498, R:0.0100, T:985.7324)
Val Loss:   19251.6422 (C:495.1486, R:0.0100, T:18756.4927)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1732.1483 (C:506.6671, R:0.0099, T:1225.4802(w:5.000)⚠️)
Batch  25/365: Loss=644.4833 (C:470.5513, R:0.0100, T:173.9309(w:5.000)⚠️)
Batch  50/365: Loss=1055.1475 (C:373.7006, R:0.0100, T:681.4458(w:5.000)⚠️)
Batch  75/365: Loss=1446.7958 (C:483.9621, R:0.0100, T:962.8326(w:5.000)⚠️)
Batch 100/365: Loss=877.0201 (C:425.4006, R:0.0100, T:451.6185(w:5.000)⚠️)
Batch 125/365: Loss=901.4271 (C:384.6539, R:0.0100, T:516.7722(w:5.000)⚠️)
Batch 150/365: Loss=903.9951 (C:454.8657, R:0.0100, T:449.1284(w:5.000)⚠️)
Batch 175/365: Loss=736.6996 (C:509.9815, R:0.0100, T:226.7170(w:5.000)⚠️)
Batch 200/365: Loss=2053.2117 (C:431.5954, R:0.0100, T:1621.6152(w:5.000)⚠️)
Batch 225/365: Loss=683.9688 (C:402.6031, R:0.0099, T:281.3647(w:5.000)⚠️)
Batch 250/365: Loss=906.8921 (C:418.4147, R:0.0099, T:488.4764(w:5.000)⚠️)
Batch 275/365: Loss=513.2193 (C:407.2688, R:0.0100, T:105.9495(w:5.000)⚠️)
Batch 300/365: Loss=1852.1251 (C:402.7144, R:0.0100, T:1449.4098(w:5.000)⚠️)
Batch 325/365: Loss=760.9290 (C:391.3831, R:0.0099, T:369.5449(w:5.000)⚠️)
Batch 350/365: Loss=1117.5931 (C:390.7458, R:0.0100, T:726.8463(w:5.000)⚠️)
📈 New best topological loss: 851.8659

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 1286.7463
  Contrastive: 434.8793
  Reconstruction: 0.0100
  Topological: 851.8659 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 13604.7290
  Contrastive: 396.8845
  Reconstruction: 0.0100
  Topological: 13207.8435 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 22/50 COMPLETE (131.8s)
Train Loss: 1286.7463 (C:434.8793, R:0.0100, T:851.8659)
Val Loss:   13604.7290 (C:396.8845, R:0.0100, T:13207.8435)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

🛑 Early stopping triggered after 22 epochs
Best model was at epoch 12 with Val Loss: 8284.4166

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 11
Epochs with topology: 12/22
Max consecutive topology epochs: 12
Best topological loss: 851.8659
Final topological loss: 851.8659
✅ SUCCESS: Topological learning achieved!
👍 GOOD: Fairly consistent topological learning (>50%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164528/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/367 batches
  Processed 51/367 batches
  Processed 101/367 batches
  Processed 151/367 batches
  Processed 201/367 batches
  Processed 251/367 batches
  Processed 301/367 batches
  Processed 351/367 batches
Extracted representations: torch.Size([549367, 50])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: -0.0122
  Adjusted Rand Score: 0.0000
  Clustering Accuracy: 0.3376
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 50])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 50])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.3732
  Per-class F1: [0.4125230202578269, 0.3641473154858749, 0.33102714209686]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009953
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 109.955 ± 132.448
  Negative distances: 110.001 ± 132.422
  Separation ratio: 1.00x
  Gap: -1001.084
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: -0.0122
  Clustering Accuracy: 0.3376
  Adjusted Rand Score: 0.0000

Classification Performance:
  Accuracy: 0.3732

Separation Quality:
  Separation Ratio: 1.00x
  Gap: -1001.084
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009953
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164528/results/evaluation_results_20250719_171535.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164528/results/evaluation_results_20250719_171535.json

Key Results:
  Separation ratio: 1.00x
  Perfect separation: False
  Classification accuracy: 0.3732

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 22
  Epochs with topological learning: 12
  Current topological loss: 851.8659
  Current topological weight: 5.0000
  ✅ Topological loss is decreasing (good progress)
✅ GOOD: Reasonable topological learning
Final topological loss: 851.8659
Epochs with topology: 12/22
⚠️  Poor clustering accuracy: 0.338

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164528/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164528

Analysis completed with exit code: 0
Time: Sat 19 Jul 17:15:37 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
