Starting Surface Distance Metric Analysis job...
Job ID: 182395
Node: gpuvm14
Time: Mon 14 Jul 16:10:07 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Mon Jul 14 16:10:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   40C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Hyperparam Optimization...

======================================================================
COARSE HYPERPARAMETER SEARCH - CONCAT EMBEDDINGS
======================================================================
Baseline accuracy: 81.67% (concat embeddings)
Embedding type: concat (best performing from initial results)
Focus: Classification performance optimization
Reconstruction weight: Fixed at 0.3 (optimize classification first)
Total combinations to test: 12
Search space:
  learning_rates: [0.0001, 0.0002]
  latent_dims: [50, 75, 100]
  batch_sizes: [1020, 1536]
======================================================================

[1/12] Testing: coarse_lr1e-04_lat50_bs1020
  Learning rate: 0.0001
  Latent dim: 50
  Batch size: 1020
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 16:13:13.156048
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,863,730
Model created with 1,863,730 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,863,730
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.072 ± 0.009
    Neg distances: 0.072 ± 0.009
    Separation ratio: 1.00x
    Gap: -0.109
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9999 (C:1.9999, R:0.0116)
Batch  25/537: Loss=1.9959 (C:1.9959, R:0.0114)
Batch  50/537: Loss=1.9799 (C:1.9799, R:0.0113)
Batch  75/537: Loss=1.9653 (C:1.9653, R:0.0111)
Batch 100/537: Loss=1.9604 (C:1.9604, R:0.0110)
Batch 125/537: Loss=1.9523 (C:1.9523, R:0.0109)
Batch 150/537: Loss=1.9447 (C:1.9447, R:0.0108)
Batch 175/537: Loss=1.9347 (C:1.9347, R:0.0108)
Batch 200/537: Loss=1.9357 (C:1.9357, R:0.0107)
Batch 225/537: Loss=1.9316 (C:1.9316, R:0.0106)
Batch 250/537: Loss=1.9248 (C:1.9248, R:0.0106)
Batch 275/537: Loss=1.9165 (C:1.9165, R:0.0106)
Batch 300/537: Loss=1.9230 (C:1.9230, R:0.0106)
Batch 325/537: Loss=1.9134 (C:1.9134, R:0.0105)
Batch 350/537: Loss=1.9105 (C:1.9105, R:0.0106)
Batch 375/537: Loss=1.8998 (C:1.8998, R:0.0106)
Batch 400/537: Loss=1.9078 (C:1.9078, R:0.0105)
Batch 425/537: Loss=1.9099 (C:1.9099, R:0.0105)
Batch 450/537: Loss=1.9014 (C:1.9014, R:0.0105)
Batch 475/537: Loss=1.9144 (C:1.9144, R:0.0105)
Batch 500/537: Loss=1.8939 (C:1.8939, R:0.0105)
Batch 525/537: Loss=1.9087 (C:1.9087, R:0.0105)

============================================================
Epoch 1/300 completed in 32.8s
Train: Loss=1.9313 (C:1.9313, R:0.0107) Ratio=1.66x
Val:   Loss=1.8916 (C:1.8916, R:0.0104) Ratio=2.16x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8916)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.8916 (C:1.8916, R:0.0105)
Batch  25/537: Loss=1.9061 (C:1.9061, R:0.0105)
Batch  50/537: Loss=1.8974 (C:1.8974, R:0.0105)
Batch  75/537: Loss=1.8874 (C:1.8874, R:0.0105)
Batch 100/537: Loss=1.8879 (C:1.8879, R:0.0105)
Batch 125/537: Loss=1.8835 (C:1.8835, R:0.0105)
Batch 150/537: Loss=1.8823 (C:1.8823, R:0.0105)
Batch 175/537: Loss=1.8973 (C:1.8973, R:0.0105)
Batch 200/537: Loss=1.8904 (C:1.8904, R:0.0105)
Batch 225/537: Loss=1.8956 (C:1.8956, R:0.0105)
Batch 250/537: Loss=1.8808 (C:1.8808, R:0.0105)
Batch 275/537: Loss=1.9073 (C:1.9073, R:0.0105)
Batch 300/537: Loss=1.8950 (C:1.8950, R:0.0105)
Batch 325/537: Loss=1.8845 (C:1.8845, R:0.0105)
Batch 350/537: Loss=1.8798 (C:1.8798, R:0.0105)
Batch 375/537: Loss=1.9108 (C:1.9108, R:0.0105)
Batch 400/537: Loss=1.8853 (C:1.8853, R:0.0105)
Batch 425/537: Loss=1.8832 (C:1.8832, R:0.0105)
Batch 450/537: Loss=1.9009 (C:1.9009, R:0.0105)
Batch 475/537: Loss=1.8719 (C:1.8719, R:0.0105)
Batch 500/537: Loss=1.8870 (C:1.8870, R:0.0105)
Batch 525/537: Loss=1.8905 (C:1.8905, R:0.0105)

============================================================
Epoch 2/300 completed in 21.7s
Train: Loss=1.8894 (C:1.8894, R:0.0105) Ratio=2.18x
Val:   Loss=1.8789 (C:1.8789, R:0.0104) Ratio=2.39x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8789)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8846 (C:1.8846, R:0.0105)
Batch  25/537: Loss=1.8772 (C:1.8772, R:0.0105)
Batch  50/537: Loss=1.8771 (C:1.8771, R:0.0105)
Batch  75/537: Loss=1.8767 (C:1.8767, R:0.0105)
Batch 100/537: Loss=1.8780 (C:1.8780, R:0.0105)
Batch 125/537: Loss=1.8809 (C:1.8809, R:0.0105)
Batch 150/537: Loss=1.8741 (C:1.8741, R:0.0106)
Batch 175/537: Loss=1.8743 (C:1.8743, R:0.0105)
Batch 200/537: Loss=1.8619 (C:1.8619, R:0.0105)
Batch 225/537: Loss=1.8736 (C:1.8736, R:0.0105)
Batch 250/537: Loss=1.8698 (C:1.8698, R:0.0106)
Batch 275/537: Loss=1.8743 (C:1.8743, R:0.0105)
Batch 300/537: Loss=1.8827 (C:1.8827, R:0.0105)
Batch 325/537: Loss=1.8754 (C:1.8754, R:0.0106)
Batch 350/537: Loss=1.8576 (C:1.8576, R:0.0105)
Batch 375/537: Loss=1.8958 (C:1.8958, R:0.0105)
Batch 400/537: Loss=1.8630 (C:1.8630, R:0.0105)
Batch 425/537: Loss=1.8702 (C:1.8702, R:0.0105)
Batch 450/537: Loss=1.8887 (C:1.8887, R:0.0105)
Batch 475/537: Loss=1.8659 (C:1.8659, R:0.0105)
Batch 500/537: Loss=1.8716 (C:1.8716, R:0.0105)
Batch 525/537: Loss=1.8687 (C:1.8687, R:0.0105)

============================================================
Epoch 3/300 completed in 21.2s
Train: Loss=1.8779 (C:1.8779, R:0.0105) Ratio=2.44x
Val:   Loss=1.8734 (C:1.8734, R:0.0104) Ratio=2.49x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8734)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.497 ± 0.533
    Neg distances: 1.331 ± 0.785
    Separation ratio: 2.68x
    Gap: -2.828
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.2610 (C:1.2610, R:0.0105)
Batch  25/537: Loss=1.2096 (C:1.2096, R:0.0105)
Batch  50/537: Loss=1.2903 (C:1.2903, R:0.0105)
Batch  75/537: Loss=1.2591 (C:1.2591, R:0.0105)
Batch 100/537: Loss=1.2470 (C:1.2470, R:0.0105)
Batch 125/537: Loss=1.2469 (C:1.2469, R:0.0105)
Batch 150/537: Loss=1.2287 (C:1.2287, R:0.0105)
Batch 175/537: Loss=1.2546 (C:1.2546, R:0.0106)
Batch 200/537: Loss=1.2580 (C:1.2580, R:0.0105)
Batch 225/537: Loss=1.2637 (C:1.2637, R:0.0105)
Batch 250/537: Loss=1.2464 (C:1.2464, R:0.0105)
Batch 275/537: Loss=1.2583 (C:1.2583, R:0.0105)
Batch 300/537: Loss=1.2437 (C:1.2437, R:0.0105)
Batch 325/537: Loss=1.2444 (C:1.2444, R:0.0105)
Batch 350/537: Loss=1.2448 (C:1.2448, R:0.0105)
Batch 375/537: Loss=1.2386 (C:1.2386, R:0.0105)
Batch 400/537: Loss=1.2458 (C:1.2458, R:0.0105)
Batch 425/537: Loss=1.2298 (C:1.2298, R:0.0105)
Batch 450/537: Loss=1.2608 (C:1.2608, R:0.0105)
Batch 475/537: Loss=1.2254 (C:1.2254, R:0.0105)
Batch 500/537: Loss=1.2075 (C:1.2075, R:0.0105)
Batch 525/537: Loss=1.2560 (C:1.2560, R:0.0105)

============================================================
Epoch 4/300 completed in 28.1s
Train: Loss=1.2480 (C:1.2480, R:0.0105) Ratio=2.54x
Val:   Loss=1.2496 (C:1.2496, R:0.0104) Ratio=2.61x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2496)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.2097 (C:1.2097, R:0.0105)
Batch  25/537: Loss=1.2014 (C:1.2014, R:0.0106)
Batch  50/537: Loss=1.2204 (C:1.2204, R:0.0105)
Batch  75/537: Loss=1.2071 (C:1.2071, R:0.0105)
Batch 100/537: Loss=1.2510 (C:1.2510, R:0.0105)
Batch 125/537: Loss=1.2371 (C:1.2371, R:0.0105)
Batch 150/537: Loss=1.2264 (C:1.2264, R:0.0105)
Batch 175/537: Loss=1.2112 (C:1.2112, R:0.0105)
Batch 200/537: Loss=1.2616 (C:1.2616, R:0.0105)
Batch 225/537: Loss=1.2507 (C:1.2507, R:0.0105)
Batch 250/537: Loss=1.2110 (C:1.2110, R:0.0106)
Batch 275/537: Loss=1.2126 (C:1.2126, R:0.0105)
Batch 300/537: Loss=1.2215 (C:1.2215, R:0.0105)
Batch 325/537: Loss=1.2252 (C:1.2252, R:0.0105)
Batch 350/537: Loss=1.2265 (C:1.2265, R:0.0105)
Batch 375/537: Loss=1.2038 (C:1.2038, R:0.0105)
Batch 400/537: Loss=1.2340 (C:1.2340, R:0.0105)
Batch 425/537: Loss=1.2631 (C:1.2631, R:0.0105)
Batch 450/537: Loss=1.2061 (C:1.2061, R:0.0105)
Batch 475/537: Loss=1.1977 (C:1.1977, R:0.0105)
Batch 500/537: Loss=1.2328 (C:1.2328, R:0.0105)
Batch 525/537: Loss=1.2064 (C:1.2064, R:0.0105)

============================================================
Epoch 5/300 completed in 21.8s
Train: Loss=1.2288 (C:1.2288, R:0.0105) Ratio=2.76x
Val:   Loss=1.2298 (C:1.2298, R:0.0104) Ratio=2.69x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2298)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.1962 (C:1.1962, R:0.0105)
Batch  25/537: Loss=1.1927 (C:1.1927, R:0.0105)
Batch  50/537: Loss=1.2053 (C:1.2053, R:0.0105)
Batch  75/537: Loss=1.1849 (C:1.1849, R:0.0105)
Batch 100/537: Loss=1.2151 (C:1.2151, R:0.0105)
Batch 125/537: Loss=1.2123 (C:1.2123, R:0.0105)
Batch 150/537: Loss=1.2191 (C:1.2191, R:0.0105)
Batch 175/537: Loss=1.2196 (C:1.2196, R:0.0105)
Batch 200/537: Loss=1.1906 (C:1.1906, R:0.0105)
Batch 225/537: Loss=1.2243 (C:1.2243, R:0.0105)
Batch 250/537: Loss=1.2304 (C:1.2304, R:0.0105)
Batch 275/537: Loss=1.2262 (C:1.2262, R:0.0105)
Batch 300/537: Loss=1.2596 (C:1.2596, R:0.0105)
Batch 325/537: Loss=1.2024 (C:1.2024, R:0.0105)
Batch 350/537: Loss=1.1808 (C:1.1808, R:0.0105)
Batch 375/537: Loss=1.2039 (C:1.2039, R:0.0105)
Batch 400/537: Loss=1.2107 (C:1.2107, R:0.0105)
Batch 425/537: Loss=1.2236 (C:1.2236, R:0.0105)
Batch 450/537: Loss=1.1987 (C:1.1987, R:0.0105)
Batch 475/537: Loss=1.2267 (C:1.2267, R:0.0105)
Batch 500/537: Loss=1.2001 (C:1.2001, R:0.0105)
Batch 525/537: Loss=1.2120 (C:1.2120, R:0.0105)

============================================================
Epoch 6/300 completed in 22.2s
Train: Loss=1.2150 (C:1.2150, R:0.0105) Ratio=2.87x
Val:   Loss=1.2257 (C:1.2257, R:0.0104) Ratio=2.76x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2257)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.445 ± 0.546
    Neg distances: 1.491 ± 0.826
    Separation ratio: 3.35x
    Gap: -2.996
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.1163 (C:1.1163, R:0.0105)
Batch  25/537: Loss=1.1065 (C:1.1065, R:0.0105)
Batch  50/537: Loss=1.1464 (C:1.1464, R:0.0105)
Batch  75/537: Loss=1.1406 (C:1.1406, R:0.0105)
Batch 100/537: Loss=1.0956 (C:1.0956, R:0.0106)
Batch 125/537: Loss=1.0732 (C:1.0732, R:0.0105)
Batch 150/537: Loss=1.0796 (C:1.0796, R:0.0105)
Batch 175/537: Loss=1.1612 (C:1.1612, R:0.0105)
Batch 200/537: Loss=1.1214 (C:1.1214, R:0.0105)
Batch 225/537: Loss=1.1496 (C:1.1496, R:0.0105)
Batch 250/537: Loss=1.1202 (C:1.1202, R:0.0105)
Batch 275/537: Loss=1.1342 (C:1.1342, R:0.0105)
Batch 300/537: Loss=1.1125 (C:1.1125, R:0.0105)
Batch 325/537: Loss=1.1066 (C:1.1066, R:0.0105)
Batch 350/537: Loss=1.1205 (C:1.1205, R:0.0105)
Batch 375/537: Loss=1.0899 (C:1.0899, R:0.0106)
Batch 400/537: Loss=1.1147 (C:1.1147, R:0.0105)
Batch 425/537: Loss=1.1226 (C:1.1226, R:0.0105)
Batch 450/537: Loss=1.1190 (C:1.1190, R:0.0105)
Batch 475/537: Loss=1.0770 (C:1.0770, R:0.0105)
Batch 500/537: Loss=1.0875 (C:1.0875, R:0.0105)
Batch 525/537: Loss=1.0976 (C:1.0976, R:0.0106)

============================================================
Epoch 7/300 completed in 28.2s
Train: Loss=1.1127 (C:1.1127, R:0.0105) Ratio=3.01x
Val:   Loss=1.1271 (C:1.1271, R:0.0104) Ratio=2.81x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1271)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.1244 (C:1.1244, R:0.0105)
Batch  25/537: Loss=1.1114 (C:1.1114, R:0.0105)
Batch  50/537: Loss=1.1330 (C:1.1330, R:0.0106)
Batch  75/537: Loss=1.1103 (C:1.1103, R:0.0105)
Batch 100/537: Loss=1.1189 (C:1.1189, R:0.0105)
Batch 125/537: Loss=1.0868 (C:1.0868, R:0.0105)
Batch 150/537: Loss=1.0544 (C:1.0544, R:0.0105)
Batch 175/537: Loss=1.1108 (C:1.1108, R:0.0106)
Batch 200/537: Loss=1.0694 (C:1.0694, R:0.0105)
Batch 225/537: Loss=1.1016 (C:1.1016, R:0.0105)
Batch 250/537: Loss=1.1274 (C:1.1274, R:0.0105)
Batch 275/537: Loss=1.0683 (C:1.0683, R:0.0105)
Batch 300/537: Loss=1.0985 (C:1.0985, R:0.0105)
Batch 325/537: Loss=1.1077 (C:1.1077, R:0.0105)
Batch 350/537: Loss=1.0641 (C:1.0641, R:0.0105)
Batch 375/537: Loss=1.1039 (C:1.1039, R:0.0105)
Batch 400/537: Loss=1.1182 (C:1.1182, R:0.0105)
Batch 425/537: Loss=1.1060 (C:1.1060, R:0.0105)
Batch 450/537: Loss=1.1000 (C:1.1000, R:0.0105)
Batch 475/537: Loss=1.0960 (C:1.0960, R:0.0105)
Batch 500/537: Loss=1.0935 (C:1.0935, R:0.0105)
Batch 525/537: Loss=1.1520 (C:1.1520, R:0.0105)

============================================================
Epoch 8/300 completed in 22.1s
Train: Loss=1.1015 (C:1.1015, R:0.0105) Ratio=3.07x
Val:   Loss=1.1284 (C:1.1284, R:0.0104) Ratio=2.82x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0876 (C:1.0876, R:0.0105)
Batch  25/537: Loss=1.0798 (C:1.0798, R:0.0105)
Batch  50/537: Loss=1.0804 (C:1.0804, R:0.0105)
Batch  75/537: Loss=1.0866 (C:1.0866, R:0.0105)
Batch 100/537: Loss=1.0881 (C:1.0881, R:0.0105)
Batch 125/537: Loss=1.1057 (C:1.1057, R:0.0105)
Batch 150/537: Loss=1.0967 (C:1.0967, R:0.0105)
Batch 175/537: Loss=1.0806 (C:1.0806, R:0.0105)
Batch 200/537: Loss=1.1039 (C:1.1039, R:0.0105)
Batch 225/537: Loss=1.0927 (C:1.0927, R:0.0105)
Batch 250/537: Loss=1.1167 (C:1.1167, R:0.0105)
Batch 275/537: Loss=1.0822 (C:1.0822, R:0.0105)
Batch 300/537: Loss=1.0815 (C:1.0815, R:0.0105)
Batch 325/537: Loss=1.0831 (C:1.0831, R:0.0105)
Batch 350/537: Loss=1.0985 (C:1.0985, R:0.0105)
Batch 375/537: Loss=1.0683 (C:1.0683, R:0.0105)
Batch 400/537: Loss=1.0858 (C:1.0858, R:0.0105)
Batch 425/537: Loss=1.0855 (C:1.0855, R:0.0105)
Batch 450/537: Loss=1.0774 (C:1.0774, R:0.0105)
Batch 475/537: Loss=1.1187 (C:1.1187, R:0.0105)
Batch 500/537: Loss=1.1336 (C:1.1336, R:0.0105)
Batch 525/537: Loss=1.0748 (C:1.0748, R:0.0105)

============================================================
Epoch 9/300 completed in 21.8s
Train: Loss=1.0925 (C:1.0925, R:0.0105) Ratio=3.22x
Val:   Loss=1.1242 (C:1.1242, R:0.0104) Ratio=2.86x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1242)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.418 ± 0.545
    Neg distances: 1.548 ± 0.832
    Separation ratio: 3.70x
    Gap: -2.841
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.0532 (C:1.0532, R:0.0105)
Batch  25/537: Loss=1.0781 (C:1.0781, R:0.0105)
Batch  50/537: Loss=1.0597 (C:1.0597, R:0.0105)
Batch  75/537: Loss=1.0550 (C:1.0550, R:0.0105)
Batch 100/537: Loss=1.0214 (C:1.0214, R:0.0106)
Batch 125/537: Loss=1.0637 (C:1.0637, R:0.0105)
Batch 150/537: Loss=1.0216 (C:1.0216, R:0.0105)
Batch 175/537: Loss=1.0405 (C:1.0405, R:0.0105)
Batch 200/537: Loss=1.0692 (C:1.0692, R:0.0105)
Batch 225/537: Loss=1.0246 (C:1.0246, R:0.0105)
Batch 250/537: Loss=1.0199 (C:1.0199, R:0.0105)
Batch 275/537: Loss=1.0478 (C:1.0478, R:0.0105)
Batch 300/537: Loss=1.0509 (C:1.0509, R:0.0105)
Batch 325/537: Loss=1.0215 (C:1.0215, R:0.0105)
Batch 350/537: Loss=1.0653 (C:1.0653, R:0.0105)
Batch 375/537: Loss=1.0543 (C:1.0543, R:0.0105)
Batch 400/537: Loss=1.0595 (C:1.0595, R:0.0105)
Batch 425/537: Loss=1.0599 (C:1.0599, R:0.0105)
Batch 450/537: Loss=1.0486 (C:1.0486, R:0.0105)
Batch 475/537: Loss=1.0557 (C:1.0557, R:0.0105)
Batch 500/537: Loss=1.0485 (C:1.0485, R:0.0105)
Batch 525/537: Loss=1.0456 (C:1.0456, R:0.0106)

============================================================
Epoch 10/300 completed in 28.5s
Train: Loss=1.0498 (C:1.0498, R:0.0105) Ratio=3.33x
Val:   Loss=1.0832 (C:1.0832, R:0.0104) Ratio=2.87x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0832)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.0341 (C:1.0341, R:0.0105)
Batch  25/537: Loss=1.0307 (C:1.0307, R:0.0105)
Batch  50/537: Loss=1.0332 (C:1.0332, R:0.0105)
Batch  75/537: Loss=1.0637 (C:1.0637, R:0.0105)
Batch 100/537: Loss=1.0362 (C:1.0362, R:0.0105)
Batch 125/537: Loss=1.0399 (C:1.0399, R:0.0105)
Batch 150/537: Loss=1.0400 (C:1.0400, R:0.0105)
Batch 175/537: Loss=1.0249 (C:1.0249, R:0.0106)
Batch 200/537: Loss=1.0461 (C:1.0461, R:0.0106)
Batch 225/537: Loss=1.0491 (C:1.0491, R:0.0105)
Batch 250/537: Loss=1.0443 (C:1.0443, R:0.0105)
Batch 275/537: Loss=1.0567 (C:1.0567, R:0.0105)
Batch 300/537: Loss=1.0353 (C:1.0353, R:0.0105)
Batch 325/537: Loss=1.0582 (C:1.0582, R:0.0105)
Batch 350/537: Loss=1.0314 (C:1.0314, R:0.0105)
Batch 375/537: Loss=1.0771 (C:1.0771, R:0.0105)
Batch 400/537: Loss=1.0233 (C:1.0233, R:0.0105)
Batch 425/537: Loss=1.0639 (C:1.0639, R:0.0105)
Batch 450/537: Loss=1.0424 (C:1.0424, R:0.0105)
Batch 475/537: Loss=1.0770 (C:1.0770, R:0.0105)
Batch 500/537: Loss=1.0383 (C:1.0383, R:0.0105)
Batch 525/537: Loss=0.9980 (C:0.9980, R:0.0105)

============================================================
Epoch 11/300 completed in 22.1s
Train: Loss=1.0437 (C:1.0437, R:0.0105) Ratio=3.40x
Val:   Loss=1.0869 (C:1.0869, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.0638 (C:1.0638, R:0.0105)
Batch  25/537: Loss=1.0692 (C:1.0692, R:0.0105)
Batch  50/537: Loss=1.0481 (C:1.0481, R:0.0105)
Batch  75/537: Loss=1.0139 (C:1.0139, R:0.0105)
Batch 100/537: Loss=1.0419 (C:1.0419, R:0.0105)
Batch 125/537: Loss=1.0582 (C:1.0582, R:0.0105)
Batch 150/537: Loss=1.0480 (C:1.0480, R:0.0105)
Batch 175/537: Loss=1.0655 (C:1.0655, R:0.0105)
Batch 200/537: Loss=1.0485 (C:1.0485, R:0.0105)
Batch 225/537: Loss=1.0421 (C:1.0421, R:0.0105)
Batch 250/537: Loss=1.0519 (C:1.0519, R:0.0105)
Batch 275/537: Loss=1.0445 (C:1.0445, R:0.0105)
Batch 300/537: Loss=1.0363 (C:1.0363, R:0.0105)
Batch 325/537: Loss=1.0387 (C:1.0387, R:0.0105)
Batch 350/537: Loss=1.0047 (C:1.0047, R:0.0105)
Batch 375/537: Loss=1.0354 (C:1.0354, R:0.0105)
Batch 400/537: Loss=0.9998 (C:0.9998, R:0.0105)
Batch 425/537: Loss=1.0409 (C:1.0409, R:0.0105)
Batch 450/537: Loss=1.0329 (C:1.0329, R:0.0105)
Batch 475/537: Loss=1.0556 (C:1.0556, R:0.0105)
Batch 500/537: Loss=1.0506 (C:1.0506, R:0.0105)
Batch 525/537: Loss=1.0374 (C:1.0374, R:0.0105)

============================================================
Epoch 12/300 completed in 21.5s
Train: Loss=1.0382 (C:1.0382, R:0.0105) Ratio=3.47x
Val:   Loss=1.0856 (C:1.0856, R:0.0104) Ratio=2.90x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.391 ± 0.544
    Neg distances: 1.595 ± 0.844
    Separation ratio: 4.08x
    Gap: -2.875
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9889 (C:0.9889, R:0.0105)
Batch  25/537: Loss=1.0268 (C:1.0268, R:0.0105)
Batch  50/537: Loss=0.9314 (C:0.9314, R:0.0106)
Batch  75/537: Loss=1.0089 (C:1.0089, R:0.0105)
Batch 100/537: Loss=1.0111 (C:1.0111, R:0.0105)
Batch 125/537: Loss=0.9775 (C:0.9775, R:0.0106)
Batch 150/537: Loss=1.0045 (C:1.0045, R:0.0105)
Batch 175/537: Loss=1.0017 (C:1.0017, R:0.0105)
Batch 200/537: Loss=1.0089 (C:1.0089, R:0.0105)
Batch 225/537: Loss=1.0168 (C:1.0168, R:0.0105)
Batch 250/537: Loss=1.0224 (C:1.0224, R:0.0105)
Batch 275/537: Loss=1.0034 (C:1.0034, R:0.0105)
Batch 300/537: Loss=0.9765 (C:0.9765, R:0.0105)
Batch 325/537: Loss=0.9499 (C:0.9499, R:0.0105)
Batch 350/537: Loss=1.0044 (C:1.0044, R:0.0105)
Batch 375/537: Loss=1.0209 (C:1.0209, R:0.0105)
Batch 400/537: Loss=1.0138 (C:1.0138, R:0.0105)
Batch 425/537: Loss=1.0220 (C:1.0220, R:0.0105)
Batch 450/537: Loss=1.0300 (C:1.0300, R:0.0105)
Batch 475/537: Loss=1.0021 (C:1.0021, R:0.0105)
Batch 500/537: Loss=0.9764 (C:0.9764, R:0.0105)
Batch 525/537: Loss=1.0363 (C:1.0363, R:0.0105)

============================================================
Epoch 13/300 completed in 26.5s
Train: Loss=1.0012 (C:1.0012, R:0.0105) Ratio=3.51x
Val:   Loss=1.0535 (C:1.0535, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0535)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=1.0169 (C:1.0169, R:0.0105)
Batch  25/537: Loss=1.0001 (C:1.0001, R:0.0105)
Batch  50/537: Loss=1.0015 (C:1.0015, R:0.0106)
Batch  75/537: Loss=0.9781 (C:0.9781, R:0.0105)
Batch 100/537: Loss=0.9997 (C:0.9997, R:0.0105)
Batch 125/537: Loss=1.0087 (C:1.0087, R:0.0105)
Batch 150/537: Loss=0.9966 (C:0.9966, R:0.0105)
Batch 175/537: Loss=1.0032 (C:1.0032, R:0.0105)
Batch 200/537: Loss=0.9983 (C:0.9983, R:0.0105)
Batch 225/537: Loss=1.0164 (C:1.0164, R:0.0105)
Batch 250/537: Loss=0.9941 (C:0.9941, R:0.0105)
Batch 275/537: Loss=0.9687 (C:0.9687, R:0.0106)
Batch 300/537: Loss=0.9807 (C:0.9807, R:0.0105)
Batch 325/537: Loss=0.9974 (C:0.9974, R:0.0105)
Batch 350/537: Loss=0.9798 (C:0.9798, R:0.0105)
Batch 375/537: Loss=0.9703 (C:0.9703, R:0.0105)
Batch 400/537: Loss=1.0137 (C:1.0137, R:0.0105)
Batch 425/537: Loss=0.9978 (C:0.9978, R:0.0105)
Batch 450/537: Loss=0.9859 (C:0.9859, R:0.0106)
Batch 475/537: Loss=1.0152 (C:1.0152, R:0.0105)
Batch 500/537: Loss=1.0181 (C:1.0181, R:0.0105)
Batch 525/537: Loss=0.9809 (C:0.9809, R:0.0105)

============================================================
Epoch 14/300 completed in 21.4s
Train: Loss=0.9968 (C:0.9968, R:0.0105) Ratio=3.61x
Val:   Loss=1.0377 (C:1.0377, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0377)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=1.0092 (C:1.0092, R:0.0105)
Batch  25/537: Loss=0.9610 (C:0.9610, R:0.0105)
Batch  50/537: Loss=0.9891 (C:0.9891, R:0.0105)
Batch  75/537: Loss=1.0123 (C:1.0123, R:0.0105)
Batch 100/537: Loss=0.9660 (C:0.9660, R:0.0105)
Batch 125/537: Loss=0.9798 (C:0.9798, R:0.0105)
Batch 150/537: Loss=0.9930 (C:0.9930, R:0.0105)
Batch 175/537: Loss=1.0197 (C:1.0197, R:0.0104)
Batch 200/537: Loss=1.0116 (C:1.0116, R:0.0105)
Batch 225/537: Loss=0.9943 (C:0.9943, R:0.0105)
Batch 250/537: Loss=1.0380 (C:1.0380, R:0.0105)
Batch 275/537: Loss=0.9959 (C:0.9959, R:0.0105)
Batch 300/537: Loss=0.9845 (C:0.9845, R:0.0105)
Batch 325/537: Loss=0.9881 (C:0.9881, R:0.0105)
Batch 350/537: Loss=1.0021 (C:1.0021, R:0.0105)
Batch 375/537: Loss=0.9919 (C:0.9919, R:0.0105)
Batch 400/537: Loss=1.0026 (C:1.0026, R:0.0105)
Batch 425/537: Loss=0.9485 (C:0.9485, R:0.0105)
Batch 450/537: Loss=0.9866 (C:0.9866, R:0.0105)
Batch 475/537: Loss=0.9733 (C:0.9733, R:0.0105)
Batch 500/537: Loss=0.9749 (C:0.9749, R:0.0106)
Batch 525/537: Loss=1.0172 (C:1.0172, R:0.0105)

============================================================
Epoch 15/300 completed in 21.7s
Train: Loss=0.9925 (C:0.9925, R:0.0105) Ratio=3.72x
Val:   Loss=1.0408 (C:1.0408, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.355 ± 0.510
    Neg distances: 1.636 ± 0.849
    Separation ratio: 4.60x
    Gap: -2.864
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.9372 (C:0.9372, R:0.0105)
Batch  25/537: Loss=0.9869 (C:0.9869, R:0.0106)
Batch  50/537: Loss=0.9381 (C:0.9381, R:0.0105)
Batch  75/537: Loss=0.9732 (C:0.9732, R:0.0105)
Batch 100/537: Loss=0.9586 (C:0.9586, R:0.0105)
Batch 125/537: Loss=0.9553 (C:0.9553, R:0.0105)
Batch 150/537: Loss=0.9695 (C:0.9695, R:0.0105)
Batch 175/537: Loss=0.9275 (C:0.9275, R:0.0105)
Batch 200/537: Loss=0.9727 (C:0.9727, R:0.0105)
Batch 225/537: Loss=0.9301 (C:0.9301, R:0.0105)
Batch 250/537: Loss=0.9281 (C:0.9281, R:0.0105)
Batch 275/537: Loss=0.9417 (C:0.9417, R:0.0105)
Batch 300/537: Loss=0.9917 (C:0.9917, R:0.0105)
Batch 325/537: Loss=0.9866 (C:0.9866, R:0.0105)
Batch 350/537: Loss=0.9469 (C:0.9469, R:0.0105)
Batch 375/537: Loss=0.9959 (C:0.9959, R:0.0105)
Batch 400/537: Loss=0.9888 (C:0.9888, R:0.0105)
Batch 425/537: Loss=1.0025 (C:1.0025, R:0.0105)
Batch 450/537: Loss=0.9627 (C:0.9627, R:0.0105)
Batch 475/537: Loss=0.9668 (C:0.9668, R:0.0105)
Batch 500/537: Loss=0.9476 (C:0.9476, R:0.0105)
Batch 525/537: Loss=0.9449 (C:0.9449, R:0.0105)

============================================================
Epoch 16/300 completed in 28.4s
Train: Loss=0.9547 (C:0.9547, R:0.0105) Ratio=3.75x
Val:   Loss=1.0148 (C:1.0148, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0148)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.9322 (C:0.9322, R:0.0105)
Batch  25/537: Loss=0.9290 (C:0.9290, R:0.0105)
Batch  50/537: Loss=0.9526 (C:0.9526, R:0.0105)
Batch  75/537: Loss=0.9263 (C:0.9263, R:0.0105)
Batch 100/537: Loss=0.9543 (C:0.9543, R:0.0105)
Batch 125/537: Loss=0.9550 (C:0.9550, R:0.0105)
Batch 150/537: Loss=0.9543 (C:0.9543, R:0.0105)
Batch 175/537: Loss=0.9539 (C:0.9539, R:0.0105)
Batch 200/537: Loss=0.9442 (C:0.9442, R:0.0105)
Batch 225/537: Loss=0.9592 (C:0.9592, R:0.0105)
Batch 250/537: Loss=0.9380 (C:0.9380, R:0.0105)
Batch 275/537: Loss=0.9650 (C:0.9650, R:0.0106)
Batch 300/537: Loss=0.9164 (C:0.9164, R:0.0105)
Batch 325/537: Loss=0.9777 (C:0.9777, R:0.0105)
Batch 350/537: Loss=0.9584 (C:0.9584, R:0.0105)
Batch 375/537: Loss=0.9372 (C:0.9372, R:0.0105)
Batch 400/537: Loss=0.9359 (C:0.9359, R:0.0105)
Batch 425/537: Loss=0.9498 (C:0.9498, R:0.0105)
Batch 450/537: Loss=0.9526 (C:0.9526, R:0.0105)
Batch 475/537: Loss=0.9697 (C:0.9697, R:0.0105)
Batch 500/537: Loss=0.9584 (C:0.9584, R:0.0105)
Batch 525/537: Loss=0.9529 (C:0.9529, R:0.0105)

============================================================
Epoch 17/300 completed in 22.3s
Train: Loss=0.9510 (C:0.9510, R:0.0105) Ratio=3.88x
Val:   Loss=1.0145 (C:1.0145, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0145)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.9484 (C:0.9484, R:0.0105)
Batch  25/537: Loss=0.9352 (C:0.9352, R:0.0105)
Batch  50/537: Loss=0.9321 (C:0.9321, R:0.0105)
Batch  75/537: Loss=0.9290 (C:0.9290, R:0.0105)
Batch 100/537: Loss=0.9709 (C:0.9709, R:0.0105)
Batch 125/537: Loss=0.9328 (C:0.9328, R:0.0105)
Batch 150/537: Loss=0.9369 (C:0.9369, R:0.0105)
Batch 175/537: Loss=0.9616 (C:0.9616, R:0.0105)
Batch 200/537: Loss=0.8994 (C:0.8994, R:0.0105)
Batch 225/537: Loss=0.9342 (C:0.9342, R:0.0105)
Batch 250/537: Loss=0.9515 (C:0.9515, R:0.0105)
Batch 275/537: Loss=0.9684 (C:0.9684, R:0.0105)
Batch 300/537: Loss=0.9438 (C:0.9438, R:0.0105)
Batch 325/537: Loss=0.9434 (C:0.9434, R:0.0105)
Batch 350/537: Loss=0.9287 (C:0.9287, R:0.0105)
Batch 375/537: Loss=0.9221 (C:0.9221, R:0.0105)
Batch 400/537: Loss=0.9185 (C:0.9185, R:0.0105)
Batch 425/537: Loss=0.9449 (C:0.9449, R:0.0105)
Batch 450/537: Loss=0.9591 (C:0.9591, R:0.0105)
Batch 475/537: Loss=0.9362 (C:0.9362, R:0.0105)
Batch 500/537: Loss=0.9426 (C:0.9426, R:0.0105)
Batch 525/537: Loss=0.9498 (C:0.9498, R:0.0105)

============================================================
Epoch 18/300 completed in 22.0s
Train: Loss=0.9466 (C:0.9466, R:0.0105) Ratio=3.94x
Val:   Loss=1.0106 (C:1.0106, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0106)
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.340 ± 0.503
    Neg distances: 1.711 ± 0.864
    Separation ratio: 5.03x
    Gap: -2.983
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.9234 (C:0.9234, R:0.0105)
Batch  25/537: Loss=0.9038 (C:0.9038, R:0.0105)
Batch  50/537: Loss=0.8977 (C:0.8977, R:0.0105)
Batch  75/537: Loss=0.9060 (C:0.9060, R:0.0105)
Batch 100/537: Loss=0.9182 (C:0.9182, R:0.0105)
Batch 125/537: Loss=0.8831 (C:0.8831, R:0.0105)
Batch 150/537: Loss=0.8879 (C:0.8879, R:0.0105)
Batch 175/537: Loss=0.8806 (C:0.8806, R:0.0105)
Batch 200/537: Loss=0.8867 (C:0.8867, R:0.0105)
Batch 225/537: Loss=0.9038 (C:0.9038, R:0.0105)
Batch 250/537: Loss=0.9155 (C:0.9155, R:0.0105)
Batch 275/537: Loss=0.9194 (C:0.9194, R:0.0105)
Batch 300/537: Loss=0.9242 (C:0.9242, R:0.0105)
Batch 325/537: Loss=0.9247 (C:0.9247, R:0.0105)
Batch 350/537: Loss=0.9010 (C:0.9010, R:0.0105)
Batch 375/537: Loss=0.9075 (C:0.9075, R:0.0105)
Batch 400/537: Loss=0.8974 (C:0.8974, R:0.0105)
Batch 425/537: Loss=0.9281 (C:0.9281, R:0.0105)
Batch 450/537: Loss=0.9156 (C:0.9156, R:0.0105)
Batch 475/537: Loss=0.9433 (C:0.9433, R:0.0105)
Batch 500/537: Loss=0.8993 (C:0.8993, R:0.0105)
Batch 525/537: Loss=0.9090 (C:0.9090, R:0.0105)

============================================================
Epoch 19/300 completed in 28.4s
Train: Loss=0.9094 (C:0.9094, R:0.0105) Ratio=3.95x
Val:   Loss=0.9754 (C:0.9754, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9754)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.9154 (C:0.9154, R:0.0105)
Batch  25/537: Loss=0.9052 (C:0.9052, R:0.0105)
Batch  50/537: Loss=0.8977 (C:0.8977, R:0.0105)
Batch  75/537: Loss=0.8842 (C:0.8842, R:0.0105)
Batch 100/537: Loss=0.8861 (C:0.8861, R:0.0105)
Batch 125/537: Loss=0.9167 (C:0.9167, R:0.0105)
Batch 150/537: Loss=0.9051 (C:0.9051, R:0.0105)
Batch 175/537: Loss=0.9192 (C:0.9192, R:0.0105)
Batch 200/537: Loss=0.8609 (C:0.8609, R:0.0105)
Batch 225/537: Loss=0.9321 (C:0.9321, R:0.0105)
Batch 250/537: Loss=0.8917 (C:0.8917, R:0.0105)
Batch 275/537: Loss=0.9279 (C:0.9279, R:0.0105)
Batch 300/537: Loss=0.8926 (C:0.8926, R:0.0105)
Batch 325/537: Loss=0.9263 (C:0.9263, R:0.0105)
Batch 350/537: Loss=0.9048 (C:0.9048, R:0.0106)
Batch 375/537: Loss=0.9020 (C:0.9020, R:0.0105)
Batch 400/537: Loss=0.8878 (C:0.8878, R:0.0105)
Batch 425/537: Loss=0.9295 (C:0.9295, R:0.0105)
Batch 450/537: Loss=0.9287 (C:0.9287, R:0.0105)
Batch 475/537: Loss=0.9201 (C:0.9201, R:0.0105)
Batch 500/537: Loss=0.9026 (C:0.9026, R:0.0105)
Batch 525/537: Loss=0.9200 (C:0.9200, R:0.0105)

============================================================
Epoch 20/300 completed in 21.8s
Train: Loss=0.9072 (C:0.9072, R:0.0105) Ratio=4.05x
Val:   Loss=0.9882 (C:0.9882, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.9145 (C:0.9145, R:0.0105)
Batch  25/537: Loss=0.8772 (C:0.8772, R:0.0105)
Batch  50/537: Loss=0.9173 (C:0.9173, R:0.0105)
Batch  75/537: Loss=0.9049 (C:0.9049, R:0.0105)
Batch 100/537: Loss=0.8565 (C:0.8565, R:0.0105)
Batch 125/537: Loss=0.9178 (C:0.9178, R:0.0105)
Batch 150/537: Loss=0.9032 (C:0.9032, R:0.0105)
Batch 175/537: Loss=0.9342 (C:0.9342, R:0.0105)
Batch 200/537: Loss=0.9083 (C:0.9083, R:0.0105)
Batch 225/537: Loss=0.8756 (C:0.8756, R:0.0105)
Batch 250/537: Loss=0.8956 (C:0.8956, R:0.0105)
Batch 275/537: Loss=0.9065 (C:0.9065, R:0.0105)
Batch 300/537: Loss=0.8756 (C:0.8756, R:0.0105)
Batch 325/537: Loss=0.9087 (C:0.9087, R:0.0105)
Batch 350/537: Loss=0.9044 (C:0.9044, R:0.0105)
Batch 375/537: Loss=0.9403 (C:0.9403, R:0.0106)
Batch 400/537: Loss=0.9012 (C:0.9012, R:0.0105)
Batch 425/537: Loss=0.9050 (C:0.9050, R:0.0105)
Batch 450/537: Loss=0.9284 (C:0.9284, R:0.0105)
Batch 475/537: Loss=0.9030 (C:0.9030, R:0.0105)
Batch 500/537: Loss=0.9013 (C:0.9013, R:0.0105)
Batch 525/537: Loss=0.9340 (C:0.9340, R:0.0105)

============================================================
Epoch 21/300 completed in 21.9s
Train: Loss=0.9038 (C:0.9038, R:0.0105) Ratio=4.05x
Val:   Loss=0.9822 (C:0.9822, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.336 ± 0.502
    Neg distances: 1.749 ± 0.877
    Separation ratio: 5.20x
    Gap: -3.072
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8604 (C:0.8604, R:0.0105)
Batch  25/537: Loss=0.8855 (C:0.8855, R:0.0105)
Batch  50/537: Loss=0.8774 (C:0.8774, R:0.0105)
Batch  75/537: Loss=0.8737 (C:0.8737, R:0.0105)
Batch 100/537: Loss=0.8643 (C:0.8643, R:0.0105)
Batch 125/537: Loss=0.8922 (C:0.8922, R:0.0105)
Batch 150/537: Loss=0.8879 (C:0.8879, R:0.0105)
Batch 175/537: Loss=0.8873 (C:0.8873, R:0.0105)
Batch 200/537: Loss=0.8869 (C:0.8869, R:0.0105)
Batch 225/537: Loss=0.8598 (C:0.8598, R:0.0105)
Batch 250/537: Loss=0.8755 (C:0.8755, R:0.0105)
Batch 275/537: Loss=0.9109 (C:0.9109, R:0.0105)
Batch 300/537: Loss=0.8866 (C:0.8866, R:0.0105)
Batch 325/537: Loss=0.9033 (C:0.9033, R:0.0105)
Batch 350/537: Loss=0.9091 (C:0.9091, R:0.0106)
Batch 375/537: Loss=0.8667 (C:0.8667, R:0.0105)
Batch 400/537: Loss=0.9406 (C:0.9406, R:0.0105)
Batch 425/537: Loss=0.9075 (C:0.9075, R:0.0105)
Batch 450/537: Loss=0.8932 (C:0.8932, R:0.0105)
Batch 475/537: Loss=0.9170 (C:0.9170, R:0.0105)
Batch 500/537: Loss=0.8968 (C:0.8968, R:0.0105)
Batch 525/537: Loss=0.9299 (C:0.9299, R:0.0105)

============================================================
Epoch 22/300 completed in 28.3s
Train: Loss=0.8913 (C:0.8913, R:0.0105) Ratio=4.08x
Val:   Loss=0.9704 (C:0.9704, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9704)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.8931 (C:0.8931, R:0.0105)
Batch  25/537: Loss=0.8701 (C:0.8701, R:0.0105)
Batch  50/537: Loss=0.8916 (C:0.8916, R:0.0105)
Batch  75/537: Loss=0.8671 (C:0.8671, R:0.0105)
Batch 100/537: Loss=0.9117 (C:0.9117, R:0.0105)
Batch 125/537: Loss=0.8801 (C:0.8801, R:0.0105)
Batch 150/537: Loss=0.8583 (C:0.8583, R:0.0105)
Batch 175/537: Loss=0.8713 (C:0.8713, R:0.0105)
Batch 200/537: Loss=0.8765 (C:0.8765, R:0.0105)
Batch 225/537: Loss=0.8999 (C:0.8999, R:0.0105)
Batch 250/537: Loss=0.8958 (C:0.8958, R:0.0105)
Batch 275/537: Loss=0.9162 (C:0.9162, R:0.0105)
Batch 300/537: Loss=0.8749 (C:0.8749, R:0.0105)
Batch 325/537: Loss=0.8942 (C:0.8942, R:0.0105)
Batch 350/537: Loss=0.8749 (C:0.8749, R:0.0105)
Batch 375/537: Loss=0.8652 (C:0.8652, R:0.0105)
Batch 400/537: Loss=0.9183 (C:0.9183, R:0.0105)
Batch 425/537: Loss=0.8959 (C:0.8959, R:0.0105)
Batch 450/537: Loss=0.8944 (C:0.8944, R:0.0105)
Batch 475/537: Loss=0.8781 (C:0.8781, R:0.0105)
Batch 500/537: Loss=0.9153 (C:0.9153, R:0.0105)
Batch 525/537: Loss=0.9110 (C:0.9110, R:0.0105)

============================================================
Epoch 23/300 completed in 21.6s
Train: Loss=0.8896 (C:0.8896, R:0.0105) Ratio=4.11x
Val:   Loss=0.9781 (C:0.9781, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8877 (C:0.8877, R:0.0106)
Batch  25/537: Loss=0.9233 (C:0.9233, R:0.0105)
Batch  50/537: Loss=0.8518 (C:0.8518, R:0.0105)
Batch  75/537: Loss=0.8829 (C:0.8829, R:0.0105)
Batch 100/537: Loss=0.8775 (C:0.8775, R:0.0105)
Batch 125/537: Loss=0.9115 (C:0.9115, R:0.0105)
Batch 150/537: Loss=0.8739 (C:0.8739, R:0.0105)
Batch 175/537: Loss=0.9395 (C:0.9395, R:0.0105)
Batch 200/537: Loss=0.8898 (C:0.8898, R:0.0105)
Batch 225/537: Loss=0.8723 (C:0.8723, R:0.0105)
Batch 250/537: Loss=0.8958 (C:0.8958, R:0.0105)
Batch 275/537: Loss=0.8491 (C:0.8491, R:0.0106)
Batch 300/537: Loss=0.9042 (C:0.9042, R:0.0105)
Batch 325/537: Loss=0.9070 (C:0.9070, R:0.0105)
Batch 350/537: Loss=0.8829 (C:0.8829, R:0.0105)
Batch 375/537: Loss=0.9026 (C:0.9026, R:0.0105)
Batch 400/537: Loss=0.9026 (C:0.9026, R:0.0105)
Batch 425/537: Loss=0.8603 (C:0.8603, R:0.0105)
Batch 450/537: Loss=0.8684 (C:0.8684, R:0.0105)
Batch 475/537: Loss=0.8718 (C:0.8718, R:0.0105)
Batch 500/537: Loss=0.9072 (C:0.9072, R:0.0105)
Batch 525/537: Loss=0.8652 (C:0.8652, R:0.0105)

============================================================
Epoch 24/300 completed in 21.1s
Train: Loss=0.8855 (C:0.8855, R:0.0105) Ratio=4.11x
Val:   Loss=0.9695 (C:0.9695, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9695)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.366 ± 0.546
    Neg distances: 1.810 ± 0.899
    Separation ratio: 4.94x
    Gap: -3.149
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.8557 (C:0.8557, R:0.0105)
Batch  25/537: Loss=0.8581 (C:0.8581, R:0.0105)
Batch  50/537: Loss=0.9147 (C:0.9147, R:0.0105)
Batch  75/537: Loss=0.8746 (C:0.8746, R:0.0105)
Batch 100/537: Loss=0.8608 (C:0.8608, R:0.0105)
Batch 125/537: Loss=0.8571 (C:0.8571, R:0.0105)
Batch 150/537: Loss=0.9377 (C:0.9377, R:0.0105)
Batch 175/537: Loss=0.8860 (C:0.8860, R:0.0105)
Batch 200/537: Loss=0.8631 (C:0.8631, R:0.0105)
Batch 225/537: Loss=0.8901 (C:0.8901, R:0.0105)
Batch 250/537: Loss=0.9050 (C:0.9050, R:0.0105)
Batch 275/537: Loss=0.8921 (C:0.8921, R:0.0105)
Batch 300/537: Loss=0.8737 (C:0.8737, R:0.0105)
Batch 325/537: Loss=0.8705 (C:0.8705, R:0.0105)
Batch 350/537: Loss=0.8865 (C:0.8865, R:0.0105)
Batch 375/537: Loss=0.8690 (C:0.8690, R:0.0105)
Batch 400/537: Loss=0.8784 (C:0.8784, R:0.0105)
Batch 425/537: Loss=0.8819 (C:0.8819, R:0.0105)
Batch 450/537: Loss=0.9018 (C:0.9018, R:0.0105)
Batch 475/537: Loss=0.8965 (C:0.8965, R:0.0105)
Batch 500/537: Loss=0.8675 (C:0.8675, R:0.0105)
Batch 525/537: Loss=0.8747 (C:0.8747, R:0.0105)

============================================================
Epoch 25/300 completed in 27.6s
Train: Loss=0.8819 (C:0.8819, R:0.0105) Ratio=4.18x
Val:   Loss=0.9656 (C:0.9656, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9656)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.8575 (C:0.8575, R:0.0105)
Batch  25/537: Loss=0.8857 (C:0.8857, R:0.0105)
Batch  50/537: Loss=0.8611 (C:0.8611, R:0.0105)
Batch  75/537: Loss=0.8736 (C:0.8736, R:0.0105)
Batch 100/537: Loss=0.8773 (C:0.8773, R:0.0105)
Batch 125/537: Loss=0.9113 (C:0.9113, R:0.0105)
Batch 150/537: Loss=0.8786 (C:0.8786, R:0.0105)
Batch 175/537: Loss=0.8866 (C:0.8866, R:0.0105)
Batch 200/537: Loss=0.8794 (C:0.8794, R:0.0105)
Batch 225/537: Loss=0.8648 (C:0.8648, R:0.0105)
Batch 250/537: Loss=0.8752 (C:0.8752, R:0.0105)
Batch 275/537: Loss=0.8821 (C:0.8821, R:0.0105)
Batch 300/537: Loss=0.8957 (C:0.8957, R:0.0105)
Batch 325/537: Loss=0.8492 (C:0.8492, R:0.0105)
Batch 350/537: Loss=0.8715 (C:0.8715, R:0.0105)
Batch 375/537: Loss=0.8606 (C:0.8606, R:0.0105)
Batch 400/537: Loss=0.8746 (C:0.8746, R:0.0105)
Batch 425/537: Loss=0.8855 (C:0.8855, R:0.0105)
Batch 450/537: Loss=0.8683 (C:0.8683, R:0.0105)
Batch 475/537: Loss=0.8695 (C:0.8695, R:0.0105)
Batch 500/537: Loss=0.8870 (C:0.8870, R:0.0105)
Batch 525/537: Loss=0.8724 (C:0.8724, R:0.0105)

============================================================
Epoch 26/300 completed in 21.9s
Train: Loss=0.8779 (C:0.8779, R:0.0105) Ratio=4.21x
Val:   Loss=0.9712 (C:0.9712, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.8946 (C:0.8946, R:0.0105)
Batch  25/537: Loss=0.8726 (C:0.8726, R:0.0105)
Batch  50/537: Loss=0.8488 (C:0.8488, R:0.0105)
Batch  75/537: Loss=0.8415 (C:0.8415, R:0.0105)
Batch 100/537: Loss=0.8827 (C:0.8827, R:0.0105)
Batch 125/537: Loss=0.8938 (C:0.8938, R:0.0105)
Batch 150/537: Loss=0.8493 (C:0.8493, R:0.0105)
Batch 175/537: Loss=0.8896 (C:0.8896, R:0.0105)
Batch 200/537: Loss=0.8496 (C:0.8496, R:0.0105)
Batch 225/537: Loss=0.8623 (C:0.8623, R:0.0105)
Batch 250/537: Loss=0.8578 (C:0.8578, R:0.0105)
Batch 275/537: Loss=0.8783 (C:0.8783, R:0.0105)
Batch 300/537: Loss=0.8859 (C:0.8859, R:0.0105)
Batch 325/537: Loss=0.8601 (C:0.8601, R:0.0105)
Batch 350/537: Loss=0.8485 (C:0.8485, R:0.0105)
Batch 375/537: Loss=0.8824 (C:0.8824, R:0.0105)
Batch 400/537: Loss=0.8567 (C:0.8567, R:0.0105)
Batch 425/537: Loss=0.8776 (C:0.8776, R:0.0105)
Batch 450/537: Loss=0.8759 (C:0.8759, R:0.0105)
Batch 475/537: Loss=0.8878 (C:0.8878, R:0.0105)
Batch 500/537: Loss=0.8874 (C:0.8874, R:0.0105)
Batch 525/537: Loss=0.8787 (C:0.8787, R:0.0105)

============================================================
Epoch 27/300 completed in 21.7s
Train: Loss=0.8761 (C:0.8761, R:0.0105) Ratio=4.39x
Val:   Loss=0.9753 (C:0.9753, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.323 ± 0.498
    Neg distances: 1.921 ± 0.904
    Separation ratio: 5.95x
    Gap: -3.242
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.7894 (C:0.7894, R:0.0105)
Batch  25/537: Loss=0.7832 (C:0.7832, R:0.0106)
Batch  50/537: Loss=0.7960 (C:0.7960, R:0.0105)
Batch  75/537: Loss=0.8028 (C:0.8028, R:0.0105)
Batch 100/537: Loss=0.7847 (C:0.7847, R:0.0105)
Batch 125/537: Loss=0.7936 (C:0.7936, R:0.0105)
Batch 150/537: Loss=0.7600 (C:0.7600, R:0.0105)
Batch 175/537: Loss=0.8013 (C:0.8013, R:0.0106)
Batch 200/537: Loss=0.8298 (C:0.8298, R:0.0105)
Batch 225/537: Loss=0.8185 (C:0.8185, R:0.0105)
Batch 250/537: Loss=0.7933 (C:0.7933, R:0.0105)
Batch 275/537: Loss=0.7969 (C:0.7969, R:0.0105)
Batch 300/537: Loss=0.8429 (C:0.8429, R:0.0105)
Batch 325/537: Loss=0.7802 (C:0.7802, R:0.0105)
Batch 350/537: Loss=0.8094 (C:0.8094, R:0.0105)
Batch 375/537: Loss=0.8009 (C:0.8009, R:0.0105)
Batch 400/537: Loss=0.7917 (C:0.7917, R:0.0105)
Batch 425/537: Loss=0.8037 (C:0.8037, R:0.0105)
Batch 450/537: Loss=0.8022 (C:0.8022, R:0.0105)
Batch 475/537: Loss=0.8133 (C:0.8133, R:0.0105)
Batch 500/537: Loss=0.8090 (C:0.8090, R:0.0105)
Batch 525/537: Loss=0.8076 (C:0.8076, R:0.0105)

============================================================
Epoch 28/300 completed in 27.7s
Train: Loss=0.8111 (C:0.8111, R:0.0105) Ratio=4.39x
Val:   Loss=0.9198 (C:0.9198, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9198)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.8040 (C:0.8040, R:0.0105)
Batch  25/537: Loss=0.8214 (C:0.8214, R:0.0105)
Batch  50/537: Loss=0.8009 (C:0.8009, R:0.0105)
Batch  75/537: Loss=0.7990 (C:0.7990, R:0.0105)
Batch 100/537: Loss=0.8378 (C:0.8378, R:0.0105)
Batch 125/537: Loss=0.7841 (C:0.7841, R:0.0105)
Batch 150/537: Loss=0.8307 (C:0.8307, R:0.0106)
Batch 175/537: Loss=0.8081 (C:0.8081, R:0.0105)
Batch 200/537: Loss=0.8261 (C:0.8261, R:0.0105)
Batch 225/537: Loss=0.8249 (C:0.8249, R:0.0105)
Batch 250/537: Loss=0.8196 (C:0.8196, R:0.0105)
Batch 275/537: Loss=0.8214 (C:0.8214, R:0.0105)
Batch 300/537: Loss=0.7840 (C:0.7840, R:0.0105)
Batch 325/537: Loss=0.8232 (C:0.8232, R:0.0105)
Batch 350/537: Loss=0.7795 (C:0.7795, R:0.0105)
Batch 375/537: Loss=0.8183 (C:0.8183, R:0.0105)
Batch 400/537: Loss=0.8320 (C:0.8320, R:0.0105)
Batch 425/537: Loss=0.8271 (C:0.8271, R:0.0105)
Batch 450/537: Loss=0.8108 (C:0.8108, R:0.0105)
Batch 475/537: Loss=0.8261 (C:0.8261, R:0.0105)
Batch 500/537: Loss=0.8006 (C:0.8006, R:0.0105)
Batch 525/537: Loss=0.8306 (C:0.8306, R:0.0106)

============================================================
Epoch 29/300 completed in 22.1s
Train: Loss=0.8078 (C:0.8078, R:0.0105) Ratio=4.26x
Val:   Loss=0.9022 (C:0.9022, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9022)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7929 (C:0.7929, R:0.0105)
Batch  25/537: Loss=0.7796 (C:0.7796, R:0.0105)
Batch  50/537: Loss=0.8099 (C:0.8099, R:0.0105)
Batch  75/537: Loss=0.8036 (C:0.8036, R:0.0105)
Batch 100/537: Loss=0.8100 (C:0.8100, R:0.0105)
Batch 125/537: Loss=0.7866 (C:0.7866, R:0.0106)
Batch 150/537: Loss=0.8238 (C:0.8238, R:0.0105)
Batch 175/537: Loss=0.7771 (C:0.7771, R:0.0105)
Batch 200/537: Loss=0.7808 (C:0.7808, R:0.0105)
Batch 225/537: Loss=0.8071 (C:0.8071, R:0.0105)
Batch 250/537: Loss=0.8185 (C:0.8185, R:0.0106)
Batch 275/537: Loss=0.8202 (C:0.8202, R:0.0105)
Batch 300/537: Loss=0.8143 (C:0.8143, R:0.0106)
Batch 325/537: Loss=0.8160 (C:0.8160, R:0.0105)
Batch 350/537: Loss=0.8409 (C:0.8409, R:0.0105)
Batch 375/537: Loss=0.8129 (C:0.8129, R:0.0105)
Batch 400/537: Loss=0.8035 (C:0.8035, R:0.0105)
Batch 425/537: Loss=0.8318 (C:0.8318, R:0.0105)
Batch 450/537: Loss=0.7936 (C:0.7936, R:0.0105)
Batch 475/537: Loss=0.8217 (C:0.8217, R:0.0105)
Batch 500/537: Loss=0.7944 (C:0.7944, R:0.0105)
Batch 525/537: Loss=0.8144 (C:0.8144, R:0.0105)

============================================================
Epoch 30/300 completed in 21.9s
Train: Loss=0.8043 (C:0.8043, R:0.0105) Ratio=4.30x
Val:   Loss=0.9019 (C:0.9019, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9019)
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.317 ± 0.502
    Neg distances: 2.018 ± 0.922
    Separation ratio: 6.36x
    Gap: -3.386
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.7737 (C:0.7737, R:0.0105)
Batch  25/537: Loss=0.7654 (C:0.7654, R:0.0105)
Batch  50/537: Loss=0.7686 (C:0.7686, R:0.0105)
Batch  75/537: Loss=0.7760 (C:0.7760, R:0.0106)
Batch 100/537: Loss=0.7825 (C:0.7825, R:0.0105)
Batch 125/537: Loss=0.7497 (C:0.7497, R:0.0105)
Batch 150/537: Loss=0.7718 (C:0.7718, R:0.0105)
Batch 175/537: Loss=0.7674 (C:0.7674, R:0.0105)
Batch 200/537: Loss=0.7543 (C:0.7543, R:0.0105)
Batch 225/537: Loss=0.7787 (C:0.7787, R:0.0105)
Batch 250/537: Loss=0.7685 (C:0.7685, R:0.0105)
Batch 275/537: Loss=0.7594 (C:0.7594, R:0.0105)
Batch 300/537: Loss=0.7725 (C:0.7725, R:0.0105)
Batch 325/537: Loss=0.7654 (C:0.7654, R:0.0105)
Batch 350/537: Loss=0.7766 (C:0.7766, R:0.0105)
Batch 375/537: Loss=0.7856 (C:0.7856, R:0.0105)
Batch 400/537: Loss=0.8109 (C:0.8109, R:0.0105)
Batch 425/537: Loss=0.7529 (C:0.7529, R:0.0105)
Batch 450/537: Loss=0.7697 (C:0.7697, R:0.0105)
Batch 475/537: Loss=0.7638 (C:0.7638, R:0.0105)
Batch 500/537: Loss=0.7756 (C:0.7756, R:0.0105)
Batch 525/537: Loss=0.7761 (C:0.7761, R:0.0105)

============================================================
Epoch 31/300 completed in 27.7s
Train: Loss=0.7700 (C:0.7700, R:0.0105) Ratio=4.43x
Val:   Loss=0.8762 (C:0.8762, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8762)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.7537 (C:0.7537, R:0.0106)
Batch  25/537: Loss=0.7411 (C:0.7411, R:0.0106)
Batch  50/537: Loss=0.7536 (C:0.7536, R:0.0105)
Batch  75/537: Loss=0.7715 (C:0.7715, R:0.0106)
Batch 100/537: Loss=0.7552 (C:0.7552, R:0.0105)
Batch 125/537: Loss=0.7645 (C:0.7645, R:0.0105)
Batch 150/537: Loss=0.7703 (C:0.7703, R:0.0105)
Batch 175/537: Loss=0.7312 (C:0.7312, R:0.0105)
Batch 200/537: Loss=0.7851 (C:0.7851, R:0.0105)
Batch 225/537: Loss=0.7732 (C:0.7732, R:0.0105)
Batch 250/537: Loss=0.7406 (C:0.7406, R:0.0105)
Batch 275/537: Loss=0.7650 (C:0.7650, R:0.0105)
Batch 300/537: Loss=0.7669 (C:0.7669, R:0.0105)
Batch 325/537: Loss=0.7796 (C:0.7796, R:0.0105)
Batch 350/537: Loss=0.7998 (C:0.7998, R:0.0105)
Batch 375/537: Loss=0.7486 (C:0.7486, R:0.0105)
Batch 400/537: Loss=0.7635 (C:0.7635, R:0.0105)
Batch 425/537: Loss=0.7679 (C:0.7679, R:0.0105)
Batch 450/537: Loss=0.7300 (C:0.7300, R:0.0105)
Batch 475/537: Loss=0.7438 (C:0.7438, R:0.0105)
Batch 500/537: Loss=0.7823 (C:0.7823, R:0.0105)
Batch 525/537: Loss=0.7758 (C:0.7758, R:0.0106)

============================================================
Epoch 32/300 completed in 21.6s
Train: Loss=0.7683 (C:0.7683, R:0.0105) Ratio=4.52x
Val:   Loss=0.8779 (C:0.8779, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.7736 (C:0.7736, R:0.0105)
Batch  25/537: Loss=0.7428 (C:0.7428, R:0.0105)
Batch  50/537: Loss=0.7403 (C:0.7403, R:0.0105)
Batch  75/537: Loss=0.7740 (C:0.7740, R:0.0105)
Batch 100/537: Loss=0.7938 (C:0.7938, R:0.0105)
Batch 125/537: Loss=0.7553 (C:0.7553, R:0.0105)
Batch 150/537: Loss=0.7444 (C:0.7444, R:0.0105)
Batch 175/537: Loss=0.7649 (C:0.7649, R:0.0105)
Batch 200/537: Loss=0.7751 (C:0.7751, R:0.0105)
Batch 225/537: Loss=0.7907 (C:0.7907, R:0.0105)
Batch 250/537: Loss=0.7410 (C:0.7410, R:0.0105)
Batch 275/537: Loss=0.7628 (C:0.7628, R:0.0105)
Batch 300/537: Loss=0.7764 (C:0.7764, R:0.0105)
Batch 325/537: Loss=0.7465 (C:0.7465, R:0.0105)
Batch 350/537: Loss=0.7519 (C:0.7519, R:0.0105)
Batch 375/537: Loss=0.7262 (C:0.7262, R:0.0106)
Batch 400/537: Loss=0.7639 (C:0.7639, R:0.0105)
Batch 425/537: Loss=0.7861 (C:0.7861, R:0.0105)
Batch 450/537: Loss=0.7652 (C:0.7652, R:0.0105)
Batch 475/537: Loss=0.7721 (C:0.7721, R:0.0106)
Batch 500/537: Loss=0.7650 (C:0.7650, R:0.0105)
Batch 525/537: Loss=0.8061 (C:0.8061, R:0.0105)

============================================================
Epoch 33/300 completed in 20.9s
Train: Loss=0.7647 (C:0.7647, R:0.0105) Ratio=4.45x
Val:   Loss=0.8717 (C:0.8717, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.8717)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.328 ± 0.524
    Neg distances: 2.060 ± 0.934
    Separation ratio: 6.28x
    Gap: -3.459
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.8042 (C:0.8042, R:0.0105)
Batch  25/537: Loss=0.7464 (C:0.7464, R:0.0105)
Batch  50/537: Loss=0.7400 (C:0.7400, R:0.0105)
Batch  75/537: Loss=0.7349 (C:0.7349, R:0.0105)
Batch 100/537: Loss=0.7527 (C:0.7527, R:0.0105)
Batch 125/537: Loss=0.7872 (C:0.7872, R:0.0105)
Batch 150/537: Loss=0.7275 (C:0.7275, R:0.0105)
Batch 175/537: Loss=0.7424 (C:0.7424, R:0.0105)
Batch 200/537: Loss=0.7303 (C:0.7303, R:0.0105)
Batch 225/537: Loss=0.7823 (C:0.7823, R:0.0105)
Batch 250/537: Loss=0.7398 (C:0.7398, R:0.0105)
Batch 275/537: Loss=0.7415 (C:0.7415, R:0.0105)
Batch 300/537: Loss=0.7603 (C:0.7603, R:0.0106)
Batch 325/537: Loss=0.7224 (C:0.7224, R:0.0105)
Batch 350/537: Loss=0.7577 (C:0.7577, R:0.0105)
Batch 375/537: Loss=0.7650 (C:0.7650, R:0.0105)
Batch 400/537: Loss=0.7435 (C:0.7435, R:0.0105)
Batch 425/537: Loss=0.7282 (C:0.7282, R:0.0105)
Batch 450/537: Loss=0.7613 (C:0.7613, R:0.0105)
Batch 475/537: Loss=0.7584 (C:0.7584, R:0.0105)
Batch 500/537: Loss=0.7661 (C:0.7661, R:0.0105)
Batch 525/537: Loss=0.7734 (C:0.7734, R:0.0105)

============================================================
Epoch 34/300 completed in 28.0s
Train: Loss=0.7536 (C:0.7536, R:0.0105) Ratio=4.50x
Val:   Loss=0.8626 (C:0.8626, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8626)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.7463 (C:0.7463, R:0.0105)
Batch  25/537: Loss=0.7332 (C:0.7332, R:0.0105)
Batch  50/537: Loss=0.7187 (C:0.7187, R:0.0105)
Batch  75/537: Loss=0.7284 (C:0.7284, R:0.0105)
Batch 100/537: Loss=0.7668 (C:0.7668, R:0.0105)
Batch 125/537: Loss=0.7511 (C:0.7511, R:0.0105)
Batch 150/537: Loss=0.7798 (C:0.7798, R:0.0105)
Batch 175/537: Loss=0.7489 (C:0.7489, R:0.0105)
Batch 200/537: Loss=0.7382 (C:0.7382, R:0.0105)
Batch 225/537: Loss=0.7725 (C:0.7725, R:0.0105)
Batch 250/537: Loss=0.7712 (C:0.7712, R:0.0105)
Batch 275/537: Loss=0.7653 (C:0.7653, R:0.0105)
Batch 300/537: Loss=0.7380 (C:0.7380, R:0.0105)
Batch 325/537: Loss=0.7602 (C:0.7602, R:0.0105)
Batch 350/537: Loss=0.7540 (C:0.7540, R:0.0105)
Batch 375/537: Loss=0.7135 (C:0.7135, R:0.0105)
Batch 400/537: Loss=0.7646 (C:0.7646, R:0.0105)
Batch 425/537: Loss=0.7802 (C:0.7802, R:0.0105)
Batch 450/537: Loss=0.7726 (C:0.7726, R:0.0105)
Batch 475/537: Loss=0.7892 (C:0.7892, R:0.0105)
Batch 500/537: Loss=0.7458 (C:0.7458, R:0.0105)
Batch 525/537: Loss=0.7620 (C:0.7620, R:0.0105)

============================================================
Epoch 35/300 completed in 21.6s
Train: Loss=0.7503 (C:0.7503, R:0.0105) Ratio=4.54x
Val:   Loss=0.8664 (C:0.8664, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.075
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.7461 (C:0.7461, R:0.0105)
Batch  25/537: Loss=0.7569 (C:0.7569, R:0.0105)
Batch  50/537: Loss=0.7221 (C:0.7221, R:0.0105)
Batch  75/537: Loss=0.7495 (C:0.7495, R:0.0105)
Batch 100/537: Loss=0.7633 (C:0.7633, R:0.0105)
Batch 125/537: Loss=0.7599 (C:0.7599, R:0.0105)
Batch 150/537: Loss=0.7239 (C:0.7239, R:0.0105)
Batch 175/537: Loss=0.7385 (C:0.7385, R:0.0105)
Batch 200/537: Loss=0.7361 (C:0.7361, R:0.0105)
Batch 225/537: Loss=0.7507 (C:0.7507, R:0.0105)
Batch 250/537: Loss=0.7389 (C:0.7389, R:0.0105)
Batch 275/537: Loss=0.7208 (C:0.7208, R:0.0105)
Batch 300/537: Loss=0.7361 (C:0.7361, R:0.0105)
Batch 325/537: Loss=0.7617 (C:0.7617, R:0.0105)
Batch 350/537: Loss=0.7637 (C:0.7637, R:0.0105)
Batch 375/537: Loss=0.7566 (C:0.7566, R:0.0105)
Batch 400/537: Loss=0.7454 (C:0.7454, R:0.0105)
Batch 425/537: Loss=0.7643 (C:0.7643, R:0.0105)
Batch 450/537: Loss=0.7758 (C:0.7758, R:0.0105)
Batch 475/537: Loss=0.7460 (C:0.7460, R:0.0105)
Batch 500/537: Loss=0.7589 (C:0.7589, R:0.0105)
Batch 525/537: Loss=0.7458 (C:0.7458, R:0.0105)

============================================================
Epoch 36/300 completed in 21.5s
Train: Loss=0.7483 (C:0.7483, R:0.0105) Ratio=4.58x
Val:   Loss=0.8662 (C:0.8662, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.090
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.321 ± 0.516
    Neg distances: 2.138 ± 0.960
    Separation ratio: 6.66x
    Gap: -3.619
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.7247 (C:0.7247, R:0.0105)
Batch  25/537: Loss=0.7277 (C:0.7277, R:0.0105)
Batch  50/537: Loss=0.7334 (C:0.7334, R:0.0105)
Batch  75/537: Loss=0.7067 (C:0.7067, R:0.0105)
Batch 100/537: Loss=0.7133 (C:0.7133, R:0.0105)
Batch 125/537: Loss=0.7440 (C:0.7440, R:0.0105)
Batch 150/537: Loss=0.7270 (C:0.7270, R:0.0105)
Batch 175/537: Loss=0.7097 (C:0.7097, R:0.0105)
Batch 200/537: Loss=0.6892 (C:0.6892, R:0.0105)
Batch 225/537: Loss=0.7245 (C:0.7245, R:0.0105)
Batch 250/537: Loss=0.6964 (C:0.6964, R:0.0105)
Batch 275/537: Loss=0.7350 (C:0.7350, R:0.0105)
Batch 300/537: Loss=0.6924 (C:0.6924, R:0.0105)
Batch 325/537: Loss=0.7266 (C:0.7266, R:0.0105)
Batch 350/537: Loss=0.7245 (C:0.7245, R:0.0105)
Batch 375/537: Loss=0.7072 (C:0.7072, R:0.0105)
Batch 400/537: Loss=0.7316 (C:0.7316, R:0.0105)
Batch 425/537: Loss=0.7171 (C:0.7171, R:0.0105)
Batch 450/537: Loss=0.7403 (C:0.7403, R:0.0105)
Batch 475/537: Loss=0.7122 (C:0.7122, R:0.0105)
Batch 500/537: Loss=0.7023 (C:0.7023, R:0.0105)
Batch 525/537: Loss=0.7068 (C:0.7068, R:0.0105)

============================================================
Epoch 37/300 completed in 27.2s
Train: Loss=0.7235 (C:0.7235, R:0.0105) Ratio=4.63x
Val:   Loss=0.8420 (C:0.8420, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8420)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.7214 (C:0.7214, R:0.0105)
Batch  25/537: Loss=0.7602 (C:0.7602, R:0.0105)
Batch  50/537: Loss=0.7301 (C:0.7301, R:0.0105)
Batch  75/537: Loss=0.7262 (C:0.7262, R:0.0105)
Batch 100/537: Loss=0.7190 (C:0.7190, R:0.0105)
Batch 125/537: Loss=0.7198 (C:0.7198, R:0.0105)
Batch 150/537: Loss=0.7540 (C:0.7540, R:0.0105)
Batch 175/537: Loss=0.7416 (C:0.7416, R:0.0105)
Batch 200/537: Loss=0.7225 (C:0.7225, R:0.0106)
Batch 225/537: Loss=0.7041 (C:0.7041, R:0.0105)
Batch 250/537: Loss=0.7301 (C:0.7301, R:0.0105)
Batch 275/537: Loss=0.7193 (C:0.7193, R:0.0105)
Batch 300/537: Loss=0.7115 (C:0.7115, R:0.0105)
Batch 325/537: Loss=0.7398 (C:0.7398, R:0.0105)
Batch 350/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 375/537: Loss=0.7364 (C:0.7364, R:0.0105)
Batch 400/537: Loss=0.7139 (C:0.7139, R:0.0105)
Batch 425/537: Loss=0.7210 (C:0.7210, R:0.0105)
Batch 450/537: Loss=0.7376 (C:0.7376, R:0.0105)
Batch 475/537: Loss=0.7016 (C:0.7016, R:0.0105)
Batch 500/537: Loss=0.7162 (C:0.7162, R:0.0105)
Batch 525/537: Loss=0.7317 (C:0.7317, R:0.0105)

============================================================
Epoch 38/300 completed in 21.6s
Train: Loss=0.7215 (C:0.7215, R:0.0105) Ratio=4.54x
Val:   Loss=0.8420 (C:0.8420, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.7203 (C:0.7203, R:0.0105)
Batch  25/537: Loss=0.7175 (C:0.7175, R:0.0105)
Batch  50/537: Loss=0.7131 (C:0.7131, R:0.0105)
Batch  75/537: Loss=0.7097 (C:0.7097, R:0.0105)
Batch 100/537: Loss=0.7182 (C:0.7182, R:0.0105)
Batch 125/537: Loss=0.7469 (C:0.7469, R:0.0105)
Batch 150/537: Loss=0.7170 (C:0.7170, R:0.0105)
Batch 175/537: Loss=0.6961 (C:0.6961, R:0.0105)
Batch 200/537: Loss=0.6992 (C:0.6992, R:0.0104)
Batch 225/537: Loss=0.6776 (C:0.6776, R:0.0105)
Batch 250/537: Loss=0.7472 (C:0.7472, R:0.0105)
Batch 275/537: Loss=0.7276 (C:0.7276, R:0.0105)
Batch 300/537: Loss=0.7373 (C:0.7373, R:0.0105)
Batch 325/537: Loss=0.7307 (C:0.7307, R:0.0105)
Batch 350/537: Loss=0.7302 (C:0.7302, R:0.0105)
Batch 375/537: Loss=0.7105 (C:0.7105, R:0.0105)
Batch 400/537: Loss=0.7270 (C:0.7270, R:0.0105)
Batch 425/537: Loss=0.7042 (C:0.7042, R:0.0105)
Batch 450/537: Loss=0.7242 (C:0.7242, R:0.0105)
Batch 475/537: Loss=0.7766 (C:0.7766, R:0.0105)
Batch 500/537: Loss=0.7263 (C:0.7263, R:0.0105)
Batch 525/537: Loss=0.7062 (C:0.7062, R:0.0105)

============================================================
Epoch 39/300 completed in 20.8s
Train: Loss=0.7202 (C:0.7202, R:0.0105) Ratio=4.62x
Val:   Loss=0.8472 (C:0.8472, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.347 ± 0.561
    Neg distances: 2.152 ± 0.961
    Separation ratio: 6.20x
    Gap: -3.661
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.6834 (C:0.6834, R:0.0106)
Batch  25/537: Loss=0.7259 (C:0.7259, R:0.0105)
Batch  50/537: Loss=0.7172 (C:0.7172, R:0.0105)
Batch  75/537: Loss=0.7122 (C:0.7122, R:0.0105)
Batch 100/537: Loss=0.7174 (C:0.7174, R:0.0105)
Batch 125/537: Loss=0.7054 (C:0.7054, R:0.0105)
Batch 150/537: Loss=0.7147 (C:0.7147, R:0.0105)
Batch 175/537: Loss=0.6960 (C:0.6960, R:0.0105)
Batch 200/537: Loss=0.7296 (C:0.7296, R:0.0105)
Batch 225/537: Loss=0.7284 (C:0.7284, R:0.0105)
Batch 250/537: Loss=0.7209 (C:0.7209, R:0.0105)
Batch 275/537: Loss=0.7016 (C:0.7016, R:0.0105)
Batch 300/537: Loss=0.7103 (C:0.7103, R:0.0105)
Batch 325/537: Loss=0.7446 (C:0.7446, R:0.0105)
Batch 350/537: Loss=0.7330 (C:0.7330, R:0.0105)
Batch 375/537: Loss=0.7308 (C:0.7308, R:0.0105)
Batch 400/537: Loss=0.7357 (C:0.7357, R:0.0105)
Batch 425/537: Loss=0.7135 (C:0.7135, R:0.0105)
Batch 450/537: Loss=0.7017 (C:0.7017, R:0.0105)
Batch 475/537: Loss=0.7218 (C:0.7218, R:0.0105)
Batch 500/537: Loss=0.7592 (C:0.7592, R:0.0106)
Batch 525/537: Loss=0.7003 (C:0.7003, R:0.0105)

============================================================
Epoch 40/300 completed in 27.3s
Train: Loss=0.7242 (C:0.7242, R:0.0105) Ratio=4.71x
Val:   Loss=0.8428 (C:0.8428, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.150
No improvement for 3 epochs
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6830 (C:0.6830, R:0.0105)
Batch  25/537: Loss=0.7034 (C:0.7034, R:0.0105)
Batch  50/537: Loss=0.6741 (C:0.6741, R:0.0105)
Batch  75/537: Loss=0.7423 (C:0.7423, R:0.0105)
Batch 100/537: Loss=0.6972 (C:0.6972, R:0.0105)
Batch 125/537: Loss=0.7096 (C:0.7096, R:0.0105)
Batch 150/537: Loss=0.7287 (C:0.7287, R:0.0105)
Batch 175/537: Loss=0.7121 (C:0.7121, R:0.0106)
Batch 200/537: Loss=0.6863 (C:0.6863, R:0.0106)
Batch 225/537: Loss=0.7058 (C:0.7058, R:0.0105)
Batch 250/537: Loss=0.7053 (C:0.7053, R:0.0105)
Batch 275/537: Loss=0.7599 (C:0.7599, R:0.0105)
Batch 300/537: Loss=0.7737 (C:0.7737, R:0.0105)
Batch 325/537: Loss=0.7004 (C:0.7004, R:0.0105)
Batch 350/537: Loss=0.7403 (C:0.7403, R:0.0105)
Batch 375/537: Loss=0.7208 (C:0.7208, R:0.0105)
Batch 400/537: Loss=0.7314 (C:0.7314, R:0.0105)
Batch 425/537: Loss=0.7209 (C:0.7209, R:0.0106)
Batch 450/537: Loss=0.7662 (C:0.7662, R:0.0105)
Batch 475/537: Loss=0.7432 (C:0.7432, R:0.0106)
Batch 500/537: Loss=0.7553 (C:0.7553, R:0.0105)
Batch 525/537: Loss=0.7162 (C:0.7162, R:0.0105)

============================================================
Epoch 41/300 completed in 21.7s
Train: Loss=0.7220 (C:0.7220, R:0.0105) Ratio=4.65x
Val:   Loss=0.8482 (C:0.8482, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.165
No improvement for 4 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.7124 (C:0.7124, R:0.0105)
Batch  25/537: Loss=0.6985 (C:0.6985, R:0.0105)
Batch  50/537: Loss=0.7056 (C:0.7056, R:0.0105)
Batch  75/537: Loss=0.6928 (C:0.6928, R:0.0105)
Batch 100/537: Loss=0.7099 (C:0.7099, R:0.0105)
Batch 125/537: Loss=0.6902 (C:0.6902, R:0.0105)
Batch 150/537: Loss=0.7333 (C:0.7333, R:0.0105)
Batch 175/537: Loss=0.6930 (C:0.6930, R:0.0105)
Batch 200/537: Loss=0.6954 (C:0.6954, R:0.0105)
Batch 225/537: Loss=0.6874 (C:0.6874, R:0.0105)
Batch 250/537: Loss=0.7323 (C:0.7323, R:0.0105)
Batch 275/537: Loss=0.6975 (C:0.6975, R:0.0105)
Batch 300/537: Loss=0.7273 (C:0.7273, R:0.0105)
Batch 325/537: Loss=0.7101 (C:0.7101, R:0.0105)
Batch 350/537: Loss=0.7493 (C:0.7493, R:0.0105)
Batch 375/537: Loss=0.7128 (C:0.7128, R:0.0105)
Batch 400/537: Loss=0.7232 (C:0.7232, R:0.0105)
Batch 425/537: Loss=0.7326 (C:0.7326, R:0.0105)
Batch 450/537: Loss=0.7179 (C:0.7179, R:0.0105)
Batch 475/537: Loss=0.7411 (C:0.7411, R:0.0105)
Batch 500/537: Loss=0.7349 (C:0.7349, R:0.0105)
Batch 525/537: Loss=0.7111 (C:0.7111, R:0.0105)

============================================================
Epoch 42/300 completed in 21.7s
Train: Loss=0.7187 (C:0.7187, R:0.0105) Ratio=4.78x
Val:   Loss=0.8508 (C:0.8508, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.180
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.309 ± 0.519
    Neg distances: 2.287 ± 0.985
    Separation ratio: 7.40x
    Gap: -3.804
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.6533 (C:0.6533, R:0.0105)
Batch  25/537: Loss=0.6306 (C:0.6306, R:0.0105)
Batch  50/537: Loss=0.6715 (C:0.6715, R:0.0105)
Batch  75/537: Loss=0.6674 (C:0.6674, R:0.0106)
Batch 100/537: Loss=0.6943 (C:0.6943, R:0.0105)
Batch 125/537: Loss=0.6507 (C:0.6507, R:0.0105)
Batch 150/537: Loss=0.6761 (C:0.6761, R:0.0105)
Batch 175/537: Loss=0.6655 (C:0.6655, R:0.0105)
Batch 200/537: Loss=0.6703 (C:0.6703, R:0.0105)
Batch 225/537: Loss=0.6502 (C:0.6502, R:0.0105)
Batch 250/537: Loss=0.6450 (C:0.6450, R:0.0105)
Batch 275/537: Loss=0.6868 (C:0.6868, R:0.0105)
Batch 300/537: Loss=0.6655 (C:0.6655, R:0.0105)
Batch 325/537: Loss=0.6551 (C:0.6551, R:0.0105)
Batch 350/537: Loss=0.6570 (C:0.6570, R:0.0105)
Batch 375/537: Loss=0.6238 (C:0.6238, R:0.0105)
Batch 400/537: Loss=0.6721 (C:0.6721, R:0.0104)
Batch 425/537: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 450/537: Loss=0.6990 (C:0.6990, R:0.0105)
Batch 475/537: Loss=0.6568 (C:0.6568, R:0.0105)
Batch 500/537: Loss=0.6703 (C:0.6703, R:0.0105)
Batch 525/537: Loss=0.6878 (C:0.6878, R:0.0105)

============================================================
Epoch 43/300 completed in 27.8s
Train: Loss=0.6594 (C:0.6594, R:0.0105) Ratio=4.71x
Val:   Loss=0.7991 (C:0.7991, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.7991)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.6515 (C:0.6515, R:0.0105)
Batch  25/537: Loss=0.6689 (C:0.6689, R:0.0105)
Batch  50/537: Loss=0.6293 (C:0.6293, R:0.0105)
Batch  75/537: Loss=0.6176 (C:0.6176, R:0.0105)
Batch 100/537: Loss=0.6718 (C:0.6718, R:0.0105)
Batch 125/537: Loss=0.6643 (C:0.6643, R:0.0105)
Batch 150/537: Loss=0.6604 (C:0.6604, R:0.0105)
Batch 175/537: Loss=0.6524 (C:0.6524, R:0.0105)
Batch 200/537: Loss=0.6947 (C:0.6947, R:0.0105)
Batch 225/537: Loss=0.6700 (C:0.6700, R:0.0105)
Batch 250/537: Loss=0.5961 (C:0.5961, R:0.0105)
Batch 275/537: Loss=0.6841 (C:0.6841, R:0.0105)
Batch 300/537: Loss=0.6277 (C:0.6277, R:0.0105)
Batch 325/537: Loss=0.6748 (C:0.6748, R:0.0105)
Batch 350/537: Loss=0.6276 (C:0.6276, R:0.0105)
Batch 375/537: Loss=0.6812 (C:0.6812, R:0.0105)
Batch 400/537: Loss=0.6832 (C:0.6832, R:0.0105)
Batch 425/537: Loss=0.6841 (C:0.6841, R:0.0105)
Batch 450/537: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 475/537: Loss=0.6508 (C:0.6508, R:0.0105)
Batch 500/537: Loss=0.6845 (C:0.6845, R:0.0105)
Batch 525/537: Loss=0.6615 (C:0.6615, R:0.0105)

============================================================
Epoch 44/300 completed in 21.0s
Train: Loss=0.6577 (C:0.6577, R:0.0105) Ratio=4.79x
Val:   Loss=0.8007 (C:0.8007, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch  25/537: Loss=0.6157 (C:0.6157, R:0.0105)
Batch  50/537: Loss=0.6146 (C:0.6146, R:0.0105)
Batch  75/537: Loss=0.6884 (C:0.6884, R:0.0105)
Batch 100/537: Loss=0.6144 (C:0.6144, R:0.0105)
Batch 125/537: Loss=0.6802 (C:0.6802, R:0.0105)
Batch 150/537: Loss=0.6113 (C:0.6113, R:0.0106)
Batch 175/537: Loss=0.6580 (C:0.6580, R:0.0105)
Batch 200/537: Loss=0.6532 (C:0.6532, R:0.0105)
Batch 225/537: Loss=0.6511 (C:0.6511, R:0.0105)
Batch 250/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 275/537: Loss=0.6397 (C:0.6397, R:0.0105)
Batch 300/537: Loss=0.6590 (C:0.6590, R:0.0105)
Batch 325/537: Loss=0.6934 (C:0.6934, R:0.0105)
Batch 350/537: Loss=0.6624 (C:0.6624, R:0.0105)
Batch 375/537: Loss=0.6419 (C:0.6419, R:0.0105)
Batch 400/537: Loss=0.6607 (C:0.6607, R:0.0105)
Batch 425/537: Loss=0.6785 (C:0.6785, R:0.0106)
Batch 450/537: Loss=0.6474 (C:0.6474, R:0.0105)
Batch 475/537: Loss=0.6834 (C:0.6834, R:0.0105)
Batch 500/537: Loss=0.6548 (C:0.6548, R:0.0105)
Batch 525/537: Loss=0.6627 (C:0.6627, R:0.0105)

============================================================
Epoch 45/300 completed in 21.5s
Train: Loss=0.6549 (C:0.6549, R:0.0105) Ratio=4.88x
Val:   Loss=0.7798 (C:0.7798, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.225
✅ New best model saved (Val Loss: 0.7798)
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.327 ± 0.553
    Neg distances: 2.320 ± 1.008
    Separation ratio: 7.10x
    Gap: -3.824
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.6666 (C:0.6666, R:0.0105)
Batch  25/537: Loss=0.5978 (C:0.5978, R:0.0105)
Batch  50/537: Loss=0.6602 (C:0.6602, R:0.0105)
Batch  75/537: Loss=0.6523 (C:0.6523, R:0.0105)
Batch 100/537: Loss=0.6538 (C:0.6538, R:0.0105)
Batch 125/537: Loss=0.6658 (C:0.6658, R:0.0105)
Batch 150/537: Loss=0.6478 (C:0.6478, R:0.0105)
Batch 175/537: Loss=0.6441 (C:0.6441, R:0.0105)
Batch 200/537: Loss=0.6818 (C:0.6818, R:0.0105)
Batch 225/537: Loss=0.6844 (C:0.6844, R:0.0105)
Batch 250/537: Loss=0.6674 (C:0.6674, R:0.0105)
Batch 275/537: Loss=0.6561 (C:0.6561, R:0.0105)
Batch 300/537: Loss=0.6586 (C:0.6586, R:0.0105)
Batch 325/537: Loss=0.6873 (C:0.6873, R:0.0105)
Batch 350/537: Loss=0.6438 (C:0.6438, R:0.0105)
Batch 375/537: Loss=0.6279 (C:0.6279, R:0.0105)
Batch 400/537: Loss=0.6720 (C:0.6720, R:0.0105)
Batch 425/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 450/537: Loss=0.6380 (C:0.6380, R:0.0105)
Batch 475/537: Loss=0.6729 (C:0.6729, R:0.0105)
Batch 500/537: Loss=0.6765 (C:0.6765, R:0.0105)
Batch 525/537: Loss=0.6445 (C:0.6445, R:0.0105)

============================================================
Epoch 46/300 completed in 27.1s
Train: Loss=0.6590 (C:0.6590, R:0.0105) Ratio=4.79x
Val:   Loss=0.7880 (C:0.7880, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.240
No improvement for 1 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.6470 (C:0.6470, R:0.0105)
Batch  25/537: Loss=0.6638 (C:0.6638, R:0.0105)
Batch  50/537: Loss=0.6534 (C:0.6534, R:0.0105)
Batch  75/537: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 100/537: Loss=0.6345 (C:0.6345, R:0.0105)
Batch 125/537: Loss=0.6565 (C:0.6565, R:0.0105)
Batch 150/537: Loss=0.6754 (C:0.6754, R:0.0105)
Batch 175/537: Loss=0.6314 (C:0.6314, R:0.0106)
Batch 200/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 225/537: Loss=0.6591 (C:0.6591, R:0.0105)
Batch 250/537: Loss=0.6605 (C:0.6605, R:0.0105)
Batch 275/537: Loss=0.6687 (C:0.6687, R:0.0105)
Batch 300/537: Loss=0.6784 (C:0.6784, R:0.0105)
Batch 325/537: Loss=0.6484 (C:0.6484, R:0.0105)
Batch 350/537: Loss=0.6642 (C:0.6642, R:0.0105)
Batch 375/537: Loss=0.6389 (C:0.6389, R:0.0105)
Batch 400/537: Loss=0.6608 (C:0.6608, R:0.0105)
Batch 425/537: Loss=0.6584 (C:0.6584, R:0.0105)
Batch 450/537: Loss=0.6601 (C:0.6601, R:0.0105)
Batch 475/537: Loss=0.6486 (C:0.6486, R:0.0105)
Batch 500/537: Loss=0.6688 (C:0.6688, R:0.0105)
Batch 525/537: Loss=0.6513 (C:0.6513, R:0.0105)

============================================================
Epoch 47/300 completed in 21.9s
Train: Loss=0.6561 (C:0.6561, R:0.0105) Ratio=4.88x
Val:   Loss=0.8012 (C:0.8012, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.255
No improvement for 2 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.6326 (C:0.6326, R:0.0105)
Batch  25/537: Loss=0.6509 (C:0.6509, R:0.0105)
Batch  50/537: Loss=0.6519 (C:0.6519, R:0.0105)
Batch  75/537: Loss=0.6560 (C:0.6560, R:0.0105)
Batch 100/537: Loss=0.6642 (C:0.6642, R:0.0105)
Batch 125/537: Loss=0.6879 (C:0.6879, R:0.0105)
Batch 150/537: Loss=0.6607 (C:0.6607, R:0.0105)
Batch 175/537: Loss=0.6437 (C:0.6437, R:0.0105)
Batch 200/537: Loss=0.6626 (C:0.6626, R:0.0105)
Batch 225/537: Loss=0.6225 (C:0.6225, R:0.0105)
Batch 250/537: Loss=0.6596 (C:0.6596, R:0.0105)
Batch 275/537: Loss=0.6665 (C:0.6665, R:0.0105)
Batch 300/537: Loss=0.6661 (C:0.6661, R:0.0105)
Batch 325/537: Loss=0.6344 (C:0.6344, R:0.0105)
Batch 350/537: Loss=0.6089 (C:0.6089, R:0.0105)
Batch 375/537: Loss=0.6169 (C:0.6169, R:0.0105)
Batch 400/537: Loss=0.6778 (C:0.6778, R:0.0105)
Batch 425/537: Loss=0.6929 (C:0.6929, R:0.0105)
Batch 450/537: Loss=0.6820 (C:0.6820, R:0.0105)
Batch 475/537: Loss=0.6761 (C:0.6761, R:0.0105)
Batch 500/537: Loss=0.6473 (C:0.6473, R:0.0105)
Batch 525/537: Loss=0.6870 (C:0.6870, R:0.0105)

============================================================
Epoch 48/300 completed in 21.6s
Train: Loss=0.6544 (C:0.6544, R:0.0105) Ratio=4.88x
Val:   Loss=0.7821 (C:0.7821, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.270
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.321 ± 0.559
    Neg distances: 2.383 ± 1.019
    Separation ratio: 7.41x
    Gap: -3.981
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.6286 (C:0.6286, R:0.0105)
Batch  25/537: Loss=0.6022 (C:0.6022, R:0.0105)
Batch  50/537: Loss=0.6139 (C:0.6139, R:0.0106)
Batch  75/537: Loss=0.6402 (C:0.6402, R:0.0105)
Batch 100/537: Loss=0.6310 (C:0.6310, R:0.0105)
Batch 125/537: Loss=0.6482 (C:0.6482, R:0.0105)
Batch 150/537: Loss=0.6315 (C:0.6315, R:0.0105)
Batch 175/537: Loss=0.6323 (C:0.6323, R:0.0105)
Batch 200/537: Loss=0.6234 (C:0.6234, R:0.0106)
Batch 225/537: Loss=0.6318 (C:0.6318, R:0.0105)
Batch 250/537: Loss=0.6092 (C:0.6092, R:0.0105)
Batch 275/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 300/537: Loss=0.6187 (C:0.6187, R:0.0105)
Batch 325/537: Loss=0.6071 (C:0.6071, R:0.0105)
Batch 350/537: Loss=0.6579 (C:0.6579, R:0.0105)
Batch 375/537: Loss=0.5945 (C:0.5945, R:0.0105)
Batch 400/537: Loss=0.6443 (C:0.6443, R:0.0105)
Batch 425/537: Loss=0.6489 (C:0.6489, R:0.0105)
Batch 450/537: Loss=0.6538 (C:0.6538, R:0.0105)
Batch 475/537: Loss=0.6747 (C:0.6747, R:0.0105)
Batch 500/537: Loss=0.6609 (C:0.6609, R:0.0105)
Batch 525/537: Loss=0.6523 (C:0.6523, R:0.0105)

============================================================
Epoch 49/300 completed in 27.6s
Train: Loss=0.6328 (C:0.6328, R:0.0105) Ratio=4.83x
Val:   Loss=0.7739 (C:0.7739, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.7739)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.6285 (C:0.6285, R:0.0105)
Batch  25/537: Loss=0.6393 (C:0.6393, R:0.0105)
Batch  50/537: Loss=0.6397 (C:0.6397, R:0.0105)
Batch  75/537: Loss=0.6424 (C:0.6424, R:0.0105)
Batch 100/537: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 125/537: Loss=0.6299 (C:0.6299, R:0.0105)
Batch 150/537: Loss=0.6324 (C:0.6324, R:0.0105)
Batch 175/537: Loss=0.6397 (C:0.6397, R:0.0105)
Batch 200/537: Loss=0.6507 (C:0.6507, R:0.0105)
Batch 225/537: Loss=0.6293 (C:0.6293, R:0.0105)
Batch 250/537: Loss=0.6604 (C:0.6604, R:0.0105)
Batch 275/537: Loss=0.6303 (C:0.6303, R:0.0105)
Batch 300/537: Loss=0.6508 (C:0.6508, R:0.0105)
Batch 325/537: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 350/537: Loss=0.6275 (C:0.6275, R:0.0105)
Batch 375/537: Loss=0.6277 (C:0.6277, R:0.0105)
Batch 400/537: Loss=0.6337 (C:0.6337, R:0.0105)
Batch 425/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch 450/537: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 475/537: Loss=0.6477 (C:0.6477, R:0.0105)
Batch 500/537: Loss=0.6430 (C:0.6430, R:0.0105)
Batch 525/537: Loss=0.6542 (C:0.6542, R:0.0105)

============================================================
Epoch 50/300 completed in 21.4s
Train: Loss=0.6302 (C:0.6302, R:0.0105) Ratio=4.81x
Val:   Loss=0.7695 (C:0.7695, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7695)
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.6229 (C:0.6229, R:0.0105)
Batch  25/537: Loss=0.6159 (C:0.6159, R:0.0105)
Batch  50/537: Loss=0.6567 (C:0.6567, R:0.0105)
Batch  75/537: Loss=0.6266 (C:0.6266, R:0.0105)
Batch 100/537: Loss=0.6281 (C:0.6281, R:0.0105)
Batch 125/537: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 150/537: Loss=0.6166 (C:0.6166, R:0.0106)
Batch 175/537: Loss=0.6043 (C:0.6043, R:0.0105)
Batch 200/537: Loss=0.6423 (C:0.6423, R:0.0105)
Batch 225/537: Loss=0.6480 (C:0.6480, R:0.0105)
Batch 250/537: Loss=0.6430 (C:0.6430, R:0.0105)
Batch 275/537: Loss=0.6424 (C:0.6424, R:0.0105)
Batch 300/537: Loss=0.6210 (C:0.6210, R:0.0106)
Batch 325/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 350/537: Loss=0.6197 (C:0.6197, R:0.0105)
Batch 375/537: Loss=0.6188 (C:0.6188, R:0.0105)
Batch 400/537: Loss=0.6258 (C:0.6258, R:0.0105)
Batch 425/537: Loss=0.6288 (C:0.6288, R:0.0105)
Batch 450/537: Loss=0.6342 (C:0.6342, R:0.0105)
Batch 475/537: Loss=0.6317 (C:0.6317, R:0.0105)
Batch 500/537: Loss=0.6219 (C:0.6219, R:0.0105)
Batch 525/537: Loss=0.6582 (C:0.6582, R:0.0105)

============================================================
Epoch 51/300 completed in 21.4s
Train: Loss=0.6289 (C:0.6289, R:0.0105) Ratio=4.90x
Val:   Loss=0.7645 (C:0.7645, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7645)
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.326 ± 0.560
    Neg distances: 2.393 ± 1.029
    Separation ratio: 7.35x
    Gap: -4.015
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.6363 (C:0.6363, R:0.0106)
Batch  25/537: Loss=0.6129 (C:0.6129, R:0.0105)
Batch  50/537: Loss=0.6404 (C:0.6404, R:0.0105)
Batch  75/537: Loss=0.6368 (C:0.6368, R:0.0105)
Batch 100/537: Loss=0.6229 (C:0.6229, R:0.0105)
Batch 125/537: Loss=0.6432 (C:0.6432, R:0.0105)
Batch 150/537: Loss=0.5796 (C:0.5796, R:0.0105)
Batch 175/537: Loss=0.6227 (C:0.6227, R:0.0105)
Batch 200/537: Loss=0.6087 (C:0.6087, R:0.0105)
Batch 225/537: Loss=0.6305 (C:0.6305, R:0.0105)
Batch 250/537: Loss=0.6263 (C:0.6263, R:0.0105)
Batch 275/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 300/537: Loss=0.6749 (C:0.6749, R:0.0105)
Batch 325/537: Loss=0.6290 (C:0.6290, R:0.0105)
Batch 350/537: Loss=0.6239 (C:0.6239, R:0.0105)
Batch 375/537: Loss=0.6080 (C:0.6080, R:0.0105)
Batch 400/537: Loss=0.6342 (C:0.6342, R:0.0105)
Batch 425/537: Loss=0.6226 (C:0.6226, R:0.0105)
Batch 450/537: Loss=0.6784 (C:0.6784, R:0.0105)
Batch 475/537: Loss=0.6527 (C:0.6527, R:0.0105)
Batch 500/537: Loss=0.6126 (C:0.6126, R:0.0105)
Batch 525/537: Loss=0.6477 (C:0.6477, R:0.0105)

============================================================
Epoch 52/300 completed in 27.9s
Train: Loss=0.6271 (C:0.6271, R:0.0105) Ratio=4.92x
Val:   Loss=0.7684 (C:0.7684, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.6199 (C:0.6199, R:0.0105)
Batch  25/537: Loss=0.6178 (C:0.6178, R:0.0105)
Batch  50/537: Loss=0.5981 (C:0.5981, R:0.0105)
Batch  75/537: Loss=0.6449 (C:0.6449, R:0.0105)
Batch 100/537: Loss=0.6264 (C:0.6264, R:0.0105)
Batch 125/537: Loss=0.6154 (C:0.6154, R:0.0105)
Batch 150/537: Loss=0.6286 (C:0.6286, R:0.0105)
Batch 175/537: Loss=0.5673 (C:0.5673, R:0.0105)
Batch 200/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch 225/537: Loss=0.6018 (C:0.6018, R:0.0105)
Batch 250/537: Loss=0.6356 (C:0.6356, R:0.0105)
Batch 275/537: Loss=0.6341 (C:0.6341, R:0.0105)
Batch 300/537: Loss=0.6122 (C:0.6122, R:0.0105)
Batch 325/537: Loss=0.6255 (C:0.6255, R:0.0105)
Batch 350/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 375/537: Loss=0.6426 (C:0.6426, R:0.0105)
Batch 400/537: Loss=0.6270 (C:0.6270, R:0.0105)
Batch 425/537: Loss=0.6479 (C:0.6479, R:0.0105)
Batch 450/537: Loss=0.6563 (C:0.6563, R:0.0105)
Batch 475/537: Loss=0.6213 (C:0.6213, R:0.0105)
Batch 500/537: Loss=0.6340 (C:0.6340, R:0.0105)
Batch 525/537: Loss=0.6354 (C:0.6354, R:0.0105)

============================================================
Epoch 53/300 completed in 21.7s
Train: Loss=0.6263 (C:0.6263, R:0.0105) Ratio=4.96x
Val:   Loss=0.7703 (C:0.7703, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.6307 (C:0.6307, R:0.0105)
Batch  25/537: Loss=0.6147 (C:0.6147, R:0.0105)
Batch  50/537: Loss=0.6024 (C:0.6024, R:0.0105)
Batch  75/537: Loss=0.6466 (C:0.6466, R:0.0105)
Batch 100/537: Loss=0.6120 (C:0.6120, R:0.0105)
Batch 125/537: Loss=0.6444 (C:0.6444, R:0.0105)
Batch 150/537: Loss=0.6481 (C:0.6481, R:0.0105)
Batch 175/537: Loss=0.6393 (C:0.6393, R:0.0105)
Batch 200/537: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 225/537: Loss=0.5987 (C:0.5987, R:0.0105)
Batch 250/537: Loss=0.6176 (C:0.6176, R:0.0105)
Batch 275/537: Loss=0.6030 (C:0.6030, R:0.0105)
Batch 300/537: Loss=0.5869 (C:0.5869, R:0.0105)
Batch 325/537: Loss=0.6180 (C:0.6180, R:0.0105)
Batch 350/537: Loss=0.6349 (C:0.6349, R:0.0105)
Batch 375/537: Loss=0.6171 (C:0.6171, R:0.0106)
Batch 400/537: Loss=0.6386 (C:0.6386, R:0.0105)
Batch 425/537: Loss=0.6422 (C:0.6422, R:0.0105)
Batch 450/537: Loss=0.6365 (C:0.6365, R:0.0105)
Batch 475/537: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 500/537: Loss=0.6072 (C:0.6072, R:0.0105)
Batch 525/537: Loss=0.6413 (C:0.6413, R:0.0105)

============================================================
Epoch 54/300 completed in 21.6s
Train: Loss=0.6240 (C:0.6240, R:0.0105) Ratio=5.06x
Val:   Loss=0.7721 (C:0.7721, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.314 ± 0.548
    Neg distances: 2.463 ± 1.045
    Separation ratio: 7.84x
    Gap: -4.055
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.5913 (C:0.5913, R:0.0105)
Batch  25/537: Loss=0.5767 (C:0.5767, R:0.0106)
Batch  50/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch  75/537: Loss=0.5741 (C:0.5741, R:0.0105)
Batch 100/537: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 125/537: Loss=0.5862 (C:0.5862, R:0.0105)
Batch 150/537: Loss=0.5984 (C:0.5984, R:0.0105)
Batch 175/537: Loss=0.5846 (C:0.5846, R:0.0105)
Batch 200/537: Loss=0.6010 (C:0.6010, R:0.0106)
Batch 225/537: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 250/537: Loss=0.5900 (C:0.5900, R:0.0105)
Batch 275/537: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 300/537: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 325/537: Loss=0.5977 (C:0.5977, R:0.0105)
Batch 350/537: Loss=0.6132 (C:0.6132, R:0.0105)
Batch 375/537: Loss=0.6041 (C:0.6041, R:0.0105)
Batch 400/537: Loss=0.6219 (C:0.6219, R:0.0105)
Batch 425/537: Loss=0.6199 (C:0.6199, R:0.0106)
Batch 450/537: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 475/537: Loss=0.6270 (C:0.6270, R:0.0105)
Batch 500/537: Loss=0.5826 (C:0.5826, R:0.0105)
Batch 525/537: Loss=0.6015 (C:0.6015, R:0.0105)

============================================================
Epoch 55/300 completed in 27.0s
Train: Loss=0.6014 (C:0.6014, R:0.0105) Ratio=5.06x
Val:   Loss=0.7503 (C:0.7503, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7503)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.6480 (C:0.6480, R:0.0105)
Batch  25/537: Loss=0.5838 (C:0.5838, R:0.0105)
Batch  50/537: Loss=0.5530 (C:0.5530, R:0.0105)
Batch  75/537: Loss=0.5854 (C:0.5854, R:0.0105)
Batch 100/537: Loss=0.6197 (C:0.6197, R:0.0105)
Batch 125/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 150/537: Loss=0.5706 (C:0.5706, R:0.0105)
Batch 175/537: Loss=0.5931 (C:0.5931, R:0.0105)
Batch 200/537: Loss=0.5819 (C:0.5819, R:0.0105)
Batch 225/537: Loss=0.6096 (C:0.6096, R:0.0105)
Batch 250/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 275/537: Loss=0.6261 (C:0.6261, R:0.0105)
Batch 300/537: Loss=0.5994 (C:0.5994, R:0.0105)
Batch 325/537: Loss=0.6165 (C:0.6165, R:0.0105)
Batch 350/537: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 375/537: Loss=0.6163 (C:0.6163, R:0.0105)
Batch 400/537: Loss=0.6055 (C:0.6055, R:0.0105)
Batch 425/537: Loss=0.5883 (C:0.5883, R:0.0105)
Batch 450/537: Loss=0.6615 (C:0.6615, R:0.0105)
Batch 475/537: Loss=0.6133 (C:0.6133, R:0.0105)
Batch 500/537: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 525/537: Loss=0.6184 (C:0.6184, R:0.0105)

============================================================
Epoch 56/300 completed in 21.6s
Train: Loss=0.6006 (C:0.6006, R:0.0105) Ratio=4.99x
Val:   Loss=0.7563 (C:0.7563, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch  25/537: Loss=0.5746 (C:0.5746, R:0.0105)
Batch  50/537: Loss=0.6303 (C:0.6303, R:0.0105)
Batch  75/537: Loss=0.5707 (C:0.5707, R:0.0105)
Batch 100/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 125/537: Loss=0.6232 (C:0.6232, R:0.0105)
Batch 150/537: Loss=0.5709 (C:0.5709, R:0.0106)
Batch 175/537: Loss=0.6309 (C:0.6309, R:0.0105)
Batch 200/537: Loss=0.6046 (C:0.6046, R:0.0105)
Batch 225/537: Loss=0.5919 (C:0.5919, R:0.0105)
Batch 250/537: Loss=0.5615 (C:0.5615, R:0.0105)
Batch 275/537: Loss=0.5573 (C:0.5573, R:0.0105)
Batch 300/537: Loss=0.6129 (C:0.6129, R:0.0106)
Batch 325/537: Loss=0.6156 (C:0.6156, R:0.0105)
Batch 350/537: Loss=0.6155 (C:0.6155, R:0.0105)
Batch 375/537: Loss=0.6002 (C:0.6002, R:0.0105)
Batch 400/537: Loss=0.5868 (C:0.5868, R:0.0105)
Batch 425/537: Loss=0.5896 (C:0.5896, R:0.0106)
Batch 450/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 475/537: Loss=0.6184 (C:0.6184, R:0.0105)
Batch 500/537: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 525/537: Loss=0.6258 (C:0.6258, R:0.0105)

============================================================
Epoch 57/300 completed in 21.7s
Train: Loss=0.5997 (C:0.5997, R:0.0105) Ratio=5.14x
Val:   Loss=0.7511 (C:0.7511, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.318 ± 0.567
    Neg distances: 2.473 ± 1.052
    Separation ratio: 7.79x
    Gap: -4.063
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.5661 (C:0.5661, R:0.0105)
Batch  25/537: Loss=0.5804 (C:0.5804, R:0.0105)
Batch  50/537: Loss=0.5646 (C:0.5646, R:0.0105)
Batch  75/537: Loss=0.5982 (C:0.5982, R:0.0105)
Batch 100/537: Loss=0.6141 (C:0.6141, R:0.0105)
Batch 125/537: Loss=0.5835 (C:0.5835, R:0.0105)
Batch 150/537: Loss=0.5779 (C:0.5779, R:0.0105)
Batch 175/537: Loss=0.5919 (C:0.5919, R:0.0105)
Batch 200/537: Loss=0.5693 (C:0.5693, R:0.0105)
Batch 225/537: Loss=0.6057 (C:0.6057, R:0.0105)
Batch 250/537: Loss=0.5840 (C:0.5840, R:0.0105)
Batch 275/537: Loss=0.6037 (C:0.6037, R:0.0105)
Batch 300/537: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 325/537: Loss=0.5964 (C:0.5964, R:0.0105)
Batch 350/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 375/537: Loss=0.6078 (C:0.6078, R:0.0105)
Batch 400/537: Loss=0.5960 (C:0.5960, R:0.0105)
Batch 425/537: Loss=0.6018 (C:0.6018, R:0.0105)
Batch 450/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 475/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 500/537: Loss=0.6020 (C:0.6020, R:0.0105)
Batch 525/537: Loss=0.6104 (C:0.6104, R:0.0105)

============================================================
Epoch 58/300 completed in 27.2s
Train: Loss=0.5969 (C:0.5969, R:0.0105) Ratio=5.19x
Val:   Loss=0.7525 (C:0.7525, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.5785 (C:0.5785, R:0.0105)
Batch  25/537: Loss=0.5704 (C:0.5704, R:0.0105)
Batch  50/537: Loss=0.6174 (C:0.6174, R:0.0105)
Batch  75/537: Loss=0.5915 (C:0.5915, R:0.0105)
Batch 100/537: Loss=0.5627 (C:0.5627, R:0.0105)
Batch 125/537: Loss=0.6091 (C:0.6091, R:0.0105)
Batch 150/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 175/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch 200/537: Loss=0.6008 (C:0.6008, R:0.0105)
Batch 225/537: Loss=0.5828 (C:0.5828, R:0.0105)
Batch 250/537: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 275/537: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 300/537: Loss=0.6157 (C:0.6157, R:0.0105)
Batch 325/537: Loss=0.5890 (C:0.5890, R:0.0105)
Batch 350/537: Loss=0.6191 (C:0.6191, R:0.0105)
Batch 375/537: Loss=0.6162 (C:0.6162, R:0.0105)
Batch 400/537: Loss=0.6040 (C:0.6040, R:0.0105)
Batch 425/537: Loss=0.6072 (C:0.6072, R:0.0105)
Batch 450/537: Loss=0.6153 (C:0.6153, R:0.0105)
Batch 475/537: Loss=0.5776 (C:0.5776, R:0.0105)
Batch 500/537: Loss=0.6108 (C:0.6108, R:0.0105)
Batch 525/537: Loss=0.6140 (C:0.6140, R:0.0105)

============================================================
Epoch 59/300 completed in 20.7s
Train: Loss=0.5949 (C:0.5949, R:0.0105) Ratio=5.10x
Val:   Loss=0.7478 (C:0.7478, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7478)
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.5818 (C:0.5818, R:0.0105)
Batch  25/537: Loss=0.5842 (C:0.5842, R:0.0106)
Batch  50/537: Loss=0.5924 (C:0.5924, R:0.0105)
Batch  75/537: Loss=0.5976 (C:0.5976, R:0.0106)
Batch 100/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 125/537: Loss=0.6097 (C:0.6097, R:0.0105)
Batch 150/537: Loss=0.5984 (C:0.5984, R:0.0105)
Batch 175/537: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 200/537: Loss=0.5901 (C:0.5901, R:0.0105)
Batch 225/537: Loss=0.5871 (C:0.5871, R:0.0105)
Batch 250/537: Loss=0.5757 (C:0.5757, R:0.0105)
Batch 275/537: Loss=0.6021 (C:0.6021, R:0.0105)
Batch 300/537: Loss=0.5755 (C:0.5755, R:0.0105)
Batch 325/537: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 350/537: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 375/537: Loss=0.5877 (C:0.5877, R:0.0105)
Batch 400/537: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 425/537: Loss=0.6077 (C:0.6077, R:0.0105)
Batch 450/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 475/537: Loss=0.5895 (C:0.5895, R:0.0105)
Batch 500/537: Loss=0.5934 (C:0.5934, R:0.0105)
Batch 525/537: Loss=0.5812 (C:0.5812, R:0.0105)

============================================================
Epoch 60/300 completed in 21.6s
Train: Loss=0.5949 (C:0.5949, R:0.0105) Ratio=5.14x
Val:   Loss=0.7436 (C:0.7436, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7436)
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.292 ± 0.539
    Neg distances: 2.505 ± 1.043
    Separation ratio: 8.58x
    Gap: -4.128
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.5690 (C:0.5690, R:0.0105)
Batch  25/537: Loss=0.5554 (C:0.5554, R:0.0105)
Batch  50/537: Loss=0.5560 (C:0.5560, R:0.0105)
Batch  75/537: Loss=0.5479 (C:0.5479, R:0.0105)
Batch 100/537: Loss=0.5625 (C:0.5625, R:0.0106)
Batch 125/537: Loss=0.5668 (C:0.5668, R:0.0105)
Batch 150/537: Loss=0.5769 (C:0.5769, R:0.0105)
Batch 175/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 200/537: Loss=0.5799 (C:0.5799, R:0.0105)
Batch 225/537: Loss=0.5561 (C:0.5561, R:0.0105)
Batch 250/537: Loss=0.5636 (C:0.5636, R:0.0105)
Batch 275/537: Loss=0.5804 (C:0.5804, R:0.0105)
Batch 300/537: Loss=0.5994 (C:0.5994, R:0.0105)
Batch 325/537: Loss=0.6065 (C:0.6065, R:0.0105)
Batch 350/537: Loss=0.5900 (C:0.5900, R:0.0105)
Batch 375/537: Loss=0.5584 (C:0.5584, R:0.0105)
Batch 400/537: Loss=0.6001 (C:0.6001, R:0.0105)
Batch 425/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 450/537: Loss=0.6089 (C:0.6089, R:0.0105)
Batch 475/537: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 500/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 525/537: Loss=0.5905 (C:0.5905, R:0.0105)

============================================================
Epoch 61/300 completed in 27.8s
Train: Loss=0.5661 (C:0.5661, R:0.0105) Ratio=5.01x
Val:   Loss=0.7243 (C:0.7243, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7243)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.5467 (C:0.5467, R:0.0105)
Batch  25/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch  50/537: Loss=0.5618 (C:0.5618, R:0.0105)
Batch  75/537: Loss=0.5262 (C:0.5262, R:0.0105)
Batch 100/537: Loss=0.5458 (C:0.5458, R:0.0105)
Batch 125/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 150/537: Loss=0.5688 (C:0.5688, R:0.0105)
Batch 175/537: Loss=0.5798 (C:0.5798, R:0.0105)
Batch 200/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch 225/537: Loss=0.5489 (C:0.5489, R:0.0105)
Batch 250/537: Loss=0.5914 (C:0.5914, R:0.0105)
Batch 275/537: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 300/537: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 325/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 350/537: Loss=0.5676 (C:0.5676, R:0.0105)
Batch 375/537: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 400/537: Loss=0.5669 (C:0.5669, R:0.0105)
Batch 425/537: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 450/537: Loss=0.5542 (C:0.5542, R:0.0105)
Batch 475/537: Loss=0.5534 (C:0.5534, R:0.0105)
Batch 500/537: Loss=0.5622 (C:0.5622, R:0.0105)
Batch 525/537: Loss=0.5753 (C:0.5753, R:0.0105)

============================================================
Epoch 62/300 completed in 21.6s
Train: Loss=0.5637 (C:0.5637, R:0.0105) Ratio=5.20x
Val:   Loss=0.7227 (C:0.7227, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7227)
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.5268 (C:0.5268, R:0.0105)
Batch  25/537: Loss=0.5713 (C:0.5713, R:0.0105)
Batch  50/537: Loss=0.5376 (C:0.5376, R:0.0105)
Batch  75/537: Loss=0.5483 (C:0.5483, R:0.0105)
Batch 100/537: Loss=0.5557 (C:0.5557, R:0.0105)
Batch 125/537: Loss=0.5411 (C:0.5411, R:0.0106)
Batch 150/537: Loss=0.5621 (C:0.5621, R:0.0105)
Batch 175/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 200/537: Loss=0.5707 (C:0.5707, R:0.0105)
Batch 225/537: Loss=0.5487 (C:0.5487, R:0.0105)
Batch 250/537: Loss=0.5750 (C:0.5750, R:0.0105)
Batch 275/537: Loss=0.5877 (C:0.5877, R:0.0105)
Batch 300/537: Loss=0.5481 (C:0.5481, R:0.0105)
Batch 325/537: Loss=0.5386 (C:0.5386, R:0.0105)
Batch 350/537: Loss=0.5266 (C:0.5266, R:0.0105)
Batch 375/537: Loss=0.5506 (C:0.5506, R:0.0105)
Batch 400/537: Loss=0.5789 (C:0.5789, R:0.0105)
Batch 425/537: Loss=0.5657 (C:0.5657, R:0.0105)
Batch 450/537: Loss=0.5953 (C:0.5953, R:0.0105)
Batch 475/537: Loss=0.5568 (C:0.5568, R:0.0105)
Batch 500/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 525/537: Loss=0.5740 (C:0.5740, R:0.0105)

============================================================
Epoch 63/300 completed in 21.3s
Train: Loss=0.5618 (C:0.5618, R:0.0105) Ratio=5.21x
Val:   Loss=0.7254 (C:0.7254, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.306 ± 0.554
    Neg distances: 2.531 ± 1.059
    Separation ratio: 8.28x
    Gap: -4.177
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.5598 (C:0.5598, R:0.0105)
Batch  25/537: Loss=0.5754 (C:0.5754, R:0.0105)
Batch  50/537: Loss=0.5724 (C:0.5724, R:0.0105)
Batch  75/537: Loss=0.5571 (C:0.5571, R:0.0105)
Batch 100/537: Loss=0.5865 (C:0.5865, R:0.0105)
Batch 125/537: Loss=0.6057 (C:0.6057, R:0.0105)
Batch 150/537: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 175/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 200/537: Loss=0.5369 (C:0.5369, R:0.0105)
Batch 225/537: Loss=0.5804 (C:0.5804, R:0.0106)
Batch 250/537: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 275/537: Loss=0.5301 (C:0.5301, R:0.0105)
Batch 300/537: Loss=0.5891 (C:0.5891, R:0.0105)
Batch 325/537: Loss=0.5549 (C:0.5549, R:0.0105)
Batch 350/537: Loss=0.5522 (C:0.5522, R:0.0105)
Batch 375/537: Loss=0.5508 (C:0.5508, R:0.0105)
Batch 400/537: Loss=0.5511 (C:0.5511, R:0.0105)
Batch 425/537: Loss=0.5681 (C:0.5681, R:0.0105)
Batch 450/537: Loss=0.5362 (C:0.5362, R:0.0105)
Batch 475/537: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 500/537: Loss=0.5815 (C:0.5815, R:0.0105)
Batch 525/537: Loss=0.5838 (C:0.5838, R:0.0105)

============================================================
Epoch 64/300 completed in 27.8s
Train: Loss=0.5653 (C:0.5653, R:0.0105) Ratio=5.16x
Val:   Loss=0.7302 (C:0.7302, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.5678 (C:0.5678, R:0.0105)
Batch  25/537: Loss=0.5222 (C:0.5222, R:0.0105)
Batch  50/537: Loss=0.5402 (C:0.5402, R:0.0105)
Batch  75/537: Loss=0.5381 (C:0.5381, R:0.0105)
Batch 100/537: Loss=0.5695 (C:0.5695, R:0.0105)
Batch 125/537: Loss=0.5438 (C:0.5438, R:0.0105)
Batch 150/537: Loss=0.5538 (C:0.5538, R:0.0104)
Batch 175/537: Loss=0.5864 (C:0.5864, R:0.0105)
Batch 200/537: Loss=0.5546 (C:0.5546, R:0.0105)
Batch 225/537: Loss=0.5697 (C:0.5697, R:0.0105)
Batch 250/537: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 275/537: Loss=0.5552 (C:0.5552, R:0.0105)
Batch 300/537: Loss=0.5275 (C:0.5275, R:0.0105)
Batch 325/537: Loss=0.5454 (C:0.5454, R:0.0105)
Batch 350/537: Loss=0.5592 (C:0.5592, R:0.0105)
Batch 375/537: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 400/537: Loss=0.5580 (C:0.5580, R:0.0105)
Batch 425/537: Loss=0.5597 (C:0.5597, R:0.0105)
Batch 450/537: Loss=0.5662 (C:0.5662, R:0.0105)
Batch 475/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 500/537: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 525/537: Loss=0.5946 (C:0.5946, R:0.0105)

============================================================
Epoch 65/300 completed in 21.5s
Train: Loss=0.5644 (C:0.5644, R:0.0105) Ratio=5.24x
Val:   Loss=0.7339 (C:0.7339, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.5951 (C:0.5951, R:0.0105)
Batch  25/537: Loss=0.5715 (C:0.5715, R:0.0105)
Batch  50/537: Loss=0.5736 (C:0.5736, R:0.0105)
Batch  75/537: Loss=0.5322 (C:0.5322, R:0.0105)
Batch 100/537: Loss=0.5568 (C:0.5568, R:0.0105)
Batch 125/537: Loss=0.5554 (C:0.5554, R:0.0105)
Batch 150/537: Loss=0.5444 (C:0.5444, R:0.0105)
Batch 175/537: Loss=0.5454 (C:0.5454, R:0.0105)
Batch 200/537: Loss=0.5563 (C:0.5563, R:0.0105)
Batch 225/537: Loss=0.5526 (C:0.5526, R:0.0105)
Batch 250/537: Loss=0.5354 (C:0.5354, R:0.0105)
Batch 275/537: Loss=0.5889 (C:0.5889, R:0.0105)
Batch 300/537: Loss=0.5694 (C:0.5694, R:0.0105)
Batch 325/537: Loss=0.5666 (C:0.5666, R:0.0106)
Batch 350/537: Loss=0.5407 (C:0.5407, R:0.0106)
Batch 375/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 400/537: Loss=0.5601 (C:0.5601, R:0.0105)
Batch 425/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 450/537: Loss=0.5571 (C:0.5571, R:0.0105)
Batch 475/537: Loss=0.5915 (C:0.5915, R:0.0106)
Batch 500/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 525/537: Loss=0.5618 (C:0.5618, R:0.0105)

============================================================
Epoch 66/300 completed in 20.8s
Train: Loss=0.5631 (C:0.5631, R:0.0105) Ratio=5.26x
Val:   Loss=0.7368 (C:0.7368, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.308 ± 0.580
    Neg distances: 2.555 ± 1.069
    Separation ratio: 8.29x
    Gap: -4.223
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.5099 (C:0.5099, R:0.0105)
Batch  25/537: Loss=0.5227 (C:0.5227, R:0.0105)
Batch  50/537: Loss=0.5462 (C:0.5462, R:0.0105)
Batch  75/537: Loss=0.5227 (C:0.5227, R:0.0105)
Batch 100/537: Loss=0.5821 (C:0.5821, R:0.0105)
Batch 125/537: Loss=0.5534 (C:0.5534, R:0.0105)
Batch 150/537: Loss=0.5525 (C:0.5525, R:0.0105)
Batch 175/537: Loss=0.5609 (C:0.5609, R:0.0105)
Batch 200/537: Loss=0.5604 (C:0.5604, R:0.0106)
Batch 225/537: Loss=0.5646 (C:0.5646, R:0.0105)
Batch 250/537: Loss=0.5369 (C:0.5369, R:0.0106)
Batch 275/537: Loss=0.5511 (C:0.5511, R:0.0105)
Batch 300/537: Loss=0.5738 (C:0.5738, R:0.0105)
Batch 325/537: Loss=0.5565 (C:0.5565, R:0.0105)
Batch 350/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 375/537: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 400/537: Loss=0.5873 (C:0.5873, R:0.0105)
Batch 425/537: Loss=0.5635 (C:0.5635, R:0.0105)
Batch 450/537: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 475/537: Loss=0.5975 (C:0.5975, R:0.0105)
Batch 500/537: Loss=0.5804 (C:0.5804, R:0.0105)
Batch 525/537: Loss=0.5638 (C:0.5638, R:0.0105)

============================================================
Epoch 67/300 completed in 27.3s
Train: Loss=0.5616 (C:0.5616, R:0.0105) Ratio=5.39x
Val:   Loss=0.7299 (C:0.7299, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.5442 (C:0.5442, R:0.0105)
Batch  25/537: Loss=0.5439 (C:0.5439, R:0.0105)
Batch  50/537: Loss=0.5693 (C:0.5693, R:0.0105)
Batch  75/537: Loss=0.5504 (C:0.5504, R:0.0105)
Batch 100/537: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 125/537: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 150/537: Loss=0.5713 (C:0.5713, R:0.0105)
Batch 175/537: Loss=0.5646 (C:0.5646, R:0.0105)
Batch 200/537: Loss=0.5595 (C:0.5595, R:0.0105)
Batch 225/537: Loss=0.5421 (C:0.5421, R:0.0105)
Batch 250/537: Loss=0.5799 (C:0.5799, R:0.0105)
Batch 275/537: Loss=0.5828 (C:0.5828, R:0.0105)
Batch 300/537: Loss=0.5626 (C:0.5626, R:0.0105)
Batch 325/537: Loss=0.5444 (C:0.5444, R:0.0105)
Batch 350/537: Loss=0.5613 (C:0.5613, R:0.0106)
Batch 375/537: Loss=0.5346 (C:0.5346, R:0.0105)
Batch 400/537: Loss=0.6201 (C:0.6201, R:0.0105)
Batch 425/537: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 450/537: Loss=0.5378 (C:0.5378, R:0.0105)
Batch 475/537: Loss=0.5754 (C:0.5754, R:0.0105)
Batch 500/537: Loss=0.5993 (C:0.5993, R:0.0105)
Batch 525/537: Loss=0.5771 (C:0.5771, R:0.0105)

============================================================
Epoch 68/300 completed in 21.2s
Train: Loss=0.5601 (C:0.5601, R:0.0105) Ratio=5.29x
Val:   Loss=0.7366 (C:0.7366, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.5501 (C:0.5501, R:0.0105)
Batch  25/537: Loss=0.5901 (C:0.5901, R:0.0105)
Batch  50/537: Loss=0.5445 (C:0.5445, R:0.0105)
Batch  75/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 100/537: Loss=0.5580 (C:0.5580, R:0.0105)
Batch 125/537: Loss=0.5487 (C:0.5487, R:0.0105)
Batch 150/537: Loss=0.5888 (C:0.5888, R:0.0105)
Batch 175/537: Loss=0.5536 (C:0.5536, R:0.0105)
Batch 200/537: Loss=0.5722 (C:0.5722, R:0.0105)
Batch 225/537: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 250/537: Loss=0.5517 (C:0.5517, R:0.0105)
Batch 275/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 300/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 325/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch 350/537: Loss=0.5713 (C:0.5713, R:0.0105)
Batch 375/537: Loss=0.5625 (C:0.5625, R:0.0105)
Batch 400/537: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 425/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 450/537: Loss=0.6137 (C:0.6137, R:0.0105)
Batch 475/537: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 500/537: Loss=0.5627 (C:0.5627, R:0.0105)
Batch 525/537: Loss=0.5516 (C:0.5516, R:0.0105)

============================================================
Epoch 69/300 completed in 21.7s
Train: Loss=0.5582 (C:0.5582, R:0.0105) Ratio=5.29x
Val:   Loss=0.7331 (C:0.7331, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.302 ± 0.552
    Neg distances: 2.585 ± 1.077
    Separation ratio: 8.55x
    Gap: -4.260
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch  25/537: Loss=0.5344 (C:0.5344, R:0.0105)
Batch  50/537: Loss=0.5579 (C:0.5579, R:0.0105)
Batch  75/537: Loss=0.5700 (C:0.5700, R:0.0105)
Batch 100/537: Loss=0.5478 (C:0.5478, R:0.0105)
Batch 125/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 150/537: Loss=0.5596 (C:0.5596, R:0.0105)
Batch 175/537: Loss=0.5250 (C:0.5250, R:0.0105)
Batch 200/537: Loss=0.5361 (C:0.5361, R:0.0105)
Batch 225/537: Loss=0.5675 (C:0.5675, R:0.0105)
Batch 250/537: Loss=0.6021 (C:0.6021, R:0.0105)
Batch 275/537: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 300/537: Loss=0.5471 (C:0.5471, R:0.0105)
Batch 325/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch 350/537: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 375/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 400/537: Loss=0.5292 (C:0.5292, R:0.0105)
Batch 425/537: Loss=0.5392 (C:0.5392, R:0.0105)
Batch 450/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 475/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 500/537: Loss=0.5768 (C:0.5768, R:0.0105)
Batch 525/537: Loss=0.5215 (C:0.5215, R:0.0105)

============================================================
Epoch 70/300 completed in 27.6s
Train: Loss=0.5516 (C:0.5516, R:0.0105) Ratio=5.27x
Val:   Loss=0.7224 (C:0.7224, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7224)
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.5711 (C:0.5711, R:0.0105)
Batch  25/537: Loss=0.5225 (C:0.5225, R:0.0105)
Batch  50/537: Loss=0.5216 (C:0.5216, R:0.0105)
Batch  75/537: Loss=0.5502 (C:0.5502, R:0.0105)
Batch 100/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 125/537: Loss=0.5691 (C:0.5691, R:0.0105)
Batch 150/537: Loss=0.5085 (C:0.5085, R:0.0105)
Batch 175/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 200/537: Loss=0.5377 (C:0.5377, R:0.0105)
Batch 225/537: Loss=0.5207 (C:0.5207, R:0.0105)
Batch 250/537: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 275/537: Loss=0.5207 (C:0.5207, R:0.0105)
Batch 300/537: Loss=0.6025 (C:0.6025, R:0.0105)
Batch 325/537: Loss=0.5278 (C:0.5278, R:0.0105)
Batch 350/537: Loss=0.5526 (C:0.5526, R:0.0105)
Batch 375/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 400/537: Loss=0.5556 (C:0.5556, R:0.0105)
Batch 425/537: Loss=0.5647 (C:0.5647, R:0.0105)
Batch 450/537: Loss=0.5522 (C:0.5522, R:0.0105)
Batch 475/537: Loss=0.5756 (C:0.5756, R:0.0106)
Batch 500/537: Loss=0.5724 (C:0.5724, R:0.0105)
Batch 525/537: Loss=0.5885 (C:0.5885, R:0.0105)

============================================================
Epoch 71/300 completed in 21.0s
Train: Loss=0.5498 (C:0.5498, R:0.0105) Ratio=5.34x
Val:   Loss=0.7171 (C:0.7171, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7171)
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch  25/537: Loss=0.5216 (C:0.5216, R:0.0105)
Batch  50/537: Loss=0.5394 (C:0.5394, R:0.0105)
Batch  75/537: Loss=0.5539 (C:0.5539, R:0.0105)
Batch 100/537: Loss=0.5578 (C:0.5578, R:0.0105)
Batch 125/537: Loss=0.5700 (C:0.5700, R:0.0105)
Batch 150/537: Loss=0.5701 (C:0.5701, R:0.0105)
Batch 175/537: Loss=0.5719 (C:0.5719, R:0.0105)
Batch 200/537: Loss=0.5665 (C:0.5665, R:0.0105)
Batch 225/537: Loss=0.5116 (C:0.5116, R:0.0105)
Batch 250/537: Loss=0.5549 (C:0.5549, R:0.0105)
Batch 275/537: Loss=0.5608 (C:0.5608, R:0.0105)
Batch 300/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 325/537: Loss=0.5403 (C:0.5403, R:0.0105)
Batch 350/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch 375/537: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 400/537: Loss=0.5744 (C:0.5744, R:0.0105)
Batch 425/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 450/537: Loss=0.5631 (C:0.5631, R:0.0105)
Batch 475/537: Loss=0.5627 (C:0.5627, R:0.0105)
Batch 500/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 525/537: Loss=0.5676 (C:0.5676, R:0.0105)

============================================================
Epoch 72/300 completed in 21.1s
Train: Loss=0.5512 (C:0.5512, R:0.0105) Ratio=5.33x
Val:   Loss=0.7223 (C:0.7223, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.298 ± 0.562
    Neg distances: 2.582 ± 1.074
    Separation ratio: 8.66x
    Gap: -4.341
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=0.5237 (C:0.5237, R:0.0105)
Batch  25/537: Loss=0.5526 (C:0.5526, R:0.0106)
Batch  50/537: Loss=0.5416 (C:0.5416, R:0.0106)
Batch  75/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 100/537: Loss=0.5575 (C:0.5575, R:0.0105)
Batch 125/537: Loss=0.5463 (C:0.5463, R:0.0105)
Batch 150/537: Loss=0.5083 (C:0.5083, R:0.0105)
Batch 175/537: Loss=0.5234 (C:0.5234, R:0.0105)
Batch 200/537: Loss=0.5508 (C:0.5508, R:0.0105)
Batch 225/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch 250/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 275/537: Loss=0.5362 (C:0.5362, R:0.0105)
Batch 300/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 325/537: Loss=0.5611 (C:0.5611, R:0.0105)
Batch 350/537: Loss=0.5377 (C:0.5377, R:0.0105)
Batch 375/537: Loss=0.5165 (C:0.5165, R:0.0105)
Batch 400/537: Loss=0.5609 (C:0.5609, R:0.0105)
Batch 425/537: Loss=0.5702 (C:0.5702, R:0.0105)
Batch 450/537: Loss=0.5302 (C:0.5302, R:0.0105)
Batch 475/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 500/537: Loss=0.5663 (C:0.5663, R:0.0105)
Batch 525/537: Loss=0.5539 (C:0.5539, R:0.0105)

============================================================
Epoch 73/300 completed in 27.7s
Train: Loss=0.5427 (C:0.5427, R:0.0105) Ratio=5.44x
Val:   Loss=0.7144 (C:0.7144, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7144)
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=0.5398 (C:0.5398, R:0.0105)
Batch  25/537: Loss=0.5044 (C:0.5044, R:0.0106)
Batch  50/537: Loss=0.5289 (C:0.5289, R:0.0105)
Batch  75/537: Loss=0.5235 (C:0.5235, R:0.0105)
Batch 100/537: Loss=0.5140 (C:0.5140, R:0.0105)
Batch 125/537: Loss=0.5401 (C:0.5401, R:0.0105)
Batch 150/537: Loss=0.4983 (C:0.4983, R:0.0105)
Batch 175/537: Loss=0.5312 (C:0.5312, R:0.0106)
Batch 200/537: Loss=0.4972 (C:0.4972, R:0.0105)
Batch 225/537: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 250/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch 275/537: Loss=0.5407 (C:0.5407, R:0.0105)
Batch 300/537: Loss=0.5225 (C:0.5225, R:0.0105)
Batch 325/537: Loss=0.5530 (C:0.5530, R:0.0105)
Batch 350/537: Loss=0.5481 (C:0.5481, R:0.0105)
Batch 375/537: Loss=0.5552 (C:0.5552, R:0.0105)
Batch 400/537: Loss=0.5683 (C:0.5683, R:0.0105)
Batch 425/537: Loss=0.5448 (C:0.5448, R:0.0105)
Batch 450/537: Loss=0.5383 (C:0.5383, R:0.0105)
Batch 475/537: Loss=0.5618 (C:0.5618, R:0.0105)
Batch 500/537: Loss=0.5660 (C:0.5660, R:0.0105)
Batch 525/537: Loss=0.5480 (C:0.5480, R:0.0105)

============================================================
Epoch 74/300 completed in 21.6s
Train: Loss=0.5428 (C:0.5428, R:0.0105) Ratio=5.40x
Val:   Loss=0.7117 (C:0.7117, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7117)
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=0.5332 (C:0.5332, R:0.0105)
Batch  25/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch  50/537: Loss=0.5189 (C:0.5189, R:0.0105)
Batch  75/537: Loss=0.5745 (C:0.5745, R:0.0105)
Batch 100/537: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 125/537: Loss=0.5296 (C:0.5296, R:0.0105)
Batch 150/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch 175/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch 200/537: Loss=0.5007 (C:0.5007, R:0.0105)
Batch 225/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 250/537: Loss=0.5312 (C:0.5312, R:0.0105)
Batch 275/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 300/537: Loss=0.5249 (C:0.5249, R:0.0105)
Batch 325/537: Loss=0.5540 (C:0.5540, R:0.0105)
Batch 350/537: Loss=0.5660 (C:0.5660, R:0.0105)
Batch 375/537: Loss=0.5336 (C:0.5336, R:0.0105)
Batch 400/537: Loss=0.5179 (C:0.5179, R:0.0105)
Batch 425/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 450/537: Loss=0.5901 (C:0.5901, R:0.0105)
Batch 475/537: Loss=0.5859 (C:0.5859, R:0.0105)
Batch 500/537: Loss=0.5655 (C:0.5655, R:0.0105)
Batch 525/537: Loss=0.5257 (C:0.5257, R:0.0105)

============================================================
Epoch 75/300 completed in 21.2s
Train: Loss=0.5417 (C:0.5417, R:0.0105) Ratio=5.39x
Val:   Loss=0.7145 (C:0.7145, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.293 ± 0.558
    Neg distances: 2.619 ± 1.085
    Separation ratio: 8.94x
    Gap: -4.317
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=0.5325 (C:0.5325, R:0.0105)
Batch  25/537: Loss=0.5255 (C:0.5255, R:0.0105)
Batch  50/537: Loss=0.4982 (C:0.4982, R:0.0105)
Batch  75/537: Loss=0.5270 (C:0.5270, R:0.0105)
Batch 100/537: Loss=0.5142 (C:0.5142, R:0.0106)
Batch 125/537: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 150/537: Loss=0.5215 (C:0.5215, R:0.0105)
Batch 175/537: Loss=0.5355 (C:0.5355, R:0.0105)
Batch 200/537: Loss=0.5023 (C:0.5023, R:0.0105)
Batch 225/537: Loss=0.5078 (C:0.5078, R:0.0105)
Batch 250/537: Loss=0.5105 (C:0.5105, R:0.0106)
Batch 275/537: Loss=0.5018 (C:0.5018, R:0.0106)
Batch 300/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 325/537: Loss=0.5030 (C:0.5030, R:0.0105)
Batch 350/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch 375/537: Loss=0.5333 (C:0.5333, R:0.0105)
Batch 400/537: Loss=0.5319 (C:0.5319, R:0.0105)
Batch 425/537: Loss=0.5244 (C:0.5244, R:0.0105)
Batch 450/537: Loss=0.5295 (C:0.5295, R:0.0105)
Batch 475/537: Loss=0.5504 (C:0.5504, R:0.0105)
Batch 500/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 525/537: Loss=0.5612 (C:0.5612, R:0.0105)

============================================================
Epoch 76/300 completed in 27.3s
Train: Loss=0.5343 (C:0.5343, R:0.0105) Ratio=5.57x
Val:   Loss=0.7183 (C:0.7183, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=0.5143 (C:0.5143, R:0.0105)
Batch  25/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch  50/537: Loss=0.5349 (C:0.5349, R:0.0105)
Batch  75/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 100/537: Loss=0.5662 (C:0.5662, R:0.0106)
Batch 125/537: Loss=0.5387 (C:0.5387, R:0.0105)
Batch 150/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 175/537: Loss=0.5284 (C:0.5284, R:0.0105)
Batch 200/537: Loss=0.5010 (C:0.5010, R:0.0105)
Batch 225/537: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 250/537: Loss=0.4914 (C:0.4914, R:0.0105)
Batch 275/537: Loss=0.5101 (C:0.5101, R:0.0105)
Batch 300/537: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 325/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch 350/537: Loss=0.5682 (C:0.5682, R:0.0105)
Batch 375/537: Loss=0.5093 (C:0.5093, R:0.0105)
Batch 400/537: Loss=0.4745 (C:0.4745, R:0.0105)
Batch 425/537: Loss=0.5327 (C:0.5327, R:0.0105)
Batch 450/537: Loss=0.5316 (C:0.5316, R:0.0105)
Batch 475/537: Loss=0.5649 (C:0.5649, R:0.0105)
Batch 500/537: Loss=0.5057 (C:0.5057, R:0.0105)
Batch 525/537: Loss=0.5063 (C:0.5063, R:0.0105)

============================================================
Epoch 77/300 completed in 21.7s
Train: Loss=0.5338 (C:0.5338, R:0.0105) Ratio=5.53x
Val:   Loss=0.7216 (C:0.7216, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=0.5338 (C:0.5338, R:0.0105)
Batch  25/537: Loss=0.5027 (C:0.5027, R:0.0105)
Batch  50/537: Loss=0.5170 (C:0.5170, R:0.0105)
Batch  75/537: Loss=0.5175 (C:0.5175, R:0.0105)
Batch 100/537: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 125/537: Loss=0.5313 (C:0.5313, R:0.0105)
Batch 150/537: Loss=0.5310 (C:0.5310, R:0.0105)
Batch 175/537: Loss=0.5082 (C:0.5082, R:0.0105)
Batch 200/537: Loss=0.5159 (C:0.5159, R:0.0106)
Batch 225/537: Loss=0.5191 (C:0.5191, R:0.0105)
Batch 250/537: Loss=0.5557 (C:0.5557, R:0.0105)
Batch 275/537: Loss=0.5395 (C:0.5395, R:0.0105)
Batch 300/537: Loss=0.5221 (C:0.5221, R:0.0105)
Batch 325/537: Loss=0.5473 (C:0.5473, R:0.0105)
Batch 350/537: Loss=0.5263 (C:0.5263, R:0.0105)
Batch 375/537: Loss=0.5038 (C:0.5038, R:0.0105)
Batch 400/537: Loss=0.5216 (C:0.5216, R:0.0105)
Batch 425/537: Loss=0.5376 (C:0.5376, R:0.0105)
Batch 450/537: Loss=0.5357 (C:0.5357, R:0.0106)
Batch 475/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 500/537: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 525/537: Loss=0.5436 (C:0.5436, R:0.0105)

============================================================
Epoch 78/300 completed in 21.3s
Train: Loss=0.5325 (C:0.5325, R:0.0105) Ratio=5.47x
Val:   Loss=0.7092 (C:0.7092, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7092)
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.292 ± 0.558
    Neg distances: 2.654 ± 1.094
    Separation ratio: 9.09x
    Gap: -4.483
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=0.5213 (C:0.5213, R:0.0105)
Batch  25/537: Loss=0.5323 (C:0.5323, R:0.0105)
Batch  50/537: Loss=0.5246 (C:0.5246, R:0.0105)
Batch  75/537: Loss=0.5552 (C:0.5552, R:0.0105)
Batch 100/537: Loss=0.5136 (C:0.5136, R:0.0105)
Batch 125/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 150/537: Loss=0.5080 (C:0.5080, R:0.0105)
Batch 175/537: Loss=0.5340 (C:0.5340, R:0.0105)
Batch 200/537: Loss=0.5257 (C:0.5257, R:0.0105)
Batch 225/537: Loss=0.5136 (C:0.5136, R:0.0105)
Batch 250/537: Loss=0.5402 (C:0.5402, R:0.0105)
Batch 275/537: Loss=0.5573 (C:0.5573, R:0.0105)
Batch 300/537: Loss=0.5171 (C:0.5171, R:0.0105)
Batch 325/537: Loss=0.5338 (C:0.5338, R:0.0105)
Batch 350/537: Loss=0.5256 (C:0.5256, R:0.0105)
Batch 375/537: Loss=0.5012 (C:0.5012, R:0.0105)
Batch 400/537: Loss=0.5208 (C:0.5208, R:0.0105)
Batch 425/537: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 450/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 475/537: Loss=0.5289 (C:0.5289, R:0.0105)
Batch 500/537: Loss=0.5316 (C:0.5316, R:0.0105)
Batch 525/537: Loss=0.4904 (C:0.4904, R:0.0105)

============================================================
Epoch 79/300 completed in 26.2s
Train: Loss=0.5235 (C:0.5235, R:0.0105) Ratio=5.57x
Val:   Loss=0.7116 (C:0.7116, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=0.5162 (C:0.5162, R:0.0105)
Batch  25/537: Loss=0.5154 (C:0.5154, R:0.0105)
Batch  50/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch  75/537: Loss=0.5185 (C:0.5185, R:0.0105)
Batch 100/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 125/537: Loss=0.5064 (C:0.5064, R:0.0105)
Batch 150/537: Loss=0.4993 (C:0.4993, R:0.0105)
Batch 175/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 200/537: Loss=0.5384 (C:0.5384, R:0.0105)
Batch 225/537: Loss=0.5192 (C:0.5192, R:0.0105)
Batch 250/537: Loss=0.5078 (C:0.5078, R:0.0105)
Batch 275/537: Loss=0.5022 (C:0.5022, R:0.0105)
Batch 300/537: Loss=0.5133 (C:0.5133, R:0.0105)
Batch 325/537: Loss=0.5414 (C:0.5414, R:0.0105)
Batch 350/537: Loss=0.5319 (C:0.5319, R:0.0105)
Batch 375/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 400/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 425/537: Loss=0.5176 (C:0.5176, R:0.0105)
Batch 450/537: Loss=0.5326 (C:0.5326, R:0.0105)
Batch 475/537: Loss=0.5235 (C:0.5235, R:0.0105)
Batch 500/537: Loss=0.5405 (C:0.5405, R:0.0105)
Batch 525/537: Loss=0.5318 (C:0.5318, R:0.0105)

============================================================
Epoch 80/300 completed in 21.5s
Train: Loss=0.5223 (C:0.5223, R:0.0105) Ratio=5.53x
Val:   Loss=0.7047 (C:0.7047, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7047)
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=0.5314 (C:0.5314, R:0.0105)
Batch  25/537: Loss=0.5299 (C:0.5299, R:0.0105)
Batch  50/537: Loss=0.5178 (C:0.5178, R:0.0105)
Batch  75/537: Loss=0.5122 (C:0.5122, R:0.0105)
Batch 100/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch 125/537: Loss=0.5044 (C:0.5044, R:0.0105)
Batch 150/537: Loss=0.5411 (C:0.5411, R:0.0106)
Batch 175/537: Loss=0.5269 (C:0.5269, R:0.0104)
Batch 200/537: Loss=0.5121 (C:0.5121, R:0.0105)
Batch 225/537: Loss=0.5342 (C:0.5342, R:0.0105)
Batch 250/537: Loss=0.5107 (C:0.5107, R:0.0105)
Batch 275/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 300/537: Loss=0.4866 (C:0.4866, R:0.0105)
Batch 325/537: Loss=0.5480 (C:0.5480, R:0.0105)
Batch 350/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 375/537: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 400/537: Loss=0.5166 (C:0.5166, R:0.0105)
Batch 425/537: Loss=0.5150 (C:0.5150, R:0.0105)
Batch 450/537: Loss=0.5328 (C:0.5328, R:0.0105)
Batch 475/537: Loss=0.5319 (C:0.5319, R:0.0105)
Batch 500/537: Loss=0.5031 (C:0.5031, R:0.0105)
Batch 525/537: Loss=0.5626 (C:0.5626, R:0.0105)

============================================================
Epoch 81/300 completed in 20.8s
Train: Loss=0.5217 (C:0.5217, R:0.0105) Ratio=5.49x
Val:   Loss=0.6998 (C:0.6998, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6998)
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.300 ± 0.574
    Neg distances: 2.683 ± 1.113
    Separation ratio: 8.94x
    Gap: -4.430
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=0.5345 (C:0.5345, R:0.0105)
Batch  25/537: Loss=0.5412 (C:0.5412, R:0.0106)
Batch  50/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch  75/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 100/537: Loss=0.5457 (C:0.5457, R:0.0105)
Batch 125/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 150/537: Loss=0.4989 (C:0.4989, R:0.0105)
Batch 175/537: Loss=0.5126 (C:0.5126, R:0.0105)
Batch 200/537: Loss=0.5585 (C:0.5585, R:0.0105)
Batch 225/537: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 250/537: Loss=0.5585 (C:0.5585, R:0.0105)
Batch 275/537: Loss=0.5196 (C:0.5196, R:0.0105)
Batch 300/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 325/537: Loss=0.5501 (C:0.5501, R:0.0105)
Batch 350/537: Loss=0.5258 (C:0.5258, R:0.0105)
Batch 375/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 400/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 425/537: Loss=0.5318 (C:0.5318, R:0.0105)
Batch 450/537: Loss=0.5528 (C:0.5528, R:0.0105)
Batch 475/537: Loss=0.5568 (C:0.5568, R:0.0105)
Batch 500/537: Loss=0.5654 (C:0.5654, R:0.0105)
Batch 525/537: Loss=0.5554 (C:0.5554, R:0.0105)

============================================================
Epoch 82/300 completed in 27.3s
Train: Loss=0.5281 (C:0.5281, R:0.0105) Ratio=5.39x
Val:   Loss=0.7106 (C:0.7106, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=0.4860 (C:0.4860, R:0.0105)
Batch  25/537: Loss=0.5190 (C:0.5190, R:0.0105)
Batch  50/537: Loss=0.5168 (C:0.5168, R:0.0105)
Batch  75/537: Loss=0.5130 (C:0.5130, R:0.0105)
Batch 100/537: Loss=0.5193 (C:0.5193, R:0.0105)
Batch 125/537: Loss=0.4860 (C:0.4860, R:0.0105)
Batch 150/537: Loss=0.5495 (C:0.5495, R:0.0105)
Batch 175/537: Loss=0.5454 (C:0.5454, R:0.0105)
Batch 200/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 225/537: Loss=0.5117 (C:0.5117, R:0.0105)
Batch 250/537: Loss=0.5464 (C:0.5464, R:0.0105)
Batch 275/537: Loss=0.5296 (C:0.5296, R:0.0105)
Batch 300/537: Loss=0.5185 (C:0.5185, R:0.0105)
Batch 325/537: Loss=0.5331 (C:0.5331, R:0.0105)
Batch 350/537: Loss=0.5186 (C:0.5186, R:0.0106)
Batch 375/537: Loss=0.5000 (C:0.5000, R:0.0105)
Batch 400/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 425/537: Loss=0.4942 (C:0.4942, R:0.0105)
Batch 450/537: Loss=0.5185 (C:0.5185, R:0.0105)
Batch 475/537: Loss=0.5350 (C:0.5350, R:0.0105)
Batch 500/537: Loss=0.5433 (C:0.5433, R:0.0105)
Batch 525/537: Loss=0.5404 (C:0.5404, R:0.0105)

============================================================
Epoch 83/300 completed in 21.8s
Train: Loss=0.5271 (C:0.5271, R:0.0105) Ratio=5.57x
Val:   Loss=0.7234 (C:0.7234, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch  25/537: Loss=0.5050 (C:0.5050, R:0.0105)
Batch  50/537: Loss=0.5535 (C:0.5535, R:0.0105)
Batch  75/537: Loss=0.5555 (C:0.5555, R:0.0105)
Batch 100/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 125/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 150/537: Loss=0.4954 (C:0.4954, R:0.0105)
Batch 175/537: Loss=0.5103 (C:0.5103, R:0.0105)
Batch 200/537: Loss=0.5567 (C:0.5567, R:0.0105)
Batch 225/537: Loss=0.5205 (C:0.5205, R:0.0105)
Batch 250/537: Loss=0.5210 (C:0.5210, R:0.0105)
Batch 275/537: Loss=0.4946 (C:0.4946, R:0.0105)
Batch 300/537: Loss=0.5547 (C:0.5547, R:0.0105)
Batch 325/537: Loss=0.5500 (C:0.5500, R:0.0105)
Batch 350/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 375/537: Loss=0.4953 (C:0.4953, R:0.0105)
Batch 400/537: Loss=0.5249 (C:0.5249, R:0.0105)
Batch 425/537: Loss=0.4943 (C:0.4943, R:0.0105)
Batch 450/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 475/537: Loss=0.5299 (C:0.5299, R:0.0105)
Batch 500/537: Loss=0.5465 (C:0.5465, R:0.0105)
Batch 525/537: Loss=0.5156 (C:0.5156, R:0.0105)

============================================================
Epoch 84/300 completed in 21.7s
Train: Loss=0.5264 (C:0.5264, R:0.0105) Ratio=5.60x
Val:   Loss=0.7124 (C:0.7124, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 85
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.302 ± 0.570
    Neg distances: 2.683 ± 1.112
    Separation ratio: 8.90x
    Gap: -4.509
    ✅ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=0.5285 (C:0.5285, R:0.0105)
Batch  25/537: Loss=0.5087 (C:0.5087, R:0.0105)
Batch  50/537: Loss=0.5327 (C:0.5327, R:0.0105)
Batch  75/537: Loss=0.5264 (C:0.5264, R:0.0105)
Batch 100/537: Loss=0.5118 (C:0.5118, R:0.0105)
Batch 125/537: Loss=0.4908 (C:0.4908, R:0.0105)
Batch 150/537: Loss=0.5156 (C:0.5156, R:0.0105)
Batch 175/537: Loss=0.5340 (C:0.5340, R:0.0105)
Batch 200/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 225/537: Loss=0.5238 (C:0.5238, R:0.0105)
Batch 250/537: Loss=0.5524 (C:0.5524, R:0.0105)
Batch 275/537: Loss=0.5034 (C:0.5034, R:0.0105)
Batch 300/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 325/537: Loss=0.5299 (C:0.5299, R:0.0105)
Batch 350/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 375/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch 400/537: Loss=0.5317 (C:0.5317, R:0.0105)
Batch 425/537: Loss=0.5178 (C:0.5178, R:0.0105)
Batch 450/537: Loss=0.5327 (C:0.5327, R:0.0105)
Batch 475/537: Loss=0.5461 (C:0.5461, R:0.0105)
Batch 500/537: Loss=0.4953 (C:0.4953, R:0.0106)
Batch 525/537: Loss=0.5221 (C:0.5221, R:0.0105)

============================================================
Epoch 85/300 completed in 27.9s
Train: Loss=0.5244 (C:0.5244, R:0.0105) Ratio=5.56x
Val:   Loss=0.7130 (C:0.7130, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=0.5487 (C:0.5487, R:0.0105)
Batch  25/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch  50/537: Loss=0.5062 (C:0.5062, R:0.0105)
Batch  75/537: Loss=0.5217 (C:0.5217, R:0.0105)
Batch 100/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 125/537: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 150/537: Loss=0.5503 (C:0.5503, R:0.0105)
Batch 175/537: Loss=0.5215 (C:0.5215, R:0.0105)
Batch 200/537: Loss=0.5137 (C:0.5137, R:0.0105)
Batch 225/537: Loss=0.5075 (C:0.5075, R:0.0105)
Batch 250/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 275/537: Loss=0.5424 (C:0.5424, R:0.0105)
Batch 300/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 325/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch 350/537: Loss=0.5354 (C:0.5354, R:0.0105)
Batch 375/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 400/537: Loss=0.5578 (C:0.5578, R:0.0105)
Batch 425/537: Loss=0.5315 (C:0.5315, R:0.0105)
Batch 450/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 475/537: Loss=0.5064 (C:0.5064, R:0.0105)
Batch 500/537: Loss=0.4978 (C:0.4978, R:0.0105)
Batch 525/537: Loss=0.5058 (C:0.5058, R:0.0105)

============================================================
Epoch 86/300 completed in 21.6s
Train: Loss=0.5222 (C:0.5222, R:0.0105) Ratio=5.66x
Val:   Loss=0.7044 (C:0.7044, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch  25/537: Loss=0.4841 (C:0.4841, R:0.0105)
Batch  50/537: Loss=0.5179 (C:0.5179, R:0.0105)
Batch  75/537: Loss=0.5149 (C:0.5149, R:0.0105)
Batch 100/537: Loss=0.4869 (C:0.4869, R:0.0105)
Batch 125/537: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 150/537: Loss=0.5139 (C:0.5139, R:0.0105)
Batch 175/537: Loss=0.5229 (C:0.5229, R:0.0105)
Batch 200/537: Loss=0.5311 (C:0.5311, R:0.0105)
Batch 225/537: Loss=0.5262 (C:0.5262, R:0.0105)
Batch 250/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 275/537: Loss=0.5162 (C:0.5162, R:0.0105)
Batch 300/537: Loss=0.5314 (C:0.5314, R:0.0105)
Batch 325/537: Loss=0.5444 (C:0.5444, R:0.0105)
Batch 350/537: Loss=0.5316 (C:0.5316, R:0.0105)
Batch 375/537: Loss=0.5348 (C:0.5348, R:0.0105)
Batch 400/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 425/537: Loss=0.5067 (C:0.5067, R:0.0105)
Batch 450/537: Loss=0.5048 (C:0.5048, R:0.0105)
Batch 475/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 500/537: Loss=0.5550 (C:0.5550, R:0.0105)
Batch 525/537: Loss=0.5409 (C:0.5409, R:0.0105)

============================================================
Epoch 87/300 completed in 21.6s
Train: Loss=0.5213 (C:0.5213, R:0.0105) Ratio=5.62x
Val:   Loss=0.7094 (C:0.7094, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 88
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.292 ± 0.585
    Neg distances: 2.716 ± 1.120
    Separation ratio: 9.30x
    Gap: -4.631
    ✅ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=0.4950 (C:0.4950, R:0.0105)
Batch  25/537: Loss=0.5304 (C:0.5304, R:0.0105)
Batch  50/537: Loss=0.5089 (C:0.5089, R:0.0105)
Batch  75/537: Loss=0.5064 (C:0.5064, R:0.0105)
Batch 100/537: Loss=0.4955 (C:0.4955, R:0.0105)
Batch 125/537: Loss=0.4961 (C:0.4961, R:0.0105)
Batch 150/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 175/537: Loss=0.5183 (C:0.5183, R:0.0105)
Batch 200/537: Loss=0.5099 (C:0.5099, R:0.0105)
Batch 225/537: Loss=0.4965 (C:0.4965, R:0.0105)
Batch 250/537: Loss=0.5193 (C:0.5193, R:0.0105)
Batch 275/537: Loss=0.5062 (C:0.5062, R:0.0105)
Batch 300/537: Loss=0.5254 (C:0.5254, R:0.0105)
Batch 325/537: Loss=0.4962 (C:0.4962, R:0.0105)
Batch 350/537: Loss=0.4945 (C:0.4945, R:0.0105)
Batch 375/537: Loss=0.5218 (C:0.5218, R:0.0105)
Batch 400/537: Loss=0.5147 (C:0.5147, R:0.0106)
Batch 425/537: Loss=0.5128 (C:0.5128, R:0.0106)
Batch 450/537: Loss=0.5125 (C:0.5125, R:0.0105)
Batch 475/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 500/537: Loss=0.5169 (C:0.5169, R:0.0105)
Batch 525/537: Loss=0.5065 (C:0.5065, R:0.0105)

============================================================
Epoch 88/300 completed in 28.2s
Train: Loss=0.5117 (C:0.5117, R:0.0105) Ratio=5.67x
Val:   Loss=0.7092 (C:0.7092, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=0.5083 (C:0.5083, R:0.0105)
Batch  25/537: Loss=0.5128 (C:0.5128, R:0.0105)
Batch  50/537: Loss=0.5013 (C:0.5013, R:0.0105)
Batch  75/537: Loss=0.5281 (C:0.5281, R:0.0105)
Batch 100/537: Loss=0.4881 (C:0.4881, R:0.0105)
Batch 125/537: Loss=0.5214 (C:0.5214, R:0.0105)
Batch 150/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch 175/537: Loss=0.5422 (C:0.5422, R:0.0105)
Batch 200/537: Loss=0.4769 (C:0.4769, R:0.0105)
Batch 225/537: Loss=0.5247 (C:0.5247, R:0.0105)
Batch 250/537: Loss=0.4924 (C:0.4924, R:0.0105)
Batch 275/537: Loss=0.4932 (C:0.4932, R:0.0105)
Batch 300/537: Loss=0.5359 (C:0.5359, R:0.0105)
Batch 325/537: Loss=0.4782 (C:0.4782, R:0.0105)
Batch 350/537: Loss=0.4948 (C:0.4948, R:0.0105)
Batch 375/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch 400/537: Loss=0.4928 (C:0.4928, R:0.0105)
Batch 425/537: Loss=0.5068 (C:0.5068, R:0.0105)
Batch 450/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 475/537: Loss=0.5185 (C:0.5185, R:0.0105)
Batch 500/537: Loss=0.4893 (C:0.4893, R:0.0105)
Batch 525/537: Loss=0.5539 (C:0.5539, R:0.0105)

============================================================
Epoch 89/300 completed in 21.3s
Train: Loss=0.5099 (C:0.5099, R:0.0105) Ratio=5.64x
Val:   Loss=0.7064 (C:0.7064, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 89 epochs
Best model was at epoch 81 with Val Loss: 0.6998

Global Dataset Training Completed!
Best epoch: 81
Best validation loss: 0.6998
Final separation ratios: Train=5.64x, Val=3.11x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 50])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4588
  Adjusted Rand Score: 0.5291
  Clustering Accuracy: 0.8123
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 50])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 50])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8164
  Per-class F1: [0.8386336866902236, 0.7518749002712621, 0.8608130081300814]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.771 ± 0.940
  Negative distances: 2.371 ± 1.261
  Separation ratio: 3.07x
  Gap: -4.399
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4588
  Clustering Accuracy: 0.8123
  Adjusted Rand Score: 0.5291

Classification Performance:
  Accuracy: 0.8164

Separation Quality:
  Separation Ratio: 3.07x
  Gap: -4.399
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313/results/evaluation_results_20250714_164909.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313/results/evaluation_results_20250714_164909.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1020_20250714_161313/final_results.json

Key Results:
  Separation ratio: 3.07x
  Perfect separation: False
  Classification accuracy: 0.8164
  NEW BEST: 0.8164% (improvement: +-80.85%)
  Saved best experiment: coarse_lr1e-04_lat50_bs1020_20250714_161313

[2/12] Testing: coarse_lr1e-04_lat50_bs1536
  Learning rate: 0.0001
  Latent dim: 50
  Batch size: 1536
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 16:49:09.497220
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1536
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 356
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 6
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1536
  Balanced sampling: True
  Train batches: 356
  Val batches: 6
  Test batches: 7
Data loading completed!
  Train: 549367 samples, 356 batches
  Val: 9842 samples, 6 batches
  Test: 9824 samples, 7 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,863,730
Model created with 1,863,730 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,863,730
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.073 ± 0.009
    Neg distances: 0.073 ± 0.009
    Separation ratio: 1.00x
    Gap: -0.103
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/356: Loss=1.9998 (C:1.9998, R:0.0117)
Batch  25/356: Loss=1.9932 (C:1.9932, R:0.0115)
Batch  50/356: Loss=1.9672 (C:1.9672, R:0.0114)
Batch  75/356: Loss=1.9568 (C:1.9568, R:0.0112)
Batch 100/356: Loss=1.9356 (C:1.9356, R:0.0110)
Batch 125/356: Loss=1.9262 (C:1.9262, R:0.0110)
Batch 150/356: Loss=1.9091 (C:1.9091, R:0.0109)
Batch 175/356: Loss=1.8900 (C:1.8900, R:0.0108)
Batch 200/356: Loss=1.8940 (C:1.8940, R:0.0107)
Batch 225/356: Loss=1.8751 (C:1.8751, R:0.0107)
Batch 250/356: Loss=1.8675 (C:1.8675, R:0.0106)
Batch 275/356: Loss=1.8575 (C:1.8575, R:0.0107)
Batch 300/356: Loss=1.8666 (C:1.8666, R:0.0106)
Batch 325/356: Loss=1.8427 (C:1.8427, R:0.0106)
Batch 350/356: Loss=1.8483 (C:1.8483, R:0.0106)

============================================================
Epoch 1/300 completed in 25.9s
Train: Loss=1.9089 (C:1.9089, R:0.0109) Ratio=1.55x
Val:   Loss=1.8408 (C:1.8408, R:0.0105) Ratio=2.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8408)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/356: Loss=1.8658 (C:1.8658, R:0.0106)
Batch  25/356: Loss=1.8476 (C:1.8476, R:0.0106)
Batch  50/356: Loss=1.8563 (C:1.8563, R:0.0105)
Batch  75/356: Loss=1.8543 (C:1.8543, R:0.0106)
Batch 100/356: Loss=1.8368 (C:1.8368, R:0.0105)
Batch 125/356: Loss=1.8198 (C:1.8198, R:0.0106)
Batch 150/356: Loss=1.8588 (C:1.8588, R:0.0105)
Batch 175/356: Loss=1.8375 (C:1.8375, R:0.0105)
Batch 200/356: Loss=1.8326 (C:1.8326, R:0.0105)
Batch 225/356: Loss=1.8339 (C:1.8339, R:0.0105)
Batch 250/356: Loss=1.8063 (C:1.8063, R:0.0105)
Batch 275/356: Loss=1.8285 (C:1.8285, R:0.0106)
Batch 300/356: Loss=1.8405 (C:1.8405, R:0.0105)
Batch 325/356: Loss=1.8349 (C:1.8349, R:0.0105)
Batch 350/356: Loss=1.8206 (C:1.8206, R:0.0105)

============================================================
Epoch 2/300 completed in 20.2s
Train: Loss=1.8362 (C:1.8362, R:0.0105) Ratio=2.05x
Val:   Loss=1.8168 (C:1.8168, R:0.0104) Ratio=2.30x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8168)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/356: Loss=1.8074 (C:1.8074, R:0.0105)
Batch  25/356: Loss=1.8134 (C:1.8134, R:0.0105)
Batch  50/356: Loss=1.8203 (C:1.8203, R:0.0105)
Batch  75/356: Loss=1.8179 (C:1.8179, R:0.0105)
Batch 100/356: Loss=1.8300 (C:1.8300, R:0.0105)
Batch 125/356: Loss=1.8204 (C:1.8204, R:0.0105)
Batch 150/356: Loss=1.8174 (C:1.8174, R:0.0105)
Batch 175/356: Loss=1.8388 (C:1.8388, R:0.0105)
Batch 200/356: Loss=1.8094 (C:1.8094, R:0.0105)
Batch 225/356: Loss=1.8186 (C:1.8186, R:0.0105)
Batch 250/356: Loss=1.8202 (C:1.8202, R:0.0105)
Batch 275/356: Loss=1.8217 (C:1.8217, R:0.0105)
Batch 300/356: Loss=1.8150 (C:1.8150, R:0.0105)
Batch 325/356: Loss=1.8017 (C:1.8017, R:0.0105)
Batch 350/356: Loss=1.8140 (C:1.8140, R:0.0105)

============================================================
Epoch 3/300 completed in 20.1s
Train: Loss=1.8157 (C:1.8157, R:0.0105) Ratio=2.27x
Val:   Loss=1.8045 (C:1.8045, R:0.0104) Ratio=2.40x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8045)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.573 ± 0.552
    Neg distances: 1.533 ± 0.838
    Separation ratio: 2.67x
    Gap: -3.414
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/356: Loss=1.2170 (C:1.2170, R:0.0105)
Batch  25/356: Loss=1.2313 (C:1.2313, R:0.0105)
Batch  50/356: Loss=1.2275 (C:1.2275, R:0.0105)
Batch  75/356: Loss=1.2544 (C:1.2544, R:0.0105)
Batch 100/356: Loss=1.2839 (C:1.2839, R:0.0105)
Batch 125/356: Loss=1.2491 (C:1.2491, R:0.0105)
Batch 150/356: Loss=1.2407 (C:1.2407, R:0.0105)
Batch 175/356: Loss=1.2618 (C:1.2618, R:0.0105)
Batch 200/356: Loss=1.2201 (C:1.2201, R:0.0105)
Batch 225/356: Loss=1.2106 (C:1.2106, R:0.0105)
Batch 250/356: Loss=1.2219 (C:1.2219, R:0.0105)
Batch 275/356: Loss=1.2285 (C:1.2285, R:0.0105)
Batch 300/356: Loss=1.2236 (C:1.2236, R:0.0105)
Batch 325/356: Loss=1.2256 (C:1.2256, R:0.0105)
Batch 350/356: Loss=1.2361 (C:1.2361, R:0.0105)

============================================================
Epoch 4/300 completed in 25.8s
Train: Loss=1.2397 (C:1.2397, R:0.0105) Ratio=2.46x
Val:   Loss=1.2314 (C:1.2314, R:0.0104) Ratio=2.52x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2314)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/356: Loss=1.2193 (C:1.2193, R:0.0105)
Batch  25/356: Loss=1.2251 (C:1.2251, R:0.0105)
Batch  50/356: Loss=1.1960 (C:1.1960, R:0.0105)
Batch  75/356: Loss=1.1821 (C:1.1821, R:0.0105)
Batch 100/356: Loss=1.2177 (C:1.2177, R:0.0105)
Batch 125/356: Loss=1.2232 (C:1.2232, R:0.0105)
Batch 150/356: Loss=1.2055 (C:1.2055, R:0.0105)
Batch 175/356: Loss=1.2092 (C:1.2092, R:0.0105)
Batch 200/356: Loss=1.1996 (C:1.1996, R:0.0105)
Batch 225/356: Loss=1.2215 (C:1.2215, R:0.0105)
Batch 250/356: Loss=1.2301 (C:1.2301, R:0.0105)
Batch 275/356: Loss=1.1925 (C:1.1925, R:0.0105)
Batch 300/356: Loss=1.2128 (C:1.2128, R:0.0105)
Batch 325/356: Loss=1.2056 (C:1.2056, R:0.0105)
Batch 350/356: Loss=1.2052 (C:1.2052, R:0.0105)

============================================================
Epoch 5/300 completed in 20.1s
Train: Loss=1.2134 (C:1.2134, R:0.0105) Ratio=2.62x
Val:   Loss=1.2042 (C:1.2042, R:0.0104) Ratio=2.63x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2042)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/356: Loss=1.2101 (C:1.2101, R:0.0105)
Batch  25/356: Loss=1.1742 (C:1.1742, R:0.0105)
Batch  50/356: Loss=1.1977 (C:1.1977, R:0.0105)
Batch  75/356: Loss=1.1994 (C:1.1994, R:0.0105)
Batch 100/356: Loss=1.1937 (C:1.1937, R:0.0105)
Batch 125/356: Loss=1.1896 (C:1.1896, R:0.0105)
Batch 150/356: Loss=1.1909 (C:1.1909, R:0.0105)
Batch 175/356: Loss=1.2185 (C:1.2185, R:0.0105)
Batch 200/356: Loss=1.1862 (C:1.1862, R:0.0105)
Batch 225/356: Loss=1.1976 (C:1.1976, R:0.0105)
Batch 250/356: Loss=1.2060 (C:1.2060, R:0.0106)
Batch 275/356: Loss=1.2015 (C:1.2015, R:0.0105)
Batch 300/356: Loss=1.1959 (C:1.1959, R:0.0105)
Batch 325/356: Loss=1.1955 (C:1.1955, R:0.0105)
Batch 350/356: Loss=1.1820 (C:1.1820, R:0.0105)

============================================================
Epoch 6/300 completed in 20.4s
Train: Loss=1.1973 (C:1.1973, R:0.0105) Ratio=2.73x
Val:   Loss=1.2103 (C:1.2103, R:0.0104) Ratio=2.68x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.522 ± 0.554
    Neg distances: 1.625 ± 0.857
    Separation ratio: 3.12x
    Gap: -3.408
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/356: Loss=1.1285 (C:1.1285, R:0.0105)
Batch  25/356: Loss=1.1178 (C:1.1178, R:0.0105)
Batch  50/356: Loss=1.1206 (C:1.1206, R:0.0105)
Batch  75/356: Loss=1.1054 (C:1.1054, R:0.0105)
Batch 100/356: Loss=1.1355 (C:1.1355, R:0.0105)
Batch 125/356: Loss=1.1132 (C:1.1132, R:0.0105)
Batch 150/356: Loss=1.1505 (C:1.1505, R:0.0105)
Batch 175/356: Loss=1.1165 (C:1.1165, R:0.0105)
Batch 200/356: Loss=1.1351 (C:1.1351, R:0.0105)
Batch 225/356: Loss=1.1477 (C:1.1477, R:0.0105)
Batch 250/356: Loss=1.1284 (C:1.1284, R:0.0105)
Batch 275/356: Loss=1.1299 (C:1.1299, R:0.0105)
Batch 300/356: Loss=1.1282 (C:1.1282, R:0.0105)
Batch 325/356: Loss=1.1092 (C:1.1092, R:0.0105)
Batch 350/356: Loss=1.1415 (C:1.1415, R:0.0106)

============================================================
Epoch 7/300 completed in 27.4s
Train: Loss=1.1330 (C:1.1330, R:0.0105) Ratio=2.87x
Val:   Loss=1.1494 (C:1.1494, R:0.0104) Ratio=2.75x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1494)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/356: Loss=1.0866 (C:1.0866, R:0.0105)
Batch  25/356: Loss=1.1341 (C:1.1341, R:0.0105)
Batch  50/356: Loss=1.0961 (C:1.0961, R:0.0105)
Batch  75/356: Loss=1.1082 (C:1.1082, R:0.0105)
Batch 100/356: Loss=1.1116 (C:1.1116, R:0.0105)
Batch 125/356: Loss=1.1533 (C:1.1533, R:0.0105)
Batch 150/356: Loss=1.1364 (C:1.1364, R:0.0105)
Batch 175/356: Loss=1.1315 (C:1.1315, R:0.0105)
Batch 200/356: Loss=1.1294 (C:1.1294, R:0.0105)
Batch 225/356: Loss=1.1341 (C:1.1341, R:0.0105)
Batch 250/356: Loss=1.1244 (C:1.1244, R:0.0105)
Batch 275/356: Loss=1.1047 (C:1.1047, R:0.0105)
Batch 300/356: Loss=1.1219 (C:1.1219, R:0.0105)
Batch 325/356: Loss=1.1286 (C:1.1286, R:0.0105)
Batch 350/356: Loss=1.1216 (C:1.1216, R:0.0105)

============================================================
Epoch 8/300 completed in 21.3s
Train: Loss=1.1215 (C:1.1215, R:0.0105) Ratio=2.94x
Val:   Loss=1.1461 (C:1.1461, R:0.0104) Ratio=2.80x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1461)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/356: Loss=1.1051 (C:1.1051, R:0.0105)
Batch  25/356: Loss=1.0787 (C:1.0787, R:0.0105)
Batch  50/356: Loss=1.1019 (C:1.1019, R:0.0105)
Batch  75/356: Loss=1.0986 (C:1.0986, R:0.0105)
Batch 100/356: Loss=1.1182 (C:1.1182, R:0.0105)
Batch 125/356: Loss=1.0864 (C:1.0864, R:0.0105)
Batch 150/356: Loss=1.1171 (C:1.1171, R:0.0105)
Batch 175/356: Loss=1.1056 (C:1.1056, R:0.0105)
Batch 200/356: Loss=1.1172 (C:1.1172, R:0.0105)
Batch 225/356: Loss=1.1323 (C:1.1323, R:0.0105)
Batch 250/356: Loss=1.0939 (C:1.0939, R:0.0105)
Batch 275/356: Loss=1.0821 (C:1.0821, R:0.0105)
Batch 300/356: Loss=1.0967 (C:1.0967, R:0.0105)
Batch 325/356: Loss=1.0987 (C:1.0987, R:0.0105)
Batch 350/356: Loss=1.1136 (C:1.1136, R:0.0105)

============================================================
Epoch 9/300 completed in 21.1s
Train: Loss=1.1125 (C:1.1125, R:0.0105) Ratio=3.09x
Val:   Loss=1.1456 (C:1.1456, R:0.0104) Ratio=2.80x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1456)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.473 ± 0.544
    Neg distances: 1.739 ± 0.877
    Separation ratio: 3.67x
    Gap: -3.337
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/356: Loss=1.0554 (C:1.0554, R:0.0105)
Batch  25/356: Loss=1.0465 (C:1.0465, R:0.0106)
Batch  50/356: Loss=1.0660 (C:1.0660, R:0.0105)
Batch  75/356: Loss=1.0784 (C:1.0784, R:0.0105)
Batch 100/356: Loss=1.0372 (C:1.0372, R:0.0105)
Batch 125/356: Loss=1.0363 (C:1.0363, R:0.0106)
Batch 150/356: Loss=1.0291 (C:1.0291, R:0.0105)
Batch 175/356: Loss=1.0595 (C:1.0595, R:0.0105)
Batch 200/356: Loss=1.0510 (C:1.0510, R:0.0105)
Batch 225/356: Loss=1.0333 (C:1.0333, R:0.0105)
Batch 250/356: Loss=1.0479 (C:1.0479, R:0.0105)
Batch 275/356: Loss=1.0660 (C:1.0660, R:0.0105)
Batch 300/356: Loss=1.0486 (C:1.0486, R:0.0105)
Batch 325/356: Loss=1.0511 (C:1.0511, R:0.0105)
Batch 350/356: Loss=1.0248 (C:1.0248, R:0.0105)

============================================================
Epoch 10/300 completed in 26.8s
Train: Loss=1.0494 (C:1.0494, R:0.0105) Ratio=3.11x
Val:   Loss=1.0833 (C:1.0833, R:0.0104) Ratio=2.82x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0833)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/356: Loss=1.0287 (C:1.0287, R:0.0105)
Batch  25/356: Loss=1.0473 (C:1.0473, R:0.0105)
Batch  50/356: Loss=1.0220 (C:1.0220, R:0.0105)
Batch  75/356: Loss=1.0711 (C:1.0711, R:0.0105)
Batch 100/356: Loss=1.0436 (C:1.0436, R:0.0105)
Batch 125/356: Loss=1.0338 (C:1.0338, R:0.0105)
Batch 150/356: Loss=1.0480 (C:1.0480, R:0.0105)
Batch 175/356: Loss=1.0462 (C:1.0462, R:0.0105)
Batch 200/356: Loss=1.0041 (C:1.0041, R:0.0105)
Batch 225/356: Loss=1.0067 (C:1.0067, R:0.0105)
Batch 250/356: Loss=1.0394 (C:1.0394, R:0.0105)
Batch 275/356: Loss=1.0150 (C:1.0150, R:0.0105)
Batch 300/356: Loss=1.0236 (C:1.0236, R:0.0105)
Batch 325/356: Loss=1.0398 (C:1.0398, R:0.0105)
Batch 350/356: Loss=1.0449 (C:1.0449, R:0.0105)

============================================================
Epoch 11/300 completed in 20.7s
Train: Loss=1.0400 (C:1.0400, R:0.0105) Ratio=3.18x
Val:   Loss=1.0818 (C:1.0818, R:0.0104) Ratio=2.83x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0818)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/356: Loss=1.0183 (C:1.0183, R:0.0105)
Batch  25/356: Loss=1.0475 (C:1.0475, R:0.0105)
Batch  50/356: Loss=1.0405 (C:1.0405, R:0.0105)
Batch  75/356: Loss=1.0553 (C:1.0553, R:0.0105)
Batch 100/356: Loss=1.0523 (C:1.0523, R:0.0105)
Batch 125/356: Loss=1.0396 (C:1.0396, R:0.0105)
Batch 150/356: Loss=1.0478 (C:1.0478, R:0.0105)
Batch 175/356: Loss=1.0480 (C:1.0480, R:0.0105)
Batch 200/356: Loss=1.0264 (C:1.0264, R:0.0105)
Batch 225/356: Loss=1.0327 (C:1.0327, R:0.0105)
Batch 250/356: Loss=1.0508 (C:1.0508, R:0.0105)
Batch 275/356: Loss=1.0158 (C:1.0158, R:0.0105)
Batch 300/356: Loss=1.0244 (C:1.0244, R:0.0105)
Batch 325/356: Loss=1.0353 (C:1.0353, R:0.0105)
Batch 350/356: Loss=1.0262 (C:1.0262, R:0.0105)

============================================================
Epoch 12/300 completed in 21.7s
Train: Loss=1.0327 (C:1.0327, R:0.0105) Ratio=3.24x
Val:   Loss=1.0793 (C:1.0793, R:0.0104) Ratio=2.87x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0793)
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.465 ± 0.553
    Neg distances: 1.802 ± 0.887
    Separation ratio: 3.88x
    Gap: -3.353
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/356: Loss=0.9817 (C:0.9817, R:0.0105)
Batch  25/356: Loss=1.0203 (C:1.0203, R:0.0105)
Batch  50/356: Loss=1.0106 (C:1.0106, R:0.0105)
Batch  75/356: Loss=1.0017 (C:1.0017, R:0.0105)
Batch 100/356: Loss=1.0289 (C:1.0289, R:0.0105)
Batch 125/356: Loss=0.9969 (C:0.9969, R:0.0105)
Batch 150/356: Loss=1.0312 (C:1.0312, R:0.0105)
Batch 175/356: Loss=1.0163 (C:1.0163, R:0.0105)
Batch 200/356: Loss=1.0008 (C:1.0008, R:0.0105)
Batch 225/356: Loss=0.9856 (C:0.9856, R:0.0105)
Batch 250/356: Loss=0.9735 (C:0.9735, R:0.0105)
Batch 275/356: Loss=1.0006 (C:1.0006, R:0.0105)
Batch 300/356: Loss=1.0321 (C:1.0321, R:0.0105)
Batch 325/356: Loss=0.9965 (C:0.9965, R:0.0105)
Batch 350/356: Loss=0.9929 (C:0.9929, R:0.0105)

============================================================
Epoch 13/300 completed in 28.2s
Train: Loss=1.0040 (C:1.0040, R:0.0105) Ratio=3.24x
Val:   Loss=1.0552 (C:1.0552, R:0.0104) Ratio=2.89x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0552)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/356: Loss=0.9717 (C:0.9717, R:0.0105)
Batch  25/356: Loss=0.9839 (C:0.9839, R:0.0105)
Batch  50/356: Loss=1.0117 (C:1.0117, R:0.0105)
Batch  75/356: Loss=0.9664 (C:0.9664, R:0.0105)
Batch 100/356: Loss=0.9953 (C:0.9953, R:0.0105)
Batch 125/356: Loss=0.9715 (C:0.9715, R:0.0105)
Batch 150/356: Loss=1.0188 (C:1.0188, R:0.0105)
Batch 175/356: Loss=0.9339 (C:0.9339, R:0.0105)
Batch 200/356: Loss=1.0093 (C:1.0093, R:0.0106)
Batch 225/356: Loss=0.9751 (C:0.9751, R:0.0105)
Batch 250/356: Loss=1.0034 (C:1.0034, R:0.0105)
Batch 275/356: Loss=0.9975 (C:0.9975, R:0.0105)
Batch 300/356: Loss=1.0102 (C:1.0102, R:0.0105)
Batch 325/356: Loss=0.9928 (C:0.9928, R:0.0105)
Batch 350/356: Loss=1.0091 (C:1.0091, R:0.0105)

============================================================
Epoch 14/300 completed in 21.2s
Train: Loss=0.9972 (C:0.9972, R:0.0105) Ratio=3.32x
Val:   Loss=1.0562 (C:1.0562, R:0.0104) Ratio=2.89x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/356: Loss=0.9763 (C:0.9763, R:0.0105)
Batch  25/356: Loss=0.9619 (C:0.9619, R:0.0105)
Batch  50/356: Loss=1.0157 (C:1.0157, R:0.0105)
Batch  75/356: Loss=0.9873 (C:0.9873, R:0.0105)
Batch 100/356: Loss=0.9997 (C:0.9997, R:0.0105)
Batch 125/356: Loss=0.9815 (C:0.9815, R:0.0105)
Batch 150/356: Loss=0.9768 (C:0.9768, R:0.0105)
Batch 175/356: Loss=0.9755 (C:0.9755, R:0.0105)
Batch 200/356: Loss=0.9972 (C:0.9972, R:0.0105)
Batch 225/356: Loss=0.9989 (C:0.9989, R:0.0105)
Batch 250/356: Loss=0.9853 (C:0.9853, R:0.0105)
Batch 275/356: Loss=0.9484 (C:0.9484, R:0.0105)
Batch 300/356: Loss=0.9783 (C:0.9783, R:0.0105)
Batch 325/356: Loss=0.9866 (C:0.9866, R:0.0105)
Batch 350/356: Loss=1.0241 (C:1.0241, R:0.0105)

============================================================
Epoch 15/300 completed in 21.1s
Train: Loss=0.9906 (C:0.9906, R:0.0105) Ratio=3.40x
Val:   Loss=1.0509 (C:1.0509, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0509)
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.455 ± 0.569
    Neg distances: 1.887 ± 0.905
    Separation ratio: 4.15x
    Gap: -3.439
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/356: Loss=0.9689 (C:0.9689, R:0.0105)
Batch  25/356: Loss=0.9473 (C:0.9473, R:0.0105)
Batch  50/356: Loss=0.9331 (C:0.9331, R:0.0105)
Batch  75/356: Loss=0.9419 (C:0.9419, R:0.0105)
Batch 100/356: Loss=0.9505 (C:0.9505, R:0.0105)
Batch 125/356: Loss=0.9719 (C:0.9719, R:0.0105)
Batch 150/356: Loss=0.9193 (C:0.9193, R:0.0105)
Batch 175/356: Loss=0.9573 (C:0.9573, R:0.0105)
Batch 200/356: Loss=0.9752 (C:0.9752, R:0.0105)
Batch 225/356: Loss=0.9402 (C:0.9402, R:0.0105)
Batch 250/356: Loss=0.9674 (C:0.9674, R:0.0105)
Batch 275/356: Loss=0.9336 (C:0.9336, R:0.0106)
Batch 300/356: Loss=0.9905 (C:0.9905, R:0.0105)
Batch 325/356: Loss=0.9496 (C:0.9496, R:0.0105)
Batch 350/356: Loss=0.9801 (C:0.9801, R:0.0105)

============================================================
Epoch 16/300 completed in 27.5s
Train: Loss=0.9594 (C:0.9594, R:0.0105) Ratio=3.45x
Val:   Loss=1.0229 (C:1.0229, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0229)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/356: Loss=0.9349 (C:0.9349, R:0.0105)
Batch  25/356: Loss=0.9482 (C:0.9482, R:0.0105)
Batch  50/356: Loss=0.9399 (C:0.9399, R:0.0105)
Batch  75/356: Loss=0.9459 (C:0.9459, R:0.0105)
Batch 100/356: Loss=0.9593 (C:0.9593, R:0.0105)
Batch 125/356: Loss=0.9461 (C:0.9461, R:0.0105)
Batch 150/356: Loss=0.9759 (C:0.9759, R:0.0105)
Batch 175/356: Loss=0.9502 (C:0.9502, R:0.0105)
Batch 200/356: Loss=0.9681 (C:0.9681, R:0.0105)
Batch 225/356: Loss=0.9962 (C:0.9962, R:0.0105)
Batch 250/356: Loss=0.9636 (C:0.9636, R:0.0105)
Batch 275/356: Loss=0.9482 (C:0.9482, R:0.0105)
Batch 300/356: Loss=0.9338 (C:0.9338, R:0.0105)
Batch 325/356: Loss=0.9783 (C:0.9783, R:0.0105)
Batch 350/356: Loss=0.9512 (C:0.9512, R:0.0105)

============================================================
Epoch 17/300 completed in 21.4s
Train: Loss=0.9530 (C:0.9530, R:0.0105) Ratio=3.45x
Val:   Loss=1.0207 (C:1.0207, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0207)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/356: Loss=0.9503 (C:0.9503, R:0.0105)
Batch  25/356: Loss=0.9407 (C:0.9407, R:0.0105)
Batch  50/356: Loss=0.9150 (C:0.9150, R:0.0105)
Batch  75/356: Loss=0.9319 (C:0.9319, R:0.0105)
Batch 100/356: Loss=0.9297 (C:0.9297, R:0.0105)
Batch 125/356: Loss=0.9783 (C:0.9783, R:0.0105)
Batch 150/356: Loss=0.9352 (C:0.9352, R:0.0105)
Batch 175/356: Loss=0.9226 (C:0.9226, R:0.0105)
Batch 200/356: Loss=0.9883 (C:0.9883, R:0.0105)
Batch 225/356: Loss=0.9410 (C:0.9410, R:0.0105)
Batch 250/356: Loss=0.9243 (C:0.9243, R:0.0105)
Batch 275/356: Loss=0.9639 (C:0.9639, R:0.0105)
Batch 300/356: Loss=0.9393 (C:0.9393, R:0.0105)
Batch 325/356: Loss=0.9520 (C:0.9520, R:0.0105)
Batch 350/356: Loss=0.9680 (C:0.9680, R:0.0105)

============================================================
Epoch 18/300 completed in 21.1s
Train: Loss=0.9500 (C:0.9500, R:0.0105) Ratio=3.57x
Val:   Loss=1.0304 (C:1.0304, R:0.0104) Ratio=2.91x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.418 ± 0.524
    Neg distances: 1.973 ± 0.912
    Separation ratio: 4.72x
    Gap: -3.523
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/356: Loss=0.9349 (C:0.9349, R:0.0105)
Batch  25/356: Loss=0.8805 (C:0.8805, R:0.0105)
Batch  50/356: Loss=0.8867 (C:0.8867, R:0.0105)
Batch  75/356: Loss=0.9006 (C:0.9006, R:0.0105)
Batch 100/356: Loss=0.9281 (C:0.9281, R:0.0105)
Batch 125/356: Loss=0.9000 (C:0.9000, R:0.0105)
Batch 150/356: Loss=0.8938 (C:0.8938, R:0.0105)
Batch 175/356: Loss=0.9193 (C:0.9193, R:0.0105)
Batch 200/356: Loss=0.9325 (C:0.9325, R:0.0105)
Batch 225/356: Loss=0.8916 (C:0.8916, R:0.0105)
Batch 250/356: Loss=0.8925 (C:0.8925, R:0.0105)
Batch 275/356: Loss=0.8999 (C:0.8999, R:0.0105)
Batch 300/356: Loss=0.9173 (C:0.9173, R:0.0105)
Batch 325/356: Loss=0.8963 (C:0.8963, R:0.0105)
Batch 350/356: Loss=0.9315 (C:0.9315, R:0.0105)

============================================================
Epoch 19/300 completed in 26.8s
Train: Loss=0.9034 (C:0.9034, R:0.0105) Ratio=3.52x
Val:   Loss=0.9827 (C:0.9827, R:0.0104) Ratio=2.90x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9827)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/356: Loss=0.8770 (C:0.8770, R:0.0106)
Batch  25/356: Loss=0.9120 (C:0.9120, R:0.0105)
Batch  50/356: Loss=0.9040 (C:0.9040, R:0.0105)
Batch  75/356: Loss=0.8946 (C:0.8946, R:0.0105)
Batch 100/356: Loss=0.9156 (C:0.9156, R:0.0105)
Batch 125/356: Loss=0.9078 (C:0.9078, R:0.0105)
Batch 150/356: Loss=0.8932 (C:0.8932, R:0.0105)
Batch 175/356: Loss=0.8854 (C:0.8854, R:0.0105)
Batch 200/356: Loss=0.8970 (C:0.8970, R:0.0105)
Batch 225/356: Loss=0.8747 (C:0.8747, R:0.0105)
Batch 250/356: Loss=0.9093 (C:0.9093, R:0.0105)
Batch 275/356: Loss=0.8978 (C:0.8978, R:0.0105)
Batch 300/356: Loss=0.9086 (C:0.9086, R:0.0105)
Batch 325/356: Loss=0.9272 (C:0.9272, R:0.0105)
Batch 350/356: Loss=0.8888 (C:0.8888, R:0.0105)

============================================================
Epoch 20/300 completed in 21.0s
Train: Loss=0.8990 (C:0.8990, R:0.0105) Ratio=3.59x
Val:   Loss=0.9778 (C:0.9778, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9778)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/356: Loss=0.8758 (C:0.8758, R:0.0105)
Batch  25/356: Loss=0.8938 (C:0.8938, R:0.0105)
Batch  50/356: Loss=0.9019 (C:0.9019, R:0.0105)
Batch  75/356: Loss=0.8936 (C:0.8936, R:0.0105)
Batch 100/356: Loss=0.9128 (C:0.9128, R:0.0105)
Batch 125/356: Loss=0.9053 (C:0.9053, R:0.0105)
Batch 150/356: Loss=0.9225 (C:0.9225, R:0.0105)
Batch 175/356: Loss=0.8893 (C:0.8893, R:0.0105)
Batch 200/356: Loss=0.9052 (C:0.9052, R:0.0105)
Batch 225/356: Loss=0.8849 (C:0.8849, R:0.0105)
Batch 250/356: Loss=0.9221 (C:0.9221, R:0.0105)
Batch 275/356: Loss=0.8906 (C:0.8906, R:0.0105)
Batch 300/356: Loss=0.8735 (C:0.8735, R:0.0105)
Batch 325/356: Loss=0.9063 (C:0.9063, R:0.0106)
Batch 350/356: Loss=0.9095 (C:0.9095, R:0.0105)

============================================================
Epoch 21/300 completed in 21.1s
Train: Loss=0.8933 (C:0.8933, R:0.0105) Ratio=3.61x
Val:   Loss=0.9842 (C:0.9842, R:0.0104) Ratio=2.94x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.420 ± 0.541
    Neg distances: 2.036 ± 0.935
    Separation ratio: 4.85x
    Gap: -3.628
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/356: Loss=0.8734 (C:0.8734, R:0.0105)
Batch  25/356: Loss=0.8871 (C:0.8871, R:0.0105)
Batch  50/356: Loss=0.8826 (C:0.8826, R:0.0105)
Batch  75/356: Loss=0.8908 (C:0.8908, R:0.0105)
Batch 100/356: Loss=0.8991 (C:0.8991, R:0.0105)
Batch 125/356: Loss=0.8968 (C:0.8968, R:0.0105)
Batch 150/356: Loss=0.8513 (C:0.8513, R:0.0106)
Batch 175/356: Loss=0.9257 (C:0.9257, R:0.0105)
Batch 200/356: Loss=0.8484 (C:0.8484, R:0.0105)
Batch 225/356: Loss=0.8518 (C:0.8518, R:0.0105)
Batch 250/356: Loss=0.8495 (C:0.8495, R:0.0105)
Batch 275/356: Loss=0.8697 (C:0.8697, R:0.0105)
Batch 300/356: Loss=0.8905 (C:0.8905, R:0.0105)
Batch 325/356: Loss=0.8629 (C:0.8629, R:0.0105)
Batch 350/356: Loss=0.8916 (C:0.8916, R:0.0105)

============================================================
Epoch 22/300 completed in 27.6s
Train: Loss=0.8773 (C:0.8773, R:0.0105) Ratio=3.67x
Val:   Loss=0.9665 (C:0.9665, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9665)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/356: Loss=0.8663 (C:0.8663, R:0.0105)
Batch  25/356: Loss=0.8514 (C:0.8514, R:0.0105)
Batch  50/356: Loss=0.8323 (C:0.8323, R:0.0105)
Batch  75/356: Loss=0.8627 (C:0.8627, R:0.0105)
Batch 100/356: Loss=0.8746 (C:0.8746, R:0.0105)
Batch 125/356: Loss=0.8568 (C:0.8568, R:0.0105)
Batch 150/356: Loss=0.8479 (C:0.8479, R:0.0105)
Batch 175/356: Loss=0.8351 (C:0.8351, R:0.0105)
Batch 200/356: Loss=0.8753 (C:0.8753, R:0.0105)
Batch 225/356: Loss=0.8624 (C:0.8624, R:0.0106)
Batch 250/356: Loss=0.9124 (C:0.9124, R:0.0105)
Batch 275/356: Loss=0.8948 (C:0.8948, R:0.0106)
Batch 300/356: Loss=0.8836 (C:0.8836, R:0.0105)
Batch 325/356: Loss=0.8875 (C:0.8875, R:0.0105)
Batch 350/356: Loss=0.8775 (C:0.8775, R:0.0105)

============================================================
Epoch 23/300 completed in 21.1s
Train: Loss=0.8706 (C:0.8706, R:0.0105) Ratio=3.75x
Val:   Loss=0.9683 (C:0.9683, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/356: Loss=0.8528 (C:0.8528, R:0.0105)
Batch  25/356: Loss=0.8678 (C:0.8678, R:0.0105)
Batch  50/356: Loss=0.8401 (C:0.8401, R:0.0105)
Batch  75/356: Loss=0.8719 (C:0.8719, R:0.0105)
Batch 100/356: Loss=0.8619 (C:0.8619, R:0.0105)
Batch 125/356: Loss=0.8440 (C:0.8440, R:0.0105)
Batch 150/356: Loss=0.8485 (C:0.8485, R:0.0105)
Batch 175/356: Loss=0.8881 (C:0.8881, R:0.0105)
Batch 200/356: Loss=0.8834 (C:0.8834, R:0.0105)
Batch 225/356: Loss=0.8850 (C:0.8850, R:0.0105)
Batch 250/356: Loss=0.9029 (C:0.9029, R:0.0105)
Batch 275/356: Loss=0.8763 (C:0.8763, R:0.0105)
Batch 300/356: Loss=0.8717 (C:0.8717, R:0.0105)
Batch 325/356: Loss=0.8593 (C:0.8593, R:0.0105)
Batch 350/356: Loss=0.8698 (C:0.8698, R:0.0105)

============================================================
Epoch 24/300 completed in 20.1s
Train: Loss=0.8669 (C:0.8669, R:0.0105) Ratio=3.78x
Val:   Loss=0.9636 (C:0.9636, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9636)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.449 ± 0.584
    Neg distances: 2.079 ± 0.963
    Separation ratio: 4.63x
    Gap: -3.706
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/356: Loss=0.8529 (C:0.8529, R:0.0105)
Batch  25/356: Loss=0.8550 (C:0.8550, R:0.0105)
Batch  50/356: Loss=0.8763 (C:0.8763, R:0.0105)
Batch  75/356: Loss=0.8625 (C:0.8625, R:0.0105)
Batch 100/356: Loss=0.8619 (C:0.8619, R:0.0105)
Batch 125/356: Loss=0.8749 (C:0.8749, R:0.0105)
Batch 150/356: Loss=0.8626 (C:0.8626, R:0.0105)
Batch 175/356: Loss=0.8828 (C:0.8828, R:0.0105)
Batch 200/356: Loss=0.8641 (C:0.8641, R:0.0105)
Batch 225/356: Loss=0.8824 (C:0.8824, R:0.0105)
Batch 250/356: Loss=0.8942 (C:0.8942, R:0.0105)
Batch 275/356: Loss=0.8778 (C:0.8778, R:0.0105)
Batch 300/356: Loss=0.8744 (C:0.8744, R:0.0105)
Batch 325/356: Loss=0.8827 (C:0.8827, R:0.0105)
Batch 350/356: Loss=0.8851 (C:0.8851, R:0.0105)

============================================================
Epoch 25/300 completed in 25.9s
Train: Loss=0.8684 (C:0.8684, R:0.0105) Ratio=3.76x
Val:   Loss=0.9598 (C:0.9598, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9598)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/356: Loss=0.8374 (C:0.8374, R:0.0106)
Batch  25/356: Loss=0.8845 (C:0.8845, R:0.0105)
Batch  50/356: Loss=0.8860 (C:0.8860, R:0.0105)
Batch  75/356: Loss=0.8828 (C:0.8828, R:0.0105)
Batch 100/356: Loss=0.8463 (C:0.8463, R:0.0105)
Batch 125/356: Loss=0.8750 (C:0.8750, R:0.0105)
Batch 150/356: Loss=0.8876 (C:0.8876, R:0.0105)
Batch 175/356: Loss=0.8642 (C:0.8642, R:0.0105)
Batch 200/356: Loss=0.8518 (C:0.8518, R:0.0105)
Batch 225/356: Loss=0.8505 (C:0.8505, R:0.0105)
Batch 250/356: Loss=0.8736 (C:0.8736, R:0.0105)
Batch 275/356: Loss=0.8830 (C:0.8830, R:0.0105)
Batch 300/356: Loss=0.8698 (C:0.8698, R:0.0105)
Batch 325/356: Loss=0.8616 (C:0.8616, R:0.0105)
Batch 350/356: Loss=0.8931 (C:0.8931, R:0.0105)

============================================================
Epoch 26/300 completed in 21.1s
Train: Loss=0.8643 (C:0.8643, R:0.0105) Ratio=3.81x
Val:   Loss=0.9567 (C:0.9567, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9567)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/356: Loss=0.8853 (C:0.8853, R:0.0105)
Batch  25/356: Loss=0.8435 (C:0.8435, R:0.0105)
Batch  50/356: Loss=0.8573 (C:0.8573, R:0.0106)
Batch  75/356: Loss=0.8649 (C:0.8649, R:0.0105)
Batch 100/356: Loss=0.8467 (C:0.8467, R:0.0105)
Batch 125/356: Loss=0.8514 (C:0.8514, R:0.0105)
Batch 150/356: Loss=0.8413 (C:0.8413, R:0.0105)
Batch 175/356: Loss=0.8674 (C:0.8674, R:0.0105)
Batch 200/356: Loss=0.8621 (C:0.8621, R:0.0105)
Batch 225/356: Loss=0.8224 (C:0.8224, R:0.0105)
Batch 250/356: Loss=0.8832 (C:0.8832, R:0.0105)
Batch 275/356: Loss=0.8546 (C:0.8546, R:0.0105)
Batch 300/356: Loss=0.8259 (C:0.8259, R:0.0105)
Batch 325/356: Loss=0.8595 (C:0.8595, R:0.0105)
Batch 350/356: Loss=0.8882 (C:0.8882, R:0.0105)

============================================================
Epoch 27/300 completed in 21.0s
Train: Loss=0.8620 (C:0.8620, R:0.0105) Ratio=3.89x
Val:   Loss=0.9615 (C:0.9615, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.442 ± 0.598
    Neg distances: 2.149 ± 0.980
    Separation ratio: 4.86x
    Gap: -3.817
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/356: Loss=0.8087 (C:0.8087, R:0.0105)
Batch  25/356: Loss=0.8540 (C:0.8540, R:0.0105)
Batch  50/356: Loss=0.8241 (C:0.8241, R:0.0105)
Batch  75/356: Loss=0.8278 (C:0.8278, R:0.0105)
Batch 100/356: Loss=0.8456 (C:0.8456, R:0.0105)
Batch 125/356: Loss=0.8558 (C:0.8558, R:0.0105)
Batch 150/356: Loss=0.8124 (C:0.8124, R:0.0105)
Batch 175/356: Loss=0.8430 (C:0.8430, R:0.0105)
Batch 200/356: Loss=0.8291 (C:0.8291, R:0.0105)
Batch 225/356: Loss=0.8509 (C:0.8509, R:0.0105)
Batch 250/356: Loss=0.8655 (C:0.8655, R:0.0105)
Batch 275/356: Loss=0.8496 (C:0.8496, R:0.0105)
Batch 300/356: Loss=0.8665 (C:0.8665, R:0.0105)
Batch 325/356: Loss=0.8610 (C:0.8610, R:0.0105)
Batch 350/356: Loss=0.8112 (C:0.8112, R:0.0105)

============================================================
Epoch 28/300 completed in 27.1s
Train: Loss=0.8366 (C:0.8366, R:0.0105) Ratio=3.92x
Val:   Loss=0.9399 (C:0.9399, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9399)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/356: Loss=0.8255 (C:0.8255, R:0.0105)
Batch  25/356: Loss=0.8479 (C:0.8479, R:0.0105)
Batch  50/356: Loss=0.8273 (C:0.8273, R:0.0105)
Batch  75/356: Loss=0.7953 (C:0.7953, R:0.0105)
Batch 100/356: Loss=0.8378 (C:0.8378, R:0.0105)
Batch 125/356: Loss=0.8478 (C:0.8478, R:0.0105)
Batch 150/356: Loss=0.8299 (C:0.8299, R:0.0105)
Batch 175/356: Loss=0.8250 (C:0.8250, R:0.0105)
Batch 200/356: Loss=0.8448 (C:0.8448, R:0.0105)
Batch 225/356: Loss=0.8421 (C:0.8421, R:0.0105)
Batch 250/356: Loss=0.8505 (C:0.8505, R:0.0105)
Batch 275/356: Loss=0.8168 (C:0.8168, R:0.0105)
Batch 300/356: Loss=0.8222 (C:0.8222, R:0.0105)
Batch 325/356: Loss=0.8335 (C:0.8335, R:0.0105)
Batch 350/356: Loss=0.8416 (C:0.8416, R:0.0105)

============================================================
Epoch 29/300 completed in 21.0s
Train: Loss=0.8335 (C:0.8335, R:0.0105) Ratio=3.95x
Val:   Loss=0.9341 (C:0.9341, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9341)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/356: Loss=0.8292 (C:0.8292, R:0.0105)
Batch  25/356: Loss=0.7845 (C:0.7845, R:0.0105)
Batch  50/356: Loss=0.8368 (C:0.8368, R:0.0105)
Batch  75/356: Loss=0.8290 (C:0.8290, R:0.0105)
Batch 100/356: Loss=0.8034 (C:0.8034, R:0.0105)
Batch 125/356: Loss=0.8171 (C:0.8171, R:0.0105)
Batch 150/356: Loss=0.8411 (C:0.8411, R:0.0105)
Batch 175/356: Loss=0.8396 (C:0.8396, R:0.0105)
Batch 200/356: Loss=0.8356 (C:0.8356, R:0.0105)
Batch 225/356: Loss=0.8381 (C:0.8381, R:0.0105)
Batch 250/356: Loss=0.8427 (C:0.8427, R:0.0105)
Batch 275/356: Loss=0.8504 (C:0.8504, R:0.0105)
Batch 300/356: Loss=0.8199 (C:0.8199, R:0.0105)
Batch 325/356: Loss=0.8616 (C:0.8616, R:0.0105)
Batch 350/356: Loss=0.8295 (C:0.8295, R:0.0106)

============================================================
Epoch 30/300 completed in 20.2s
Train: Loss=0.8283 (C:0.8283, R:0.0105) Ratio=4.01x
Val:   Loss=0.9437 (C:0.9437, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.416 ± 0.577
    Neg distances: 2.250 ± 1.006
    Separation ratio: 5.41x
    Gap: -3.928
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/356: Loss=0.7514 (C:0.7514, R:0.0105)
Batch  25/356: Loss=0.8067 (C:0.8067, R:0.0105)
Batch  50/356: Loss=0.7712 (C:0.7712, R:0.0105)
Batch  75/356: Loss=0.7528 (C:0.7528, R:0.0105)
Batch 100/356: Loss=0.8227 (C:0.8227, R:0.0105)
Batch 125/356: Loss=0.7926 (C:0.7926, R:0.0105)
Batch 150/356: Loss=0.7916 (C:0.7916, R:0.0105)
Batch 175/356: Loss=0.8035 (C:0.8035, R:0.0105)
Batch 200/356: Loss=0.8056 (C:0.8056, R:0.0105)
Batch 225/356: Loss=0.8316 (C:0.8316, R:0.0105)
Batch 250/356: Loss=0.7756 (C:0.7756, R:0.0105)
Batch 275/356: Loss=0.7994 (C:0.7994, R:0.0105)
Batch 300/356: Loss=0.7849 (C:0.7849, R:0.0105)
Batch 325/356: Loss=0.7971 (C:0.7971, R:0.0105)
Batch 350/356: Loss=0.7805 (C:0.7805, R:0.0105)

============================================================
Epoch 31/300 completed in 26.5s
Train: Loss=0.7921 (C:0.7921, R:0.0105) Ratio=4.05x
Val:   Loss=0.9198 (C:0.9198, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.9198)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/356: Loss=0.7992 (C:0.7992, R:0.0105)
Batch  25/356: Loss=0.7754 (C:0.7754, R:0.0105)
Batch  50/356: Loss=0.7458 (C:0.7458, R:0.0105)
Batch  75/356: Loss=0.7647 (C:0.7647, R:0.0106)
Batch 100/356: Loss=0.7908 (C:0.7908, R:0.0105)
Batch 125/356: Loss=0.8013 (C:0.8013, R:0.0105)
Batch 150/356: Loss=0.7806 (C:0.7806, R:0.0105)
Batch 175/356: Loss=0.7505 (C:0.7505, R:0.0105)
Batch 200/356: Loss=0.7939 (C:0.7939, R:0.0105)
Batch 225/356: Loss=0.7814 (C:0.7814, R:0.0105)
Batch 250/356: Loss=0.8034 (C:0.8034, R:0.0105)
Batch 275/356: Loss=0.7858 (C:0.7858, R:0.0105)
Batch 300/356: Loss=0.8120 (C:0.8120, R:0.0105)
Batch 325/356: Loss=0.7880 (C:0.7880, R:0.0105)
Batch 350/356: Loss=0.7774 (C:0.7774, R:0.0105)

============================================================
Epoch 32/300 completed in 21.3s
Train: Loss=0.7870 (C:0.7870, R:0.0105) Ratio=4.10x
Val:   Loss=0.9100 (C:0.9100, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.030
✅ New best model saved (Val Loss: 0.9100)
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/356: Loss=0.7828 (C:0.7828, R:0.0105)
Batch  25/356: Loss=0.7823 (C:0.7823, R:0.0105)
Batch  50/356: Loss=0.7943 (C:0.7943, R:0.0105)
Batch  75/356: Loss=0.8170 (C:0.8170, R:0.0105)
Batch 100/356: Loss=0.7931 (C:0.7931, R:0.0105)
Batch 125/356: Loss=0.8016 (C:0.8016, R:0.0105)
Batch 150/356: Loss=0.7823 (C:0.7823, R:0.0105)
Batch 175/356: Loss=0.7958 (C:0.7958, R:0.0105)
Batch 200/356: Loss=0.7748 (C:0.7748, R:0.0105)
Batch 225/356: Loss=0.7996 (C:0.7996, R:0.0105)
Batch 250/356: Loss=0.8042 (C:0.8042, R:0.0105)
Batch 275/356: Loss=0.7808 (C:0.7808, R:0.0105)
Batch 300/356: Loss=0.7900 (C:0.7900, R:0.0105)
Batch 325/356: Loss=0.7795 (C:0.7795, R:0.0105)
Batch 350/356: Loss=0.7903 (C:0.7903, R:0.0105)

============================================================
Epoch 33/300 completed in 21.1s
Train: Loss=0.7849 (C:0.7849, R:0.0105) Ratio=4.02x
Val:   Loss=0.9096 (C:0.9096, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.9096)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.420 ± 0.592
    Neg distances: 2.319 ± 1.027
    Separation ratio: 5.52x
    Gap: -4.085
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/356: Loss=0.7481 (C:0.7481, R:0.0105)
Batch  25/356: Loss=0.7524 (C:0.7524, R:0.0105)
Batch  50/356: Loss=0.7672 (C:0.7672, R:0.0105)
Batch  75/356: Loss=0.7633 (C:0.7633, R:0.0105)
Batch 100/356: Loss=0.7679 (C:0.7679, R:0.0105)
Batch 125/356: Loss=0.7838 (C:0.7838, R:0.0105)
Batch 150/356: Loss=0.7737 (C:0.7737, R:0.0105)
Batch 175/356: Loss=0.7821 (C:0.7821, R:0.0105)
Batch 200/356: Loss=0.7537 (C:0.7537, R:0.0105)
Batch 225/356: Loss=0.7966 (C:0.7966, R:0.0105)
Batch 250/356: Loss=0.7726 (C:0.7726, R:0.0105)
Batch 275/356: Loss=0.7526 (C:0.7526, R:0.0105)
Batch 300/356: Loss=0.7918 (C:0.7918, R:0.0105)
Batch 325/356: Loss=0.7610 (C:0.7610, R:0.0105)
Batch 350/356: Loss=0.8026 (C:0.8026, R:0.0105)

============================================================
Epoch 34/300 completed in 27.4s
Train: Loss=0.7694 (C:0.7694, R:0.0105) Ratio=4.10x
Val:   Loss=0.8993 (C:0.8993, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8993)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/356: Loss=0.7460 (C:0.7460, R:0.0105)
Batch  25/356: Loss=0.7560 (C:0.7560, R:0.0105)
Batch  50/356: Loss=0.7657 (C:0.7657, R:0.0105)
Batch  75/356: Loss=0.7706 (C:0.7706, R:0.0105)
Batch 100/356: Loss=0.7537 (C:0.7537, R:0.0105)
Batch 125/356: Loss=0.7714 (C:0.7714, R:0.0105)
Batch 150/356: Loss=0.7562 (C:0.7562, R:0.0105)
Batch 175/356: Loss=0.7772 (C:0.7772, R:0.0106)
Batch 200/356: Loss=0.7610 (C:0.7610, R:0.0105)
Batch 225/356: Loss=0.7770 (C:0.7770, R:0.0105)
Batch 250/356: Loss=0.8085 (C:0.8085, R:0.0105)
Batch 275/356: Loss=0.7647 (C:0.7647, R:0.0105)
Batch 300/356: Loss=0.7671 (C:0.7671, R:0.0105)
Batch 325/356: Loss=0.8150 (C:0.8150, R:0.0106)
Batch 350/356: Loss=0.7775 (C:0.7775, R:0.0105)

============================================================
Epoch 35/300 completed in 20.6s
Train: Loss=0.7667 (C:0.7667, R:0.0105) Ratio=4.09x
Val:   Loss=0.8900 (C:0.8900, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 0.8900)
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/356: Loss=0.7538 (C:0.7538, R:0.0105)
Batch  25/356: Loss=0.7280 (C:0.7280, R:0.0105)
Batch  50/356: Loss=0.7323 (C:0.7323, R:0.0105)
Batch  75/356: Loss=0.7382 (C:0.7382, R:0.0105)
Batch 100/356: Loss=0.7759 (C:0.7759, R:0.0105)
Batch 125/356: Loss=0.7679 (C:0.7679, R:0.0105)
Batch 150/356: Loss=0.7298 (C:0.7298, R:0.0105)
Batch 175/356: Loss=0.7432 (C:0.7432, R:0.0105)
Batch 200/356: Loss=0.7123 (C:0.7123, R:0.0105)
Batch 225/356: Loss=0.7563 (C:0.7563, R:0.0105)
Batch 250/356: Loss=0.7628 (C:0.7628, R:0.0105)
Batch 275/356: Loss=0.7482 (C:0.7482, R:0.0105)
Batch 300/356: Loss=0.8134 (C:0.8134, R:0.0105)
Batch 325/356: Loss=0.8039 (C:0.8039, R:0.0105)
Batch 350/356: Loss=0.7353 (C:0.7353, R:0.0106)

============================================================
Epoch 36/300 completed in 20.2s
Train: Loss=0.7630 (C:0.7630, R:0.0105) Ratio=4.25x
Val:   Loss=0.9027 (C:0.9027, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.090
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.389 ± 0.590
    Neg distances: 2.415 ± 1.044
    Separation ratio: 6.20x
    Gap: -4.198
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/356: Loss=0.7018 (C:0.7018, R:0.0105)
Batch  25/356: Loss=0.7400 (C:0.7400, R:0.0105)
Batch  50/356: Loss=0.7160 (C:0.7160, R:0.0105)
Batch  75/356: Loss=0.7073 (C:0.7073, R:0.0105)
Batch 100/356: Loss=0.6975 (C:0.6975, R:0.0105)
Batch 125/356: Loss=0.7014 (C:0.7014, R:0.0105)
Batch 150/356: Loss=0.7342 (C:0.7342, R:0.0105)
Batch 175/356: Loss=0.7294 (C:0.7294, R:0.0105)
Batch 200/356: Loss=0.7441 (C:0.7441, R:0.0105)
Batch 225/356: Loss=0.7819 (C:0.7819, R:0.0105)
Batch 250/356: Loss=0.7131 (C:0.7131, R:0.0105)
Batch 275/356: Loss=0.7042 (C:0.7042, R:0.0105)
Batch 300/356: Loss=0.7460 (C:0.7460, R:0.0105)
Batch 325/356: Loss=0.7301 (C:0.7301, R:0.0105)
Batch 350/356: Loss=0.7358 (C:0.7358, R:0.0105)

============================================================
Epoch 37/300 completed in 27.2s
Train: Loss=0.7268 (C:0.7268, R:0.0105) Ratio=4.24x
Val:   Loss=0.8704 (C:0.8704, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8704)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/356: Loss=0.6789 (C:0.6789, R:0.0105)
Batch  25/356: Loss=0.6959 (C:0.6959, R:0.0105)
Batch  50/356: Loss=0.7183 (C:0.7183, R:0.0105)
Batch  75/356: Loss=0.7355 (C:0.7355, R:0.0105)
Batch 100/356: Loss=0.7158 (C:0.7158, R:0.0105)
Batch 125/356: Loss=0.7203 (C:0.7203, R:0.0105)
Batch 150/356: Loss=0.7409 (C:0.7409, R:0.0105)
Batch 175/356: Loss=0.7730 (C:0.7730, R:0.0105)
Batch 200/356: Loss=0.7311 (C:0.7311, R:0.0105)
Batch 225/356: Loss=0.7396 (C:0.7396, R:0.0105)
Batch 250/356: Loss=0.6993 (C:0.6993, R:0.0105)
Batch 275/356: Loss=0.7471 (C:0.7471, R:0.0106)
Batch 300/356: Loss=0.7147 (C:0.7147, R:0.0105)
Batch 325/356: Loss=0.7305 (C:0.7305, R:0.0105)
Batch 350/356: Loss=0.7120 (C:0.7120, R:0.0105)

============================================================
Epoch 38/300 completed in 21.1s
Train: Loss=0.7238 (C:0.7238, R:0.0105) Ratio=4.25x
Val:   Loss=0.8657 (C:0.8657, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.120
✅ New best model saved (Val Loss: 0.8657)
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/356: Loss=0.6453 (C:0.6453, R:0.0105)
Batch  25/356: Loss=0.7318 (C:0.7318, R:0.0105)
Batch  50/356: Loss=0.7015 (C:0.7015, R:0.0105)
Batch  75/356: Loss=0.6874 (C:0.6874, R:0.0105)
Batch 100/356: Loss=0.7432 (C:0.7432, R:0.0105)
Batch 125/356: Loss=0.7042 (C:0.7042, R:0.0105)
Batch 150/356: Loss=0.7482 (C:0.7482, R:0.0105)
Batch 175/356: Loss=0.7167 (C:0.7167, R:0.0105)
Batch 200/356: Loss=0.7173 (C:0.7173, R:0.0106)
Batch 225/356: Loss=0.7160 (C:0.7160, R:0.0105)
Batch 250/356: Loss=0.7203 (C:0.7203, R:0.0105)
Batch 275/356: Loss=0.7535 (C:0.7535, R:0.0105)
Batch 300/356: Loss=0.7242 (C:0.7242, R:0.0105)
Batch 325/356: Loss=0.7269 (C:0.7269, R:0.0105)
Batch 350/356: Loss=0.7501 (C:0.7501, R:0.0105)

============================================================
Epoch 39/300 completed in 21.0s
Train: Loss=0.7206 (C:0.7206, R:0.0105) Ratio=4.26x
Val:   Loss=0.8593 (C:0.8593, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.135
✅ New best model saved (Val Loss: 0.8593)
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.389 ± 0.585
    Neg distances: 2.455 ± 1.058
    Separation ratio: 6.32x
    Gap: -4.227
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/356: Loss=0.7170 (C:0.7170, R:0.0105)
Batch  25/356: Loss=0.6941 (C:0.6941, R:0.0105)
Batch  50/356: Loss=0.6720 (C:0.6720, R:0.0105)
Batch  75/356: Loss=0.7074 (C:0.7074, R:0.0105)
Batch 100/356: Loss=0.7024 (C:0.7024, R:0.0105)
Batch 125/356: Loss=0.7199 (C:0.7199, R:0.0105)
Batch 150/356: Loss=0.6706 (C:0.6706, R:0.0105)
Batch 175/356: Loss=0.7075 (C:0.7075, R:0.0105)
Batch 200/356: Loss=0.7172 (C:0.7172, R:0.0105)
Batch 225/356: Loss=0.7012 (C:0.7012, R:0.0105)
Batch 250/356: Loss=0.7238 (C:0.7238, R:0.0105)
Batch 275/356: Loss=0.6935 (C:0.6935, R:0.0105)
Batch 300/356: Loss=0.7148 (C:0.7148, R:0.0105)
Batch 325/356: Loss=0.7373 (C:0.7373, R:0.0105)
Batch 350/356: Loss=0.7151 (C:0.7151, R:0.0105)

============================================================
Epoch 40/300 completed in 27.3s
Train: Loss=0.7115 (C:0.7115, R:0.0105) Ratio=4.27x
Val:   Loss=0.8569 (C:0.8569, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.8569)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/356: Loss=0.6825 (C:0.6825, R:0.0105)
Batch  25/356: Loss=0.7512 (C:0.7512, R:0.0105)
Batch  50/356: Loss=0.6839 (C:0.6839, R:0.0105)
Batch  75/356: Loss=0.6681 (C:0.6681, R:0.0105)
Batch 100/356: Loss=0.6901 (C:0.6901, R:0.0105)
Batch 125/356: Loss=0.6852 (C:0.6852, R:0.0105)
Batch 150/356: Loss=0.7193 (C:0.7193, R:0.0105)
Batch 175/356: Loss=0.7184 (C:0.7184, R:0.0105)
Batch 200/356: Loss=0.7354 (C:0.7354, R:0.0105)
Batch 225/356: Loss=0.6986 (C:0.6986, R:0.0105)
Batch 250/356: Loss=0.6921 (C:0.6921, R:0.0105)
Batch 275/356: Loss=0.7327 (C:0.7327, R:0.0105)
Batch 300/356: Loss=0.7396 (C:0.7396, R:0.0105)
Batch 325/356: Loss=0.7213 (C:0.7213, R:0.0105)
Batch 350/356: Loss=0.7063 (C:0.7063, R:0.0105)

============================================================
Epoch 41/300 completed in 21.2s
Train: Loss=0.7062 (C:0.7062, R:0.0105) Ratio=4.36x
Val:   Loss=0.8529 (C:0.8529, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.8529)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/356: Loss=0.6820 (C:0.6820, R:0.0106)
Batch  25/356: Loss=0.6722 (C:0.6722, R:0.0105)
Batch  50/356: Loss=0.6487 (C:0.6487, R:0.0105)
Batch  75/356: Loss=0.6885 (C:0.6885, R:0.0105)
Batch 100/356: Loss=0.6658 (C:0.6658, R:0.0105)
Batch 125/356: Loss=0.6889 (C:0.6889, R:0.0105)
Batch 150/356: Loss=0.7228 (C:0.7228, R:0.0105)
Batch 175/356: Loss=0.6887 (C:0.6887, R:0.0105)
Batch 200/356: Loss=0.6955 (C:0.6955, R:0.0105)
Batch 225/356: Loss=0.7020 (C:0.7020, R:0.0105)
Batch 250/356: Loss=0.7115 (C:0.7115, R:0.0105)
Batch 275/356: Loss=0.6963 (C:0.6963, R:0.0105)
Batch 300/356: Loss=0.7111 (C:0.7111, R:0.0105)
Batch 325/356: Loss=0.7225 (C:0.7225, R:0.0105)
Batch 350/356: Loss=0.7167 (C:0.7167, R:0.0105)

============================================================
Epoch 42/300 completed in 21.1s
Train: Loss=0.7057 (C:0.7057, R:0.0105) Ratio=4.43x
Val:   Loss=0.8533 (C:0.8533, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.180
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.401 ± 0.597
    Neg distances: 2.445 ± 1.064
    Separation ratio: 6.10x
    Gap: -4.274
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/356: Loss=0.6738 (C:0.6738, R:0.0105)
Batch  25/356: Loss=0.6898 (C:0.6898, R:0.0105)
Batch  50/356: Loss=0.6827 (C:0.6827, R:0.0105)
Batch  75/356: Loss=0.7023 (C:0.7023, R:0.0105)
Batch 100/356: Loss=0.7406 (C:0.7406, R:0.0105)
Batch 125/356: Loss=0.6865 (C:0.6865, R:0.0105)
Batch 150/356: Loss=0.7136 (C:0.7136, R:0.0105)
Batch 175/356: Loss=0.7385 (C:0.7385, R:0.0105)
Batch 200/356: Loss=0.7350 (C:0.7350, R:0.0105)
Batch 225/356: Loss=0.7162 (C:0.7162, R:0.0105)
Batch 250/356: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 275/356: Loss=0.6912 (C:0.6912, R:0.0105)
Batch 300/356: Loss=0.6748 (C:0.6748, R:0.0105)
Batch 325/356: Loss=0.7340 (C:0.7340, R:0.0105)
Batch 350/356: Loss=0.7134 (C:0.7134, R:0.0105)

============================================================
Epoch 43/300 completed in 26.3s
Train: Loss=0.7075 (C:0.7075, R:0.0105) Ratio=4.36x
Val:   Loss=0.8521 (C:0.8521, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.8521)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/356: Loss=0.6637 (C:0.6637, R:0.0105)
Batch  25/356: Loss=0.7028 (C:0.7028, R:0.0105)
Batch  50/356: Loss=0.6712 (C:0.6712, R:0.0105)
Batch  75/356: Loss=0.6987 (C:0.6987, R:0.0105)
Batch 100/356: Loss=0.7378 (C:0.7378, R:0.0105)
Batch 125/356: Loss=0.6901 (C:0.6901, R:0.0105)
Batch 150/356: Loss=0.7027 (C:0.7027, R:0.0105)
Batch 175/356: Loss=0.6961 (C:0.6961, R:0.0105)
Batch 200/356: Loss=0.6856 (C:0.6856, R:0.0105)
Batch 225/356: Loss=0.6967 (C:0.6967, R:0.0105)
Batch 250/356: Loss=0.7142 (C:0.7142, R:0.0105)
Batch 275/356: Loss=0.7139 (C:0.7139, R:0.0105)
Batch 300/356: Loss=0.6954 (C:0.6954, R:0.0105)
Batch 325/356: Loss=0.7701 (C:0.7701, R:0.0105)
Batch 350/356: Loss=0.7062 (C:0.7062, R:0.0105)

============================================================
Epoch 44/300 completed in 20.6s
Train: Loss=0.7040 (C:0.7040, R:0.0105) Ratio=4.43x
Val:   Loss=0.8600 (C:0.8600, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/356: Loss=0.6782 (C:0.6782, R:0.0105)
Batch  25/356: Loss=0.6957 (C:0.6957, R:0.0105)
Batch  50/356: Loss=0.7308 (C:0.7308, R:0.0105)
Batch  75/356: Loss=0.7012 (C:0.7012, R:0.0105)
Batch 100/356: Loss=0.7026 (C:0.7026, R:0.0105)
Batch 125/356: Loss=0.7020 (C:0.7020, R:0.0106)
Batch 150/356: Loss=0.6825 (C:0.6825, R:0.0105)
Batch 175/356: Loss=0.7091 (C:0.7091, R:0.0105)
Batch 200/356: Loss=0.7119 (C:0.7119, R:0.0105)
Batch 225/356: Loss=0.7114 (C:0.7114, R:0.0105)
Batch 250/356: Loss=0.7319 (C:0.7319, R:0.0105)
Batch 275/356: Loss=0.7034 (C:0.7034, R:0.0105)
Batch 300/356: Loss=0.6575 (C:0.6575, R:0.0105)
Batch 325/356: Loss=0.7016 (C:0.7016, R:0.0105)
Batch 350/356: Loss=0.6763 (C:0.6763, R:0.0105)

============================================================
Epoch 45/300 completed in 21.2s
Train: Loss=0.7017 (C:0.7017, R:0.0105) Ratio=4.44x
Val:   Loss=0.8619 (C:0.8619, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.225
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.393 ± 0.561
    Neg distances: 2.487 ± 1.075
    Separation ratio: 6.34x
    Gap: -4.338
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/356: Loss=0.6761 (C:0.6761, R:0.0105)
Batch  25/356: Loss=0.6801 (C:0.6801, R:0.0105)
Batch  50/356: Loss=0.6652 (C:0.6652, R:0.0105)
Batch  75/356: Loss=0.7137 (C:0.7137, R:0.0105)
Batch 100/356: Loss=0.6801 (C:0.6801, R:0.0105)
Batch 125/356: Loss=0.6632 (C:0.6632, R:0.0105)
Batch 150/356: Loss=0.6899 (C:0.6899, R:0.0105)
Batch 175/356: Loss=0.6963 (C:0.6963, R:0.0105)
Batch 200/356: Loss=0.6994 (C:0.6994, R:0.0105)
Batch 225/356: Loss=0.6754 (C:0.6754, R:0.0105)
Batch 250/356: Loss=0.7071 (C:0.7071, R:0.0105)
Batch 275/356: Loss=0.6678 (C:0.6678, R:0.0105)
Batch 300/356: Loss=0.6662 (C:0.6662, R:0.0105)
Batch 325/356: Loss=0.7089 (C:0.7089, R:0.0105)
Batch 350/356: Loss=0.6969 (C:0.6969, R:0.0105)

============================================================
Epoch 46/300 completed in 26.9s
Train: Loss=0.6903 (C:0.6903, R:0.0105) Ratio=4.50x
Val:   Loss=0.8541 (C:0.8541, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.240
No improvement for 3 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/356: Loss=0.6879 (C:0.6879, R:0.0105)
Batch  25/356: Loss=0.6671 (C:0.6671, R:0.0105)
Batch  50/356: Loss=0.6866 (C:0.6866, R:0.0105)
Batch  75/356: Loss=0.6796 (C:0.6796, R:0.0105)
Batch 100/356: Loss=0.6916 (C:0.6916, R:0.0105)
Batch 125/356: Loss=0.7029 (C:0.7029, R:0.0105)
Batch 150/356: Loss=0.6942 (C:0.6942, R:0.0105)
Batch 175/356: Loss=0.6720 (C:0.6720, R:0.0106)
Batch 200/356: Loss=0.6953 (C:0.6953, R:0.0105)
Batch 225/356: Loss=0.7358 (C:0.7358, R:0.0105)
Batch 250/356: Loss=0.6993 (C:0.6993, R:0.0105)
Batch 275/356: Loss=0.7129 (C:0.7129, R:0.0106)
Batch 300/356: Loss=0.6733 (C:0.6733, R:0.0105)
Batch 325/356: Loss=0.6919 (C:0.6919, R:0.0105)
Batch 350/356: Loss=0.6793 (C:0.6793, R:0.0105)

============================================================
Epoch 47/300 completed in 21.1s
Train: Loss=0.6877 (C:0.6877, R:0.0105) Ratio=4.48x
Val:   Loss=0.8481 (C:0.8481, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.255
✅ New best model saved (Val Loss: 0.8481)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/356: Loss=0.6899 (C:0.6899, R:0.0105)
Batch  25/356: Loss=0.6691 (C:0.6691, R:0.0105)
Batch  50/356: Loss=0.6797 (C:0.6797, R:0.0105)
Batch  75/356: Loss=0.7015 (C:0.7015, R:0.0105)
Batch 100/356: Loss=0.7023 (C:0.7023, R:0.0105)
Batch 125/356: Loss=0.6954 (C:0.6954, R:0.0105)
Batch 150/356: Loss=0.6564 (C:0.6564, R:0.0105)
Batch 175/356: Loss=0.6997 (C:0.6997, R:0.0105)
Batch 200/356: Loss=0.7105 (C:0.7105, R:0.0105)
Batch 225/356: Loss=0.6791 (C:0.6791, R:0.0105)
Batch 250/356: Loss=0.6858 (C:0.6858, R:0.0105)
Batch 275/356: Loss=0.6933 (C:0.6933, R:0.0105)
Batch 300/356: Loss=0.7016 (C:0.7016, R:0.0105)
Batch 325/356: Loss=0.7044 (C:0.7044, R:0.0105)
Batch 350/356: Loss=0.7128 (C:0.7128, R:0.0105)

============================================================
Epoch 48/300 completed in 21.1s
Train: Loss=0.6865 (C:0.6865, R:0.0105) Ratio=4.46x
Val:   Loss=0.8466 (C:0.8466, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.270
✅ New best model saved (Val Loss: 0.8466)
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.374 ± 0.581
    Neg distances: 2.535 ± 1.081
    Separation ratio: 6.78x
    Gap: -4.313
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/356: Loss=0.6556 (C:0.6556, R:0.0105)
Batch  25/356: Loss=0.6534 (C:0.6534, R:0.0105)
Batch  50/356: Loss=0.6895 (C:0.6895, R:0.0105)
Batch  75/356: Loss=0.6504 (C:0.6504, R:0.0105)
Batch 100/356: Loss=0.6723 (C:0.6723, R:0.0105)
Batch 125/356: Loss=0.6431 (C:0.6431, R:0.0105)
Batch 150/356: Loss=0.6873 (C:0.6873, R:0.0105)
Batch 175/356: Loss=0.6306 (C:0.6306, R:0.0105)
Batch 200/356: Loss=0.6683 (C:0.6683, R:0.0105)
Batch 225/356: Loss=0.6782 (C:0.6782, R:0.0105)
Batch 250/356: Loss=0.6564 (C:0.6564, R:0.0105)
Batch 275/356: Loss=0.7015 (C:0.7015, R:0.0105)
Batch 300/356: Loss=0.6649 (C:0.6649, R:0.0105)
Batch 325/356: Loss=0.6924 (C:0.6924, R:0.0105)
Batch 350/356: Loss=0.6361 (C:0.6361, R:0.0106)

============================================================
Epoch 49/300 completed in 26.4s
Train: Loss=0.6671 (C:0.6671, R:0.0105) Ratio=4.57x
Val:   Loss=0.8228 (C:0.8228, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.8228)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/356: Loss=0.6086 (C:0.6086, R:0.0106)
Batch  25/356: Loss=0.6711 (C:0.6711, R:0.0105)
Batch  50/356: Loss=0.6648 (C:0.6648, R:0.0105)
Batch  75/356: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 100/356: Loss=0.6602 (C:0.6602, R:0.0105)
Batch 125/356: Loss=0.6437 (C:0.6437, R:0.0105)
Batch 150/356: Loss=0.6663 (C:0.6663, R:0.0105)
Batch 175/356: Loss=0.6298 (C:0.6298, R:0.0105)
Batch 200/356: Loss=0.6302 (C:0.6302, R:0.0105)
Batch 225/356: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 250/356: Loss=0.6753 (C:0.6753, R:0.0105)
Batch 275/356: Loss=0.6717 (C:0.6717, R:0.0105)
Batch 300/356: Loss=0.6686 (C:0.6686, R:0.0105)
Batch 325/356: Loss=0.6851 (C:0.6851, R:0.0105)
Batch 350/356: Loss=0.6965 (C:0.6965, R:0.0105)

============================================================
Epoch 50/300 completed in 20.2s
Train: Loss=0.6649 (C:0.6649, R:0.0105) Ratio=4.57x
Val:   Loss=0.8345 (C:0.8345, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/356: Loss=0.6539 (C:0.6539, R:0.0105)
Batch  25/356: Loss=0.6396 (C:0.6396, R:0.0105)
Batch  50/356: Loss=0.6524 (C:0.6524, R:0.0105)
Batch  75/356: Loss=0.6735 (C:0.6735, R:0.0105)
Batch 100/356: Loss=0.6782 (C:0.6782, R:0.0105)
Batch 125/356: Loss=0.6850 (C:0.6850, R:0.0105)
Batch 150/356: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 175/356: Loss=0.6336 (C:0.6336, R:0.0105)
Batch 200/356: Loss=0.6686 (C:0.6686, R:0.0105)
Batch 225/356: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 250/356: Loss=0.6881 (C:0.6881, R:0.0105)
Batch 275/356: Loss=0.6683 (C:0.6683, R:0.0105)
Batch 300/356: Loss=0.6676 (C:0.6676, R:0.0105)
Batch 325/356: Loss=0.6589 (C:0.6589, R:0.0105)
Batch 350/356: Loss=0.6698 (C:0.6698, R:0.0105)

============================================================
Epoch 51/300 completed in 21.3s
Train: Loss=0.6614 (C:0.6614, R:0.0105) Ratio=4.61x
Val:   Loss=0.8340 (C:0.8340, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.361 ± 0.572
    Neg distances: 2.550 ± 1.081
    Separation ratio: 7.07x
    Gap: -4.508
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/356: Loss=0.6267 (C:0.6267, R:0.0105)
Batch  25/356: Loss=0.6497 (C:0.6497, R:0.0105)
Batch  50/356: Loss=0.6568 (C:0.6568, R:0.0106)
Batch  75/356: Loss=0.6303 (C:0.6303, R:0.0105)
Batch 100/356: Loss=0.6162 (C:0.6162, R:0.0105)
Batch 125/356: Loss=0.6318 (C:0.6318, R:0.0105)
Batch 150/356: Loss=0.6696 (C:0.6696, R:0.0105)
Batch 175/356: Loss=0.6397 (C:0.6397, R:0.0105)
Batch 200/356: Loss=0.6850 (C:0.6850, R:0.0105)
Batch 225/356: Loss=0.6523 (C:0.6523, R:0.0105)
Batch 250/356: Loss=0.6447 (C:0.6447, R:0.0105)
Batch 275/356: Loss=0.6644 (C:0.6644, R:0.0105)
Batch 300/356: Loss=0.6516 (C:0.6516, R:0.0105)
Batch 325/356: Loss=0.6513 (C:0.6513, R:0.0105)
Batch 350/356: Loss=0.6207 (C:0.6207, R:0.0105)

============================================================
Epoch 52/300 completed in 27.0s
Train: Loss=0.6510 (C:0.6510, R:0.0105) Ratio=4.71x
Val:   Loss=0.8268 (C:0.8268, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/356: Loss=0.6600 (C:0.6600, R:0.0105)
Batch  25/356: Loss=0.6462 (C:0.6462, R:0.0105)
Batch  50/356: Loss=0.6153 (C:0.6153, R:0.0105)
Batch  75/356: Loss=0.6314 (C:0.6314, R:0.0105)
Batch 100/356: Loss=0.6423 (C:0.6423, R:0.0105)
Batch 125/356: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 150/356: Loss=0.6628 (C:0.6628, R:0.0105)
Batch 175/356: Loss=0.6577 (C:0.6577, R:0.0105)
Batch 200/356: Loss=0.6642 (C:0.6642, R:0.0105)
Batch 225/356: Loss=0.6850 (C:0.6850, R:0.0105)
Batch 250/356: Loss=0.6721 (C:0.6721, R:0.0106)
Batch 275/356: Loss=0.6751 (C:0.6751, R:0.0105)
Batch 300/356: Loss=0.6186 (C:0.6186, R:0.0105)
Batch 325/356: Loss=0.6282 (C:0.6282, R:0.0105)
Batch 350/356: Loss=0.6687 (C:0.6687, R:0.0105)

============================================================
Epoch 53/300 completed in 20.1s
Train: Loss=0.6486 (C:0.6486, R:0.0105) Ratio=4.60x
Val:   Loss=0.8237 (C:0.8237, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/356: Loss=0.6701 (C:0.6701, R:0.0105)
Batch  25/356: Loss=0.6001 (C:0.6001, R:0.0105)
Batch  50/356: Loss=0.6488 (C:0.6488, R:0.0105)
Batch  75/356: Loss=0.6016 (C:0.6016, R:0.0105)
Batch 100/356: Loss=0.6326 (C:0.6326, R:0.0105)
Batch 125/356: Loss=0.6129 (C:0.6129, R:0.0105)
Batch 150/356: Loss=0.6194 (C:0.6194, R:0.0105)
Batch 175/356: Loss=0.6300 (C:0.6300, R:0.0105)
Batch 200/356: Loss=0.6678 (C:0.6678, R:0.0105)
Batch 225/356: Loss=0.6367 (C:0.6367, R:0.0105)
Batch 250/356: Loss=0.6438 (C:0.6438, R:0.0105)
Batch 275/356: Loss=0.6366 (C:0.6366, R:0.0105)
Batch 300/356: Loss=0.6257 (C:0.6257, R:0.0105)
Batch 325/356: Loss=0.6386 (C:0.6386, R:0.0105)
Batch 350/356: Loss=0.6316 (C:0.6316, R:0.0105)

============================================================
Epoch 54/300 completed in 20.1s
Train: Loss=0.6454 (C:0.6454, R:0.0105) Ratio=4.78x
Val:   Loss=0.8217 (C:0.8217, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8217)
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.361 ± 0.567
    Neg distances: 2.542 ± 1.076
    Separation ratio: 7.04x
    Gap: -4.366
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/356: Loss=0.6636 (C:0.6636, R:0.0105)
Batch  25/356: Loss=0.6512 (C:0.6512, R:0.0105)
Batch  50/356: Loss=0.6596 (C:0.6596, R:0.0105)
Batch  75/356: Loss=0.6658 (C:0.6658, R:0.0105)
Batch 100/356: Loss=0.6310 (C:0.6310, R:0.0105)
Batch 125/356: Loss=0.6245 (C:0.6245, R:0.0105)
Batch 150/356: Loss=0.6267 (C:0.6267, R:0.0105)
Batch 175/356: Loss=0.6611 (C:0.6611, R:0.0105)
Batch 200/356: Loss=0.6505 (C:0.6505, R:0.0105)
Batch 225/356: Loss=0.6597 (C:0.6597, R:0.0105)
Batch 250/356: Loss=0.6562 (C:0.6562, R:0.0105)
Batch 275/356: Loss=0.6203 (C:0.6203, R:0.0105)
Batch 300/356: Loss=0.6836 (C:0.6836, R:0.0105)
Batch 325/356: Loss=0.6341 (C:0.6341, R:0.0105)
Batch 350/356: Loss=0.6343 (C:0.6343, R:0.0105)

============================================================
Epoch 55/300 completed in 27.4s
Train: Loss=0.6444 (C:0.6444, R:0.0105) Ratio=4.66x
Val:   Loss=0.8242 (C:0.8242, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/356: Loss=0.6786 (C:0.6786, R:0.0105)
Batch  25/356: Loss=0.6317 (C:0.6317, R:0.0105)
Batch  50/356: Loss=0.6432 (C:0.6432, R:0.0105)
Batch  75/356: Loss=0.6577 (C:0.6577, R:0.0105)
Batch 100/356: Loss=0.6124 (C:0.6124, R:0.0105)
Batch 125/356: Loss=0.6395 (C:0.6395, R:0.0105)
Batch 150/356: Loss=0.6554 (C:0.6554, R:0.0105)
Batch 175/356: Loss=0.6147 (C:0.6147, R:0.0105)
Batch 200/356: Loss=0.6314 (C:0.6314, R:0.0105)
Batch 225/356: Loss=0.6589 (C:0.6589, R:0.0105)
Batch 250/356: Loss=0.6248 (C:0.6248, R:0.0105)
Batch 275/356: Loss=0.6672 (C:0.6672, R:0.0105)
Batch 300/356: Loss=0.6565 (C:0.6565, R:0.0105)
Batch 325/356: Loss=0.6318 (C:0.6318, R:0.0105)
Batch 350/356: Loss=0.6557 (C:0.6557, R:0.0105)

============================================================
Epoch 56/300 completed in 21.0s
Train: Loss=0.6429 (C:0.6429, R:0.0105) Ratio=4.67x
Val:   Loss=0.8277 (C:0.8277, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/356: Loss=0.6346 (C:0.6346, R:0.0105)
Batch  25/356: Loss=0.6679 (C:0.6679, R:0.0105)
Batch  50/356: Loss=0.5976 (C:0.5976, R:0.0105)
Batch  75/356: Loss=0.6131 (C:0.6131, R:0.0105)
Batch 100/356: Loss=0.6521 (C:0.6521, R:0.0105)
Batch 125/356: Loss=0.6104 (C:0.6104, R:0.0105)
Batch 150/356: Loss=0.6655 (C:0.6655, R:0.0105)
Batch 175/356: Loss=0.6644 (C:0.6644, R:0.0105)
Batch 200/356: Loss=0.6416 (C:0.6416, R:0.0105)
Batch 225/356: Loss=0.6674 (C:0.6674, R:0.0105)
Batch 250/356: Loss=0.6429 (C:0.6429, R:0.0105)
Batch 275/356: Loss=0.6815 (C:0.6815, R:0.0105)
Batch 300/356: Loss=0.6297 (C:0.6297, R:0.0105)
Batch 325/356: Loss=0.6533 (C:0.6533, R:0.0105)
Batch 350/356: Loss=0.6507 (C:0.6507, R:0.0105)

============================================================
Epoch 57/300 completed in 21.2s
Train: Loss=0.6403 (C:0.6403, R:0.0105) Ratio=4.65x
Val:   Loss=0.8333 (C:0.8333, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.376 ± 0.605
    Neg distances: 2.565 ± 1.094
    Separation ratio: 6.82x
    Gap: -4.479
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/356: Loss=0.6464 (C:0.6464, R:0.0105)
Batch  25/356: Loss=0.6368 (C:0.6368, R:0.0105)
Batch  50/356: Loss=0.6373 (C:0.6373, R:0.0105)
Batch  75/356: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 100/356: Loss=0.6565 (C:0.6565, R:0.0105)
Batch 125/356: Loss=0.6651 (C:0.6651, R:0.0105)
Batch 150/356: Loss=0.6414 (C:0.6414, R:0.0105)
Batch 175/356: Loss=0.6643 (C:0.6643, R:0.0105)
Batch 200/356: Loss=0.6511 (C:0.6511, R:0.0105)
Batch 225/356: Loss=0.6927 (C:0.6927, R:0.0105)
Batch 250/356: Loss=0.6490 (C:0.6490, R:0.0105)
Batch 275/356: Loss=0.6767 (C:0.6767, R:0.0106)
Batch 300/356: Loss=0.6202 (C:0.6202, R:0.0105)
Batch 325/356: Loss=0.6209 (C:0.6209, R:0.0105)
Batch 350/356: Loss=0.6736 (C:0.6736, R:0.0105)

============================================================
Epoch 58/300 completed in 26.8s
Train: Loss=0.6460 (C:0.6460, R:0.0105) Ratio=4.69x
Val:   Loss=0.8243 (C:0.8243, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/356: Loss=0.6650 (C:0.6650, R:0.0105)
Batch  25/356: Loss=0.6383 (C:0.6383, R:0.0105)
Batch  50/356: Loss=0.6357 (C:0.6357, R:0.0105)
Batch  75/356: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 100/356: Loss=0.6430 (C:0.6430, R:0.0105)
Batch 125/356: Loss=0.6447 (C:0.6447, R:0.0105)
Batch 150/356: Loss=0.6426 (C:0.6426, R:0.0105)
Batch 175/356: Loss=0.6391 (C:0.6391, R:0.0105)
Batch 200/356: Loss=0.6424 (C:0.6424, R:0.0105)
Batch 225/356: Loss=0.6555 (C:0.6555, R:0.0105)
Batch 250/356: Loss=0.6391 (C:0.6391, R:0.0105)
Batch 275/356: Loss=0.6582 (C:0.6582, R:0.0105)
Batch 300/356: Loss=0.6454 (C:0.6454, R:0.0105)
Batch 325/356: Loss=0.6623 (C:0.6623, R:0.0105)
Batch 350/356: Loss=0.6398 (C:0.6398, R:0.0105)

============================================================
Epoch 59/300 completed in 21.2s
Train: Loss=0.6411 (C:0.6411, R:0.0105) Ratio=4.78x
Val:   Loss=0.8289 (C:0.8289, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/356: Loss=0.6181 (C:0.6181, R:0.0105)
Batch  25/356: Loss=0.6342 (C:0.6342, R:0.0105)
Batch  50/356: Loss=0.6001 (C:0.6001, R:0.0105)
Batch  75/356: Loss=0.6403 (C:0.6403, R:0.0105)
Batch 100/356: Loss=0.6633 (C:0.6633, R:0.0105)
Batch 125/356: Loss=0.6579 (C:0.6579, R:0.0105)
Batch 150/356: Loss=0.6269 (C:0.6269, R:0.0105)
Batch 175/356: Loss=0.6798 (C:0.6798, R:0.0105)
Batch 200/356: Loss=0.6267 (C:0.6267, R:0.0105)
Batch 225/356: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 250/356: Loss=0.6524 (C:0.6524, R:0.0105)
Batch 275/356: Loss=0.6393 (C:0.6393, R:0.0105)
Batch 300/356: Loss=0.6410 (C:0.6410, R:0.0105)
Batch 325/356: Loss=0.6624 (C:0.6624, R:0.0105)
Batch 350/356: Loss=0.6451 (C:0.6451, R:0.0105)

============================================================
Epoch 60/300 completed in 21.1s
Train: Loss=0.6412 (C:0.6412, R:0.0105) Ratio=4.81x
Val:   Loss=0.8425 (C:0.8425, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 6 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.354 ± 0.592
    Neg distances: 2.598 ± 1.096
    Separation ratio: 7.34x
    Gap: -4.435
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch  25/356: Loss=0.6355 (C:0.6355, R:0.0105)
Batch  50/356: Loss=0.6442 (C:0.6442, R:0.0105)
Batch  75/356: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 100/356: Loss=0.6175 (C:0.6175, R:0.0105)
Batch 125/356: Loss=0.6226 (C:0.6226, R:0.0105)
Batch 150/356: Loss=0.6132 (C:0.6132, R:0.0105)
Batch 175/356: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 200/356: Loss=0.6149 (C:0.6149, R:0.0105)
Batch 225/356: Loss=0.5832 (C:0.5832, R:0.0105)
Batch 250/356: Loss=0.6223 (C:0.6223, R:0.0105)
Batch 275/356: Loss=0.6255 (C:0.6255, R:0.0105)
Batch 300/356: Loss=0.6273 (C:0.6273, R:0.0105)
Batch 325/356: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 350/356: Loss=0.6593 (C:0.6593, R:0.0105)

============================================================
Epoch 61/300 completed in 27.1s
Train: Loss=0.6226 (C:0.6226, R:0.0105) Ratio=4.90x
Val:   Loss=0.8150 (C:0.8150, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8150)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/356: Loss=0.6064 (C:0.6064, R:0.0105)
Batch  25/356: Loss=0.6231 (C:0.6231, R:0.0105)
Batch  50/356: Loss=0.6021 (C:0.6021, R:0.0105)
Batch  75/356: Loss=0.5940 (C:0.5940, R:0.0105)
Batch 100/356: Loss=0.6271 (C:0.6271, R:0.0105)
Batch 125/356: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 150/356: Loss=0.6404 (C:0.6404, R:0.0105)
Batch 175/356: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 200/356: Loss=0.6068 (C:0.6068, R:0.0105)
Batch 225/356: Loss=0.6036 (C:0.6036, R:0.0105)
Batch 250/356: Loss=0.6256 (C:0.6256, R:0.0105)
Batch 275/356: Loss=0.6457 (C:0.6457, R:0.0105)
Batch 300/356: Loss=0.6112 (C:0.6112, R:0.0105)
Batch 325/356: Loss=0.5898 (C:0.5898, R:0.0105)
Batch 350/356: Loss=0.6450 (C:0.6450, R:0.0105)

============================================================
Epoch 62/300 completed in 20.3s
Train: Loss=0.6210 (C:0.6210, R:0.0105) Ratio=4.84x
Val:   Loss=0.8213 (C:0.8213, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/356: Loss=0.6314 (C:0.6314, R:0.0105)
Batch  25/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch  50/356: Loss=0.6239 (C:0.6239, R:0.0105)
Batch  75/356: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 100/356: Loss=0.6122 (C:0.6122, R:0.0105)
Batch 125/356: Loss=0.6216 (C:0.6216, R:0.0105)
Batch 150/356: Loss=0.6211 (C:0.6211, R:0.0106)
Batch 175/356: Loss=0.6501 (C:0.6501, R:0.0105)
Batch 200/356: Loss=0.6236 (C:0.6236, R:0.0105)
Batch 225/356: Loss=0.6026 (C:0.6026, R:0.0105)
Batch 250/356: Loss=0.6160 (C:0.6160, R:0.0105)
Batch 275/356: Loss=0.6614 (C:0.6614, R:0.0105)
Batch 300/356: Loss=0.6317 (C:0.6317, R:0.0105)
Batch 325/356: Loss=0.6316 (C:0.6316, R:0.0105)
Batch 350/356: Loss=0.6158 (C:0.6158, R:0.0105)

============================================================
Epoch 63/300 completed in 20.4s
Train: Loss=0.6197 (C:0.6197, R:0.0105) Ratio=4.83x
Val:   Loss=0.8088 (C:0.8088, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8088)
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.348 ± 0.568
    Neg distances: 2.624 ± 1.108
    Separation ratio: 7.55x
    Gap: -4.438
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/356: Loss=0.6338 (C:0.6338, R:0.0105)
Batch  25/356: Loss=0.6210 (C:0.6210, R:0.0105)
Batch  50/356: Loss=0.6227 (C:0.6227, R:0.0105)
Batch  75/356: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 100/356: Loss=0.6128 (C:0.6128, R:0.0105)
Batch 125/356: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 150/356: Loss=0.6136 (C:0.6136, R:0.0105)
Batch 175/356: Loss=0.6206 (C:0.6206, R:0.0105)
Batch 200/356: Loss=0.6016 (C:0.6016, R:0.0105)
Batch 225/356: Loss=0.6070 (C:0.6070, R:0.0105)
Batch 250/356: Loss=0.6384 (C:0.6384, R:0.0105)
Batch 275/356: Loss=0.6453 (C:0.6453, R:0.0105)
Batch 300/356: Loss=0.6115 (C:0.6115, R:0.0105)
Batch 325/356: Loss=0.6248 (C:0.6248, R:0.0105)
Batch 350/356: Loss=0.6173 (C:0.6173, R:0.0105)

============================================================
Epoch 64/300 completed in 27.2s
Train: Loss=0.6149 (C:0.6149, R:0.0105) Ratio=4.77x
Val:   Loss=0.8182 (C:0.8182, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/356: Loss=0.6054 (C:0.6054, R:0.0105)
Batch  25/356: Loss=0.6004 (C:0.6004, R:0.0105)
Batch  50/356: Loss=0.5944 (C:0.5944, R:0.0105)
Batch  75/356: Loss=0.5679 (C:0.5679, R:0.0106)
Batch 100/356: Loss=0.6302 (C:0.6302, R:0.0105)
Batch 125/356: Loss=0.6135 (C:0.6135, R:0.0105)
Batch 150/356: Loss=0.6453 (C:0.6453, R:0.0105)
Batch 175/356: Loss=0.5978 (C:0.5978, R:0.0105)
Batch 200/356: Loss=0.6029 (C:0.6029, R:0.0105)
Batch 225/356: Loss=0.6075 (C:0.6075, R:0.0105)
Batch 250/356: Loss=0.6664 (C:0.6664, R:0.0105)
Batch 275/356: Loss=0.6301 (C:0.6301, R:0.0105)
Batch 300/356: Loss=0.6495 (C:0.6495, R:0.0105)
Batch 325/356: Loss=0.6141 (C:0.6141, R:0.0105)
Batch 350/356: Loss=0.6375 (C:0.6375, R:0.0105)

============================================================
Epoch 65/300 completed in 21.1s
Train: Loss=0.6117 (C:0.6117, R:0.0105) Ratio=4.85x
Val:   Loss=0.8131 (C:0.8131, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/356: Loss=0.6089 (C:0.6089, R:0.0105)
Batch  25/356: Loss=0.6128 (C:0.6128, R:0.0105)
Batch  50/356: Loss=0.5676 (C:0.5676, R:0.0105)
Batch  75/356: Loss=0.6247 (C:0.6247, R:0.0105)
Batch 100/356: Loss=0.6141 (C:0.6141, R:0.0105)
Batch 125/356: Loss=0.6175 (C:0.6175, R:0.0105)
Batch 150/356: Loss=0.6125 (C:0.6125, R:0.0105)
Batch 175/356: Loss=0.6121 (C:0.6121, R:0.0105)
Batch 200/356: Loss=0.6208 (C:0.6208, R:0.0105)
Batch 225/356: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 250/356: Loss=0.5744 (C:0.5744, R:0.0105)
Batch 275/356: Loss=0.6278 (C:0.6278, R:0.0105)
Batch 300/356: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 325/356: Loss=0.6271 (C:0.6271, R:0.0105)
Batch 350/356: Loss=0.6253 (C:0.6253, R:0.0105)

============================================================
Epoch 66/300 completed in 21.3s
Train: Loss=0.6096 (C:0.6096, R:0.0105) Ratio=4.94x
Val:   Loss=0.8150 (C:0.8150, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.338 ± 0.572
    Neg distances: 2.635 ± 1.095
    Separation ratio: 7.79x
    Gap: -4.496
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/356: Loss=0.5896 (C:0.5896, R:0.0105)
Batch  25/356: Loss=0.5839 (C:0.5839, R:0.0105)
Batch  50/356: Loss=0.5764 (C:0.5764, R:0.0105)
Batch  75/356: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 100/356: Loss=0.6002 (C:0.6002, R:0.0105)
Batch 125/356: Loss=0.5952 (C:0.5952, R:0.0105)
Batch 150/356: Loss=0.5802 (C:0.5802, R:0.0105)
Batch 175/356: Loss=0.5868 (C:0.5868, R:0.0105)
Batch 200/356: Loss=0.5952 (C:0.5952, R:0.0105)
Batch 225/356: Loss=0.6001 (C:0.6001, R:0.0105)
Batch 250/356: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 275/356: Loss=0.5751 (C:0.5751, R:0.0105)
Batch 300/356: Loss=0.6009 (C:0.6009, R:0.0105)
Batch 325/356: Loss=0.6124 (C:0.6124, R:0.0105)
Batch 350/356: Loss=0.5685 (C:0.5685, R:0.0105)

============================================================
Epoch 67/300 completed in 27.4s
Train: Loss=0.5994 (C:0.5994, R:0.0105) Ratio=4.98x
Val:   Loss=0.8091 (C:0.8091, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/356: Loss=0.5737 (C:0.5737, R:0.0105)
Batch  25/356: Loss=0.5867 (C:0.5867, R:0.0105)
Batch  50/356: Loss=0.5793 (C:0.5793, R:0.0105)
Batch  75/356: Loss=0.5663 (C:0.5663, R:0.0105)
Batch 100/356: Loss=0.6000 (C:0.6000, R:0.0105)
Batch 125/356: Loss=0.6076 (C:0.6076, R:0.0105)
Batch 150/356: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 175/356: Loss=0.6281 (C:0.6281, R:0.0105)
Batch 200/356: Loss=0.6036 (C:0.6036, R:0.0105)
Batch 225/356: Loss=0.5567 (C:0.5567, R:0.0105)
Batch 250/356: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 275/356: Loss=0.5933 (C:0.5933, R:0.0105)
Batch 300/356: Loss=0.6418 (C:0.6418, R:0.0105)
Batch 325/356: Loss=0.6078 (C:0.6078, R:0.0105)
Batch 350/356: Loss=0.5869 (C:0.5869, R:0.0105)

============================================================
Epoch 68/300 completed in 20.6s
Train: Loss=0.5971 (C:0.5971, R:0.0105) Ratio=5.01x
Val:   Loss=0.8004 (C:0.8004, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8004)
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/356: Loss=0.5374 (C:0.5374, R:0.0105)
Batch  25/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch  50/356: Loss=0.5962 (C:0.5962, R:0.0105)
Batch  75/356: Loss=0.5948 (C:0.5948, R:0.0105)
Batch 100/356: Loss=0.6110 (C:0.6110, R:0.0106)
Batch 125/356: Loss=0.5896 (C:0.5896, R:0.0105)
Batch 150/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch 175/356: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 200/356: Loss=0.6247 (C:0.6247, R:0.0105)
Batch 225/356: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 250/356: Loss=0.6283 (C:0.6283, R:0.0105)
Batch 275/356: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 300/356: Loss=0.5919 (C:0.5919, R:0.0105)
Batch 325/356: Loss=0.5988 (C:0.5988, R:0.0105)
Batch 350/356: Loss=0.5843 (C:0.5843, R:0.0105)

============================================================
Epoch 69/300 completed in 21.0s
Train: Loss=0.5953 (C:0.5953, R:0.0105) Ratio=5.00x
Val:   Loss=0.8066 (C:0.8066, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.325 ± 0.540
    Neg distances: 2.651 ± 1.100
    Separation ratio: 8.16x
    Gap: -4.633
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/356: Loss=0.5701 (C:0.5701, R:0.0105)
Batch  25/356: Loss=0.5756 (C:0.5756, R:0.0105)
Batch  50/356: Loss=0.5898 (C:0.5898, R:0.0105)
Batch  75/356: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 100/356: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 125/356: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 150/356: Loss=0.5969 (C:0.5969, R:0.0105)
Batch 175/356: Loss=0.6372 (C:0.6372, R:0.0105)
Batch 200/356: Loss=0.5684 (C:0.5684, R:0.0105)
Batch 225/356: Loss=0.6024 (C:0.6024, R:0.0105)
Batch 250/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch 275/356: Loss=0.5677 (C:0.5677, R:0.0105)
Batch 300/356: Loss=0.5922 (C:0.5922, R:0.0105)
Batch 325/356: Loss=0.5965 (C:0.5965, R:0.0105)
Batch 350/356: Loss=0.5732 (C:0.5732, R:0.0105)

============================================================
Epoch 70/300 completed in 26.6s
Train: Loss=0.5866 (C:0.5866, R:0.0105) Ratio=4.99x
Val:   Loss=0.8063 (C:0.8063, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/356: Loss=0.5683 (C:0.5683, R:0.0105)
Batch  25/356: Loss=0.5750 (C:0.5750, R:0.0105)
Batch  50/356: Loss=0.5756 (C:0.5756, R:0.0105)
Batch  75/356: Loss=0.5651 (C:0.5651, R:0.0105)
Batch 100/356: Loss=0.5889 (C:0.5889, R:0.0105)
Batch 125/356: Loss=0.5692 (C:0.5692, R:0.0105)
Batch 150/356: Loss=0.5959 (C:0.5959, R:0.0105)
Batch 175/356: Loss=0.6038 (C:0.6038, R:0.0105)
Batch 200/356: Loss=0.5764 (C:0.5764, R:0.0105)
Batch 225/356: Loss=0.5576 (C:0.5576, R:0.0105)
Batch 250/356: Loss=0.6110 (C:0.6110, R:0.0105)
Batch 275/356: Loss=0.5755 (C:0.5755, R:0.0105)
Batch 300/356: Loss=0.6397 (C:0.6397, R:0.0105)
Batch 325/356: Loss=0.5975 (C:0.5975, R:0.0105)
Batch 350/356: Loss=0.5885 (C:0.5885, R:0.0105)

============================================================
Epoch 71/300 completed in 21.1s
Train: Loss=0.5832 (C:0.5832, R:0.0105) Ratio=5.05x
Val:   Loss=0.8089 (C:0.8089, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/356: Loss=0.5951 (C:0.5951, R:0.0105)
Batch  25/356: Loss=0.5761 (C:0.5761, R:0.0105)
Batch  50/356: Loss=0.6043 (C:0.6043, R:0.0105)
Batch  75/356: Loss=0.5703 (C:0.5703, R:0.0105)
Batch 100/356: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 125/356: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 150/356: Loss=0.5854 (C:0.5854, R:0.0105)
Batch 175/356: Loss=0.5995 (C:0.5995, R:0.0105)
Batch 200/356: Loss=0.5741 (C:0.5741, R:0.0105)
Batch 225/356: Loss=0.5823 (C:0.5823, R:0.0105)
Batch 250/356: Loss=0.5804 (C:0.5804, R:0.0105)
Batch 275/356: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 300/356: Loss=0.6029 (C:0.6029, R:0.0105)
Batch 325/356: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 350/356: Loss=0.6180 (C:0.6180, R:0.0105)

============================================================
Epoch 72/300 completed in 20.9s
Train: Loss=0.5843 (C:0.5843, R:0.0105) Ratio=5.05x
Val:   Loss=0.8028 (C:0.8028, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.307 ± 0.540
    Neg distances: 2.685 ± 1.098
    Separation ratio: 8.73x
    Gap: -4.530
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/356: Loss=0.5663 (C:0.5663, R:0.0105)
Batch  25/356: Loss=0.5469 (C:0.5469, R:0.0105)
Batch  50/356: Loss=0.5731 (C:0.5731, R:0.0105)
Batch  75/356: Loss=0.5677 (C:0.5677, R:0.0105)
Batch 100/356: Loss=0.5555 (C:0.5555, R:0.0105)
Batch 125/356: Loss=0.5848 (C:0.5848, R:0.0105)
Batch 150/356: Loss=0.5724 (C:0.5724, R:0.0105)
Batch 175/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 200/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 225/356: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 250/356: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 275/356: Loss=0.5587 (C:0.5587, R:0.0105)
Batch 300/356: Loss=0.5840 (C:0.5840, R:0.0105)
Batch 325/356: Loss=0.5981 (C:0.5981, R:0.0105)
Batch 350/356: Loss=0.5599 (C:0.5599, R:0.0105)

============================================================
Epoch 73/300 completed in 27.1s
Train: Loss=0.5677 (C:0.5677, R:0.0105) Ratio=5.04x
Val:   Loss=0.7869 (C:0.7869, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7869)
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/356: Loss=0.5589 (C:0.5589, R:0.0105)
Batch  25/356: Loss=0.5796 (C:0.5796, R:0.0105)
Batch  50/356: Loss=0.5354 (C:0.5354, R:0.0105)
Batch  75/356: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 100/356: Loss=0.5512 (C:0.5512, R:0.0105)
Batch 125/356: Loss=0.5704 (C:0.5704, R:0.0105)
Batch 150/356: Loss=0.5661 (C:0.5661, R:0.0105)
Batch 175/356: Loss=0.5865 (C:0.5865, R:0.0105)
Batch 200/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 225/356: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 250/356: Loss=0.5595 (C:0.5595, R:0.0105)
Batch 275/356: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 300/356: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 325/356: Loss=0.5595 (C:0.5595, R:0.0105)
Batch 350/356: Loss=0.6045 (C:0.6045, R:0.0105)

============================================================
Epoch 74/300 completed in 20.5s
Train: Loss=0.5677 (C:0.5677, R:0.0105) Ratio=5.02x
Val:   Loss=0.7987 (C:0.7987, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/356: Loss=0.5600 (C:0.5600, R:0.0105)
Batch  25/356: Loss=0.5676 (C:0.5676, R:0.0105)
Batch  50/356: Loss=0.5431 (C:0.5431, R:0.0105)
Batch  75/356: Loss=0.5749 (C:0.5749, R:0.0105)
Batch 100/356: Loss=0.5713 (C:0.5713, R:0.0105)
Batch 125/356: Loss=0.5496 (C:0.5496, R:0.0105)
Batch 150/356: Loss=0.5737 (C:0.5737, R:0.0105)
Batch 175/356: Loss=0.5759 (C:0.5759, R:0.0105)
Batch 200/356: Loss=0.5898 (C:0.5898, R:0.0105)
Batch 225/356: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 250/356: Loss=0.5713 (C:0.5713, R:0.0105)
Batch 275/356: Loss=0.5455 (C:0.5455, R:0.0105)
Batch 300/356: Loss=0.5941 (C:0.5941, R:0.0105)
Batch 325/356: Loss=0.5830 (C:0.5830, R:0.0105)
Batch 350/356: Loss=0.5607 (C:0.5607, R:0.0105)

============================================================
Epoch 75/300 completed in 21.2s
Train: Loss=0.5651 (C:0.5651, R:0.0105) Ratio=5.11x
Val:   Loss=0.7937 (C:0.7937, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.298 ± 0.557
    Neg distances: 2.701 ± 1.097
    Separation ratio: 9.06x
    Gap: -4.604
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/356: Loss=0.5604 (C:0.5604, R:0.0105)
Batch  25/356: Loss=0.5564 (C:0.5564, R:0.0105)
Batch  50/356: Loss=0.5473 (C:0.5473, R:0.0105)
Batch  75/356: Loss=0.5738 (C:0.5738, R:0.0105)
Batch 100/356: Loss=0.5394 (C:0.5394, R:0.0105)
Batch 125/356: Loss=0.5564 (C:0.5564, R:0.0105)
Batch 150/356: Loss=0.5727 (C:0.5727, R:0.0105)
Batch 175/356: Loss=0.5478 (C:0.5478, R:0.0105)
Batch 200/356: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 225/356: Loss=0.5712 (C:0.5712, R:0.0105)
Batch 250/356: Loss=0.5570 (C:0.5570, R:0.0105)
Batch 275/356: Loss=0.5573 (C:0.5573, R:0.0106)
Batch 300/356: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 325/356: Loss=0.5422 (C:0.5422, R:0.0105)
Batch 350/356: Loss=0.5476 (C:0.5476, R:0.0105)

============================================================
Epoch 76/300 completed in 27.3s
Train: Loss=0.5563 (C:0.5563, R:0.0105) Ratio=5.10x
Val:   Loss=0.7770 (C:0.7770, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7770)
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/356: Loss=0.5329 (C:0.5329, R:0.0105)
Batch  25/356: Loss=0.5557 (C:0.5557, R:0.0105)
Batch  50/356: Loss=0.5601 (C:0.5601, R:0.0105)
Batch  75/356: Loss=0.5111 (C:0.5111, R:0.0105)
Batch 100/356: Loss=0.5661 (C:0.5661, R:0.0105)
Batch 125/356: Loss=0.5436 (C:0.5436, R:0.0105)
Batch 150/356: Loss=0.5840 (C:0.5840, R:0.0105)
Batch 175/356: Loss=0.5760 (C:0.5760, R:0.0105)
Batch 200/356: Loss=0.5369 (C:0.5369, R:0.0105)
Batch 225/356: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 250/356: Loss=0.5465 (C:0.5465, R:0.0105)
Batch 275/356: Loss=0.5824 (C:0.5824, R:0.0105)
Batch 300/356: Loss=0.5720 (C:0.5720, R:0.0105)
Batch 325/356: Loss=0.5650 (C:0.5650, R:0.0105)
Batch 350/356: Loss=0.5523 (C:0.5523, R:0.0105)

============================================================
Epoch 77/300 completed in 21.2s
Train: Loss=0.5551 (C:0.5551, R:0.0105) Ratio=5.13x
Val:   Loss=0.7832 (C:0.7832, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/356: Loss=0.5264 (C:0.5264, R:0.0105)
Batch  25/356: Loss=0.5363 (C:0.5363, R:0.0105)
Batch  50/356: Loss=0.5405 (C:0.5405, R:0.0105)
Batch  75/356: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 100/356: Loss=0.5271 (C:0.5271, R:0.0105)
Batch 125/356: Loss=0.5854 (C:0.5854, R:0.0105)
Batch 150/356: Loss=0.5750 (C:0.5750, R:0.0105)
Batch 175/356: Loss=0.5387 (C:0.5387, R:0.0105)
Batch 200/356: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 225/356: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 250/356: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 275/356: Loss=0.5556 (C:0.5556, R:0.0105)
Batch 300/356: Loss=0.5816 (C:0.5816, R:0.0105)
Batch 325/356: Loss=0.5623 (C:0.5623, R:0.0105)
Batch 350/356: Loss=0.5606 (C:0.5606, R:0.0105)

============================================================
Epoch 78/300 completed in 21.4s
Train: Loss=0.5541 (C:0.5541, R:0.0105) Ratio=5.23x
Val:   Loss=0.7861 (C:0.7861, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.297 ± 0.550
    Neg distances: 2.729 ± 1.108
    Separation ratio: 9.19x
    Gap: -4.606
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/356: Loss=0.5164 (C:0.5164, R:0.0105)
Batch  25/356: Loss=0.5129 (C:0.5129, R:0.0105)
Batch  50/356: Loss=0.5623 (C:0.5623, R:0.0105)
Batch  75/356: Loss=0.5411 (C:0.5411, R:0.0105)
Batch 100/356: Loss=0.5499 (C:0.5499, R:0.0105)
Batch 125/356: Loss=0.5568 (C:0.5568, R:0.0105)
Batch 150/356: Loss=0.5498 (C:0.5498, R:0.0105)
Batch 175/356: Loss=0.5434 (C:0.5434, R:0.0105)
Batch 200/356: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 225/356: Loss=0.5504 (C:0.5504, R:0.0105)
Batch 250/356: Loss=0.5549 (C:0.5549, R:0.0105)
Batch 275/356: Loss=0.5528 (C:0.5528, R:0.0105)
Batch 300/356: Loss=0.5573 (C:0.5573, R:0.0105)
Batch 325/356: Loss=0.5363 (C:0.5363, R:0.0105)
Batch 350/356: Loss=0.5982 (C:0.5982, R:0.0105)

============================================================
Epoch 79/300 completed in 27.6s
Train: Loss=0.5496 (C:0.5496, R:0.0105) Ratio=5.20x
Val:   Loss=0.7884 (C:0.7884, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/356: Loss=0.5530 (C:0.5530, R:0.0105)
Batch  25/356: Loss=0.5618 (C:0.5618, R:0.0106)
Batch  50/356: Loss=0.5383 (C:0.5383, R:0.0105)
Batch  75/356: Loss=0.5522 (C:0.5522, R:0.0105)
Batch 100/356: Loss=0.5345 (C:0.5345, R:0.0105)
Batch 125/356: Loss=0.5688 (C:0.5688, R:0.0105)
Batch 150/356: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 175/356: Loss=0.5649 (C:0.5649, R:0.0105)
Batch 200/356: Loss=0.5701 (C:0.5701, R:0.0105)
Batch 225/356: Loss=0.5443 (C:0.5443, R:0.0105)
Batch 250/356: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 275/356: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 300/356: Loss=0.5569 (C:0.5569, R:0.0105)
Batch 325/356: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 350/356: Loss=0.5733 (C:0.5733, R:0.0105)

============================================================
Epoch 80/300 completed in 20.8s
Train: Loss=0.5487 (C:0.5487, R:0.0105) Ratio=5.18x
Val:   Loss=0.7786 (C:0.7786, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 4 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/356: Loss=0.5152 (C:0.5152, R:0.0105)
Batch  25/356: Loss=0.5395 (C:0.5395, R:0.0106)
Batch  50/356: Loss=0.5952 (C:0.5952, R:0.0105)
Batch  75/356: Loss=0.5556 (C:0.5556, R:0.0105)
Batch 100/356: Loss=0.5330 (C:0.5330, R:0.0105)
Batch 125/356: Loss=0.5593 (C:0.5593, R:0.0105)
Batch 150/356: Loss=0.5246 (C:0.5246, R:0.0105)
Batch 175/356: Loss=0.5369 (C:0.5369, R:0.0105)
Batch 200/356: Loss=0.5413 (C:0.5413, R:0.0105)
Batch 225/356: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 250/356: Loss=0.5222 (C:0.5222, R:0.0105)
Batch 275/356: Loss=0.5456 (C:0.5456, R:0.0105)
Batch 300/356: Loss=0.5523 (C:0.5523, R:0.0105)
Batch 325/356: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 350/356: Loss=0.5437 (C:0.5437, R:0.0105)

============================================================
Epoch 81/300 completed in 21.3s
Train: Loss=0.5473 (C:0.5473, R:0.0105) Ratio=5.29x
Val:   Loss=0.7813 (C:0.7813, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.308 ± 0.580
    Neg distances: 2.714 ± 1.106
    Separation ratio: 8.82x
    Gap: -4.626
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/356: Loss=0.5246 (C:0.5246, R:0.0105)
Batch  25/356: Loss=0.5499 (C:0.5499, R:0.0105)
Batch  50/356: Loss=0.5226 (C:0.5226, R:0.0105)
Batch  75/356: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 100/356: Loss=0.5639 (C:0.5639, R:0.0105)
Batch 125/356: Loss=0.5670 (C:0.5670, R:0.0105)
Batch 150/356: Loss=0.5753 (C:0.5753, R:0.0105)
Batch 175/356: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 200/356: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 225/356: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 250/356: Loss=0.5497 (C:0.5497, R:0.0105)
Batch 275/356: Loss=0.5235 (C:0.5235, R:0.0105)
Batch 300/356: Loss=0.5915 (C:0.5915, R:0.0105)
Batch 325/356: Loss=0.5338 (C:0.5338, R:0.0105)
Batch 350/356: Loss=0.5610 (C:0.5610, R:0.0105)

============================================================
Epoch 82/300 completed in 27.4s
Train: Loss=0.5533 (C:0.5533, R:0.0105) Ratio=5.31x
Val:   Loss=0.7882 (C:0.7882, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/356: Loss=0.5561 (C:0.5561, R:0.0105)
Batch  25/356: Loss=0.5310 (C:0.5310, R:0.0105)
Batch  50/356: Loss=0.5456 (C:0.5456, R:0.0105)
Batch  75/356: Loss=0.5585 (C:0.5585, R:0.0105)
Batch 100/356: Loss=0.5433 (C:0.5433, R:0.0105)
Batch 125/356: Loss=0.5265 (C:0.5265, R:0.0105)
Batch 150/356: Loss=0.5113 (C:0.5113, R:0.0105)
Batch 175/356: Loss=0.5714 (C:0.5714, R:0.0105)
Batch 200/356: Loss=0.5358 (C:0.5358, R:0.0105)
Batch 225/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch 250/356: Loss=0.5626 (C:0.5626, R:0.0105)
Batch 275/356: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 300/356: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 325/356: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 350/356: Loss=0.5704 (C:0.5704, R:0.0105)

============================================================
Epoch 83/300 completed in 21.1s
Train: Loss=0.5510 (C:0.5510, R:0.0105) Ratio=5.31x
Val:   Loss=0.7891 (C:0.7891, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/356: Loss=0.5553 (C:0.5553, R:0.0105)
Batch  25/356: Loss=0.5465 (C:0.5465, R:0.0105)
Batch  50/356: Loss=0.5500 (C:0.5500, R:0.0105)
Batch  75/356: Loss=0.5242 (C:0.5242, R:0.0105)
Batch 100/356: Loss=0.5627 (C:0.5627, R:0.0105)
Batch 125/356: Loss=0.5513 (C:0.5513, R:0.0105)
Batch 150/356: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 175/356: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 200/356: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 225/356: Loss=0.5716 (C:0.5716, R:0.0105)
Batch 250/356: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 275/356: Loss=0.5479 (C:0.5479, R:0.0105)
Batch 300/356: Loss=0.5717 (C:0.5717, R:0.0105)
Batch 325/356: Loss=0.5596 (C:0.5596, R:0.0105)
Batch 350/356: Loss=0.5485 (C:0.5485, R:0.0105)

============================================================
Epoch 84/300 completed in 21.2s
Train: Loss=0.5510 (C:0.5510, R:0.0105) Ratio=5.27x
Val:   Loss=0.7913 (C:0.7913, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 84 epochs
Best model was at epoch 76 with Val Loss: 0.7770

Global Dataset Training Completed!
Best epoch: 76
Best validation loss: 0.7770
Final separation ratios: Train=5.27x, Val=2.99x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/7 batches
Extracted representations: torch.Size([9824, 50])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4583
  Adjusted Rand Score: 0.5301
  Clustering Accuracy: 0.8147
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
Extracted representations: torch.Size([546816, 50])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/6 batches
Extracted representations: torch.Size([9216, 50])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9216 samples
Classification Results:
  Accuracy: 0.8137
  Per-class F1: [0.8339833813803629, 0.7531133250311333, 0.8577974144984454]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.781 ± 0.906
  Negative distances: 2.347 ± 1.246
  Separation ratio: 3.01x
  Gap: -4.660
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4583
  Clustering Accuracy: 0.8147
  Adjusted Rand Score: 0.5301

Classification Performance:
  Accuracy: 0.8137

Separation Quality:
  Separation Ratio: 3.01x
  Gap: -4.660
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909/results/evaluation_results_20250714_172135.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909/results/evaluation_results_20250714_172135.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat50_bs1536_20250714_164909/final_results.json

Key Results:
  Separation ratio: 3.01x
  Perfect separation: False
  Classification accuracy: 0.8137
  Result: 0.8137% (improvement: +-80.86%)
  Cleaning up: coarse_lr1e-04_lat50_bs1536_20250714_164909

[3/12] Testing: coarse_lr1e-04_lat75_bs1020
  Learning rate: 0.0001
  Latent dim: 75
  Batch size: 1020
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 17:21:35.315549
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,876,555
Model created with 1,876,555 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,876,555
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.093 ± 0.011
    Neg distances: 0.093 ± 0.011
    Separation ratio: 1.00x
    Gap: -0.121
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9999 (C:1.9999, R:0.0117)
Batch  25/537: Loss=1.9953 (C:1.9953, R:0.0114)
Batch  50/537: Loss=1.9768 (C:1.9768, R:0.0113)
Batch  75/537: Loss=1.9736 (C:1.9736, R:0.0111)
Batch 100/537: Loss=1.9641 (C:1.9641, R:0.0110)
Batch 125/537: Loss=1.9593 (C:1.9593, R:0.0109)
Batch 150/537: Loss=1.9574 (C:1.9574, R:0.0108)
Batch 175/537: Loss=1.9482 (C:1.9482, R:0.0108)
Batch 200/537: Loss=1.9364 (C:1.9364, R:0.0107)
Batch 225/537: Loss=1.9218 (C:1.9218, R:0.0107)
Batch 250/537: Loss=1.9251 (C:1.9251, R:0.0107)
Batch 275/537: Loss=1.9395 (C:1.9395, R:0.0106)
Batch 300/537: Loss=1.9231 (C:1.9231, R:0.0106)
Batch 325/537: Loss=1.9183 (C:1.9183, R:0.0106)
Batch 350/537: Loss=1.9090 (C:1.9090, R:0.0105)
Batch 375/537: Loss=1.9069 (C:1.9069, R:0.0106)
Batch 400/537: Loss=1.9168 (C:1.9168, R:0.0106)
Batch 425/537: Loss=1.9131 (C:1.9131, R:0.0106)
Batch 450/537: Loss=1.9124 (C:1.9124, R:0.0105)
Batch 475/537: Loss=1.9012 (C:1.9012, R:0.0105)
Batch 500/537: Loss=1.8894 (C:1.8894, R:0.0105)
Batch 525/537: Loss=1.9053 (C:1.9053, R:0.0105)

============================================================
Epoch 1/300 completed in 27.8s
Train: Loss=1.9332 (C:1.9332, R:0.0108) Ratio=1.63x
Val:   Loss=1.8915 (C:1.8915, R:0.0105) Ratio=2.17x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8915)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.9058 (C:1.9058, R:0.0105)
Batch  25/537: Loss=1.8925 (C:1.8925, R:0.0105)
Batch  50/537: Loss=1.8918 (C:1.8918, R:0.0106)
Batch  75/537: Loss=1.8978 (C:1.8978, R:0.0105)
Batch 100/537: Loss=1.8981 (C:1.8981, R:0.0105)
Batch 125/537: Loss=1.8826 (C:1.8826, R:0.0105)
Batch 150/537: Loss=1.8931 (C:1.8931, R:0.0105)
Batch 175/537: Loss=1.8875 (C:1.8875, R:0.0105)
Batch 200/537: Loss=1.8998 (C:1.8998, R:0.0105)
Batch 225/537: Loss=1.8810 (C:1.8810, R:0.0105)
Batch 250/537: Loss=1.8998 (C:1.8998, R:0.0105)
Batch 275/537: Loss=1.8857 (C:1.8857, R:0.0105)
Batch 300/537: Loss=1.8994 (C:1.8994, R:0.0105)
Batch 325/537: Loss=1.9003 (C:1.9003, R:0.0105)
Batch 350/537: Loss=1.8810 (C:1.8810, R:0.0105)
Batch 375/537: Loss=1.8895 (C:1.8895, R:0.0105)
Batch 400/537: Loss=1.8789 (C:1.8789, R:0.0105)
Batch 425/537: Loss=1.8843 (C:1.8843, R:0.0105)
Batch 450/537: Loss=1.8998 (C:1.8998, R:0.0105)
Batch 475/537: Loss=1.8819 (C:1.8819, R:0.0105)
Batch 500/537: Loss=1.8838 (C:1.8838, R:0.0105)
Batch 525/537: Loss=1.8794 (C:1.8794, R:0.0105)

============================================================
Epoch 2/300 completed in 21.7s
Train: Loss=1.8914 (C:1.8914, R:0.0105) Ratio=2.20x
Val:   Loss=1.8795 (C:1.8795, R:0.0104) Ratio=2.40x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8795)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8758 (C:1.8758, R:0.0105)
Batch  25/537: Loss=1.8849 (C:1.8849, R:0.0105)
Batch  50/537: Loss=1.8792 (C:1.8792, R:0.0105)
Batch  75/537: Loss=1.8864 (C:1.8864, R:0.0105)
Batch 100/537: Loss=1.8868 (C:1.8868, R:0.0105)
Batch 125/537: Loss=1.8788 (C:1.8788, R:0.0105)
Batch 150/537: Loss=1.8923 (C:1.8923, R:0.0105)
Batch 175/537: Loss=1.8817 (C:1.8817, R:0.0105)
Batch 200/537: Loss=1.8818 (C:1.8818, R:0.0105)
Batch 225/537: Loss=1.8708 (C:1.8708, R:0.0105)
Batch 250/537: Loss=1.8819 (C:1.8819, R:0.0106)
Batch 275/537: Loss=1.8760 (C:1.8760, R:0.0105)
Batch 300/537: Loss=1.8804 (C:1.8804, R:0.0105)
Batch 325/537: Loss=1.8717 (C:1.8717, R:0.0105)
Batch 350/537: Loss=1.8796 (C:1.8796, R:0.0105)
Batch 375/537: Loss=1.8785 (C:1.8785, R:0.0105)
Batch 400/537: Loss=1.8796 (C:1.8796, R:0.0105)
Batch 425/537: Loss=1.8811 (C:1.8811, R:0.0105)
Batch 450/537: Loss=1.8849 (C:1.8849, R:0.0105)
Batch 475/537: Loss=1.8843 (C:1.8843, R:0.0105)
Batch 500/537: Loss=1.8759 (C:1.8759, R:0.0105)
Batch 525/537: Loss=1.8868 (C:1.8868, R:0.0105)

============================================================
Epoch 3/300 completed in 20.9s
Train: Loss=1.8794 (C:1.8794, R:0.0105) Ratio=2.38x
Val:   Loss=1.8731 (C:1.8731, R:0.0104) Ratio=2.56x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8731)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.561 ± 0.569
    Neg distances: 1.558 ± 0.866
    Separation ratio: 2.78x
    Gap: -3.257
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.2081 (C:1.2081, R:0.0106)
Batch  25/537: Loss=1.2318 (C:1.2318, R:0.0105)
Batch  50/537: Loss=1.2106 (C:1.2106, R:0.0105)
Batch  75/537: Loss=1.2300 (C:1.2300, R:0.0105)
Batch 100/537: Loss=1.2285 (C:1.2285, R:0.0105)
Batch 125/537: Loss=1.2058 (C:1.2058, R:0.0105)
Batch 150/537: Loss=1.2348 (C:1.2348, R:0.0105)
Batch 175/537: Loss=1.1769 (C:1.1769, R:0.0105)
Batch 200/537: Loss=1.2027 (C:1.2027, R:0.0105)
Batch 225/537: Loss=1.2094 (C:1.2094, R:0.0105)
Batch 250/537: Loss=1.2288 (C:1.2288, R:0.0105)
Batch 275/537: Loss=1.2388 (C:1.2388, R:0.0105)
Batch 300/537: Loss=1.2276 (C:1.2276, R:0.0105)
Batch 325/537: Loss=1.1822 (C:1.1822, R:0.0105)
Batch 350/537: Loss=1.2127 (C:1.2127, R:0.0105)
Batch 375/537: Loss=1.2125 (C:1.2125, R:0.0105)
Batch 400/537: Loss=1.1945 (C:1.1945, R:0.0105)
Batch 425/537: Loss=1.2237 (C:1.2237, R:0.0105)
Batch 450/537: Loss=1.2113 (C:1.2113, R:0.0106)
Batch 475/537: Loss=1.2153 (C:1.2153, R:0.0105)
Batch 500/537: Loss=1.1928 (C:1.1928, R:0.0106)
Batch 525/537: Loss=1.2224 (C:1.2224, R:0.0105)

============================================================
Epoch 4/300 completed in 26.7s
Train: Loss=1.2115 (C:1.2115, R:0.0105) Ratio=2.55x
Val:   Loss=1.1972 (C:1.1972, R:0.0104) Ratio=2.66x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1972)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.1681 (C:1.1681, R:0.0105)
Batch  25/537: Loss=1.1937 (C:1.1937, R:0.0105)
Batch  50/537: Loss=1.1798 (C:1.1798, R:0.0105)
Batch  75/537: Loss=1.1795 (C:1.1795, R:0.0105)
Batch 100/537: Loss=1.2063 (C:1.2063, R:0.0105)
Batch 125/537: Loss=1.2004 (C:1.2004, R:0.0105)
Batch 150/537: Loss=1.1832 (C:1.1832, R:0.0105)
Batch 175/537: Loss=1.2114 (C:1.2114, R:0.0105)
Batch 200/537: Loss=1.1780 (C:1.1780, R:0.0105)
Batch 225/537: Loss=1.1829 (C:1.1829, R:0.0105)
Batch 250/537: Loss=1.1678 (C:1.1678, R:0.0105)
Batch 275/537: Loss=1.2131 (C:1.2131, R:0.0105)
Batch 300/537: Loss=1.1897 (C:1.1897, R:0.0105)
Batch 325/537: Loss=1.2055 (C:1.2055, R:0.0105)
Batch 350/537: Loss=1.1904 (C:1.1904, R:0.0105)
Batch 375/537: Loss=1.1652 (C:1.1652, R:0.0105)
Batch 400/537: Loss=1.2083 (C:1.2083, R:0.0105)
Batch 425/537: Loss=1.1748 (C:1.1748, R:0.0105)
Batch 450/537: Loss=1.1740 (C:1.1740, R:0.0105)
Batch 475/537: Loss=1.1844 (C:1.1844, R:0.0105)
Batch 500/537: Loss=1.2223 (C:1.2223, R:0.0105)
Batch 525/537: Loss=1.1681 (C:1.1681, R:0.0105)

============================================================
Epoch 5/300 completed in 21.0s
Train: Loss=1.1883 (C:1.1883, R:0.0105) Ratio=2.76x
Val:   Loss=1.1805 (C:1.1805, R:0.0104) Ratio=2.77x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1805)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.1542 (C:1.1542, R:0.0105)
Batch  25/537: Loss=1.1622 (C:1.1622, R:0.0105)
Batch  50/537: Loss=1.1835 (C:1.1835, R:0.0105)
Batch  75/537: Loss=1.1702 (C:1.1702, R:0.0105)
Batch 100/537: Loss=1.1708 (C:1.1708, R:0.0105)
Batch 125/537: Loss=1.1691 (C:1.1691, R:0.0105)
Batch 150/537: Loss=1.1887 (C:1.1887, R:0.0105)
Batch 175/537: Loss=1.1662 (C:1.1662, R:0.0105)
Batch 200/537: Loss=1.1649 (C:1.1649, R:0.0105)
Batch 225/537: Loss=1.1750 (C:1.1750, R:0.0105)
Batch 250/537: Loss=1.1751 (C:1.1751, R:0.0105)
Batch 275/537: Loss=1.1848 (C:1.1848, R:0.0106)
Batch 300/537: Loss=1.1989 (C:1.1989, R:0.0105)
Batch 325/537: Loss=1.1568 (C:1.1568, R:0.0105)
Batch 350/537: Loss=1.1802 (C:1.1802, R:0.0105)
Batch 375/537: Loss=1.1744 (C:1.1744, R:0.0105)
Batch 400/537: Loss=1.1463 (C:1.1463, R:0.0105)
Batch 425/537: Loss=1.1446 (C:1.1446, R:0.0105)
Batch 450/537: Loss=1.1741 (C:1.1741, R:0.0105)
Batch 475/537: Loss=1.1130 (C:1.1130, R:0.0105)
Batch 500/537: Loss=1.1802 (C:1.1802, R:0.0105)
Batch 525/537: Loss=1.1691 (C:1.1691, R:0.0105)

============================================================
Epoch 6/300 completed in 21.8s
Train: Loss=1.1748 (C:1.1748, R:0.0105) Ratio=2.96x
Val:   Loss=1.1811 (C:1.1811, R:0.0104) Ratio=2.83x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.480 ± 0.561
    Neg distances: 1.669 ± 0.867
    Separation ratio: 3.48x
    Gap: -3.142
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.0694 (C:1.0694, R:0.0105)
Batch  25/537: Loss=1.0663 (C:1.0663, R:0.0105)
Batch  50/537: Loss=1.1059 (C:1.1059, R:0.0105)
Batch  75/537: Loss=1.1164 (C:1.1164, R:0.0105)
Batch 100/537: Loss=1.0610 (C:1.0610, R:0.0105)
Batch 125/537: Loss=1.0418 (C:1.0418, R:0.0105)
Batch 150/537: Loss=1.0899 (C:1.0899, R:0.0105)
Batch 175/537: Loss=1.0796 (C:1.0796, R:0.0105)
Batch 200/537: Loss=1.0632 (C:1.0632, R:0.0105)
Batch 225/537: Loss=1.0833 (C:1.0833, R:0.0105)
Batch 250/537: Loss=1.0783 (C:1.0783, R:0.0105)
Batch 275/537: Loss=1.0893 (C:1.0893, R:0.0105)
Batch 300/537: Loss=1.0654 (C:1.0654, R:0.0106)
Batch 325/537: Loss=1.1062 (C:1.1062, R:0.0105)
Batch 350/537: Loss=1.0920 (C:1.0920, R:0.0105)
Batch 375/537: Loss=1.0705 (C:1.0705, R:0.0105)
Batch 400/537: Loss=1.0821 (C:1.0821, R:0.0105)
Batch 425/537: Loss=1.0692 (C:1.0692, R:0.0106)
Batch 450/537: Loss=1.0788 (C:1.0788, R:0.0105)
Batch 475/537: Loss=1.0702 (C:1.0702, R:0.0105)
Batch 500/537: Loss=1.0295 (C:1.0295, R:0.0105)
Batch 525/537: Loss=1.0585 (C:1.0585, R:0.0105)

============================================================
Epoch 7/300 completed in 27.9s
Train: Loss=1.0791 (C:1.0791, R:0.0105) Ratio=2.99x
Val:   Loss=1.0917 (C:1.0917, R:0.0104) Ratio=2.88x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0917)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.0555 (C:1.0555, R:0.0105)
Batch  25/537: Loss=1.0693 (C:1.0693, R:0.0105)
Batch  50/537: Loss=1.0718 (C:1.0718, R:0.0105)
Batch  75/537: Loss=1.0985 (C:1.0985, R:0.0105)
Batch 100/537: Loss=1.0333 (C:1.0333, R:0.0105)
Batch 125/537: Loss=1.0440 (C:1.0440, R:0.0105)
Batch 150/537: Loss=1.0392 (C:1.0392, R:0.0105)
Batch 175/537: Loss=1.0864 (C:1.0864, R:0.0105)
Batch 200/537: Loss=1.0673 (C:1.0673, R:0.0105)
Batch 225/537: Loss=1.0922 (C:1.0922, R:0.0105)
Batch 250/537: Loss=1.0547 (C:1.0547, R:0.0105)
Batch 275/537: Loss=1.0649 (C:1.0649, R:0.0105)
Batch 300/537: Loss=1.0745 (C:1.0745, R:0.0105)
Batch 325/537: Loss=1.0849 (C:1.0849, R:0.0105)
Batch 350/537: Loss=1.0965 (C:1.0965, R:0.0105)
Batch 375/537: Loss=1.0602 (C:1.0602, R:0.0105)
Batch 400/537: Loss=1.0953 (C:1.0953, R:0.0105)
Batch 425/537: Loss=1.0541 (C:1.0541, R:0.0105)
Batch 450/537: Loss=1.0249 (C:1.0249, R:0.0105)
Batch 475/537: Loss=1.0425 (C:1.0425, R:0.0105)
Batch 500/537: Loss=1.0744 (C:1.0744, R:0.0105)
Batch 525/537: Loss=1.1174 (C:1.1174, R:0.0105)

============================================================
Epoch 8/300 completed in 22.1s
Train: Loss=1.0681 (C:1.0681, R:0.0105) Ratio=3.06x
Val:   Loss=1.0907 (C:1.0907, R:0.0104) Ratio=2.89x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0907)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0540 (C:1.0540, R:0.0105)
Batch  25/537: Loss=1.0576 (C:1.0576, R:0.0105)
Batch  50/537: Loss=1.0384 (C:1.0384, R:0.0105)
Batch  75/537: Loss=1.0506 (C:1.0506, R:0.0105)
Batch 100/537: Loss=1.0507 (C:1.0507, R:0.0105)
Batch 125/537: Loss=1.0366 (C:1.0366, R:0.0105)
Batch 150/537: Loss=1.0539 (C:1.0539, R:0.0105)
Batch 175/537: Loss=1.0971 (C:1.0971, R:0.0105)
Batch 200/537: Loss=1.0756 (C:1.0756, R:0.0105)
Batch 225/537: Loss=1.0398 (C:1.0398, R:0.0105)
Batch 250/537: Loss=1.0394 (C:1.0394, R:0.0105)
Batch 275/537: Loss=1.0598 (C:1.0598, R:0.0105)
Batch 300/537: Loss=1.0674 (C:1.0674, R:0.0105)
Batch 325/537: Loss=1.0638 (C:1.0638, R:0.0105)
Batch 350/537: Loss=1.0729 (C:1.0729, R:0.0105)
Batch 375/537: Loss=1.0748 (C:1.0748, R:0.0105)
Batch 400/537: Loss=1.0779 (C:1.0779, R:0.0105)
Batch 425/537: Loss=1.0529 (C:1.0529, R:0.0105)
Batch 450/537: Loss=1.0436 (C:1.0436, R:0.0106)
Batch 475/537: Loss=1.0646 (C:1.0646, R:0.0105)
Batch 500/537: Loss=1.0738 (C:1.0738, R:0.0105)
Batch 525/537: Loss=1.0682 (C:1.0682, R:0.0105)

============================================================
Epoch 9/300 completed in 22.2s
Train: Loss=1.0618 (C:1.0618, R:0.0105) Ratio=3.18x
Val:   Loss=1.0749 (C:1.0749, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0749)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.449 ± 0.555
    Neg distances: 1.709 ± 0.861
    Separation ratio: 3.81x
    Gap: -3.111
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.0262 (C:1.0262, R:0.0105)
Batch  25/537: Loss=0.9898 (C:0.9898, R:0.0105)
Batch  50/537: Loss=1.0321 (C:1.0321, R:0.0105)
Batch  75/537: Loss=0.9991 (C:0.9991, R:0.0105)
Batch 100/537: Loss=1.0083 (C:1.0083, R:0.0105)
Batch 125/537: Loss=1.0222 (C:1.0222, R:0.0105)
Batch 150/537: Loss=1.0132 (C:1.0132, R:0.0105)
Batch 175/537: Loss=0.9982 (C:0.9982, R:0.0105)
Batch 200/537: Loss=1.0293 (C:1.0293, R:0.0105)
Batch 225/537: Loss=1.0205 (C:1.0205, R:0.0105)
Batch 250/537: Loss=1.0251 (C:1.0251, R:0.0105)
Batch 275/537: Loss=1.0496 (C:1.0496, R:0.0105)
Batch 300/537: Loss=1.0346 (C:1.0346, R:0.0105)
Batch 325/537: Loss=0.9770 (C:0.9770, R:0.0106)
Batch 350/537: Loss=1.0045 (C:1.0045, R:0.0105)
Batch 375/537: Loss=1.0393 (C:1.0393, R:0.0105)
Batch 400/537: Loss=1.0333 (C:1.0333, R:0.0105)
Batch 425/537: Loss=1.0226 (C:1.0226, R:0.0106)
Batch 450/537: Loss=1.0278 (C:1.0278, R:0.0105)
Batch 475/537: Loss=1.0409 (C:1.0409, R:0.0105)
Batch 500/537: Loss=1.0192 (C:1.0192, R:0.0105)
Batch 525/537: Loss=1.0543 (C:1.0543, R:0.0105)

============================================================
Epoch 10/300 completed in 27.7s
Train: Loss=1.0236 (C:1.0236, R:0.0105) Ratio=3.24x
Val:   Loss=1.0481 (C:1.0481, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0481)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.0115 (C:1.0115, R:0.0105)
Batch  25/537: Loss=1.0317 (C:1.0317, R:0.0105)
Batch  50/537: Loss=1.0696 (C:1.0696, R:0.0105)
Batch  75/537: Loss=0.9970 (C:0.9970, R:0.0105)
Batch 100/537: Loss=1.0125 (C:1.0125, R:0.0105)
Batch 125/537: Loss=1.0083 (C:1.0083, R:0.0105)
Batch 150/537: Loss=1.0107 (C:1.0107, R:0.0106)
Batch 175/537: Loss=1.0223 (C:1.0223, R:0.0105)
Batch 200/537: Loss=1.0304 (C:1.0304, R:0.0105)
Batch 225/537: Loss=1.0492 (C:1.0492, R:0.0105)
Batch 250/537: Loss=1.0442 (C:1.0442, R:0.0105)
Batch 275/537: Loss=1.0310 (C:1.0310, R:0.0105)
Batch 300/537: Loss=1.0200 (C:1.0200, R:0.0105)
Batch 325/537: Loss=1.0064 (C:1.0064, R:0.0105)
Batch 350/537: Loss=1.0098 (C:1.0098, R:0.0105)
Batch 375/537: Loss=1.0293 (C:1.0293, R:0.0105)
Batch 400/537: Loss=1.0106 (C:1.0106, R:0.0105)
Batch 425/537: Loss=1.0300 (C:1.0300, R:0.0105)
Batch 450/537: Loss=1.0585 (C:1.0585, R:0.0105)
Batch 475/537: Loss=1.0355 (C:1.0355, R:0.0105)
Batch 500/537: Loss=1.0380 (C:1.0380, R:0.0105)
Batch 525/537: Loss=1.0344 (C:1.0344, R:0.0105)

============================================================
Epoch 11/300 completed in 21.3s
Train: Loss=1.0158 (C:1.0158, R:0.0105) Ratio=3.24x
Val:   Loss=1.0442 (C:1.0442, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0442)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.0127 (C:1.0127, R:0.0105)
Batch  25/537: Loss=0.9860 (C:0.9860, R:0.0105)
Batch  50/537: Loss=1.0262 (C:1.0262, R:0.0106)
Batch  75/537: Loss=1.0014 (C:1.0014, R:0.0105)
Batch 100/537: Loss=1.0078 (C:1.0078, R:0.0105)
Batch 125/537: Loss=0.9828 (C:0.9828, R:0.0105)
Batch 150/537: Loss=1.0054 (C:1.0054, R:0.0105)
Batch 175/537: Loss=0.9886 (C:0.9886, R:0.0105)
Batch 200/537: Loss=0.9856 (C:0.9856, R:0.0105)
Batch 225/537: Loss=1.0393 (C:1.0393, R:0.0105)
Batch 250/537: Loss=1.0161 (C:1.0161, R:0.0105)
Batch 275/537: Loss=1.0183 (C:1.0183, R:0.0105)
Batch 300/537: Loss=1.0387 (C:1.0387, R:0.0105)
Batch 325/537: Loss=0.9863 (C:0.9863, R:0.0105)
Batch 350/537: Loss=1.0260 (C:1.0260, R:0.0105)
Batch 375/537: Loss=1.0102 (C:1.0102, R:0.0105)
Batch 400/537: Loss=1.0116 (C:1.0116, R:0.0105)
Batch 425/537: Loss=0.9993 (C:0.9993, R:0.0105)
Batch 450/537: Loss=0.9747 (C:0.9747, R:0.0105)
Batch 475/537: Loss=1.0045 (C:1.0045, R:0.0105)
Batch 500/537: Loss=1.0226 (C:1.0226, R:0.0105)
Batch 525/537: Loss=0.9932 (C:0.9932, R:0.0105)

============================================================
Epoch 12/300 completed in 20.9s
Train: Loss=1.0101 (C:1.0101, R:0.0105) Ratio=3.39x
Val:   Loss=1.0486 (C:1.0486, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.435 ± 0.558
    Neg distances: 1.826 ± 0.883
    Separation ratio: 4.19x
    Gap: -3.219
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9372 (C:0.9372, R:0.0105)
Batch  25/537: Loss=0.9410 (C:0.9410, R:0.0105)
Batch  50/537: Loss=0.9609 (C:0.9609, R:0.0105)
Batch  75/537: Loss=0.9420 (C:0.9420, R:0.0105)
Batch 100/537: Loss=0.9317 (C:0.9317, R:0.0106)
Batch 125/537: Loss=0.9193 (C:0.9193, R:0.0105)
Batch 150/537: Loss=0.9454 (C:0.9454, R:0.0105)
Batch 175/537: Loss=0.9294 (C:0.9294, R:0.0105)
Batch 200/537: Loss=0.9621 (C:0.9621, R:0.0105)
Batch 225/537: Loss=0.9619 (C:0.9619, R:0.0105)
Batch 250/537: Loss=0.9490 (C:0.9490, R:0.0105)
Batch 275/537: Loss=0.9574 (C:0.9574, R:0.0105)
Batch 300/537: Loss=0.9552 (C:0.9552, R:0.0105)
Batch 325/537: Loss=0.9673 (C:0.9673, R:0.0105)
Batch 350/537: Loss=0.9472 (C:0.9472, R:0.0105)
Batch 375/537: Loss=0.9494 (C:0.9494, R:0.0105)
Batch 400/537: Loss=1.0171 (C:1.0171, R:0.0105)
Batch 425/537: Loss=0.9672 (C:0.9672, R:0.0105)
Batch 450/537: Loss=0.9374 (C:0.9374, R:0.0105)
Batch 475/537: Loss=1.0245 (C:1.0245, R:0.0105)
Batch 500/537: Loss=0.9681 (C:0.9681, R:0.0105)
Batch 525/537: Loss=0.9802 (C:0.9802, R:0.0105)

============================================================
Epoch 13/300 completed in 27.5s
Train: Loss=0.9600 (C:0.9600, R:0.0105) Ratio=3.41x
Val:   Loss=1.0036 (C:1.0036, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0036)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9570 (C:0.9570, R:0.0105)
Batch  25/537: Loss=0.9543 (C:0.9543, R:0.0105)
Batch  50/537: Loss=0.9500 (C:0.9500, R:0.0105)
Batch  75/537: Loss=0.9600 (C:0.9600, R:0.0105)
Batch 100/537: Loss=0.9210 (C:0.9210, R:0.0105)
Batch 125/537: Loss=0.9798 (C:0.9798, R:0.0105)
Batch 150/537: Loss=0.9545 (C:0.9545, R:0.0105)
Batch 175/537: Loss=0.9657 (C:0.9657, R:0.0105)
Batch 200/537: Loss=0.9487 (C:0.9487, R:0.0105)
Batch 225/537: Loss=0.9673 (C:0.9673, R:0.0105)
Batch 250/537: Loss=0.9339 (C:0.9339, R:0.0105)
Batch 275/537: Loss=0.9588 (C:0.9588, R:0.0105)
Batch 300/537: Loss=0.9326 (C:0.9326, R:0.0105)
Batch 325/537: Loss=0.9499 (C:0.9499, R:0.0105)
Batch 350/537: Loss=0.9866 (C:0.9866, R:0.0105)
Batch 375/537: Loss=0.9856 (C:0.9856, R:0.0105)
Batch 400/537: Loss=0.9953 (C:0.9953, R:0.0105)
Batch 425/537: Loss=0.9718 (C:0.9718, R:0.0105)
Batch 450/537: Loss=0.9558 (C:0.9558, R:0.0105)
Batch 475/537: Loss=0.9689 (C:0.9689, R:0.0105)
Batch 500/537: Loss=0.9480 (C:0.9480, R:0.0105)
Batch 525/537: Loss=0.9759 (C:0.9759, R:0.0105)

============================================================
Epoch 14/300 completed in 22.0s
Train: Loss=0.9546 (C:0.9546, R:0.0105) Ratio=3.48x
Val:   Loss=0.9956 (C:0.9956, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9956)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.9172 (C:0.9172, R:0.0105)
Batch  25/537: Loss=0.9028 (C:0.9028, R:0.0105)
Batch  50/537: Loss=0.9525 (C:0.9525, R:0.0105)
Batch  75/537: Loss=0.9775 (C:0.9775, R:0.0105)
Batch 100/537: Loss=0.9307 (C:0.9307, R:0.0105)
Batch 125/537: Loss=0.9397 (C:0.9397, R:0.0105)
Batch 150/537: Loss=0.9600 (C:0.9600, R:0.0105)
Batch 175/537: Loss=0.9482 (C:0.9482, R:0.0105)
Batch 200/537: Loss=0.9358 (C:0.9358, R:0.0105)
Batch 225/537: Loss=0.9237 (C:0.9237, R:0.0105)
Batch 250/537: Loss=0.9446 (C:0.9446, R:0.0106)
Batch 275/537: Loss=0.9450 (C:0.9450, R:0.0105)
Batch 300/537: Loss=0.9391 (C:0.9391, R:0.0105)
Batch 325/537: Loss=0.9381 (C:0.9381, R:0.0105)
Batch 350/537: Loss=0.9805 (C:0.9805, R:0.0105)
Batch 375/537: Loss=0.9301 (C:0.9301, R:0.0105)
Batch 400/537: Loss=0.9329 (C:0.9329, R:0.0105)
Batch 425/537: Loss=0.9457 (C:0.9457, R:0.0105)
Batch 450/537: Loss=0.9483 (C:0.9483, R:0.0105)
Batch 475/537: Loss=0.9427 (C:0.9427, R:0.0105)
Batch 500/537: Loss=0.9525 (C:0.9525, R:0.0105)
Batch 525/537: Loss=0.9761 (C:0.9761, R:0.0105)

============================================================
Epoch 15/300 completed in 21.8s
Train: Loss=0.9491 (C:0.9491, R:0.0105) Ratio=3.54x
Val:   Loss=0.9947 (C:0.9947, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9947)
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.421 ± 0.553
    Neg distances: 1.905 ± 0.898
    Separation ratio: 4.52x
    Gap: -3.320
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.8793 (C:0.8793, R:0.0105)
Batch  25/537: Loss=0.8922 (C:0.8922, R:0.0105)
Batch  50/537: Loss=0.9175 (C:0.9175, R:0.0105)
Batch  75/537: Loss=0.9052 (C:0.9052, R:0.0105)
Batch 100/537: Loss=0.9173 (C:0.9173, R:0.0105)
Batch 125/537: Loss=0.9291 (C:0.9291, R:0.0105)
Batch 150/537: Loss=0.8871 (C:0.8871, R:0.0105)
Batch 175/537: Loss=0.8980 (C:0.8980, R:0.0105)
Batch 200/537: Loss=0.8861 (C:0.8861, R:0.0105)
Batch 225/537: Loss=0.9427 (C:0.9427, R:0.0105)
Batch 250/537: Loss=0.8668 (C:0.8668, R:0.0105)
Batch 275/537: Loss=0.9533 (C:0.9533, R:0.0106)
Batch 300/537: Loss=0.9047 (C:0.9047, R:0.0105)
Batch 325/537: Loss=0.8926 (C:0.8926, R:0.0105)
Batch 350/537: Loss=0.9115 (C:0.9115, R:0.0105)
Batch 375/537: Loss=0.9313 (C:0.9313, R:0.0105)
Batch 400/537: Loss=0.9219 (C:0.9219, R:0.0105)
Batch 425/537: Loss=0.8914 (C:0.8914, R:0.0105)
Batch 450/537: Loss=0.9220 (C:0.9220, R:0.0105)
Batch 475/537: Loss=0.9211 (C:0.9211, R:0.0105)
Batch 500/537: Loss=0.9202 (C:0.9202, R:0.0105)
Batch 525/537: Loss=0.8916 (C:0.8916, R:0.0105)

============================================================
Epoch 16/300 completed in 27.6s
Train: Loss=0.9118 (C:0.9118, R:0.0105) Ratio=3.60x
Val:   Loss=0.9785 (C:0.9785, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9785)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.8899 (C:0.8899, R:0.0105)
Batch  25/537: Loss=0.9076 (C:0.9076, R:0.0106)
Batch  50/537: Loss=0.8782 (C:0.8782, R:0.0105)
Batch  75/537: Loss=0.9320 (C:0.9320, R:0.0105)
Batch 100/537: Loss=0.9090 (C:0.9090, R:0.0105)
Batch 125/537: Loss=0.9170 (C:0.9170, R:0.0105)
Batch 150/537: Loss=0.9021 (C:0.9021, R:0.0105)
Batch 175/537: Loss=0.9039 (C:0.9039, R:0.0105)
Batch 200/537: Loss=0.8833 (C:0.8833, R:0.0105)
Batch 225/537: Loss=0.9057 (C:0.9057, R:0.0105)
Batch 250/537: Loss=0.9131 (C:0.9131, R:0.0105)
Batch 275/537: Loss=0.8978 (C:0.8978, R:0.0105)
Batch 300/537: Loss=0.9139 (C:0.9139, R:0.0105)
Batch 325/537: Loss=0.8947 (C:0.8947, R:0.0105)
Batch 350/537: Loss=0.9061 (C:0.9061, R:0.0105)
Batch 375/537: Loss=0.9209 (C:0.9209, R:0.0105)
Batch 400/537: Loss=0.8963 (C:0.8963, R:0.0105)
Batch 425/537: Loss=0.9122 (C:0.9122, R:0.0105)
Batch 450/537: Loss=0.9137 (C:0.9137, R:0.0105)
Batch 475/537: Loss=0.8911 (C:0.8911, R:0.0105)
Batch 500/537: Loss=0.9056 (C:0.9056, R:0.0105)
Batch 525/537: Loss=0.9085 (C:0.9085, R:0.0105)

============================================================
Epoch 17/300 completed in 20.9s
Train: Loss=0.9075 (C:0.9075, R:0.0105) Ratio=3.68x
Val:   Loss=0.9568 (C:0.9568, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9568)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.8830 (C:0.8830, R:0.0105)
Batch  25/537: Loss=0.9158 (C:0.9158, R:0.0105)
Batch  50/537: Loss=0.9006 (C:0.9006, R:0.0105)
Batch  75/537: Loss=0.9037 (C:0.9037, R:0.0105)
Batch 100/537: Loss=0.9246 (C:0.9246, R:0.0105)
Batch 125/537: Loss=0.9342 (C:0.9342, R:0.0105)
Batch 150/537: Loss=0.8865 (C:0.8865, R:0.0105)
Batch 175/537: Loss=0.9019 (C:0.9019, R:0.0105)
Batch 200/537: Loss=0.9107 (C:0.9107, R:0.0105)
Batch 225/537: Loss=0.8791 (C:0.8791, R:0.0105)
Batch 250/537: Loss=0.8967 (C:0.8967, R:0.0105)
Batch 275/537: Loss=0.8815 (C:0.8815, R:0.0105)
Batch 300/537: Loss=0.8991 (C:0.8991, R:0.0105)
Batch 325/537: Loss=0.8682 (C:0.8682, R:0.0105)
Batch 350/537: Loss=0.8741 (C:0.8741, R:0.0105)
Batch 375/537: Loss=0.9158 (C:0.9158, R:0.0105)
Batch 400/537: Loss=0.9070 (C:0.9070, R:0.0106)
Batch 425/537: Loss=0.9467 (C:0.9467, R:0.0105)
Batch 450/537: Loss=0.8960 (C:0.8960, R:0.0105)
Batch 475/537: Loss=0.9428 (C:0.9428, R:0.0105)
Batch 500/537: Loss=0.9316 (C:0.9316, R:0.0105)
Batch 525/537: Loss=0.9007 (C:0.9007, R:0.0105)

============================================================
Epoch 18/300 completed in 21.1s
Train: Loss=0.9036 (C:0.9036, R:0.0105) Ratio=3.71x
Val:   Loss=0.9668 (C:0.9668, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.400 ± 0.551
    Neg distances: 1.968 ± 0.910
    Separation ratio: 4.92x
    Gap: -3.361
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.8544 (C:0.8544, R:0.0105)
Batch  25/537: Loss=0.8462 (C:0.8462, R:0.0105)
Batch  50/537: Loss=0.8569 (C:0.8569, R:0.0105)
Batch  75/537: Loss=0.8763 (C:0.8763, R:0.0105)
Batch 100/537: Loss=0.8619 (C:0.8619, R:0.0105)
Batch 125/537: Loss=0.8176 (C:0.8176, R:0.0105)
Batch 150/537: Loss=0.8736 (C:0.8736, R:0.0105)
Batch 175/537: Loss=0.8389 (C:0.8389, R:0.0105)
Batch 200/537: Loss=0.8748 (C:0.8748, R:0.0106)
Batch 225/537: Loss=0.8846 (C:0.8846, R:0.0105)
Batch 250/537: Loss=0.8575 (C:0.8575, R:0.0105)
Batch 275/537: Loss=0.8702 (C:0.8702, R:0.0106)
Batch 300/537: Loss=0.8666 (C:0.8666, R:0.0105)
Batch 325/537: Loss=0.8761 (C:0.8761, R:0.0105)
Batch 350/537: Loss=0.8572 (C:0.8572, R:0.0105)
Batch 375/537: Loss=0.8672 (C:0.8672, R:0.0106)
Batch 400/537: Loss=0.8662 (C:0.8662, R:0.0105)
Batch 425/537: Loss=0.8735 (C:0.8735, R:0.0105)
Batch 450/537: Loss=0.8833 (C:0.8833, R:0.0105)
Batch 475/537: Loss=0.8800 (C:0.8800, R:0.0105)
Batch 500/537: Loss=0.8220 (C:0.8220, R:0.0105)
Batch 525/537: Loss=0.8711 (C:0.8711, R:0.0104)

============================================================
Epoch 19/300 completed in 28.0s
Train: Loss=0.8683 (C:0.8683, R:0.0105) Ratio=3.77x
Val:   Loss=0.9228 (C:0.9228, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9228)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.8752 (C:0.8752, R:0.0105)
Batch  25/537: Loss=0.8541 (C:0.8541, R:0.0105)
Batch  50/537: Loss=0.8721 (C:0.8721, R:0.0105)
Batch  75/537: Loss=0.8723 (C:0.8723, R:0.0105)
Batch 100/537: Loss=0.8800 (C:0.8800, R:0.0105)
Batch 125/537: Loss=0.8925 (C:0.8925, R:0.0105)
Batch 150/537: Loss=0.8846 (C:0.8846, R:0.0105)
Batch 175/537: Loss=0.8826 (C:0.8826, R:0.0105)
Batch 200/537: Loss=0.8691 (C:0.8691, R:0.0105)
Batch 225/537: Loss=0.8650 (C:0.8650, R:0.0105)
Batch 250/537: Loss=0.8594 (C:0.8594, R:0.0105)
Batch 275/537: Loss=0.8762 (C:0.8762, R:0.0105)
Batch 300/537: Loss=0.8644 (C:0.8644, R:0.0105)
Batch 325/537: Loss=0.8690 (C:0.8690, R:0.0105)
Batch 350/537: Loss=0.8899 (C:0.8899, R:0.0106)
Batch 375/537: Loss=0.8542 (C:0.8542, R:0.0105)
Batch 400/537: Loss=0.8832 (C:0.8832, R:0.0105)
Batch 425/537: Loss=0.8502 (C:0.8502, R:0.0105)
Batch 450/537: Loss=0.8821 (C:0.8821, R:0.0105)
Batch 475/537: Loss=0.8530 (C:0.8530, R:0.0105)
Batch 500/537: Loss=0.8962 (C:0.8962, R:0.0105)
Batch 525/537: Loss=0.8612 (C:0.8612, R:0.0106)

============================================================
Epoch 20/300 completed in 22.1s
Train: Loss=0.8648 (C:0.8648, R:0.0105) Ratio=3.73x
Val:   Loss=0.9405 (C:0.9405, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8370 (C:0.8370, R:0.0105)
Batch  25/537: Loss=0.8598 (C:0.8598, R:0.0105)
Batch  50/537: Loss=0.8427 (C:0.8427, R:0.0105)
Batch  75/537: Loss=0.8474 (C:0.8474, R:0.0105)
Batch 100/537: Loss=0.8745 (C:0.8745, R:0.0105)
Batch 125/537: Loss=0.8729 (C:0.8729, R:0.0105)
Batch 150/537: Loss=0.8643 (C:0.8643, R:0.0105)
Batch 175/537: Loss=0.8583 (C:0.8583, R:0.0105)
Batch 200/537: Loss=0.8908 (C:0.8908, R:0.0105)
Batch 225/537: Loss=0.8636 (C:0.8636, R:0.0105)
Batch 250/537: Loss=0.8651 (C:0.8651, R:0.0105)
Batch 275/537: Loss=0.8401 (C:0.8401, R:0.0105)
Batch 300/537: Loss=0.8785 (C:0.8785, R:0.0105)
Batch 325/537: Loss=0.8315 (C:0.8315, R:0.0105)
Batch 350/537: Loss=0.8377 (C:0.8377, R:0.0105)
Batch 375/537: Loss=0.8857 (C:0.8857, R:0.0105)
Batch 400/537: Loss=0.8817 (C:0.8817, R:0.0105)
Batch 425/537: Loss=0.8381 (C:0.8381, R:0.0105)
Batch 450/537: Loss=0.8507 (C:0.8507, R:0.0105)
Batch 475/537: Loss=0.8569 (C:0.8569, R:0.0105)
Batch 500/537: Loss=0.9100 (C:0.9100, R:0.0105)
Batch 525/537: Loss=0.8605 (C:0.8605, R:0.0105)

============================================================
Epoch 21/300 completed in 22.1s
Train: Loss=0.8597 (C:0.8597, R:0.0105) Ratio=3.81x
Val:   Loss=0.9270 (C:0.9270, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.405 ± 0.579
    Neg distances: 2.057 ± 0.936
    Separation ratio: 5.08x
    Gap: -3.576
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8405 (C:0.8405, R:0.0105)
Batch  25/537: Loss=0.8122 (C:0.8122, R:0.0106)
Batch  50/537: Loss=0.8084 (C:0.8084, R:0.0105)
Batch  75/537: Loss=0.8477 (C:0.8477, R:0.0105)
Batch 100/537: Loss=0.8223 (C:0.8223, R:0.0105)
Batch 125/537: Loss=0.8410 (C:0.8410, R:0.0105)
Batch 150/537: Loss=0.8232 (C:0.8232, R:0.0105)
Batch 175/537: Loss=0.8167 (C:0.8167, R:0.0106)
Batch 200/537: Loss=0.8631 (C:0.8631, R:0.0105)
Batch 225/537: Loss=0.8483 (C:0.8483, R:0.0105)
Batch 250/537: Loss=0.8690 (C:0.8690, R:0.0105)
Batch 275/537: Loss=0.8405 (C:0.8405, R:0.0105)
Batch 300/537: Loss=0.8404 (C:0.8404, R:0.0105)
Batch 325/537: Loss=0.8106 (C:0.8106, R:0.0105)
Batch 350/537: Loss=0.8899 (C:0.8899, R:0.0105)
Batch 375/537: Loss=0.8010 (C:0.8010, R:0.0105)
Batch 400/537: Loss=0.8447 (C:0.8447, R:0.0105)
Batch 425/537: Loss=0.8269 (C:0.8269, R:0.0105)
Batch 450/537: Loss=0.8220 (C:0.8220, R:0.0105)
Batch 475/537: Loss=0.8456 (C:0.8456, R:0.0105)
Batch 500/537: Loss=0.8360 (C:0.8360, R:0.0105)
Batch 525/537: Loss=0.8814 (C:0.8814, R:0.0105)

============================================================
Epoch 22/300 completed in 27.6s
Train: Loss=0.8341 (C:0.8341, R:0.0105) Ratio=3.84x
Val:   Loss=0.9025 (C:0.9025, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9025)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.8365 (C:0.8365, R:0.0105)
Batch  25/537: Loss=0.8288 (C:0.8288, R:0.0105)
Batch  50/537: Loss=0.8140 (C:0.8140, R:0.0105)
Batch  75/537: Loss=0.7952 (C:0.7952, R:0.0105)
Batch 100/537: Loss=0.8049 (C:0.8049, R:0.0106)
Batch 125/537: Loss=0.8151 (C:0.8151, R:0.0105)
Batch 150/537: Loss=0.8275 (C:0.8275, R:0.0105)
Batch 175/537: Loss=0.8491 (C:0.8491, R:0.0105)
Batch 200/537: Loss=0.8549 (C:0.8549, R:0.0105)
Batch 225/537: Loss=0.8347 (C:0.8347, R:0.0105)
Batch 250/537: Loss=0.8051 (C:0.8051, R:0.0105)
Batch 275/537: Loss=0.8410 (C:0.8410, R:0.0105)
Batch 300/537: Loss=0.8047 (C:0.8047, R:0.0106)
Batch 325/537: Loss=0.8074 (C:0.8074, R:0.0105)
Batch 350/537: Loss=0.8138 (C:0.8138, R:0.0105)
Batch 375/537: Loss=0.8598 (C:0.8598, R:0.0105)
Batch 400/537: Loss=0.8921 (C:0.8921, R:0.0105)
Batch 425/537: Loss=0.8313 (C:0.8313, R:0.0105)
Batch 450/537: Loss=0.8368 (C:0.8368, R:0.0105)
Batch 475/537: Loss=0.8123 (C:0.8123, R:0.0105)
Batch 500/537: Loss=0.8827 (C:0.8827, R:0.0105)
Batch 525/537: Loss=0.8527 (C:0.8527, R:0.0105)

============================================================
Epoch 23/300 completed in 21.9s
Train: Loss=0.8300 (C:0.8300, R:0.0105) Ratio=3.86x
Val:   Loss=0.9036 (C:0.9036, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8245 (C:0.8245, R:0.0105)
Batch  25/537: Loss=0.8284 (C:0.8284, R:0.0105)
Batch  50/537: Loss=0.8183 (C:0.8183, R:0.0105)
Batch  75/537: Loss=0.8075 (C:0.8075, R:0.0105)
Batch 100/537: Loss=0.8041 (C:0.8041, R:0.0105)
Batch 125/537: Loss=0.8330 (C:0.8330, R:0.0105)
Batch 150/537: Loss=0.8307 (C:0.8307, R:0.0105)
Batch 175/537: Loss=0.7979 (C:0.7979, R:0.0105)
Batch 200/537: Loss=0.8200 (C:0.8200, R:0.0105)
Batch 225/537: Loss=0.8242 (C:0.8242, R:0.0105)
Batch 250/537: Loss=0.8365 (C:0.8365, R:0.0105)
Batch 275/537: Loss=0.8248 (C:0.8248, R:0.0105)
Batch 300/537: Loss=0.8270 (C:0.8270, R:0.0105)
Batch 325/537: Loss=0.8473 (C:0.8473, R:0.0105)
Batch 350/537: Loss=0.8642 (C:0.8642, R:0.0105)
Batch 375/537: Loss=0.8252 (C:0.8252, R:0.0105)
Batch 400/537: Loss=0.8039 (C:0.8039, R:0.0105)
Batch 425/537: Loss=0.8446 (C:0.8446, R:0.0105)
Batch 450/537: Loss=0.8462 (C:0.8462, R:0.0105)
Batch 475/537: Loss=0.7856 (C:0.7856, R:0.0105)
Batch 500/537: Loss=0.8485 (C:0.8485, R:0.0105)
Batch 525/537: Loss=0.8037 (C:0.8037, R:0.0105)

============================================================
Epoch 24/300 completed in 21.5s
Train: Loss=0.8265 (C:0.8265, R:0.0105) Ratio=3.95x
Val:   Loss=0.9042 (C:0.9042, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.388 ± 0.547
    Neg distances: 2.148 ± 0.960
    Separation ratio: 5.54x
    Gap: -3.721
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.7755 (C:0.7755, R:0.0105)
Batch  25/537: Loss=0.7595 (C:0.7595, R:0.0105)
Batch  50/537: Loss=0.8191 (C:0.8191, R:0.0105)
Batch  75/537: Loss=0.7904 (C:0.7904, R:0.0105)
Batch 100/537: Loss=0.7783 (C:0.7783, R:0.0105)
Batch 125/537: Loss=0.8098 (C:0.8098, R:0.0105)
Batch 150/537: Loss=0.8025 (C:0.8025, R:0.0105)
Batch 175/537: Loss=0.7676 (C:0.7676, R:0.0105)
Batch 200/537: Loss=0.7660 (C:0.7660, R:0.0105)
Batch 225/537: Loss=0.8170 (C:0.8170, R:0.0105)
Batch 250/537: Loss=0.8079 (C:0.8079, R:0.0105)
Batch 275/537: Loss=0.8202 (C:0.8202, R:0.0105)
Batch 300/537: Loss=0.8440 (C:0.8440, R:0.0105)
Batch 325/537: Loss=0.7688 (C:0.7688, R:0.0105)
Batch 350/537: Loss=0.7687 (C:0.7687, R:0.0105)
Batch 375/537: Loss=0.8074 (C:0.8074, R:0.0105)
Batch 400/537: Loss=0.7912 (C:0.7912, R:0.0105)
Batch 425/537: Loss=0.8083 (C:0.8083, R:0.0105)
Batch 450/537: Loss=0.8112 (C:0.8112, R:0.0105)
Batch 475/537: Loss=0.8040 (C:0.8040, R:0.0105)
Batch 500/537: Loss=0.7903 (C:0.7903, R:0.0105)
Batch 525/537: Loss=0.7485 (C:0.7485, R:0.0105)

============================================================
Epoch 25/300 completed in 27.1s
Train: Loss=0.7874 (C:0.7874, R:0.0105) Ratio=3.87x
Val:   Loss=0.8683 (C:0.8683, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8683)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.7391 (C:0.7391, R:0.0105)
Batch  25/537: Loss=0.7524 (C:0.7524, R:0.0105)
Batch  50/537: Loss=0.7845 (C:0.7845, R:0.0105)
Batch  75/537: Loss=0.7377 (C:0.7377, R:0.0105)
Batch 100/537: Loss=0.7696 (C:0.7696, R:0.0105)
Batch 125/537: Loss=0.7362 (C:0.7362, R:0.0105)
Batch 150/537: Loss=0.7544 (C:0.7544, R:0.0105)
Batch 175/537: Loss=0.8074 (C:0.8074, R:0.0105)
Batch 200/537: Loss=0.7864 (C:0.7864, R:0.0105)
Batch 225/537: Loss=0.7642 (C:0.7642, R:0.0106)
Batch 250/537: Loss=0.8081 (C:0.8081, R:0.0105)
Batch 275/537: Loss=0.7839 (C:0.7839, R:0.0106)
Batch 300/537: Loss=0.7912 (C:0.7912, R:0.0105)
Batch 325/537: Loss=0.8036 (C:0.8036, R:0.0105)
Batch 350/537: Loss=0.7913 (C:0.7913, R:0.0105)
Batch 375/537: Loss=0.7996 (C:0.7996, R:0.0105)
Batch 400/537: Loss=0.7848 (C:0.7848, R:0.0105)
Batch 425/537: Loss=0.8098 (C:0.8098, R:0.0105)
Batch 450/537: Loss=0.7597 (C:0.7597, R:0.0105)
Batch 475/537: Loss=0.7830 (C:0.7830, R:0.0105)
Batch 500/537: Loss=0.8058 (C:0.8058, R:0.0105)
Batch 525/537: Loss=0.8158 (C:0.8158, R:0.0105)

============================================================
Epoch 26/300 completed in 21.5s
Train: Loss=0.7830 (C:0.7830, R:0.0105) Ratio=4.05x
Val:   Loss=0.8660 (C:0.8660, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8660)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.7383 (C:0.7383, R:0.0105)
Batch  25/537: Loss=0.7863 (C:0.7863, R:0.0105)
Batch  50/537: Loss=0.7651 (C:0.7651, R:0.0105)
Batch  75/537: Loss=0.7664 (C:0.7664, R:0.0105)
Batch 100/537: Loss=0.7776 (C:0.7776, R:0.0105)
Batch 125/537: Loss=0.7770 (C:0.7770, R:0.0105)
Batch 150/537: Loss=0.7914 (C:0.7914, R:0.0105)
Batch 175/537: Loss=0.7898 (C:0.7898, R:0.0105)
Batch 200/537: Loss=0.7670 (C:0.7670, R:0.0105)
Batch 225/537: Loss=0.7995 (C:0.7995, R:0.0105)
Batch 250/537: Loss=0.8080 (C:0.8080, R:0.0105)
Batch 275/537: Loss=0.8044 (C:0.8044, R:0.0105)
Batch 300/537: Loss=0.7940 (C:0.7940, R:0.0105)
Batch 325/537: Loss=0.7511 (C:0.7511, R:0.0105)
Batch 350/537: Loss=0.7632 (C:0.7632, R:0.0105)
Batch 375/537: Loss=0.7970 (C:0.7970, R:0.0105)
Batch 400/537: Loss=0.7840 (C:0.7840, R:0.0105)
Batch 425/537: Loss=0.8206 (C:0.8206, R:0.0105)
Batch 450/537: Loss=0.7967 (C:0.7967, R:0.0105)
Batch 475/537: Loss=0.7611 (C:0.7611, R:0.0105)
Batch 500/537: Loss=0.8093 (C:0.8093, R:0.0105)
Batch 525/537: Loss=0.7952 (C:0.7952, R:0.0105)

============================================================
Epoch 27/300 completed in 22.2s
Train: Loss=0.7811 (C:0.7811, R:0.0105) Ratio=3.98x
Val:   Loss=0.8709 (C:0.8709, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.388 ± 0.566
    Neg distances: 2.260 ± 0.995
    Separation ratio: 5.82x
    Gap: -3.854
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.7903 (C:0.7903, R:0.0105)
Batch  25/537: Loss=0.7492 (C:0.7492, R:0.0105)
Batch  50/537: Loss=0.7464 (C:0.7464, R:0.0105)
Batch  75/537: Loss=0.7081 (C:0.7081, R:0.0105)
Batch 100/537: Loss=0.7529 (C:0.7529, R:0.0105)
Batch 125/537: Loss=0.7709 (C:0.7709, R:0.0106)
Batch 150/537: Loss=0.7669 (C:0.7669, R:0.0105)
Batch 175/537: Loss=0.7300 (C:0.7300, R:0.0105)
Batch 200/537: Loss=0.7503 (C:0.7503, R:0.0105)
Batch 225/537: Loss=0.7737 (C:0.7737, R:0.0105)
Batch 250/537: Loss=0.7265 (C:0.7265, R:0.0105)
Batch 275/537: Loss=0.7653 (C:0.7653, R:0.0105)
Batch 300/537: Loss=0.7246 (C:0.7246, R:0.0105)
Batch 325/537: Loss=0.7559 (C:0.7559, R:0.0105)
Batch 350/537: Loss=0.7685 (C:0.7685, R:0.0105)
Batch 375/537: Loss=0.7726 (C:0.7726, R:0.0105)
Batch 400/537: Loss=0.7093 (C:0.7093, R:0.0105)
Batch 425/537: Loss=0.7332 (C:0.7332, R:0.0105)
Batch 450/537: Loss=0.7516 (C:0.7516, R:0.0105)
Batch 475/537: Loss=0.7568 (C:0.7568, R:0.0105)
Batch 500/537: Loss=0.7549 (C:0.7549, R:0.0105)
Batch 525/537: Loss=0.7647 (C:0.7647, R:0.0105)

============================================================
Epoch 28/300 completed in 28.7s
Train: Loss=0.7493 (C:0.7493, R:0.0105) Ratio=4.09x
Val:   Loss=0.8351 (C:0.8351, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8351)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.7274 (C:0.7274, R:0.0105)
Batch  25/537: Loss=0.7259 (C:0.7259, R:0.0105)
Batch  50/537: Loss=0.7476 (C:0.7476, R:0.0105)
Batch  75/537: Loss=0.7740 (C:0.7740, R:0.0105)
Batch 100/537: Loss=0.6906 (C:0.6906, R:0.0106)
Batch 125/537: Loss=0.7352 (C:0.7352, R:0.0105)
Batch 150/537: Loss=0.7575 (C:0.7575, R:0.0105)
Batch 175/537: Loss=0.7204 (C:0.7204, R:0.0105)
Batch 200/537: Loss=0.7315 (C:0.7315, R:0.0105)
Batch 225/537: Loss=0.7099 (C:0.7099, R:0.0105)
Batch 250/537: Loss=0.7366 (C:0.7366, R:0.0105)
Batch 275/537: Loss=0.7826 (C:0.7826, R:0.0105)
Batch 300/537: Loss=0.7458 (C:0.7458, R:0.0106)
Batch 325/537: Loss=0.7593 (C:0.7593, R:0.0105)
Batch 350/537: Loss=0.7452 (C:0.7452, R:0.0105)
Batch 375/537: Loss=0.7546 (C:0.7546, R:0.0105)
Batch 400/537: Loss=0.7543 (C:0.7543, R:0.0105)
Batch 425/537: Loss=0.7537 (C:0.7537, R:0.0105)
Batch 450/537: Loss=0.7489 (C:0.7489, R:0.0105)
Batch 475/537: Loss=0.7270 (C:0.7270, R:0.0105)
Batch 500/537: Loss=0.7442 (C:0.7442, R:0.0105)
Batch 525/537: Loss=0.7961 (C:0.7961, R:0.0105)

============================================================
Epoch 29/300 completed in 21.2s
Train: Loss=0.7458 (C:0.7458, R:0.0105) Ratio=4.07x
Val:   Loss=0.8313 (C:0.8313, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8313)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7105 (C:0.7105, R:0.0105)
Batch  25/537: Loss=0.7222 (C:0.7222, R:0.0105)
Batch  50/537: Loss=0.7341 (C:0.7341, R:0.0105)
Batch  75/537: Loss=0.7367 (C:0.7367, R:0.0105)
Batch 100/537: Loss=0.7202 (C:0.7202, R:0.0105)
Batch 125/537: Loss=0.7494 (C:0.7494, R:0.0105)
Batch 150/537: Loss=0.7222 (C:0.7222, R:0.0105)
Batch 175/537: Loss=0.7443 (C:0.7443, R:0.0105)
Batch 200/537: Loss=0.7289 (C:0.7289, R:0.0105)
Batch 225/537: Loss=0.7634 (C:0.7634, R:0.0105)
Batch 250/537: Loss=0.7141 (C:0.7141, R:0.0105)
Batch 275/537: Loss=0.7468 (C:0.7468, R:0.0105)
Batch 300/537: Loss=0.7398 (C:0.7398, R:0.0105)
Batch 325/537: Loss=0.7514 (C:0.7514, R:0.0105)
Batch 350/537: Loss=0.7385 (C:0.7385, R:0.0105)
Batch 375/537: Loss=0.7483 (C:0.7483, R:0.0105)
Batch 400/537: Loss=0.7471 (C:0.7471, R:0.0106)
Batch 425/537: Loss=0.7403 (C:0.7403, R:0.0105)
Batch 450/537: Loss=0.7514 (C:0.7514, R:0.0105)
Batch 475/537: Loss=0.7783 (C:0.7783, R:0.0105)
Batch 500/537: Loss=0.7115 (C:0.7115, R:0.0105)
Batch 525/537: Loss=0.7193 (C:0.7193, R:0.0105)

============================================================
Epoch 30/300 completed in 21.5s
Train: Loss=0.7440 (C:0.7440, R:0.0105) Ratio=4.17x
Val:   Loss=0.8409 (C:0.8409, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.396 ± 0.592
    Neg distances: 2.284 ± 1.017
    Separation ratio: 5.77x
    Gap: -3.938
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.7474 (C:0.7474, R:0.0105)
Batch  25/537: Loss=0.7217 (C:0.7217, R:0.0105)
Batch  50/537: Loss=0.7654 (C:0.7654, R:0.0105)
Batch  75/537: Loss=0.6916 (C:0.6916, R:0.0105)
Batch 100/537: Loss=0.7403 (C:0.7403, R:0.0105)
Batch 125/537: Loss=0.6813 (C:0.6813, R:0.0105)
Batch 150/537: Loss=0.7341 (C:0.7341, R:0.0105)
Batch 175/537: Loss=0.7613 (C:0.7613, R:0.0105)
Batch 200/537: Loss=0.7147 (C:0.7147, R:0.0105)
Batch 225/537: Loss=0.7503 (C:0.7503, R:0.0105)
Batch 250/537: Loss=0.7517 (C:0.7517, R:0.0105)
Batch 275/537: Loss=0.7591 (C:0.7591, R:0.0105)
Batch 300/537: Loss=0.7755 (C:0.7755, R:0.0105)
Batch 325/537: Loss=0.7591 (C:0.7591, R:0.0105)
Batch 350/537: Loss=0.7107 (C:0.7107, R:0.0105)
Batch 375/537: Loss=0.7468 (C:0.7468, R:0.0105)
Batch 400/537: Loss=0.7198 (C:0.7198, R:0.0105)
Batch 425/537: Loss=0.7189 (C:0.7189, R:0.0105)
Batch 450/537: Loss=0.7525 (C:0.7525, R:0.0105)
Batch 475/537: Loss=0.7686 (C:0.7686, R:0.0105)
Batch 500/537: Loss=0.7647 (C:0.7647, R:0.0105)
Batch 525/537: Loss=0.7396 (C:0.7396, R:0.0105)

============================================================
Epoch 31/300 completed in 27.7s
Train: Loss=0.7401 (C:0.7401, R:0.0105) Ratio=4.17x
Val:   Loss=0.8322 (C:0.8322, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.015
No improvement for 2 epochs
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.7245 (C:0.7245, R:0.0105)
Batch  25/537: Loss=0.7299 (C:0.7299, R:0.0106)
Batch  50/537: Loss=0.7597 (C:0.7597, R:0.0105)
Batch  75/537: Loss=0.7531 (C:0.7531, R:0.0105)
Batch 100/537: Loss=0.6959 (C:0.6959, R:0.0105)
Batch 125/537: Loss=0.7511 (C:0.7511, R:0.0105)
Batch 150/537: Loss=0.7425 (C:0.7425, R:0.0105)
Batch 175/537: Loss=0.7463 (C:0.7463, R:0.0105)
Batch 200/537: Loss=0.7531 (C:0.7531, R:0.0105)
Batch 225/537: Loss=0.7226 (C:0.7226, R:0.0105)
Batch 250/537: Loss=0.7397 (C:0.7397, R:0.0105)
Batch 275/537: Loss=0.7409 (C:0.7409, R:0.0105)
Batch 300/537: Loss=0.7590 (C:0.7590, R:0.0105)
Batch 325/537: Loss=0.7554 (C:0.7554, R:0.0105)
Batch 350/537: Loss=0.7481 (C:0.7481, R:0.0105)
Batch 375/537: Loss=0.7421 (C:0.7421, R:0.0105)
Batch 400/537: Loss=0.7214 (C:0.7214, R:0.0105)
Batch 425/537: Loss=0.7390 (C:0.7390, R:0.0105)
Batch 450/537: Loss=0.7248 (C:0.7248, R:0.0106)
Batch 475/537: Loss=0.7633 (C:0.7633, R:0.0105)
Batch 500/537: Loss=0.7627 (C:0.7627, R:0.0105)
Batch 525/537: Loss=0.7655 (C:0.7655, R:0.0106)

============================================================
Epoch 32/300 completed in 21.0s
Train: Loss=0.7372 (C:0.7372, R:0.0105) Ratio=4.17x
Val:   Loss=0.8351 (C:0.8351, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.030
No improvement for 3 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.7464 (C:0.7464, R:0.0105)
Batch  25/537: Loss=0.7566 (C:0.7566, R:0.0105)
Batch  50/537: Loss=0.7290 (C:0.7290, R:0.0105)
Batch  75/537: Loss=0.7475 (C:0.7475, R:0.0105)
Batch 100/537: Loss=0.7689 (C:0.7689, R:0.0105)
Batch 125/537: Loss=0.7251 (C:0.7251, R:0.0105)
Batch 150/537: Loss=0.7035 (C:0.7035, R:0.0106)
Batch 175/537: Loss=0.6894 (C:0.6894, R:0.0105)
Batch 200/537: Loss=0.6995 (C:0.6995, R:0.0105)
Batch 225/537: Loss=0.7228 (C:0.7228, R:0.0105)
Batch 250/537: Loss=0.7189 (C:0.7189, R:0.0105)
Batch 275/537: Loss=0.7418 (C:0.7418, R:0.0105)
Batch 300/537: Loss=0.7268 (C:0.7268, R:0.0105)
Batch 325/537: Loss=0.7592 (C:0.7592, R:0.0105)
Batch 350/537: Loss=0.7312 (C:0.7312, R:0.0105)
Batch 375/537: Loss=0.7322 (C:0.7322, R:0.0105)
Batch 400/537: Loss=0.7465 (C:0.7465, R:0.0105)
Batch 425/537: Loss=0.7074 (C:0.7074, R:0.0105)
Batch 450/537: Loss=0.7294 (C:0.7294, R:0.0106)
Batch 475/537: Loss=0.7580 (C:0.7580, R:0.0105)
Batch 500/537: Loss=0.7414 (C:0.7414, R:0.0105)
Batch 525/537: Loss=0.7190 (C:0.7190, R:0.0105)

============================================================
Epoch 33/300 completed in 21.0s
Train: Loss=0.7352 (C:0.7352, R:0.0105) Ratio=4.29x
Val:   Loss=0.8299 (C:0.8299, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.8299)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.393 ± 0.614
    Neg distances: 2.344 ± 1.030
    Separation ratio: 5.96x
    Gap: -4.004
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.6930 (C:0.6930, R:0.0105)
Batch  25/537: Loss=0.7203 (C:0.7203, R:0.0105)
Batch  50/537: Loss=0.7063 (C:0.7063, R:0.0105)
Batch  75/537: Loss=0.6866 (C:0.6866, R:0.0105)
Batch 100/537: Loss=0.7210 (C:0.7210, R:0.0105)
Batch 125/537: Loss=0.6744 (C:0.6744, R:0.0105)
Batch 150/537: Loss=0.7143 (C:0.7143, R:0.0105)
Batch 175/537: Loss=0.7232 (C:0.7232, R:0.0105)
Batch 200/537: Loss=0.6984 (C:0.6984, R:0.0105)
Batch 225/537: Loss=0.7161 (C:0.7161, R:0.0105)
Batch 250/537: Loss=0.7047 (C:0.7047, R:0.0105)
Batch 275/537: Loss=0.6949 (C:0.6949, R:0.0105)
Batch 300/537: Loss=0.6792 (C:0.6792, R:0.0105)
Batch 325/537: Loss=0.7292 (C:0.7292, R:0.0105)
Batch 350/537: Loss=0.7084 (C:0.7084, R:0.0105)
Batch 375/537: Loss=0.7079 (C:0.7079, R:0.0105)
Batch 400/537: Loss=0.7057 (C:0.7057, R:0.0105)
Batch 425/537: Loss=0.7112 (C:0.7112, R:0.0105)
Batch 450/537: Loss=0.7159 (C:0.7159, R:0.0105)
Batch 475/537: Loss=0.7116 (C:0.7116, R:0.0105)
Batch 500/537: Loss=0.7404 (C:0.7404, R:0.0105)
Batch 525/537: Loss=0.7071 (C:0.7071, R:0.0106)

============================================================
Epoch 34/300 completed in 26.8s
Train: Loss=0.7122 (C:0.7122, R:0.0105) Ratio=4.25x
Val:   Loss=0.8075 (C:0.8075, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8075)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.6895 (C:0.6895, R:0.0105)
Batch  25/537: Loss=0.7201 (C:0.7201, R:0.0105)
Batch  50/537: Loss=0.6944 (C:0.6944, R:0.0105)
Batch  75/537: Loss=0.7284 (C:0.7284, R:0.0105)
Batch 100/537: Loss=0.7127 (C:0.7127, R:0.0105)
Batch 125/537: Loss=0.7260 (C:0.7260, R:0.0105)
Batch 150/537: Loss=0.6854 (C:0.6854, R:0.0105)
Batch 175/537: Loss=0.7205 (C:0.7205, R:0.0105)
Batch 200/537: Loss=0.7020 (C:0.7020, R:0.0105)
Batch 225/537: Loss=0.7023 (C:0.7023, R:0.0105)
Batch 250/537: Loss=0.7101 (C:0.7101, R:0.0105)
Batch 275/537: Loss=0.7018 (C:0.7018, R:0.0105)
Batch 300/537: Loss=0.6778 (C:0.6778, R:0.0105)
Batch 325/537: Loss=0.7061 (C:0.7061, R:0.0105)
Batch 350/537: Loss=0.7366 (C:0.7366, R:0.0105)
Batch 375/537: Loss=0.6756 (C:0.6756, R:0.0105)
Batch 400/537: Loss=0.7162 (C:0.7162, R:0.0105)
Batch 425/537: Loss=0.6932 (C:0.6932, R:0.0105)
Batch 450/537: Loss=0.7144 (C:0.7144, R:0.0105)
Batch 475/537: Loss=0.6947 (C:0.6947, R:0.0105)
Batch 500/537: Loss=0.7252 (C:0.7252, R:0.0105)
Batch 525/537: Loss=0.6707 (C:0.6707, R:0.0105)

============================================================
Epoch 35/300 completed in 21.4s
Train: Loss=0.7090 (C:0.7090, R:0.0105) Ratio=4.33x
Val:   Loss=0.8134 (C:0.8134, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.075
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.7029 (C:0.7029, R:0.0105)
Batch  25/537: Loss=0.6748 (C:0.6748, R:0.0105)
Batch  50/537: Loss=0.6942 (C:0.6942, R:0.0105)
Batch  75/537: Loss=0.7327 (C:0.7327, R:0.0105)
Batch 100/537: Loss=0.7101 (C:0.7101, R:0.0105)
Batch 125/537: Loss=0.7086 (C:0.7086, R:0.0105)
Batch 150/537: Loss=0.6884 (C:0.6884, R:0.0106)
Batch 175/537: Loss=0.7291 (C:0.7291, R:0.0105)
Batch 200/537: Loss=0.6729 (C:0.6729, R:0.0105)
Batch 225/537: Loss=0.6932 (C:0.6932, R:0.0105)
Batch 250/537: Loss=0.7245 (C:0.7245, R:0.0105)
Batch 275/537: Loss=0.6940 (C:0.6940, R:0.0105)
Batch 300/537: Loss=0.7149 (C:0.7149, R:0.0105)
Batch 325/537: Loss=0.6957 (C:0.6957, R:0.0105)
Batch 350/537: Loss=0.7328 (C:0.7328, R:0.0105)
Batch 375/537: Loss=0.6877 (C:0.6877, R:0.0105)
Batch 400/537: Loss=0.7356 (C:0.7356, R:0.0105)
Batch 425/537: Loss=0.7280 (C:0.7280, R:0.0105)
Batch 450/537: Loss=0.7152 (C:0.7152, R:0.0105)
Batch 475/537: Loss=0.7474 (C:0.7474, R:0.0105)
Batch 500/537: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 525/537: Loss=0.7304 (C:0.7304, R:0.0105)

============================================================
Epoch 36/300 completed in 22.1s
Train: Loss=0.7066 (C:0.7066, R:0.0105) Ratio=4.28x
Val:   Loss=0.8245 (C:0.8245, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.090
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.388 ± 0.619
    Neg distances: 2.424 ± 1.058
    Separation ratio: 6.24x
    Gap: -4.105
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.7016 (C:0.7016, R:0.0105)
Batch  25/537: Loss=0.6941 (C:0.6941, R:0.0105)
Batch  50/537: Loss=0.6465 (C:0.6465, R:0.0105)
Batch  75/537: Loss=0.6380 (C:0.6380, R:0.0105)
Batch 100/537: Loss=0.6763 (C:0.6763, R:0.0105)
Batch 125/537: Loss=0.6652 (C:0.6652, R:0.0105)
Batch 150/537: Loss=0.6870 (C:0.6870, R:0.0105)
Batch 175/537: Loss=0.6515 (C:0.6515, R:0.0106)
Batch 200/537: Loss=0.6846 (C:0.6846, R:0.0105)
Batch 225/537: Loss=0.6970 (C:0.6970, R:0.0105)
Batch 250/537: Loss=0.6773 (C:0.6773, R:0.0105)
Batch 275/537: Loss=0.6823 (C:0.6823, R:0.0105)
Batch 300/537: Loss=0.6680 (C:0.6680, R:0.0105)
Batch 325/537: Loss=0.6944 (C:0.6944, R:0.0106)
Batch 350/537: Loss=0.6958 (C:0.6958, R:0.0105)
Batch 375/537: Loss=0.7028 (C:0.7028, R:0.0105)
Batch 400/537: Loss=0.6741 (C:0.6741, R:0.0105)
Batch 425/537: Loss=0.6796 (C:0.6796, R:0.0105)
Batch 450/537: Loss=0.6837 (C:0.6837, R:0.0105)
Batch 475/537: Loss=0.6673 (C:0.6673, R:0.0105)
Batch 500/537: Loss=0.7027 (C:0.7027, R:0.0105)
Batch 525/537: Loss=0.6911 (C:0.6911, R:0.0105)

============================================================
Epoch 37/300 completed in 26.7s
Train: Loss=0.6846 (C:0.6846, R:0.0105) Ratio=4.39x
Val:   Loss=0.7909 (C:0.7909, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.7909)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.6695 (C:0.6695, R:0.0105)
Batch  25/537: Loss=0.6931 (C:0.6931, R:0.0105)
Batch  50/537: Loss=0.6495 (C:0.6495, R:0.0105)
Batch  75/537: Loss=0.6666 (C:0.6666, R:0.0105)
Batch 100/537: Loss=0.6900 (C:0.6900, R:0.0105)
Batch 125/537: Loss=0.7167 (C:0.7167, R:0.0105)
Batch 150/537: Loss=0.6868 (C:0.6868, R:0.0105)
Batch 175/537: Loss=0.7096 (C:0.7096, R:0.0105)
Batch 200/537: Loss=0.6637 (C:0.6637, R:0.0105)
Batch 225/537: Loss=0.6820 (C:0.6820, R:0.0105)
Batch 250/537: Loss=0.6358 (C:0.6358, R:0.0105)
Batch 275/537: Loss=0.6859 (C:0.6859, R:0.0105)
Batch 300/537: Loss=0.6774 (C:0.6774, R:0.0105)
Batch 325/537: Loss=0.6950 (C:0.6950, R:0.0106)
Batch 350/537: Loss=0.6612 (C:0.6612, R:0.0105)
Batch 375/537: Loss=0.6928 (C:0.6928, R:0.0105)
Batch 400/537: Loss=0.7038 (C:0.7038, R:0.0105)
Batch 425/537: Loss=0.6785 (C:0.6785, R:0.0105)
Batch 450/537: Loss=0.6609 (C:0.6609, R:0.0105)
Batch 475/537: Loss=0.6859 (C:0.6859, R:0.0105)
Batch 500/537: Loss=0.7226 (C:0.7226, R:0.0105)
Batch 525/537: Loss=0.6665 (C:0.6665, R:0.0105)

============================================================
Epoch 38/300 completed in 21.1s
Train: Loss=0.6833 (C:0.6833, R:0.0105) Ratio=4.44x
Val:   Loss=0.7910 (C:0.7910, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.6679 (C:0.6679, R:0.0105)
Batch  25/537: Loss=0.6313 (C:0.6313, R:0.0105)
Batch  50/537: Loss=0.6306 (C:0.6306, R:0.0105)
Batch  75/537: Loss=0.6553 (C:0.6553, R:0.0105)
Batch 100/537: Loss=0.6847 (C:0.6847, R:0.0105)
Batch 125/537: Loss=0.6207 (C:0.6207, R:0.0105)
Batch 150/537: Loss=0.6854 (C:0.6854, R:0.0105)
Batch 175/537: Loss=0.6769 (C:0.6769, R:0.0105)
Batch 200/537: Loss=0.6877 (C:0.6877, R:0.0105)
Batch 225/537: Loss=0.6496 (C:0.6496, R:0.0105)
Batch 250/537: Loss=0.6583 (C:0.6583, R:0.0104)
Batch 275/537: Loss=0.6907 (C:0.6907, R:0.0105)
Batch 300/537: Loss=0.6881 (C:0.6881, R:0.0105)
Batch 325/537: Loss=0.7161 (C:0.7161, R:0.0105)
Batch 350/537: Loss=0.7009 (C:0.7009, R:0.0105)
Batch 375/537: Loss=0.6819 (C:0.6819, R:0.0105)
Batch 400/537: Loss=0.6611 (C:0.6611, R:0.0105)
Batch 425/537: Loss=0.6614 (C:0.6614, R:0.0105)
Batch 450/537: Loss=0.6944 (C:0.6944, R:0.0105)
Batch 475/537: Loss=0.7110 (C:0.7110, R:0.0105)
Batch 500/537: Loss=0.7049 (C:0.7049, R:0.0105)
Batch 525/537: Loss=0.7097 (C:0.7097, R:0.0105)

============================================================
Epoch 39/300 completed in 20.9s
Train: Loss=0.6799 (C:0.6799, R:0.0105) Ratio=4.48x
Val:   Loss=0.7889 (C:0.7889, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.135
✅ New best model saved (Val Loss: 0.7889)
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.372 ± 0.597
    Neg distances: 2.471 ± 1.063
    Separation ratio: 6.64x
    Gap: -4.175
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.7036 (C:0.7036, R:0.0105)
Batch  25/537: Loss=0.6354 (C:0.6354, R:0.0105)
Batch  50/537: Loss=0.6191 (C:0.6191, R:0.0105)
Batch  75/537: Loss=0.6649 (C:0.6649, R:0.0105)
Batch 100/537: Loss=0.6491 (C:0.6491, R:0.0105)
Batch 125/537: Loss=0.6577 (C:0.6577, R:0.0105)
Batch 150/537: Loss=0.6396 (C:0.6396, R:0.0105)
Batch 175/537: Loss=0.6167 (C:0.6167, R:0.0105)
Batch 200/537: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 225/537: Loss=0.7068 (C:0.7068, R:0.0105)
Batch 250/537: Loss=0.6641 (C:0.6641, R:0.0106)
Batch 275/537: Loss=0.6296 (C:0.6296, R:0.0106)
Batch 300/537: Loss=0.6415 (C:0.6415, R:0.0105)
Batch 325/537: Loss=0.6794 (C:0.6794, R:0.0105)
Batch 350/537: Loss=0.6700 (C:0.6700, R:0.0105)
Batch 375/537: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 400/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 425/537: Loss=0.6782 (C:0.6782, R:0.0105)
Batch 450/537: Loss=0.6572 (C:0.6572, R:0.0105)
Batch 475/537: Loss=0.6425 (C:0.6425, R:0.0105)
Batch 500/537: Loss=0.6895 (C:0.6895, R:0.0105)
Batch 525/537: Loss=0.6570 (C:0.6570, R:0.0105)

============================================================
Epoch 40/300 completed in 26.8s
Train: Loss=0.6596 (C:0.6596, R:0.0105) Ratio=4.39x
Val:   Loss=0.7701 (C:0.7701, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.7701)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6846 (C:0.6846, R:0.0105)
Batch  25/537: Loss=0.6233 (C:0.6233, R:0.0105)
Batch  50/537: Loss=0.7107 (C:0.7107, R:0.0105)
Batch  75/537: Loss=0.6290 (C:0.6290, R:0.0105)
Batch 100/537: Loss=0.6972 (C:0.6972, R:0.0105)
Batch 125/537: Loss=0.6385 (C:0.6385, R:0.0105)
Batch 150/537: Loss=0.6499 (C:0.6499, R:0.0105)
Batch 175/537: Loss=0.6783 (C:0.6783, R:0.0105)
Batch 200/537: Loss=0.6473 (C:0.6473, R:0.0105)
Batch 225/537: Loss=0.6778 (C:0.6778, R:0.0105)
Batch 250/537: Loss=0.6728 (C:0.6728, R:0.0105)
Batch 275/537: Loss=0.6695 (C:0.6695, R:0.0105)
Batch 300/537: Loss=0.6816 (C:0.6816, R:0.0105)
Batch 325/537: Loss=0.6559 (C:0.6559, R:0.0105)
Batch 350/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 375/537: Loss=0.6299 (C:0.6299, R:0.0106)
Batch 400/537: Loss=0.6411 (C:0.6411, R:0.0105)
Batch 425/537: Loss=0.6644 (C:0.6644, R:0.0105)
Batch 450/537: Loss=0.6545 (C:0.6545, R:0.0105)
Batch 475/537: Loss=0.6879 (C:0.6879, R:0.0105)
Batch 500/537: Loss=0.6173 (C:0.6173, R:0.0105)
Batch 525/537: Loss=0.6249 (C:0.6249, R:0.0105)

============================================================
Epoch 41/300 completed in 21.7s
Train: Loss=0.6563 (C:0.6563, R:0.0105) Ratio=4.50x
Val:   Loss=0.7722 (C:0.7722, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.165
No improvement for 1 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.6399 (C:0.6399, R:0.0105)
Batch  25/537: Loss=0.6251 (C:0.6251, R:0.0105)
Batch  50/537: Loss=0.6577 (C:0.6577, R:0.0105)
Batch  75/537: Loss=0.6264 (C:0.6264, R:0.0105)
Batch 100/537: Loss=0.6674 (C:0.6674, R:0.0105)
Batch 125/537: Loss=0.6510 (C:0.6510, R:0.0105)
Batch 150/537: Loss=0.6448 (C:0.6448, R:0.0105)
Batch 175/537: Loss=0.6680 (C:0.6680, R:0.0105)
Batch 200/537: Loss=0.6253 (C:0.6253, R:0.0105)
Batch 225/537: Loss=0.6893 (C:0.6893, R:0.0105)
Batch 250/537: Loss=0.6239 (C:0.6239, R:0.0105)
Batch 275/537: Loss=0.6635 (C:0.6635, R:0.0105)
Batch 300/537: Loss=0.6733 (C:0.6733, R:0.0105)
Batch 325/537: Loss=0.6758 (C:0.6758, R:0.0105)
Batch 350/537: Loss=0.6482 (C:0.6482, R:0.0106)
Batch 375/537: Loss=0.6465 (C:0.6465, R:0.0105)
Batch 400/537: Loss=0.6281 (C:0.6281, R:0.0105)
Batch 425/537: Loss=0.6130 (C:0.6130, R:0.0105)
Batch 450/537: Loss=0.6859 (C:0.6859, R:0.0105)
Batch 475/537: Loss=0.6618 (C:0.6618, R:0.0105)
Batch 500/537: Loss=0.6648 (C:0.6648, R:0.0105)
Batch 525/537: Loss=0.6730 (C:0.6730, R:0.0105)

============================================================
Epoch 42/300 completed in 21.3s
Train: Loss=0.6554 (C:0.6554, R:0.0105) Ratio=4.55x
Val:   Loss=0.7717 (C:0.7717, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.180
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.342 ± 0.565
    Neg distances: 2.536 ± 1.082
    Separation ratio: 7.42x
    Gap: -4.202
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.6148 (C:0.6148, R:0.0105)
Batch  25/537: Loss=0.6276 (C:0.6276, R:0.0105)
Batch  50/537: Loss=0.6192 (C:0.6192, R:0.0105)
Batch  75/537: Loss=0.6007 (C:0.6007, R:0.0105)
Batch 100/537: Loss=0.6499 (C:0.6499, R:0.0105)
Batch 125/537: Loss=0.5917 (C:0.5917, R:0.0105)
Batch 150/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 175/537: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 200/537: Loss=0.5956 (C:0.5956, R:0.0105)
Batch 225/537: Loss=0.6168 (C:0.6168, R:0.0105)
Batch 250/537: Loss=0.5908 (C:0.5908, R:0.0105)
Batch 275/537: Loss=0.6199 (C:0.6199, R:0.0105)
Batch 300/537: Loss=0.6077 (C:0.6077, R:0.0106)
Batch 325/537: Loss=0.6146 (C:0.6146, R:0.0105)
Batch 350/537: Loss=0.6178 (C:0.6178, R:0.0105)
Batch 375/537: Loss=0.6095 (C:0.6095, R:0.0105)
Batch 400/537: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 425/537: Loss=0.5977 (C:0.5977, R:0.0105)
Batch 450/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch 475/537: Loss=0.6375 (C:0.6375, R:0.0105)
Batch 500/537: Loss=0.6530 (C:0.6530, R:0.0105)
Batch 525/537: Loss=0.6398 (C:0.6398, R:0.0105)

============================================================
Epoch 43/300 completed in 26.4s
Train: Loss=0.6238 (C:0.6238, R:0.0105) Ratio=4.64x
Val:   Loss=0.7517 (C:0.7517, R:0.0104) Ratio=3.18x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.7517)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.5877 (C:0.5877, R:0.0105)
Batch  25/537: Loss=0.6301 (C:0.6301, R:0.0105)
Batch  50/537: Loss=0.6359 (C:0.6359, R:0.0105)
Batch  75/537: Loss=0.6158 (C:0.6158, R:0.0105)
Batch 100/537: Loss=0.6069 (C:0.6069, R:0.0105)
Batch 125/537: Loss=0.5981 (C:0.5981, R:0.0105)
Batch 150/537: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 175/537: Loss=0.6466 (C:0.6466, R:0.0105)
Batch 200/537: Loss=0.6208 (C:0.6208, R:0.0105)
Batch 225/537: Loss=0.6154 (C:0.6154, R:0.0105)
Batch 250/537: Loss=0.6739 (C:0.6739, R:0.0105)
Batch 275/537: Loss=0.6243 (C:0.6243, R:0.0105)
Batch 300/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 325/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 350/537: Loss=0.6217 (C:0.6217, R:0.0105)
Batch 375/537: Loss=0.6308 (C:0.6308, R:0.0105)
Batch 400/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch 425/537: Loss=0.6209 (C:0.6209, R:0.0105)
Batch 450/537: Loss=0.6427 (C:0.6427, R:0.0105)
Batch 475/537: Loss=0.6342 (C:0.6342, R:0.0105)
Batch 500/537: Loss=0.6751 (C:0.6751, R:0.0106)
Batch 525/537: Loss=0.6292 (C:0.6292, R:0.0105)

============================================================
Epoch 44/300 completed in 20.9s
Train: Loss=0.6229 (C:0.6229, R:0.0105) Ratio=4.52x
Val:   Loss=0.7521 (C:0.7521, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.6234 (C:0.6234, R:0.0105)
Batch  25/537: Loss=0.6273 (C:0.6273, R:0.0105)
Batch  50/537: Loss=0.5948 (C:0.5948, R:0.0105)
Batch  75/537: Loss=0.6437 (C:0.6437, R:0.0105)
Batch 100/537: Loss=0.5648 (C:0.5648, R:0.0105)
Batch 125/537: Loss=0.5919 (C:0.5919, R:0.0105)
Batch 150/537: Loss=0.6227 (C:0.6227, R:0.0105)
Batch 175/537: Loss=0.6183 (C:0.6183, R:0.0105)
Batch 200/537: Loss=0.6400 (C:0.6400, R:0.0105)
Batch 225/537: Loss=0.5892 (C:0.5892, R:0.0105)
Batch 250/537: Loss=0.6293 (C:0.6293, R:0.0106)
Batch 275/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 300/537: Loss=0.6379 (C:0.6379, R:0.0105)
Batch 325/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 350/537: Loss=0.6270 (C:0.6270, R:0.0105)
Batch 375/537: Loss=0.6252 (C:0.6252, R:0.0105)
Batch 400/537: Loss=0.6099 (C:0.6099, R:0.0105)
Batch 425/537: Loss=0.6482 (C:0.6482, R:0.0105)
Batch 450/537: Loss=0.6145 (C:0.6145, R:0.0105)
Batch 475/537: Loss=0.6387 (C:0.6387, R:0.0105)
Batch 500/537: Loss=0.5977 (C:0.5977, R:0.0105)
Batch 525/537: Loss=0.6108 (C:0.6108, R:0.0105)

============================================================
Epoch 45/300 completed in 21.0s
Train: Loss=0.6197 (C:0.6197, R:0.0105) Ratio=4.64x
Val:   Loss=0.7489 (C:0.7489, R:0.0104) Ratio=3.20x
Reconstruction weight: 0.225
✅ New best model saved (Val Loss: 0.7489)
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.351 ± 0.600
    Neg distances: 2.530 ± 1.080
    Separation ratio: 7.21x
    Gap: -4.273
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.5939 (C:0.5939, R:0.0105)
Batch  25/537: Loss=0.6093 (C:0.6093, R:0.0105)
Batch  50/537: Loss=0.6151 (C:0.6151, R:0.0105)
Batch  75/537: Loss=0.5915 (C:0.5915, R:0.0105)
Batch 100/537: Loss=0.6301 (C:0.6301, R:0.0106)
Batch 125/537: Loss=0.5948 (C:0.5948, R:0.0105)
Batch 150/537: Loss=0.5732 (C:0.5732, R:0.0105)
Batch 175/537: Loss=0.6162 (C:0.6162, R:0.0105)
Batch 200/537: Loss=0.6048 (C:0.6048, R:0.0105)
Batch 225/537: Loss=0.6467 (C:0.6467, R:0.0105)
Batch 250/537: Loss=0.5959 (C:0.5959, R:0.0105)
Batch 275/537: Loss=0.6063 (C:0.6063, R:0.0105)
Batch 300/537: Loss=0.6296 (C:0.6296, R:0.0105)
Batch 325/537: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 350/537: Loss=0.6401 (C:0.6401, R:0.0105)
Batch 375/537: Loss=0.6203 (C:0.6203, R:0.0106)
Batch 400/537: Loss=0.6334 (C:0.6334, R:0.0105)
Batch 425/537: Loss=0.5792 (C:0.5792, R:0.0105)
Batch 450/537: Loss=0.6446 (C:0.6446, R:0.0105)
Batch 475/537: Loss=0.6420 (C:0.6420, R:0.0105)
Batch 500/537: Loss=0.6503 (C:0.6503, R:0.0105)
Batch 525/537: Loss=0.6212 (C:0.6212, R:0.0105)

============================================================
Epoch 46/300 completed in 26.5s
Train: Loss=0.6180 (C:0.6180, R:0.0105) Ratio=4.62x
Val:   Loss=0.7462 (C:0.7462, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.240
✅ New best model saved (Val Loss: 0.7462)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch  25/537: Loss=0.5826 (C:0.5826, R:0.0105)
Batch  50/537: Loss=0.6042 (C:0.6042, R:0.0105)
Batch  75/537: Loss=0.6053 (C:0.6053, R:0.0105)
Batch 100/537: Loss=0.6197 (C:0.6197, R:0.0105)
Batch 125/537: Loss=0.6182 (C:0.6182, R:0.0105)
Batch 150/537: Loss=0.6306 (C:0.6306, R:0.0105)
Batch 175/537: Loss=0.5967 (C:0.5967, R:0.0105)
Batch 200/537: Loss=0.6430 (C:0.6430, R:0.0105)
Batch 225/537: Loss=0.6804 (C:0.6804, R:0.0105)
Batch 250/537: Loss=0.6149 (C:0.6149, R:0.0105)
Batch 275/537: Loss=0.6130 (C:0.6130, R:0.0105)
Batch 300/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 325/537: Loss=0.6184 (C:0.6184, R:0.0105)
Batch 350/537: Loss=0.6190 (C:0.6190, R:0.0105)
Batch 375/537: Loss=0.6321 (C:0.6321, R:0.0105)
Batch 400/537: Loss=0.5906 (C:0.5906, R:0.0105)
Batch 425/537: Loss=0.6130 (C:0.6130, R:0.0106)
Batch 450/537: Loss=0.6147 (C:0.6147, R:0.0105)
Batch 475/537: Loss=0.6192 (C:0.6192, R:0.0105)
Batch 500/537: Loss=0.6140 (C:0.6140, R:0.0105)
Batch 525/537: Loss=0.5962 (C:0.5962, R:0.0106)

============================================================
Epoch 47/300 completed in 21.2s
Train: Loss=0.6154 (C:0.6154, R:0.0105) Ratio=4.61x
Val:   Loss=0.7446 (C:0.7446, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.255
✅ New best model saved (Val Loss: 0.7446)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.6073 (C:0.6073, R:0.0105)
Batch  25/537: Loss=0.6168 (C:0.6168, R:0.0105)
Batch  50/537: Loss=0.5971 (C:0.5971, R:0.0106)
Batch  75/537: Loss=0.6034 (C:0.6034, R:0.0105)
Batch 100/537: Loss=0.6142 (C:0.6142, R:0.0105)
Batch 125/537: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 150/537: Loss=0.5930 (C:0.5930, R:0.0105)
Batch 175/537: Loss=0.6008 (C:0.6008, R:0.0105)
Batch 200/537: Loss=0.6577 (C:0.6577, R:0.0105)
Batch 225/537: Loss=0.6242 (C:0.6242, R:0.0105)
Batch 250/537: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 275/537: Loss=0.6279 (C:0.6279, R:0.0105)
Batch 300/537: Loss=0.6168 (C:0.6168, R:0.0105)
Batch 325/537: Loss=0.5971 (C:0.5971, R:0.0105)
Batch 350/537: Loss=0.6333 (C:0.6333, R:0.0105)
Batch 375/537: Loss=0.6281 (C:0.6281, R:0.0105)
Batch 400/537: Loss=0.6538 (C:0.6538, R:0.0105)
Batch 425/537: Loss=0.6410 (C:0.6410, R:0.0105)
Batch 450/537: Loss=0.6713 (C:0.6713, R:0.0105)
Batch 475/537: Loss=0.6246 (C:0.6246, R:0.0105)
Batch 500/537: Loss=0.6353 (C:0.6353, R:0.0105)
Batch 525/537: Loss=0.6403 (C:0.6403, R:0.0105)

============================================================
Epoch 48/300 completed in 20.8s
Train: Loss=0.6159 (C:0.6159, R:0.0105) Ratio=4.63x
Val:   Loss=0.7501 (C:0.7501, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.270
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.365 ± 0.612
    Neg distances: 2.572 ± 1.102
    Separation ratio: 7.04x
    Gap: -4.363
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.6325 (C:0.6325, R:0.0105)
Batch  25/537: Loss=0.5817 (C:0.5817, R:0.0105)
Batch  50/537: Loss=0.6047 (C:0.6047, R:0.0105)
Batch  75/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 100/537: Loss=0.5469 (C:0.5469, R:0.0105)
Batch 125/537: Loss=0.6620 (C:0.6620, R:0.0105)
Batch 150/537: Loss=0.5932 (C:0.5932, R:0.0105)
Batch 175/537: Loss=0.6410 (C:0.6410, R:0.0104)
Batch 200/537: Loss=0.6105 (C:0.6105, R:0.0105)
Batch 225/537: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 250/537: Loss=0.6344 (C:0.6344, R:0.0105)
Batch 275/537: Loss=0.6747 (C:0.6747, R:0.0105)
Batch 300/537: Loss=0.6131 (C:0.6131, R:0.0105)
Batch 325/537: Loss=0.6327 (C:0.6327, R:0.0105)
Batch 350/537: Loss=0.6519 (C:0.6519, R:0.0105)
Batch 375/537: Loss=0.6256 (C:0.6256, R:0.0105)
Batch 400/537: Loss=0.6371 (C:0.6371, R:0.0105)
Batch 425/537: Loss=0.6379 (C:0.6379, R:0.0105)
Batch 450/537: Loss=0.5945 (C:0.5945, R:0.0105)
Batch 475/537: Loss=0.6223 (C:0.6223, R:0.0105)
Batch 500/537: Loss=0.6403 (C:0.6403, R:0.0105)
Batch 525/537: Loss=0.6377 (C:0.6377, R:0.0105)

============================================================
Epoch 49/300 completed in 26.7s
Train: Loss=0.6211 (C:0.6211, R:0.0105) Ratio=4.70x
Val:   Loss=0.7550 (C:0.7550, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.285
No improvement for 2 epochs
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.6091 (C:0.6091, R:0.0105)
Batch  25/537: Loss=0.5965 (C:0.5965, R:0.0105)
Batch  50/537: Loss=0.6062 (C:0.6062, R:0.0105)
Batch  75/537: Loss=0.5890 (C:0.5890, R:0.0105)
Batch 100/537: Loss=0.6076 (C:0.6076, R:0.0105)
Batch 125/537: Loss=0.6288 (C:0.6288, R:0.0105)
Batch 150/537: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 175/537: Loss=0.6280 (C:0.6280, R:0.0105)
Batch 200/537: Loss=0.6388 (C:0.6388, R:0.0105)
Batch 225/537: Loss=0.6033 (C:0.6033, R:0.0105)
Batch 250/537: Loss=0.6062 (C:0.6062, R:0.0105)
Batch 275/537: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 300/537: Loss=0.6446 (C:0.6446, R:0.0105)
Batch 325/537: Loss=0.6179 (C:0.6179, R:0.0105)
Batch 350/537: Loss=0.5891 (C:0.5891, R:0.0105)
Batch 375/537: Loss=0.6038 (C:0.6038, R:0.0105)
Batch 400/537: Loss=0.6096 (C:0.6096, R:0.0105)
Batch 425/537: Loss=0.6403 (C:0.6403, R:0.0105)
Batch 450/537: Loss=0.6516 (C:0.6516, R:0.0105)
Batch 475/537: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 500/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 525/537: Loss=0.6393 (C:0.6393, R:0.0105)

============================================================
Epoch 50/300 completed in 21.1s
Train: Loss=0.6171 (C:0.6171, R:0.0105) Ratio=4.73x
Val:   Loss=0.7580 (C:0.7580, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.6078 (C:0.6078, R:0.0105)
Batch  25/537: Loss=0.5986 (C:0.5986, R:0.0105)
Batch  50/537: Loss=0.5997 (C:0.5997, R:0.0105)
Batch  75/537: Loss=0.6326 (C:0.6326, R:0.0105)
Batch 100/537: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 125/537: Loss=0.6228 (C:0.6228, R:0.0105)
Batch 150/537: Loss=0.6237 (C:0.6237, R:0.0106)
Batch 175/537: Loss=0.6024 (C:0.6024, R:0.0105)
Batch 200/537: Loss=0.5941 (C:0.5941, R:0.0105)
Batch 225/537: Loss=0.6006 (C:0.6006, R:0.0105)
Batch 250/537: Loss=0.6040 (C:0.6040, R:0.0105)
Batch 275/537: Loss=0.5784 (C:0.5784, R:0.0105)
Batch 300/537: Loss=0.5991 (C:0.5991, R:0.0105)
Batch 325/537: Loss=0.6171 (C:0.6171, R:0.0105)
Batch 350/537: Loss=0.6469 (C:0.6469, R:0.0105)
Batch 375/537: Loss=0.6113 (C:0.6113, R:0.0105)
Batch 400/537: Loss=0.6268 (C:0.6268, R:0.0105)
Batch 425/537: Loss=0.6579 (C:0.6579, R:0.0105)
Batch 450/537: Loss=0.6569 (C:0.6569, R:0.0105)
Batch 475/537: Loss=0.6128 (C:0.6128, R:0.0105)
Batch 500/537: Loss=0.6279 (C:0.6279, R:0.0105)
Batch 525/537: Loss=0.6412 (C:0.6412, R:0.0105)

============================================================
Epoch 51/300 completed in 20.9s
Train: Loss=0.6160 (C:0.6160, R:0.0105) Ratio=4.75x
Val:   Loss=0.7515 (C:0.7515, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.329 ± 0.583
    Neg distances: 2.573 ± 1.080
    Separation ratio: 7.82x
    Gap: -4.340
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.5950 (C:0.5950, R:0.0105)
Batch  25/537: Loss=0.5488 (C:0.5488, R:0.0106)
Batch  50/537: Loss=0.5706 (C:0.5706, R:0.0105)
Batch  75/537: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 100/537: Loss=0.6189 (C:0.6189, R:0.0105)
Batch 125/537: Loss=0.5525 (C:0.5525, R:0.0105)
Batch 150/537: Loss=0.5649 (C:0.5649, R:0.0105)
Batch 175/537: Loss=0.5737 (C:0.5737, R:0.0105)
Batch 200/537: Loss=0.6008 (C:0.6008, R:0.0105)
Batch 225/537: Loss=0.5835 (C:0.5835, R:0.0106)
Batch 250/537: Loss=0.6006 (C:0.6006, R:0.0105)
Batch 275/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 300/537: Loss=0.5971 (C:0.5971, R:0.0105)
Batch 325/537: Loss=0.5757 (C:0.5757, R:0.0105)
Batch 350/537: Loss=0.6074 (C:0.6074, R:0.0105)
Batch 375/537: Loss=0.6237 (C:0.6237, R:0.0105)
Batch 400/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 425/537: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 450/537: Loss=0.5431 (C:0.5431, R:0.0105)
Batch 475/537: Loss=0.5960 (C:0.5960, R:0.0105)
Batch 500/537: Loss=0.5535 (C:0.5535, R:0.0105)
Batch 525/537: Loss=0.5775 (C:0.5775, R:0.0105)

============================================================
Epoch 52/300 completed in 27.2s
Train: Loss=0.5869 (C:0.5869, R:0.0105) Ratio=4.78x
Val:   Loss=0.7175 (C:0.7175, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7175)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.5609 (C:0.5609, R:0.0105)
Batch  25/537: Loss=0.5904 (C:0.5904, R:0.0105)
Batch  50/537: Loss=0.6110 (C:0.6110, R:0.0105)
Batch  75/537: Loss=0.6310 (C:0.6310, R:0.0105)
Batch 100/537: Loss=0.5794 (C:0.5794, R:0.0105)
Batch 125/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 150/537: Loss=0.6215 (C:0.6215, R:0.0105)
Batch 175/537: Loss=0.5854 (C:0.5854, R:0.0105)
Batch 200/537: Loss=0.5825 (C:0.5825, R:0.0105)
Batch 225/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 250/537: Loss=0.5958 (C:0.5958, R:0.0105)
Batch 275/537: Loss=0.5910 (C:0.5910, R:0.0105)
Batch 300/537: Loss=0.5685 (C:0.5685, R:0.0105)
Batch 325/537: Loss=0.5923 (C:0.5923, R:0.0105)
Batch 350/537: Loss=0.6145 (C:0.6145, R:0.0105)
Batch 375/537: Loss=0.6010 (C:0.6010, R:0.0105)
Batch 400/537: Loss=0.5834 (C:0.5834, R:0.0105)
Batch 425/537: Loss=0.5604 (C:0.5604, R:0.0105)
Batch 450/537: Loss=0.5661 (C:0.5661, R:0.0105)
Batch 475/537: Loss=0.5381 (C:0.5381, R:0.0105)
Batch 500/537: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 525/537: Loss=0.5678 (C:0.5678, R:0.0105)

============================================================
Epoch 53/300 completed in 21.4s
Train: Loss=0.5853 (C:0.5853, R:0.0105) Ratio=4.78x
Val:   Loss=0.7233 (C:0.7233, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.5700 (C:0.5700, R:0.0105)
Batch  25/537: Loss=0.5869 (C:0.5869, R:0.0105)
Batch  50/537: Loss=0.5623 (C:0.5623, R:0.0105)
Batch  75/537: Loss=0.5732 (C:0.5732, R:0.0105)
Batch 100/537: Loss=0.6382 (C:0.6382, R:0.0105)
Batch 125/537: Loss=0.6113 (C:0.6113, R:0.0105)
Batch 150/537: Loss=0.5796 (C:0.5796, R:0.0105)
Batch 175/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch 200/537: Loss=0.5716 (C:0.5716, R:0.0105)
Batch 225/537: Loss=0.5895 (C:0.5895, R:0.0106)
Batch 250/537: Loss=0.5841 (C:0.5841, R:0.0105)
Batch 275/537: Loss=0.6044 (C:0.6044, R:0.0105)
Batch 300/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 325/537: Loss=0.6083 (C:0.6083, R:0.0105)
Batch 350/537: Loss=0.5925 (C:0.5925, R:0.0105)
Batch 375/537: Loss=0.5703 (C:0.5703, R:0.0105)
Batch 400/537: Loss=0.5732 (C:0.5732, R:0.0105)
Batch 425/537: Loss=0.5825 (C:0.5825, R:0.0105)
Batch 450/537: Loss=0.5831 (C:0.5831, R:0.0105)
Batch 475/537: Loss=0.5994 (C:0.5994, R:0.0105)
Batch 500/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 525/537: Loss=0.5826 (C:0.5826, R:0.0105)

============================================================
Epoch 54/300 completed in 21.0s
Train: Loss=0.5840 (C:0.5840, R:0.0105) Ratio=4.76x
Val:   Loss=0.7223 (C:0.7223, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.324 ± 0.574
    Neg distances: 2.601 ± 1.087
    Separation ratio: 8.02x
    Gap: -4.364
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.5631 (C:0.5631, R:0.0106)
Batch  25/537: Loss=0.5626 (C:0.5626, R:0.0105)
Batch  50/537: Loss=0.5707 (C:0.5707, R:0.0105)
Batch  75/537: Loss=0.5585 (C:0.5585, R:0.0105)
Batch 100/537: Loss=0.5591 (C:0.5591, R:0.0105)
Batch 125/537: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 150/537: Loss=0.5846 (C:0.5846, R:0.0105)
Batch 175/537: Loss=0.5822 (C:0.5822, R:0.0105)
Batch 200/537: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 225/537: Loss=0.5690 (C:0.5690, R:0.0105)
Batch 250/537: Loss=0.5883 (C:0.5883, R:0.0105)
Batch 275/537: Loss=0.5564 (C:0.5564, R:0.0105)
Batch 300/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 325/537: Loss=0.5827 (C:0.5827, R:0.0105)
Batch 350/537: Loss=0.5926 (C:0.5926, R:0.0104)
Batch 375/537: Loss=0.5590 (C:0.5590, R:0.0106)
Batch 400/537: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 425/537: Loss=0.5681 (C:0.5681, R:0.0105)
Batch 450/537: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 475/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 500/537: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 525/537: Loss=0.5776 (C:0.5776, R:0.0105)

============================================================
Epoch 55/300 completed in 26.5s
Train: Loss=0.5743 (C:0.5743, R:0.0105) Ratio=4.88x
Val:   Loss=0.7205 (C:0.7205, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.5933 (C:0.5933, R:0.0105)
Batch  25/537: Loss=0.5459 (C:0.5459, R:0.0105)
Batch  50/537: Loss=0.5940 (C:0.5940, R:0.0105)
Batch  75/537: Loss=0.5544 (C:0.5544, R:0.0105)
Batch 100/537: Loss=0.5473 (C:0.5473, R:0.0105)
Batch 125/537: Loss=0.5769 (C:0.5769, R:0.0105)
Batch 150/537: Loss=0.5636 (C:0.5636, R:0.0105)
Batch 175/537: Loss=0.5441 (C:0.5441, R:0.0105)
Batch 200/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 225/537: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 250/537: Loss=0.5519 (C:0.5519, R:0.0105)
Batch 275/537: Loss=0.5783 (C:0.5783, R:0.0105)
Batch 300/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 325/537: Loss=0.5854 (C:0.5854, R:0.0105)
Batch 350/537: Loss=0.5662 (C:0.5662, R:0.0105)
Batch 375/537: Loss=0.5816 (C:0.5816, R:0.0106)
Batch 400/537: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 425/537: Loss=0.5772 (C:0.5772, R:0.0105)
Batch 450/537: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 475/537: Loss=0.6286 (C:0.6286, R:0.0105)
Batch 500/537: Loss=0.5653 (C:0.5653, R:0.0105)
Batch 525/537: Loss=0.5521 (C:0.5521, R:0.0105)

============================================================
Epoch 56/300 completed in 20.9s
Train: Loss=0.5736 (C:0.5736, R:0.0105) Ratio=4.88x
Val:   Loss=0.7260 (C:0.7260, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.5815 (C:0.5815, R:0.0105)
Batch  25/537: Loss=0.5613 (C:0.5613, R:0.0105)
Batch  50/537: Loss=0.5598 (C:0.5598, R:0.0105)
Batch  75/537: Loss=0.5776 (C:0.5776, R:0.0105)
Batch 100/537: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 125/537: Loss=0.5719 (C:0.5719, R:0.0106)
Batch 150/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 175/537: Loss=0.5813 (C:0.5813, R:0.0105)
Batch 200/537: Loss=0.5703 (C:0.5703, R:0.0105)
Batch 225/537: Loss=0.5707 (C:0.5707, R:0.0106)
Batch 250/537: Loss=0.5853 (C:0.5853, R:0.0106)
Batch 275/537: Loss=0.5859 (C:0.5859, R:0.0105)
Batch 300/537: Loss=0.5523 (C:0.5523, R:0.0105)
Batch 325/537: Loss=0.5314 (C:0.5314, R:0.0105)
Batch 350/537: Loss=0.5536 (C:0.5536, R:0.0105)
Batch 375/537: Loss=0.5764 (C:0.5764, R:0.0105)
Batch 400/537: Loss=0.5832 (C:0.5832, R:0.0105)
Batch 425/537: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 450/537: Loss=0.6057 (C:0.6057, R:0.0105)
Batch 475/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 500/537: Loss=0.5678 (C:0.5678, R:0.0106)
Batch 525/537: Loss=0.5922 (C:0.5922, R:0.0106)

============================================================
Epoch 57/300 completed in 20.8s
Train: Loss=0.5699 (C:0.5699, R:0.0105) Ratio=4.96x
Val:   Loss=0.7128 (C:0.7128, R:0.0104) Ratio=3.18x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7128)
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.335 ± 0.612
    Neg distances: 2.609 ± 1.099
    Separation ratio: 7.79x
    Gap: -4.396
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.5724 (C:0.5724, R:0.0105)
Batch  25/537: Loss=0.5796 (C:0.5796, R:0.0105)
Batch  50/537: Loss=0.5718 (C:0.5718, R:0.0105)
Batch  75/537: Loss=0.5802 (C:0.5802, R:0.0105)
Batch 100/537: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 125/537: Loss=0.5876 (C:0.5876, R:0.0105)
Batch 150/537: Loss=0.5676 (C:0.5676, R:0.0105)
Batch 175/537: Loss=0.5462 (C:0.5462, R:0.0106)
Batch 200/537: Loss=0.6032 (C:0.6032, R:0.0105)
Batch 225/537: Loss=0.5900 (C:0.5900, R:0.0105)
Batch 250/537: Loss=0.5782 (C:0.5782, R:0.0105)
Batch 275/537: Loss=0.5959 (C:0.5959, R:0.0105)
Batch 300/537: Loss=0.6126 (C:0.6126, R:0.0106)
Batch 325/537: Loss=0.5622 (C:0.5622, R:0.0106)
Batch 350/537: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 375/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 400/537: Loss=0.5650 (C:0.5650, R:0.0105)
Batch 425/537: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 450/537: Loss=0.5506 (C:0.5506, R:0.0105)
Batch 475/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch 500/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 525/537: Loss=0.5951 (C:0.5951, R:0.0105)

============================================================
Epoch 58/300 completed in 26.4s
Train: Loss=0.5767 (C:0.5767, R:0.0105) Ratio=4.93x
Val:   Loss=0.7218 (C:0.7218, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.5975 (C:0.5975, R:0.0105)
Batch  25/537: Loss=0.5709 (C:0.5709, R:0.0105)
Batch  50/537: Loss=0.5426 (C:0.5426, R:0.0105)
Batch  75/537: Loss=0.5862 (C:0.5862, R:0.0106)
Batch 100/537: Loss=0.5476 (C:0.5476, R:0.0105)
Batch 125/537: Loss=0.5725 (C:0.5725, R:0.0105)
Batch 150/537: Loss=0.6205 (C:0.6205, R:0.0105)
Batch 175/537: Loss=0.5725 (C:0.5725, R:0.0105)
Batch 200/537: Loss=0.6037 (C:0.6037, R:0.0105)
Batch 225/537: Loss=0.5445 (C:0.5445, R:0.0105)
Batch 250/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 275/537: Loss=0.5846 (C:0.5846, R:0.0105)
Batch 300/537: Loss=0.5628 (C:0.5628, R:0.0105)
Batch 325/537: Loss=0.5653 (C:0.5653, R:0.0105)
Batch 350/537: Loss=0.5538 (C:0.5538, R:0.0105)
Batch 375/537: Loss=0.5911 (C:0.5911, R:0.0105)
Batch 400/537: Loss=0.5963 (C:0.5963, R:0.0105)
Batch 425/537: Loss=0.6042 (C:0.6042, R:0.0105)
Batch 450/537: Loss=0.5592 (C:0.5592, R:0.0105)
Batch 475/537: Loss=0.6176 (C:0.6176, R:0.0105)
Batch 500/537: Loss=0.6017 (C:0.6017, R:0.0105)
Batch 525/537: Loss=0.5631 (C:0.5631, R:0.0105)

============================================================
Epoch 59/300 completed in 20.9s
Train: Loss=0.5766 (C:0.5766, R:0.0105) Ratio=4.91x
Val:   Loss=0.7284 (C:0.7284, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.5686 (C:0.5686, R:0.0105)
Batch  25/537: Loss=0.5714 (C:0.5714, R:0.0105)
Batch  50/537: Loss=0.5618 (C:0.5618, R:0.0105)
Batch  75/537: Loss=0.5409 (C:0.5409, R:0.0105)
Batch 100/537: Loss=0.5852 (C:0.5852, R:0.0106)
Batch 125/537: Loss=0.5442 (C:0.5442, R:0.0105)
Batch 150/537: Loss=0.5657 (C:0.5657, R:0.0105)
Batch 175/537: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 200/537: Loss=0.5692 (C:0.5692, R:0.0105)
Batch 225/537: Loss=0.5833 (C:0.5833, R:0.0105)
Batch 250/537: Loss=0.5797 (C:0.5797, R:0.0105)
Batch 275/537: Loss=0.6096 (C:0.6096, R:0.0105)
Batch 300/537: Loss=0.5133 (C:0.5133, R:0.0105)
Batch 325/537: Loss=0.5597 (C:0.5597, R:0.0105)
Batch 350/537: Loss=0.5717 (C:0.5717, R:0.0105)
Batch 375/537: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 400/537: Loss=0.5860 (C:0.5860, R:0.0105)
Batch 425/537: Loss=0.5934 (C:0.5934, R:0.0105)
Batch 450/537: Loss=0.6098 (C:0.6098, R:0.0106)
Batch 475/537: Loss=0.5655 (C:0.5655, R:0.0105)
Batch 500/537: Loss=0.5904 (C:0.5904, R:0.0105)
Batch 525/537: Loss=0.5797 (C:0.5797, R:0.0105)

============================================================
Epoch 60/300 completed in 20.9s
Train: Loss=0.5752 (C:0.5752, R:0.0105) Ratio=5.04x
Val:   Loss=0.7291 (C:0.7291, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 3 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.331 ± 0.591
    Neg distances: 2.604 ± 1.095
    Separation ratio: 7.87x
    Gap: -4.448
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch  25/537: Loss=0.6035 (C:0.6035, R:0.0105)
Batch  50/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch  75/537: Loss=0.5269 (C:0.5269, R:0.0105)
Batch 100/537: Loss=0.5588 (C:0.5588, R:0.0106)
Batch 125/537: Loss=0.5093 (C:0.5093, R:0.0106)
Batch 150/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 175/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch 200/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 225/537: Loss=0.6127 (C:0.6127, R:0.0105)
Batch 250/537: Loss=0.5336 (C:0.5336, R:0.0105)
Batch 275/537: Loss=0.5407 (C:0.5407, R:0.0105)
Batch 300/537: Loss=0.5803 (C:0.5803, R:0.0105)
Batch 325/537: Loss=0.5817 (C:0.5817, R:0.0105)
Batch 350/537: Loss=0.6057 (C:0.6057, R:0.0105)
Batch 375/537: Loss=0.6228 (C:0.6228, R:0.0105)
Batch 400/537: Loss=0.5871 (C:0.5871, R:0.0105)
Batch 425/537: Loss=0.5701 (C:0.5701, R:0.0105)
Batch 450/537: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 475/537: Loss=0.5918 (C:0.5918, R:0.0105)
Batch 500/537: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 525/537: Loss=0.5897 (C:0.5897, R:0.0105)

============================================================
Epoch 61/300 completed in 26.6s
Train: Loss=0.5693 (C:0.5693, R:0.0105) Ratio=4.99x
Val:   Loss=0.7236 (C:0.7236, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.5448 (C:0.5448, R:0.0105)
Batch  25/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch  50/537: Loss=0.5665 (C:0.5665, R:0.0105)
Batch  75/537: Loss=0.5105 (C:0.5105, R:0.0105)
Batch 100/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 125/537: Loss=0.5920 (C:0.5920, R:0.0105)
Batch 150/537: Loss=0.5859 (C:0.5859, R:0.0105)
Batch 175/537: Loss=0.5961 (C:0.5961, R:0.0105)
Batch 200/537: Loss=0.5828 (C:0.5828, R:0.0105)
Batch 225/537: Loss=0.5600 (C:0.5600, R:0.0105)
Batch 250/537: Loss=0.5981 (C:0.5981, R:0.0105)
Batch 275/537: Loss=0.5632 (C:0.5632, R:0.0105)
Batch 300/537: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 325/537: Loss=0.5267 (C:0.5267, R:0.0105)
Batch 350/537: Loss=0.6498 (C:0.6498, R:0.0106)
Batch 375/537: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 400/537: Loss=0.5393 (C:0.5393, R:0.0105)
Batch 425/537: Loss=0.5628 (C:0.5628, R:0.0105)
Batch 450/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 475/537: Loss=0.5655 (C:0.5655, R:0.0105)
Batch 500/537: Loss=0.5676 (C:0.5676, R:0.0105)
Batch 525/537: Loss=0.5593 (C:0.5593, R:0.0105)

============================================================
Epoch 62/300 completed in 20.9s
Train: Loss=0.5674 (C:0.5674, R:0.0105) Ratio=5.02x
Val:   Loss=0.7173 (C:0.7173, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.5435 (C:0.5435, R:0.0105)
Batch  25/537: Loss=0.5401 (C:0.5401, R:0.0105)
Batch  50/537: Loss=0.6066 (C:0.6066, R:0.0105)
Batch  75/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 100/537: Loss=0.5764 (C:0.5764, R:0.0105)
Batch 125/537: Loss=0.5964 (C:0.5964, R:0.0105)
Batch 150/537: Loss=0.5760 (C:0.5760, R:0.0105)
Batch 175/537: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 200/537: Loss=0.5319 (C:0.5319, R:0.0105)
Batch 225/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 250/537: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 275/537: Loss=0.5882 (C:0.5882, R:0.0105)
Batch 300/537: Loss=0.5678 (C:0.5678, R:0.0105)
Batch 325/537: Loss=0.6025 (C:0.6025, R:0.0105)
Batch 350/537: Loss=0.5725 (C:0.5725, R:0.0105)
Batch 375/537: Loss=0.5581 (C:0.5581, R:0.0105)
Batch 400/537: Loss=0.5799 (C:0.5799, R:0.0105)
Batch 425/537: Loss=0.5897 (C:0.5897, R:0.0105)
Batch 450/537: Loss=0.6092 (C:0.6092, R:0.0105)
Batch 475/537: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 500/537: Loss=0.5532 (C:0.5532, R:0.0106)
Batch 525/537: Loss=0.5584 (C:0.5584, R:0.0105)

============================================================
Epoch 63/300 completed in 20.9s
Train: Loss=0.5681 (C:0.5681, R:0.0105) Ratio=5.06x
Val:   Loss=0.7141 (C:0.7141, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.305 ± 0.571
    Neg distances: 2.616 ± 1.083
    Separation ratio: 8.58x
    Gap: -4.486
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.5251 (C:0.5251, R:0.0105)
Batch  25/537: Loss=0.5349 (C:0.5349, R:0.0105)
Batch  50/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch  75/537: Loss=0.5488 (C:0.5488, R:0.0105)
Batch 100/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 125/537: Loss=0.5306 (C:0.5306, R:0.0105)
Batch 150/537: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 175/537: Loss=0.5248 (C:0.5248, R:0.0105)
Batch 200/537: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 225/537: Loss=0.5625 (C:0.5625, R:0.0105)
Batch 250/537: Loss=0.5408 (C:0.5408, R:0.0105)
Batch 275/537: Loss=0.5431 (C:0.5431, R:0.0105)
Batch 300/537: Loss=0.5712 (C:0.5712, R:0.0105)
Batch 325/537: Loss=0.5467 (C:0.5467, R:0.0105)
Batch 350/537: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 375/537: Loss=0.5235 (C:0.5235, R:0.0105)
Batch 400/537: Loss=0.5577 (C:0.5577, R:0.0105)
Batch 425/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch 450/537: Loss=0.5451 (C:0.5451, R:0.0105)
Batch 475/537: Loss=0.5442 (C:0.5442, R:0.0105)
Batch 500/537: Loss=0.5400 (C:0.5400, R:0.0105)
Batch 525/537: Loss=0.5523 (C:0.5523, R:0.0105)

============================================================
Epoch 64/300 completed in 27.1s
Train: Loss=0.5453 (C:0.5453, R:0.0105) Ratio=4.99x
Val:   Loss=0.6985 (C:0.6985, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6985)
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.5449 (C:0.5449, R:0.0105)
Batch  25/537: Loss=0.5557 (C:0.5557, R:0.0105)
Batch  50/537: Loss=0.5686 (C:0.5686, R:0.0105)
Batch  75/537: Loss=0.5750 (C:0.5750, R:0.0105)
Batch 100/537: Loss=0.5683 (C:0.5683, R:0.0105)
Batch 125/537: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 150/537: Loss=0.5238 (C:0.5238, R:0.0105)
Batch 175/537: Loss=0.5530 (C:0.5530, R:0.0105)
Batch 200/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch 225/537: Loss=0.5552 (C:0.5552, R:0.0105)
Batch 250/537: Loss=0.5414 (C:0.5414, R:0.0105)
Batch 275/537: Loss=0.5292 (C:0.5292, R:0.0105)
Batch 300/537: Loss=0.5585 (C:0.5585, R:0.0105)
Batch 325/537: Loss=0.5479 (C:0.5479, R:0.0105)
Batch 350/537: Loss=0.5833 (C:0.5833, R:0.0105)
Batch 375/537: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 400/537: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 425/537: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 450/537: Loss=0.5474 (C:0.5474, R:0.0105)
Batch 475/537: Loss=0.5463 (C:0.5463, R:0.0105)
Batch 500/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 525/537: Loss=0.5592 (C:0.5592, R:0.0106)

============================================================
Epoch 65/300 completed in 21.3s
Train: Loss=0.5451 (C:0.5451, R:0.0105) Ratio=4.94x
Val:   Loss=0.6982 (C:0.6982, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6982)
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.5461 (C:0.5461, R:0.0105)
Batch  25/537: Loss=0.5350 (C:0.5350, R:0.0105)
Batch  50/537: Loss=0.5354 (C:0.5354, R:0.0105)
Batch  75/537: Loss=0.5116 (C:0.5116, R:0.0105)
Batch 100/537: Loss=0.5214 (C:0.5214, R:0.0105)
Batch 125/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 150/537: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 175/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 200/537: Loss=0.5367 (C:0.5367, R:0.0105)
Batch 225/537: Loss=0.5389 (C:0.5389, R:0.0105)
Batch 250/537: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 275/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 300/537: Loss=0.5493 (C:0.5493, R:0.0105)
Batch 325/537: Loss=0.5077 (C:0.5077, R:0.0105)
Batch 350/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 375/537: Loss=0.5514 (C:0.5514, R:0.0105)
Batch 400/537: Loss=0.5700 (C:0.5700, R:0.0105)
Batch 425/537: Loss=0.5368 (C:0.5368, R:0.0105)
Batch 450/537: Loss=0.5413 (C:0.5413, R:0.0105)
Batch 475/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 500/537: Loss=0.5885 (C:0.5885, R:0.0105)
Batch 525/537: Loss=0.5465 (C:0.5465, R:0.0105)

============================================================
Epoch 66/300 completed in 21.8s
Train: Loss=0.5418 (C:0.5418, R:0.0105) Ratio=5.13x
Val:   Loss=0.7060 (C:0.7060, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.314 ± 0.575
    Neg distances: 2.637 ± 1.097
    Separation ratio: 8.40x
    Gap: -4.541
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.5170 (C:0.5170, R:0.0105)
Batch  25/537: Loss=0.5402 (C:0.5402, R:0.0105)
Batch  50/537: Loss=0.5366 (C:0.5366, R:0.0105)
Batch  75/537: Loss=0.5427 (C:0.5427, R:0.0105)
Batch 100/537: Loss=0.5288 (C:0.5288, R:0.0105)
Batch 125/537: Loss=0.5400 (C:0.5400, R:0.0105)
Batch 150/537: Loss=0.5402 (C:0.5402, R:0.0105)
Batch 175/537: Loss=0.5367 (C:0.5367, R:0.0105)
Batch 200/537: Loss=0.5252 (C:0.5252, R:0.0105)
Batch 225/537: Loss=0.5541 (C:0.5541, R:0.0106)
Batch 250/537: Loss=0.5668 (C:0.5668, R:0.0105)
Batch 275/537: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 300/537: Loss=0.5333 (C:0.5333, R:0.0105)
Batch 325/537: Loss=0.5526 (C:0.5526, R:0.0105)
Batch 350/537: Loss=0.6077 (C:0.6077, R:0.0105)
Batch 375/537: Loss=0.5051 (C:0.5051, R:0.0105)
Batch 400/537: Loss=0.6042 (C:0.6042, R:0.0105)
Batch 425/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch 450/537: Loss=0.5607 (C:0.5607, R:0.0105)
Batch 475/537: Loss=0.5473 (C:0.5473, R:0.0105)
Batch 500/537: Loss=0.5245 (C:0.5245, R:0.0105)
Batch 525/537: Loss=0.6059 (C:0.6059, R:0.0105)

============================================================
Epoch 67/300 completed in 27.7s
Train: Loss=0.5475 (C:0.5475, R:0.0105) Ratio=5.19x
Val:   Loss=0.7071 (C:0.7071, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.5178 (C:0.5178, R:0.0105)
Batch  25/537: Loss=0.5648 (C:0.5648, R:0.0105)
Batch  50/537: Loss=0.5022 (C:0.5022, R:0.0105)
Batch  75/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 100/537: Loss=0.5386 (C:0.5386, R:0.0105)
Batch 125/537: Loss=0.4934 (C:0.4934, R:0.0105)
Batch 150/537: Loss=0.5372 (C:0.5372, R:0.0105)
Batch 175/537: Loss=0.5414 (C:0.5414, R:0.0105)
Batch 200/537: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 225/537: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 250/537: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 275/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 300/537: Loss=0.5836 (C:0.5836, R:0.0105)
Batch 325/537: Loss=0.5473 (C:0.5473, R:0.0105)
Batch 350/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch 375/537: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 400/537: Loss=0.5523 (C:0.5523, R:0.0105)
Batch 425/537: Loss=0.5238 (C:0.5238, R:0.0105)
Batch 450/537: Loss=0.5551 (C:0.5551, R:0.0105)
Batch 475/537: Loss=0.5646 (C:0.5646, R:0.0105)
Batch 500/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 525/537: Loss=0.5754 (C:0.5754, R:0.0106)

============================================================
Epoch 68/300 completed in 21.0s
Train: Loss=0.5455 (C:0.5455, R:0.0105) Ratio=5.10x
Val:   Loss=0.7076 (C:0.7076, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.5101 (C:0.5101, R:0.0105)
Batch  25/537: Loss=0.5385 (C:0.5385, R:0.0105)
Batch  50/537: Loss=0.5438 (C:0.5438, R:0.0105)
Batch  75/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 100/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch 125/537: Loss=0.5015 (C:0.5015, R:0.0105)
Batch 150/537: Loss=0.5662 (C:0.5662, R:0.0105)
Batch 175/537: Loss=0.5267 (C:0.5267, R:0.0105)
Batch 200/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 225/537: Loss=0.5985 (C:0.5985, R:0.0105)
Batch 250/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 275/537: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 300/537: Loss=0.5893 (C:0.5893, R:0.0105)
Batch 325/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 350/537: Loss=0.5845 (C:0.5845, R:0.0105)
Batch 375/537: Loss=0.5458 (C:0.5458, R:0.0106)
Batch 400/537: Loss=0.5526 (C:0.5526, R:0.0105)
Batch 425/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 450/537: Loss=0.5351 (C:0.5351, R:0.0105)
Batch 475/537: Loss=0.5516 (C:0.5516, R:0.0105)
Batch 500/537: Loss=0.5788 (C:0.5788, R:0.0105)
Batch 525/537: Loss=0.5184 (C:0.5184, R:0.0105)

============================================================
Epoch 69/300 completed in 21.0s
Train: Loss=0.5434 (C:0.5434, R:0.0105) Ratio=5.19x
Val:   Loss=0.7088 (C:0.7088, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.305 ± 0.600
    Neg distances: 2.677 ± 1.106
    Separation ratio: 8.78x
    Gap: -4.529
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.4921 (C:0.4921, R:0.0105)
Batch  25/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch  50/537: Loss=0.5396 (C:0.5396, R:0.0105)
Batch  75/537: Loss=0.4972 (C:0.4972, R:0.0105)
Batch 100/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch 125/537: Loss=0.5173 (C:0.5173, R:0.0105)
Batch 150/537: Loss=0.5278 (C:0.5278, R:0.0105)
Batch 175/537: Loss=0.5061 (C:0.5061, R:0.0105)
Batch 200/537: Loss=0.5182 (C:0.5182, R:0.0105)
Batch 225/537: Loss=0.5376 (C:0.5376, R:0.0105)
Batch 250/537: Loss=0.5594 (C:0.5594, R:0.0105)
Batch 275/537: Loss=0.5126 (C:0.5126, R:0.0105)
Batch 300/537: Loss=0.5323 (C:0.5323, R:0.0106)
Batch 325/537: Loss=0.5412 (C:0.5412, R:0.0105)
Batch 350/537: Loss=0.5344 (C:0.5344, R:0.0105)
Batch 375/537: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 400/537: Loss=0.5244 (C:0.5244, R:0.0105)
Batch 425/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 450/537: Loss=0.5165 (C:0.5165, R:0.0105)
Batch 475/537: Loss=0.4920 (C:0.4920, R:0.0105)
Batch 500/537: Loss=0.5546 (C:0.5546, R:0.0105)
Batch 525/537: Loss=0.5457 (C:0.5457, R:0.0105)

============================================================
Epoch 70/300 completed in 26.9s
Train: Loss=0.5335 (C:0.5335, R:0.0105) Ratio=5.27x
Val:   Loss=0.6968 (C:0.6968, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6968)
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.5425 (C:0.5425, R:0.0105)
Batch  25/537: Loss=0.4978 (C:0.4978, R:0.0105)
Batch  50/537: Loss=0.5154 (C:0.5154, R:0.0105)
Batch  75/537: Loss=0.5557 (C:0.5557, R:0.0105)
Batch 100/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 125/537: Loss=0.5445 (C:0.5445, R:0.0105)
Batch 150/537: Loss=0.4889 (C:0.4889, R:0.0105)
Batch 175/537: Loss=0.4977 (C:0.4977, R:0.0105)
Batch 200/537: Loss=0.5245 (C:0.5245, R:0.0105)
Batch 225/537: Loss=0.5531 (C:0.5531, R:0.0105)
Batch 250/537: Loss=0.5339 (C:0.5339, R:0.0105)
Batch 275/537: Loss=0.5127 (C:0.5127, R:0.0105)
Batch 300/537: Loss=0.5247 (C:0.5247, R:0.0105)
Batch 325/537: Loss=0.5516 (C:0.5516, R:0.0106)
Batch 350/537: Loss=0.5363 (C:0.5363, R:0.0105)
Batch 375/537: Loss=0.5231 (C:0.5231, R:0.0105)
Batch 400/537: Loss=0.5557 (C:0.5557, R:0.0105)
Batch 425/537: Loss=0.5188 (C:0.5188, R:0.0105)
Batch 450/537: Loss=0.5164 (C:0.5164, R:0.0105)
Batch 475/537: Loss=0.5478 (C:0.5478, R:0.0105)
Batch 500/537: Loss=0.5483 (C:0.5483, R:0.0105)
Batch 525/537: Loss=0.5449 (C:0.5449, R:0.0105)

============================================================
Epoch 71/300 completed in 22.1s
Train: Loss=0.5327 (C:0.5327, R:0.0105) Ratio=5.22x
Val:   Loss=0.7043 (C:0.7043, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.5193 (C:0.5193, R:0.0105)
Batch  25/537: Loss=0.5044 (C:0.5044, R:0.0105)
Batch  50/537: Loss=0.5148 (C:0.5148, R:0.0105)
Batch  75/537: Loss=0.5449 (C:0.5449, R:0.0106)
Batch 100/537: Loss=0.5547 (C:0.5547, R:0.0105)
Batch 125/537: Loss=0.5443 (C:0.5443, R:0.0105)
Batch 150/537: Loss=0.5249 (C:0.5249, R:0.0105)
Batch 175/537: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 200/537: Loss=0.5306 (C:0.5306, R:0.0105)
Batch 225/537: Loss=0.4969 (C:0.4969, R:0.0105)
Batch 250/537: Loss=0.5687 (C:0.5687, R:0.0105)
Batch 275/537: Loss=0.5348 (C:0.5348, R:0.0105)
Batch 300/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 325/537: Loss=0.5267 (C:0.5267, R:0.0105)
Batch 350/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 375/537: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 400/537: Loss=0.6276 (C:0.6276, R:0.0105)
Batch 425/537: Loss=0.5411 (C:0.5411, R:0.0105)
Batch 450/537: Loss=0.5363 (C:0.5363, R:0.0105)
Batch 475/537: Loss=0.5291 (C:0.5291, R:0.0105)
Batch 500/537: Loss=0.5032 (C:0.5032, R:0.0105)
Batch 525/537: Loss=0.5132 (C:0.5132, R:0.0105)

============================================================
Epoch 72/300 completed in 21.7s
Train: Loss=0.5316 (C:0.5316, R:0.0105) Ratio=5.19x
Val:   Loss=0.7061 (C:0.7061, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.287 ± 0.546
    Neg distances: 2.717 ± 1.109
    Separation ratio: 9.48x
    Gap: -4.497
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=0.5246 (C:0.5246, R:0.0105)
Batch  25/537: Loss=0.4869 (C:0.4869, R:0.0105)
Batch  50/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch  75/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch 100/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 125/537: Loss=0.5117 (C:0.5117, R:0.0105)
Batch 150/537: Loss=0.4914 (C:0.4914, R:0.0105)
Batch 175/537: Loss=0.5016 (C:0.5016, R:0.0106)
Batch 200/537: Loss=0.5546 (C:0.5546, R:0.0105)
Batch 225/537: Loss=0.5433 (C:0.5433, R:0.0105)
Batch 250/537: Loss=0.5065 (C:0.5065, R:0.0105)
Batch 275/537: Loss=0.5381 (C:0.5381, R:0.0105)
Batch 300/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 325/537: Loss=0.5657 (C:0.5657, R:0.0105)
Batch 350/537: Loss=0.5339 (C:0.5339, R:0.0105)
Batch 375/537: Loss=0.5120 (C:0.5120, R:0.0105)
Batch 400/537: Loss=0.4981 (C:0.4981, R:0.0106)
Batch 425/537: Loss=0.4998 (C:0.4998, R:0.0105)
Batch 450/537: Loss=0.5066 (C:0.5066, R:0.0105)
Batch 475/537: Loss=0.5466 (C:0.5466, R:0.0105)
Batch 500/537: Loss=0.5300 (C:0.5300, R:0.0105)
Batch 525/537: Loss=0.5110 (C:0.5110, R:0.0105)

============================================================
Epoch 73/300 completed in 26.8s
Train: Loss=0.5162 (C:0.5162, R:0.0105) Ratio=5.13x
Val:   Loss=0.6945 (C:0.6945, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6945)
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=0.4825 (C:0.4825, R:0.0105)
Batch  25/537: Loss=0.5002 (C:0.5002, R:0.0105)
Batch  50/537: Loss=0.5291 (C:0.5291, R:0.0105)
Batch  75/537: Loss=0.5043 (C:0.5043, R:0.0105)
Batch 100/537: Loss=0.5218 (C:0.5218, R:0.0105)
Batch 125/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch 150/537: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 175/537: Loss=0.5214 (C:0.5214, R:0.0106)
Batch 200/537: Loss=0.5339 (C:0.5339, R:0.0105)
Batch 225/537: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 250/537: Loss=0.4906 (C:0.4906, R:0.0105)
Batch 275/537: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 300/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 325/537: Loss=0.5498 (C:0.5498, R:0.0105)
Batch 350/537: Loss=0.5261 (C:0.5261, R:0.0105)
Batch 375/537: Loss=0.5572 (C:0.5572, R:0.0105)
Batch 400/537: Loss=0.5204 (C:0.5204, R:0.0105)
Batch 425/537: Loss=0.5291 (C:0.5291, R:0.0105)
Batch 450/537: Loss=0.5375 (C:0.5375, R:0.0105)
Batch 475/537: Loss=0.5335 (C:0.5335, R:0.0105)
Batch 500/537: Loss=0.5187 (C:0.5187, R:0.0105)
Batch 525/537: Loss=0.5242 (C:0.5242, R:0.0105)

============================================================
Epoch 74/300 completed in 21.1s
Train: Loss=0.5143 (C:0.5143, R:0.0105) Ratio=5.17x
Val:   Loss=0.6936 (C:0.6936, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6936)
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=0.4954 (C:0.4954, R:0.0105)
Batch  25/537: Loss=0.4853 (C:0.4853, R:0.0105)
Batch  50/537: Loss=0.5114 (C:0.5114, R:0.0106)
Batch  75/537: Loss=0.5028 (C:0.5028, R:0.0105)
Batch 100/537: Loss=0.5313 (C:0.5313, R:0.0105)
Batch 125/537: Loss=0.4802 (C:0.4802, R:0.0105)
Batch 150/537: Loss=0.4812 (C:0.4812, R:0.0105)
Batch 175/537: Loss=0.5222 (C:0.5222, R:0.0105)
Batch 200/537: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 225/537: Loss=0.5026 (C:0.5026, R:0.0105)
Batch 250/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 275/537: Loss=0.4766 (C:0.4766, R:0.0105)
Batch 300/537: Loss=0.4950 (C:0.4950, R:0.0105)
Batch 325/537: Loss=0.5118 (C:0.5118, R:0.0105)
Batch 350/537: Loss=0.5221 (C:0.5221, R:0.0105)
Batch 375/537: Loss=0.5406 (C:0.5406, R:0.0105)
Batch 400/537: Loss=0.4868 (C:0.4868, R:0.0105)
Batch 425/537: Loss=0.5173 (C:0.5173, R:0.0105)
Batch 450/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 475/537: Loss=0.5085 (C:0.5085, R:0.0105)
Batch 500/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 525/537: Loss=0.5415 (C:0.5415, R:0.0106)

============================================================
Epoch 75/300 completed in 21.8s
Train: Loss=0.5137 (C:0.5137, R:0.0105) Ratio=5.34x
Val:   Loss=0.6901 (C:0.6901, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6901)
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.296 ± 0.583
    Neg distances: 2.754 ± 1.126
    Separation ratio: 9.31x
    Gap: -4.538
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=0.5015 (C:0.5015, R:0.0105)
Batch  25/537: Loss=0.5319 (C:0.5319, R:0.0105)
Batch  50/537: Loss=0.4938 (C:0.4938, R:0.0105)
Batch  75/537: Loss=0.5244 (C:0.5244, R:0.0105)
Batch 100/537: Loss=0.4674 (C:0.4674, R:0.0105)
Batch 125/537: Loss=0.5120 (C:0.5120, R:0.0105)
Batch 150/537: Loss=0.4932 (C:0.4932, R:0.0105)
Batch 175/537: Loss=0.4871 (C:0.4871, R:0.0105)
Batch 200/537: Loss=0.4915 (C:0.4915, R:0.0105)
Batch 225/537: Loss=0.5114 (C:0.5114, R:0.0105)
Batch 250/537: Loss=0.5430 (C:0.5430, R:0.0106)
Batch 275/537: Loss=0.5112 (C:0.5112, R:0.0105)
Batch 300/537: Loss=0.5206 (C:0.5206, R:0.0105)
Batch 325/537: Loss=0.5362 (C:0.5362, R:0.0105)
Batch 350/537: Loss=0.5346 (C:0.5346, R:0.0105)
Batch 375/537: Loss=0.5254 (C:0.5254, R:0.0105)
Batch 400/537: Loss=0.5052 (C:0.5052, R:0.0105)
Batch 425/537: Loss=0.5076 (C:0.5076, R:0.0105)
Batch 450/537: Loss=0.5398 (C:0.5398, R:0.0105)
Batch 475/537: Loss=0.5221 (C:0.5221, R:0.0105)
Batch 500/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 525/537: Loss=0.5400 (C:0.5400, R:0.0105)

============================================================
Epoch 76/300 completed in 27.2s
Train: Loss=0.5174 (C:0.5174, R:0.0105) Ratio=5.33x
Val:   Loss=0.6995 (C:0.6995, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=0.4813 (C:0.4813, R:0.0105)
Batch  25/537: Loss=0.5153 (C:0.5153, R:0.0105)
Batch  50/537: Loss=0.5050 (C:0.5050, R:0.0105)
Batch  75/537: Loss=0.5074 (C:0.5074, R:0.0105)
Batch 100/537: Loss=0.4960 (C:0.4960, R:0.0105)
Batch 125/537: Loss=0.4983 (C:0.4983, R:0.0105)
Batch 150/537: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 175/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 200/537: Loss=0.4825 (C:0.4825, R:0.0105)
Batch 225/537: Loss=0.5198 (C:0.5198, R:0.0105)
Batch 250/537: Loss=0.5266 (C:0.5266, R:0.0105)
Batch 275/537: Loss=0.5278 (C:0.5278, R:0.0105)
Batch 300/537: Loss=0.5572 (C:0.5572, R:0.0105)
Batch 325/537: Loss=0.5164 (C:0.5164, R:0.0105)
Batch 350/537: Loss=0.5802 (C:0.5802, R:0.0105)
Batch 375/537: Loss=0.5112 (C:0.5112, R:0.0106)
Batch 400/537: Loss=0.5516 (C:0.5516, R:0.0105)
Batch 425/537: Loss=0.5205 (C:0.5205, R:0.0105)
Batch 450/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 475/537: Loss=0.5036 (C:0.5036, R:0.0105)
Batch 500/537: Loss=0.5349 (C:0.5349, R:0.0105)
Batch 525/537: Loss=0.5231 (C:0.5231, R:0.0105)

============================================================
Epoch 77/300 completed in 21.0s
Train: Loss=0.5152 (C:0.5152, R:0.0105) Ratio=5.33x
Val:   Loss=0.6921 (C:0.6921, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=0.4853 (C:0.4853, R:0.0105)
Batch  25/537: Loss=0.5164 (C:0.5164, R:0.0105)
Batch  50/537: Loss=0.5164 (C:0.5164, R:0.0105)
Batch  75/537: Loss=0.5288 (C:0.5288, R:0.0105)
Batch 100/537: Loss=0.5107 (C:0.5107, R:0.0105)
Batch 125/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch 150/537: Loss=0.5171 (C:0.5171, R:0.0105)
Batch 175/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch 200/537: Loss=0.4881 (C:0.4881, R:0.0105)
Batch 225/537: Loss=0.5185 (C:0.5185, R:0.0105)
Batch 250/537: Loss=0.5357 (C:0.5357, R:0.0105)
Batch 275/537: Loss=0.4703 (C:0.4703, R:0.0106)
Batch 300/537: Loss=0.5036 (C:0.5036, R:0.0105)
Batch 325/537: Loss=0.5252 (C:0.5252, R:0.0105)
Batch 350/537: Loss=0.5322 (C:0.5322, R:0.0105)
Batch 375/537: Loss=0.5400 (C:0.5400, R:0.0105)
Batch 400/537: Loss=0.5051 (C:0.5051, R:0.0105)
Batch 425/537: Loss=0.4988 (C:0.4988, R:0.0105)
Batch 450/537: Loss=0.5248 (C:0.5248, R:0.0105)
Batch 475/537: Loss=0.5228 (C:0.5228, R:0.0105)
Batch 500/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch 525/537: Loss=0.5446 (C:0.5446, R:0.0105)

============================================================
Epoch 78/300 completed in 21.0s
Train: Loss=0.5137 (C:0.5137, R:0.0105) Ratio=5.26x
Val:   Loss=0.6982 (C:0.6982, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.286 ± 0.568
    Neg distances: 2.720 ± 1.111
    Separation ratio: 9.50x
    Gap: -4.481
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch  25/537: Loss=0.4423 (C:0.4423, R:0.0105)
Batch  50/537: Loss=0.5107 (C:0.5107, R:0.0105)
Batch  75/537: Loss=0.4770 (C:0.4770, R:0.0105)
Batch 100/537: Loss=0.5323 (C:0.5323, R:0.0105)
Batch 125/537: Loss=0.4780 (C:0.4780, R:0.0105)
Batch 150/537: Loss=0.5189 (C:0.5189, R:0.0105)
Batch 175/537: Loss=0.5304 (C:0.5304, R:0.0105)
Batch 200/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch 225/537: Loss=0.5080 (C:0.5080, R:0.0105)
Batch 250/537: Loss=0.5465 (C:0.5465, R:0.0105)
Batch 275/537: Loss=0.4913 (C:0.4913, R:0.0105)
Batch 300/537: Loss=0.5111 (C:0.5111, R:0.0105)
Batch 325/537: Loss=0.5008 (C:0.5008, R:0.0105)
Batch 350/537: Loss=0.5151 (C:0.5151, R:0.0105)
Batch 375/537: Loss=0.5254 (C:0.5254, R:0.0105)
Batch 400/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch 425/537: Loss=0.5303 (C:0.5303, R:0.0105)
Batch 450/537: Loss=0.5276 (C:0.5276, R:0.0105)
Batch 475/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 500/537: Loss=0.4922 (C:0.4922, R:0.0105)
Batch 525/537: Loss=0.5431 (C:0.5431, R:0.0105)

============================================================
Epoch 79/300 completed in 26.6s
Train: Loss=0.5078 (C:0.5078, R:0.0105) Ratio=5.34x
Val:   Loss=0.6920 (C:0.6920, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=0.4990 (C:0.4990, R:0.0106)
Batch  25/537: Loss=0.5024 (C:0.5024, R:0.0105)
Batch  50/537: Loss=0.4880 (C:0.4880, R:0.0105)
Batch  75/537: Loss=0.4832 (C:0.4832, R:0.0105)
Batch 100/537: Loss=0.5140 (C:0.5140, R:0.0105)
Batch 125/537: Loss=0.4911 (C:0.4911, R:0.0105)
Batch 150/537: Loss=0.4723 (C:0.4723, R:0.0105)
Batch 175/537: Loss=0.4615 (C:0.4615, R:0.0105)
Batch 200/537: Loss=0.4756 (C:0.4756, R:0.0105)
Batch 225/537: Loss=0.5025 (C:0.5025, R:0.0105)
Batch 250/537: Loss=0.5389 (C:0.5389, R:0.0105)
Batch 275/537: Loss=0.4959 (C:0.4959, R:0.0105)
Batch 300/537: Loss=0.5139 (C:0.5139, R:0.0105)
Batch 325/537: Loss=0.4811 (C:0.4811, R:0.0105)
Batch 350/537: Loss=0.4865 (C:0.4865, R:0.0105)
Batch 375/537: Loss=0.5061 (C:0.5061, R:0.0105)
Batch 400/537: Loss=0.4996 (C:0.4996, R:0.0105)
Batch 425/537: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 450/537: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 475/537: Loss=0.5195 (C:0.5195, R:0.0105)
Batch 500/537: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 525/537: Loss=0.5132 (C:0.5132, R:0.0105)

============================================================
Epoch 80/300 completed in 20.9s
Train: Loss=0.5072 (C:0.5072, R:0.0105) Ratio=5.40x
Val:   Loss=0.6947 (C:0.6947, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 5 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=0.5114 (C:0.5114, R:0.0105)
Batch  25/537: Loss=0.4809 (C:0.4809, R:0.0105)
Batch  50/537: Loss=0.5023 (C:0.5023, R:0.0105)
Batch  75/537: Loss=0.5020 (C:0.5020, R:0.0105)
Batch 100/537: Loss=0.5354 (C:0.5354, R:0.0105)
Batch 125/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch 150/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 175/537: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 200/537: Loss=0.4699 (C:0.4699, R:0.0105)
Batch 225/537: Loss=0.4943 (C:0.4943, R:0.0105)
Batch 250/537: Loss=0.4888 (C:0.4888, R:0.0105)
Batch 275/537: Loss=0.5221 (C:0.5221, R:0.0105)
Batch 300/537: Loss=0.4701 (C:0.4701, R:0.0105)
Batch 325/537: Loss=0.4970 (C:0.4970, R:0.0106)
Batch 350/537: Loss=0.5020 (C:0.5020, R:0.0105)
Batch 375/537: Loss=0.4748 (C:0.4748, R:0.0105)
Batch 400/537: Loss=0.5384 (C:0.5384, R:0.0105)
Batch 425/537: Loss=0.5297 (C:0.5297, R:0.0105)
Batch 450/537: Loss=0.5328 (C:0.5328, R:0.0105)
Batch 475/537: Loss=0.5256 (C:0.5256, R:0.0105)
Batch 500/537: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 525/537: Loss=0.4946 (C:0.4946, R:0.0105)

============================================================
Epoch 81/300 completed in 20.9s
Train: Loss=0.5051 (C:0.5051, R:0.0105) Ratio=5.39x
Val:   Loss=0.6904 (C:0.6904, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.278 ± 0.564
    Neg distances: 2.725 ± 1.111
    Separation ratio: 9.80x
    Gap: -4.545
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=0.5022 (C:0.5022, R:0.0105)
Batch  25/537: Loss=0.4903 (C:0.4903, R:0.0105)
Batch  50/537: Loss=0.4755 (C:0.4755, R:0.0105)
Batch  75/537: Loss=0.4828 (C:0.4828, R:0.0105)
Batch 100/537: Loss=0.4673 (C:0.4673, R:0.0105)
Batch 125/537: Loss=0.5155 (C:0.5155, R:0.0105)
Batch 150/537: Loss=0.4751 (C:0.4751, R:0.0105)
Batch 175/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 200/537: Loss=0.5064 (C:0.5064, R:0.0106)
Batch 225/537: Loss=0.5219 (C:0.5219, R:0.0105)
Batch 250/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 275/537: Loss=0.5024 (C:0.5024, R:0.0105)
Batch 300/537: Loss=0.5037 (C:0.5037, R:0.0105)
Batch 325/537: Loss=0.4914 (C:0.4914, R:0.0105)
Batch 350/537: Loss=0.5099 (C:0.5099, R:0.0105)
Batch 375/537: Loss=0.4873 (C:0.4873, R:0.0105)
Batch 400/537: Loss=0.4905 (C:0.4905, R:0.0105)
Batch 425/537: Loss=0.5097 (C:0.5097, R:0.0105)
Batch 450/537: Loss=0.5034 (C:0.5034, R:0.0105)
Batch 475/537: Loss=0.4878 (C:0.4878, R:0.0105)
Batch 500/537: Loss=0.4963 (C:0.4963, R:0.0105)
Batch 525/537: Loss=0.4857 (C:0.4857, R:0.0105)

============================================================
Epoch 82/300 completed in 27.3s
Train: Loss=0.4988 (C:0.4988, R:0.0105) Ratio=5.50x
Val:   Loss=0.6874 (C:0.6874, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6874)
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=0.5153 (C:0.5153, R:0.0105)
Batch  25/537: Loss=0.4967 (C:0.4967, R:0.0105)
Batch  50/537: Loss=0.4842 (C:0.4842, R:0.0105)
Batch  75/537: Loss=0.4697 (C:0.4697, R:0.0105)
Batch 100/537: Loss=0.4860 (C:0.4860, R:0.0105)
Batch 125/537: Loss=0.4999 (C:0.4999, R:0.0105)
Batch 150/537: Loss=0.5686 (C:0.5686, R:0.0105)
Batch 175/537: Loss=0.5080 (C:0.5080, R:0.0105)
Batch 200/537: Loss=0.4695 (C:0.4695, R:0.0105)
Batch 225/537: Loss=0.4785 (C:0.4785, R:0.0105)
Batch 250/537: Loss=0.5078 (C:0.5078, R:0.0105)
Batch 275/537: Loss=0.5086 (C:0.5086, R:0.0105)
Batch 300/537: Loss=0.5065 (C:0.5065, R:0.0105)
Batch 325/537: Loss=0.4982 (C:0.4982, R:0.0105)
Batch 350/537: Loss=0.4890 (C:0.4890, R:0.0105)
Batch 375/537: Loss=0.4938 (C:0.4938, R:0.0105)
Batch 400/537: Loss=0.4924 (C:0.4924, R:0.0105)
Batch 425/537: Loss=0.4888 (C:0.4888, R:0.0105)
Batch 450/537: Loss=0.4968 (C:0.4968, R:0.0105)
Batch 475/537: Loss=0.4718 (C:0.4718, R:0.0105)
Batch 500/537: Loss=0.5303 (C:0.5303, R:0.0105)
Batch 525/537: Loss=0.5373 (C:0.5373, R:0.0105)

============================================================
Epoch 83/300 completed in 21.8s
Train: Loss=0.4978 (C:0.4978, R:0.0105) Ratio=5.53x
Val:   Loss=0.6890 (C:0.6890, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=0.5015 (C:0.5015, R:0.0105)
Batch  25/537: Loss=0.5066 (C:0.5066, R:0.0105)
Batch  50/537: Loss=0.5121 (C:0.5121, R:0.0105)
Batch  75/537: Loss=0.4848 (C:0.4848, R:0.0105)
Batch 100/537: Loss=0.4960 (C:0.4960, R:0.0105)
Batch 125/537: Loss=0.4836 (C:0.4836, R:0.0105)
Batch 150/537: Loss=0.4866 (C:0.4866, R:0.0105)
Batch 175/537: Loss=0.4845 (C:0.4845, R:0.0105)
Batch 200/537: Loss=0.5126 (C:0.5126, R:0.0105)
Batch 225/537: Loss=0.4824 (C:0.4824, R:0.0105)
Batch 250/537: Loss=0.5108 (C:0.5108, R:0.0105)
Batch 275/537: Loss=0.4934 (C:0.4934, R:0.0105)
Batch 300/537: Loss=0.5014 (C:0.5014, R:0.0105)
Batch 325/537: Loss=0.4783 (C:0.4783, R:0.0105)
Batch 350/537: Loss=0.4319 (C:0.4319, R:0.0105)
Batch 375/537: Loss=0.5013 (C:0.5013, R:0.0105)
Batch 400/537: Loss=0.4675 (C:0.4675, R:0.0105)
Batch 425/537: Loss=0.4897 (C:0.4897, R:0.0106)
Batch 450/537: Loss=0.5021 (C:0.5021, R:0.0105)
Batch 475/537: Loss=0.5088 (C:0.5088, R:0.0105)
Batch 500/537: Loss=0.5318 (C:0.5318, R:0.0105)
Batch 525/537: Loss=0.5037 (C:0.5037, R:0.0105)

============================================================
Epoch 84/300 completed in 22.0s
Train: Loss=0.4977 (C:0.4977, R:0.0105) Ratio=5.51x
Val:   Loss=0.6946 (C:0.6946, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 85
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.272 ± 0.532
    Neg distances: 2.725 ± 1.108
    Separation ratio: 10.01x
    Gap: -4.577
    ✅ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=0.4812 (C:0.4812, R:0.0105)
Batch  25/537: Loss=0.4827 (C:0.4827, R:0.0105)
Batch  50/537: Loss=0.5036 (C:0.5036, R:0.0105)
Batch  75/537: Loss=0.4811 (C:0.4811, R:0.0105)
Batch 100/537: Loss=0.5012 (C:0.5012, R:0.0105)
Batch 125/537: Loss=0.4825 (C:0.4825, R:0.0105)
Batch 150/537: Loss=0.4753 (C:0.4753, R:0.0105)
Batch 175/537: Loss=0.5003 (C:0.5003, R:0.0105)
Batch 200/537: Loss=0.4858 (C:0.4858, R:0.0106)
Batch 225/537: Loss=0.5252 (C:0.5252, R:0.0105)
Batch 250/537: Loss=0.4922 (C:0.4922, R:0.0105)
Batch 275/537: Loss=0.5177 (C:0.5177, R:0.0105)
Batch 300/537: Loss=0.4913 (C:0.4913, R:0.0105)
Batch 325/537: Loss=0.5350 (C:0.5350, R:0.0105)
Batch 350/537: Loss=0.5331 (C:0.5331, R:0.0105)
Batch 375/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 400/537: Loss=0.4548 (C:0.4548, R:0.0106)
Batch 425/537: Loss=0.4856 (C:0.4856, R:0.0105)
Batch 450/537: Loss=0.4556 (C:0.4556, R:0.0105)
Batch 475/537: Loss=0.4593 (C:0.4593, R:0.0105)
Batch 500/537: Loss=0.4555 (C:0.4555, R:0.0105)
Batch 525/537: Loss=0.4958 (C:0.4958, R:0.0105)

============================================================
Epoch 85/300 completed in 27.4s
Train: Loss=0.4908 (C:0.4908, R:0.0105) Ratio=5.44x
Val:   Loss=0.6869 (C:0.6869, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6869)
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=0.4454 (C:0.4454, R:0.0105)
Batch  25/537: Loss=0.4773 (C:0.4773, R:0.0105)
Batch  50/537: Loss=0.4635 (C:0.4635, R:0.0105)
Batch  75/537: Loss=0.4774 (C:0.4774, R:0.0105)
Batch 100/537: Loss=0.4711 (C:0.4711, R:0.0105)
Batch 125/537: Loss=0.5014 (C:0.5014, R:0.0105)
Batch 150/537: Loss=0.4683 (C:0.4683, R:0.0105)
Batch 175/537: Loss=0.4938 (C:0.4938, R:0.0105)
Batch 200/537: Loss=0.5234 (C:0.5234, R:0.0105)
Batch 225/537: Loss=0.4725 (C:0.4725, R:0.0106)
Batch 250/537: Loss=0.5230 (C:0.5230, R:0.0105)
Batch 275/537: Loss=0.4664 (C:0.4664, R:0.0105)
Batch 300/537: Loss=0.5151 (C:0.5151, R:0.0105)
Batch 325/537: Loss=0.4766 (C:0.4766, R:0.0105)
Batch 350/537: Loss=0.4760 (C:0.4760, R:0.0105)
Batch 375/537: Loss=0.4722 (C:0.4722, R:0.0105)
Batch 400/537: Loss=0.4564 (C:0.4564, R:0.0105)
Batch 425/537: Loss=0.4738 (C:0.4738, R:0.0105)
Batch 450/537: Loss=0.5027 (C:0.5027, R:0.0105)
Batch 475/537: Loss=0.4623 (C:0.4623, R:0.0105)
Batch 500/537: Loss=0.4999 (C:0.4999, R:0.0105)
Batch 525/537: Loss=0.4731 (C:0.4731, R:0.0105)

============================================================
Epoch 86/300 completed in 21.2s
Train: Loss=0.4901 (C:0.4901, R:0.0105) Ratio=5.60x
Val:   Loss=0.6941 (C:0.6941, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=0.4706 (C:0.4706, R:0.0105)
Batch  25/537: Loss=0.4791 (C:0.4791, R:0.0105)
Batch  50/537: Loss=0.4820 (C:0.4820, R:0.0106)
Batch  75/537: Loss=0.4589 (C:0.4589, R:0.0105)
Batch 100/537: Loss=0.5107 (C:0.5107, R:0.0105)
Batch 125/537: Loss=0.5125 (C:0.5125, R:0.0106)
Batch 150/537: Loss=0.4637 (C:0.4637, R:0.0105)
Batch 175/537: Loss=0.4775 (C:0.4775, R:0.0105)
Batch 200/537: Loss=0.4883 (C:0.4883, R:0.0105)
Batch 225/537: Loss=0.5187 (C:0.5187, R:0.0105)
Batch 250/537: Loss=0.4814 (C:0.4814, R:0.0105)
Batch 275/537: Loss=0.4632 (C:0.4632, R:0.0105)
Batch 300/537: Loss=0.4531 (C:0.4531, R:0.0105)
Batch 325/537: Loss=0.4812 (C:0.4812, R:0.0105)
Batch 350/537: Loss=0.5066 (C:0.5066, R:0.0106)
Batch 375/537: Loss=0.4999 (C:0.4999, R:0.0105)
Batch 400/537: Loss=0.4876 (C:0.4876, R:0.0105)
Batch 425/537: Loss=0.4599 (C:0.4599, R:0.0105)
Batch 450/537: Loss=0.4946 (C:0.4946, R:0.0105)
Batch 475/537: Loss=0.4708 (C:0.4708, R:0.0105)
Batch 500/537: Loss=0.5028 (C:0.5028, R:0.0105)
Batch 525/537: Loss=0.5082 (C:0.5082, R:0.0106)

============================================================
Epoch 87/300 completed in 21.4s
Train: Loss=0.4896 (C:0.4896, R:0.0105) Ratio=5.53x
Val:   Loss=0.6871 (C:0.6871, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 88
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.256 ± 0.557
    Neg distances: 2.761 ± 1.110
    Separation ratio: 10.80x
    Gap: -4.616
    ✅ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=0.4728 (C:0.4728, R:0.0105)
Batch  25/537: Loss=0.5033 (C:0.5033, R:0.0105)
Batch  50/537: Loss=0.4488 (C:0.4488, R:0.0105)
Batch  75/537: Loss=0.4446 (C:0.4446, R:0.0105)
Batch 100/537: Loss=0.4608 (C:0.4608, R:0.0105)
Batch 125/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch 150/537: Loss=0.4985 (C:0.4985, R:0.0105)
Batch 175/537: Loss=0.4901 (C:0.4901, R:0.0105)
Batch 200/537: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 225/537: Loss=0.4826 (C:0.4826, R:0.0105)
Batch 250/537: Loss=0.4804 (C:0.4804, R:0.0105)
Batch 275/537: Loss=0.4665 (C:0.4665, R:0.0105)
Batch 300/537: Loss=0.4316 (C:0.4316, R:0.0105)
Batch 325/537: Loss=0.4477 (C:0.4477, R:0.0105)
Batch 350/537: Loss=0.5008 (C:0.5008, R:0.0105)
Batch 375/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch 400/537: Loss=0.4701 (C:0.4701, R:0.0105)
Batch 425/537: Loss=0.4385 (C:0.4385, R:0.0105)
Batch 450/537: Loss=0.4733 (C:0.4733, R:0.0105)
Batch 475/537: Loss=0.4999 (C:0.4999, R:0.0105)
Batch 500/537: Loss=0.5030 (C:0.5030, R:0.0105)
Batch 525/537: Loss=0.4881 (C:0.4881, R:0.0105)

============================================================
Epoch 88/300 completed in 27.6s
Train: Loss=0.4731 (C:0.4731, R:0.0105) Ratio=5.50x
Val:   Loss=0.6645 (C:0.6645, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6645)
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=0.4350 (C:0.4350, R:0.0105)
Batch  25/537: Loss=0.4583 (C:0.4583, R:0.0105)
Batch  50/537: Loss=0.4682 (C:0.4682, R:0.0105)
Batch  75/537: Loss=0.4780 (C:0.4780, R:0.0105)
Batch 100/537: Loss=0.4749 (C:0.4749, R:0.0105)
Batch 125/537: Loss=0.5250 (C:0.5250, R:0.0105)
Batch 150/537: Loss=0.4513 (C:0.4513, R:0.0105)
Batch 175/537: Loss=0.4546 (C:0.4546, R:0.0105)
Batch 200/537: Loss=0.5043 (C:0.5043, R:0.0105)
Batch 225/537: Loss=0.4721 (C:0.4721, R:0.0105)
Batch 250/537: Loss=0.4496 (C:0.4496, R:0.0105)
Batch 275/537: Loss=0.4443 (C:0.4443, R:0.0105)
Batch 300/537: Loss=0.4749 (C:0.4749, R:0.0105)
Batch 325/537: Loss=0.4856 (C:0.4856, R:0.0105)
Batch 350/537: Loss=0.4689 (C:0.4689, R:0.0105)
Batch 375/537: Loss=0.4742 (C:0.4742, R:0.0105)
Batch 400/537: Loss=0.4338 (C:0.4338, R:0.0105)
Batch 425/537: Loss=0.4949 (C:0.4949, R:0.0105)
Batch 450/537: Loss=0.4640 (C:0.4640, R:0.0105)
Batch 475/537: Loss=0.4994 (C:0.4994, R:0.0105)
Batch 500/537: Loss=0.4924 (C:0.4924, R:0.0105)
Batch 525/537: Loss=0.4711 (C:0.4711, R:0.0105)

============================================================
Epoch 89/300 completed in 21.3s
Train: Loss=0.4729 (C:0.4729, R:0.0105) Ratio=5.57x
Val:   Loss=0.6611 (C:0.6611, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6611)
============================================================

Epoch 90 Training
----------------------------------------
Batch   0/537: Loss=0.4539 (C:0.4539, R:0.0105)
Batch  25/537: Loss=0.4495 (C:0.4495, R:0.0105)
Batch  50/537: Loss=0.4632 (C:0.4632, R:0.0105)
Batch  75/537: Loss=0.4471 (C:0.4471, R:0.0105)
Batch 100/537: Loss=0.4837 (C:0.4837, R:0.0105)
Batch 125/537: Loss=0.4845 (C:0.4845, R:0.0105)
Batch 150/537: Loss=0.4641 (C:0.4641, R:0.0105)
Batch 175/537: Loss=0.4484 (C:0.4484, R:0.0105)
Batch 200/537: Loss=0.4700 (C:0.4700, R:0.0105)
Batch 225/537: Loss=0.4783 (C:0.4783, R:0.0105)
Batch 250/537: Loss=0.4755 (C:0.4755, R:0.0105)
Batch 275/537: Loss=0.4654 (C:0.4654, R:0.0105)
Batch 300/537: Loss=0.4535 (C:0.4535, R:0.0105)
Batch 325/537: Loss=0.5008 (C:0.5008, R:0.0105)
Batch 350/537: Loss=0.4593 (C:0.4593, R:0.0105)
Batch 375/537: Loss=0.4663 (C:0.4663, R:0.0105)
Batch 400/537: Loss=0.4661 (C:0.4661, R:0.0105)
Batch 425/537: Loss=0.4831 (C:0.4831, R:0.0105)
Batch 450/537: Loss=0.4952 (C:0.4952, R:0.0105)
Batch 475/537: Loss=0.4860 (C:0.4860, R:0.0105)
Batch 500/537: Loss=0.5110 (C:0.5110, R:0.0105)
Batch 525/537: Loss=0.5283 (C:0.5283, R:0.0105)

============================================================
Epoch 90/300 completed in 21.5s
Train: Loss=0.4738 (C:0.4738, R:0.0105) Ratio=5.61x
Val:   Loss=0.6705 (C:0.6705, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 91
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.267 ± 0.562
    Neg distances: 2.782 ± 1.128
    Separation ratio: 10.41x
    Gap: -4.683
    ✅ Excellent global separation!

Epoch 91 Training
----------------------------------------
Batch   0/537: Loss=0.5039 (C:0.5039, R:0.0105)
Batch  25/537: Loss=0.4860 (C:0.4860, R:0.0106)
Batch  50/537: Loss=0.4532 (C:0.4532, R:0.0105)
Batch  75/537: Loss=0.4594 (C:0.4594, R:0.0105)
Batch 100/537: Loss=0.4969 (C:0.4969, R:0.0106)
Batch 125/537: Loss=0.4831 (C:0.4831, R:0.0105)
Batch 150/537: Loss=0.4747 (C:0.4747, R:0.0105)
Batch 175/537: Loss=0.4788 (C:0.4788, R:0.0105)
Batch 200/537: Loss=0.4919 (C:0.4919, R:0.0105)
Batch 225/537: Loss=0.4749 (C:0.4749, R:0.0105)
Batch 250/537: Loss=0.4845 (C:0.4845, R:0.0105)
Batch 275/537: Loss=0.4815 (C:0.4815, R:0.0105)
Batch 300/537: Loss=0.4707 (C:0.4707, R:0.0105)
Batch 325/537: Loss=0.4704 (C:0.4704, R:0.0105)
Batch 350/537: Loss=0.4316 (C:0.4316, R:0.0105)
Batch 375/537: Loss=0.4589 (C:0.4589, R:0.0105)
Batch 400/537: Loss=0.4737 (C:0.4737, R:0.0105)
Batch 425/537: Loss=0.5106 (C:0.5106, R:0.0105)
Batch 450/537: Loss=0.5125 (C:0.5125, R:0.0105)
Batch 475/537: Loss=0.4958 (C:0.4958, R:0.0105)
Batch 500/537: Loss=0.4783 (C:0.4783, R:0.0105)
Batch 525/537: Loss=0.4668 (C:0.4668, R:0.0105)

============================================================
Epoch 91/300 completed in 27.7s
Train: Loss=0.4802 (C:0.4802, R:0.0105) Ratio=5.65x
Val:   Loss=0.6735 (C:0.6735, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 92 Training
----------------------------------------
Batch   0/537: Loss=0.5157 (C:0.5157, R:0.0105)
Batch  25/537: Loss=0.4561 (C:0.4561, R:0.0105)
Batch  50/537: Loss=0.4813 (C:0.4813, R:0.0105)
Batch  75/537: Loss=0.4734 (C:0.4734, R:0.0105)
Batch 100/537: Loss=0.4578 (C:0.4578, R:0.0105)
Batch 125/537: Loss=0.4743 (C:0.4743, R:0.0105)
Batch 150/537: Loss=0.4489 (C:0.4489, R:0.0105)
Batch 175/537: Loss=0.4826 (C:0.4826, R:0.0105)
Batch 200/537: Loss=0.4943 (C:0.4943, R:0.0105)
Batch 225/537: Loss=0.4760 (C:0.4760, R:0.0105)
Batch 250/537: Loss=0.4895 (C:0.4895, R:0.0105)
Batch 275/537: Loss=0.4794 (C:0.4794, R:0.0105)
Batch 300/537: Loss=0.4809 (C:0.4809, R:0.0105)
Batch 325/537: Loss=0.4732 (C:0.4732, R:0.0105)
Batch 350/537: Loss=0.4731 (C:0.4731, R:0.0105)
Batch 375/537: Loss=0.4599 (C:0.4599, R:0.0105)
Batch 400/537: Loss=0.5270 (C:0.5270, R:0.0105)
Batch 425/537: Loss=0.5132 (C:0.5132, R:0.0105)
Batch 450/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch 475/537: Loss=0.4352 (C:0.4352, R:0.0105)
Batch 500/537: Loss=0.4830 (C:0.4830, R:0.0105)
Batch 525/537: Loss=0.4743 (C:0.4743, R:0.0105)

============================================================
Epoch 92/300 completed in 21.1s
Train: Loss=0.4806 (C:0.4806, R:0.0105) Ratio=5.61x
Val:   Loss=0.6720 (C:0.6720, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 93 Training
----------------------------------------
Batch   0/537: Loss=0.4827 (C:0.4827, R:0.0105)
Batch  25/537: Loss=0.4691 (C:0.4691, R:0.0105)
Batch  50/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch  75/537: Loss=0.4800 (C:0.4800, R:0.0105)
Batch 100/537: Loss=0.4760 (C:0.4760, R:0.0105)
Batch 125/537: Loss=0.4785 (C:0.4785, R:0.0105)
Batch 150/537: Loss=0.4821 (C:0.4821, R:0.0105)
Batch 175/537: Loss=0.4781 (C:0.4781, R:0.0105)
Batch 200/537: Loss=0.4637 (C:0.4637, R:0.0105)
Batch 225/537: Loss=0.4665 (C:0.4665, R:0.0105)
Batch 250/537: Loss=0.4579 (C:0.4579, R:0.0105)
Batch 275/537: Loss=0.4671 (C:0.4671, R:0.0105)
Batch 300/537: Loss=0.4953 (C:0.4953, R:0.0105)
Batch 325/537: Loss=0.4738 (C:0.4738, R:0.0105)
Batch 350/537: Loss=0.5104 (C:0.5104, R:0.0106)
Batch 375/537: Loss=0.4761 (C:0.4761, R:0.0105)
Batch 400/537: Loss=0.4928 (C:0.4928, R:0.0105)
Batch 425/537: Loss=0.4898 (C:0.4898, R:0.0105)
Batch 450/537: Loss=0.4731 (C:0.4731, R:0.0105)
Batch 475/537: Loss=0.4549 (C:0.4549, R:0.0105)
Batch 500/537: Loss=0.4911 (C:0.4911, R:0.0105)
Batch 525/537: Loss=0.5124 (C:0.5124, R:0.0105)

============================================================
Epoch 93/300 completed in 21.5s
Train: Loss=0.4797 (C:0.4797, R:0.0105) Ratio=5.54x
Val:   Loss=0.6837 (C:0.6837, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 94
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.262 ± 0.568
    Neg distances: 2.765 ± 1.115
    Separation ratio: 10.55x
    Gap: -4.626
    ✅ Excellent global separation!

Epoch 94 Training
----------------------------------------
Batch   0/537: Loss=0.4479 (C:0.4479, R:0.0105)
Batch  25/537: Loss=0.4406 (C:0.4406, R:0.0105)
Batch  50/537: Loss=0.4287 (C:0.4287, R:0.0105)
Batch  75/537: Loss=0.4702 (C:0.4702, R:0.0105)
Batch 100/537: Loss=0.4629 (C:0.4629, R:0.0105)
Batch 125/537: Loss=0.4648 (C:0.4648, R:0.0105)
Batch 150/537: Loss=0.4183 (C:0.4183, R:0.0105)
Batch 175/537: Loss=0.4655 (C:0.4655, R:0.0105)
Batch 200/537: Loss=0.4652 (C:0.4652, R:0.0105)
Batch 225/537: Loss=0.4793 (C:0.4793, R:0.0105)
Batch 250/537: Loss=0.5060 (C:0.5060, R:0.0105)
Batch 275/537: Loss=0.4977 (C:0.4977, R:0.0105)
Batch 300/537: Loss=0.4750 (C:0.4750, R:0.0105)
Batch 325/537: Loss=0.4841 (C:0.4841, R:0.0105)
Batch 350/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 375/537: Loss=0.4944 (C:0.4944, R:0.0105)
Batch 400/537: Loss=0.4661 (C:0.4661, R:0.0105)
Batch 425/537: Loss=0.4780 (C:0.4780, R:0.0105)
Batch 450/537: Loss=0.4971 (C:0.4971, R:0.0105)
Batch 475/537: Loss=0.5064 (C:0.5064, R:0.0105)
Batch 500/537: Loss=0.4843 (C:0.4843, R:0.0105)
Batch 525/537: Loss=0.4812 (C:0.4812, R:0.0105)

============================================================
Epoch 94/300 completed in 27.1s
Train: Loss=0.4738 (C:0.4738, R:0.0105) Ratio=5.69x
Val:   Loss=0.6736 (C:0.6736, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 95 Training
----------------------------------------
Batch   0/537: Loss=0.4807 (C:0.4807, R:0.0106)
Batch  25/537: Loss=0.4343 (C:0.4343, R:0.0105)
Batch  50/537: Loss=0.4499 (C:0.4499, R:0.0105)
Batch  75/537: Loss=0.4721 (C:0.4721, R:0.0105)
Batch 100/537: Loss=0.4410 (C:0.4410, R:0.0105)
Batch 125/537: Loss=0.4628 (C:0.4628, R:0.0105)
Batch 150/537: Loss=0.4883 (C:0.4883, R:0.0105)
Batch 175/537: Loss=0.4590 (C:0.4590, R:0.0105)
Batch 200/537: Loss=0.4356 (C:0.4356, R:0.0105)
Batch 225/537: Loss=0.4615 (C:0.4615, R:0.0105)
Batch 250/537: Loss=0.4603 (C:0.4603, R:0.0105)
Batch 275/537: Loss=0.4733 (C:0.4733, R:0.0105)
Batch 300/537: Loss=0.4836 (C:0.4836, R:0.0105)
Batch 325/537: Loss=0.4649 (C:0.4649, R:0.0105)
Batch 350/537: Loss=0.4699 (C:0.4699, R:0.0105)
Batch 375/537: Loss=0.4638 (C:0.4638, R:0.0105)
Batch 400/537: Loss=0.4670 (C:0.4670, R:0.0105)
Batch 425/537: Loss=0.4700 (C:0.4700, R:0.0105)
Batch 450/537: Loss=0.4348 (C:0.4348, R:0.0105)
Batch 475/537: Loss=0.4737 (C:0.4737, R:0.0105)
Batch 500/537: Loss=0.4747 (C:0.4747, R:0.0105)
Batch 525/537: Loss=0.4627 (C:0.4627, R:0.0105)

============================================================
Epoch 95/300 completed in 20.9s
Train: Loss=0.4731 (C:0.4731, R:0.0105) Ratio=5.84x
Val:   Loss=0.6751 (C:0.6751, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 96 Training
----------------------------------------
Batch   0/537: Loss=0.4320 (C:0.4320, R:0.0105)
Batch  25/537: Loss=0.4491 (C:0.4491, R:0.0105)
Batch  50/537: Loss=0.4386 (C:0.4386, R:0.0105)
Batch  75/537: Loss=0.4799 (C:0.4799, R:0.0105)
Batch 100/537: Loss=0.4338 (C:0.4338, R:0.0105)
Batch 125/537: Loss=0.4844 (C:0.4844, R:0.0105)
Batch 150/537: Loss=0.4683 (C:0.4683, R:0.0105)
Batch 175/537: Loss=0.4531 (C:0.4531, R:0.0105)
Batch 200/537: Loss=0.4766 (C:0.4766, R:0.0105)
Batch 225/537: Loss=0.4642 (C:0.4642, R:0.0105)
Batch 250/537: Loss=0.4883 (C:0.4883, R:0.0105)
Batch 275/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch 300/537: Loss=0.4810 (C:0.4810, R:0.0105)
Batch 325/537: Loss=0.4615 (C:0.4615, R:0.0105)
Batch 350/537: Loss=0.5109 (C:0.5109, R:0.0105)
Batch 375/537: Loss=0.4750 (C:0.4750, R:0.0105)
Batch 400/537: Loss=0.4488 (C:0.4488, R:0.0105)
Batch 425/537: Loss=0.4911 (C:0.4911, R:0.0105)
Batch 450/537: Loss=0.4902 (C:0.4902, R:0.0105)
Batch 475/537: Loss=0.4779 (C:0.4779, R:0.0105)
Batch 500/537: Loss=0.4731 (C:0.4731, R:0.0105)
Batch 525/537: Loss=0.4886 (C:0.4886, R:0.0105)

============================================================
Epoch 96/300 completed in 20.9s
Train: Loss=0.4719 (C:0.4719, R:0.0105) Ratio=5.68x
Val:   Loss=0.6799 (C:0.6799, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

🌍 Updating global dataset at epoch 97
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.248 ± 0.507
    Neg distances: 2.735 ± 1.091
    Separation ratio: 11.03x
    Gap: -4.629
    ✅ Excellent global separation!

Epoch 97 Training
----------------------------------------
Batch   0/537: Loss=0.4733 (C:0.4733, R:0.0105)
Batch  25/537: Loss=0.4300 (C:0.4300, R:0.0105)
Batch  50/537: Loss=0.4405 (C:0.4405, R:0.0105)
Batch  75/537: Loss=0.4292 (C:0.4292, R:0.0105)
Batch 100/537: Loss=0.4736 (C:0.4736, R:0.0105)
Batch 125/537: Loss=0.4645 (C:0.4645, R:0.0105)
Batch 150/537: Loss=0.4769 (C:0.4769, R:0.0106)
Batch 175/537: Loss=0.4323 (C:0.4323, R:0.0105)
Batch 200/537: Loss=0.4561 (C:0.4561, R:0.0105)
Batch 225/537: Loss=0.4530 (C:0.4530, R:0.0105)
Batch 250/537: Loss=0.4156 (C:0.4156, R:0.0105)
Batch 275/537: Loss=0.4296 (C:0.4296, R:0.0105)
Batch 300/537: Loss=0.4569 (C:0.4569, R:0.0105)
Batch 325/537: Loss=0.4699 (C:0.4699, R:0.0105)
Batch 350/537: Loss=0.4387 (C:0.4387, R:0.0105)
Batch 375/537: Loss=0.4890 (C:0.4890, R:0.0105)
Batch 400/537: Loss=0.4581 (C:0.4581, R:0.0105)
Batch 425/537: Loss=0.4605 (C:0.4605, R:0.0105)
Batch 450/537: Loss=0.4739 (C:0.4739, R:0.0105)
Batch 475/537: Loss=0.4629 (C:0.4629, R:0.0105)
Batch 500/537: Loss=0.4399 (C:0.4399, R:0.0105)
Batch 525/537: Loss=0.4727 (C:0.4727, R:0.0105)

============================================================
Epoch 97/300 completed in 26.4s
Train: Loss=0.4607 (C:0.4607, R:0.0105) Ratio=5.76x
Val:   Loss=0.6625 (C:0.6625, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 97 epochs
Best model was at epoch 89 with Val Loss: 0.6611

Global Dataset Training Completed!
Best epoch: 89
Best validation loss: 0.6611
Final separation ratios: Train=5.76x, Val=3.11x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4686
  Adjusted Rand Score: 0.5317
  Clustering Accuracy: 0.8161
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8197
  Per-class F1: [0.8410169491525423, 0.7622747047855811, 0.8602257636122178]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.785 ± 0.949
  Negative distances: 2.404 ± 1.282
  Separation ratio: 3.06x
  Gap: -4.674
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4686
  Clustering Accuracy: 0.8161
  Adjusted Rand Score: 0.5317

Classification Performance:
  Accuracy: 0.8197

Separation Quality:
  Separation Ratio: 3.06x
  Gap: -4.674
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135/results/evaluation_results_20250714_175934.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135/results/evaluation_results_20250714_175934.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135/final_results.json

Key Results:
  Separation ratio: 3.06x
  Perfect separation: False
  Classification accuracy: 0.8197
  Removing previous best: coarse_lr1e-04_lat50_bs1020_20250714_161313
  NEW BEST: 0.8197% (improvement: +-80.85%)
  Saved best experiment: coarse_lr1e-04_lat75_bs1020_20250714_172135

[4/12] Testing: coarse_lr1e-04_lat75_bs1536
  Learning rate: 0.0001
  Latent dim: 75
  Batch size: 1536
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 17:59:35.106601
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1536
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 356
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 6
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1536
  Balanced sampling: True
  Train batches: 356
  Val batches: 6
  Test batches: 7
Data loading completed!
  Train: 549367 samples, 356 batches
  Val: 9842 samples, 6 batches
  Test: 9824 samples, 7 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,876,555
Model created with 1,876,555 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,876,555
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.091 ± 0.011
    Neg distances: 0.091 ± 0.011
    Separation ratio: 1.00x
    Gap: -0.126
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/356: Loss=1.9999 (C:1.9999, R:0.0116)
Batch  25/356: Loss=1.9919 (C:1.9919, R:0.0114)
Batch  50/356: Loss=1.9688 (C:1.9688, R:0.0113)
Batch  75/356: Loss=1.9522 (C:1.9522, R:0.0112)
Batch 100/356: Loss=1.9417 (C:1.9417, R:0.0111)
Batch 125/356: Loss=1.9268 (C:1.9268, R:0.0109)
Batch 150/356: Loss=1.9101 (C:1.9101, R:0.0108)
Batch 175/356: Loss=1.8885 (C:1.8885, R:0.0108)
Batch 200/356: Loss=1.8991 (C:1.8991, R:0.0107)
Batch 225/356: Loss=1.8879 (C:1.8879, R:0.0107)
Batch 250/356: Loss=1.8670 (C:1.8670, R:0.0106)
Batch 275/356: Loss=1.8664 (C:1.8664, R:0.0107)
Batch 300/356: Loss=1.8608 (C:1.8608, R:0.0106)
Batch 325/356: Loss=1.8494 (C:1.8494, R:0.0106)
Batch 350/356: Loss=1.8598 (C:1.8598, R:0.0106)

============================================================
Epoch 1/300 completed in 26.5s
Train: Loss=1.9107 (C:1.9107, R:0.0109) Ratio=1.54x
Val:   Loss=1.8445 (C:1.8445, R:0.0105) Ratio=2.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8445)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/356: Loss=1.8499 (C:1.8499, R:0.0106)
Batch  25/356: Loss=1.8512 (C:1.8512, R:0.0106)
Batch  50/356: Loss=1.8431 (C:1.8431, R:0.0105)
Batch  75/356: Loss=1.8495 (C:1.8495, R:0.0105)
Batch 100/356: Loss=1.8360 (C:1.8360, R:0.0105)
Batch 125/356: Loss=1.8372 (C:1.8372, R:0.0105)
Batch 150/356: Loss=1.8345 (C:1.8345, R:0.0105)
Batch 175/356: Loss=1.8358 (C:1.8358, R:0.0105)
Batch 200/356: Loss=1.8361 (C:1.8361, R:0.0105)
Batch 225/356: Loss=1.8369 (C:1.8369, R:0.0105)
Batch 250/356: Loss=1.8502 (C:1.8502, R:0.0105)
Batch 275/356: Loss=1.8471 (C:1.8471, R:0.0105)
Batch 300/356: Loss=1.8438 (C:1.8438, R:0.0105)
Batch 325/356: Loss=1.8261 (C:1.8261, R:0.0105)
Batch 350/356: Loss=1.8325 (C:1.8325, R:0.0105)

============================================================
Epoch 2/300 completed in 20.8s
Train: Loss=1.8387 (C:1.8387, R:0.0105) Ratio=2.08x
Val:   Loss=1.8210 (C:1.8210, R:0.0104) Ratio=2.31x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8210)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/356: Loss=1.8112 (C:1.8112, R:0.0105)
Batch  25/356: Loss=1.8167 (C:1.8167, R:0.0105)
Batch  50/356: Loss=1.8194 (C:1.8194, R:0.0105)
Batch  75/356: Loss=1.8049 (C:1.8049, R:0.0105)
Batch 100/356: Loss=1.8182 (C:1.8182, R:0.0105)
Batch 125/356: Loss=1.8272 (C:1.8272, R:0.0105)
Batch 150/356: Loss=1.8045 (C:1.8045, R:0.0105)
Batch 175/356: Loss=1.8265 (C:1.8265, R:0.0105)
Batch 200/356: Loss=1.8143 (C:1.8143, R:0.0105)
Batch 225/356: Loss=1.8239 (C:1.8239, R:0.0105)
Batch 250/356: Loss=1.8244 (C:1.8244, R:0.0105)
Batch 275/356: Loss=1.8202 (C:1.8202, R:0.0105)
Batch 300/356: Loss=1.8168 (C:1.8168, R:0.0105)
Batch 325/356: Loss=1.8028 (C:1.8028, R:0.0105)
Batch 350/356: Loss=1.8253 (C:1.8253, R:0.0105)

============================================================
Epoch 3/300 completed in 20.7s
Train: Loss=1.8183 (C:1.8183, R:0.0105) Ratio=2.32x
Val:   Loss=1.8071 (C:1.8071, R:0.0104) Ratio=2.43x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8071)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.562 ± 0.557
    Neg distances: 1.490 ± 0.830
    Separation ratio: 2.65x
    Gap: -3.214
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/356: Loss=1.2555 (C:1.2555, R:0.0105)
Batch  25/356: Loss=1.2493 (C:1.2493, R:0.0105)
Batch  50/356: Loss=1.2160 (C:1.2160, R:0.0105)
Batch  75/356: Loss=1.2427 (C:1.2427, R:0.0105)
Batch 100/356: Loss=1.2332 (C:1.2332, R:0.0105)
Batch 125/356: Loss=1.2480 (C:1.2480, R:0.0105)
Batch 150/356: Loss=1.2470 (C:1.2470, R:0.0105)
Batch 175/356: Loss=1.2271 (C:1.2271, R:0.0105)
Batch 200/356: Loss=1.2581 (C:1.2581, R:0.0105)
Batch 225/356: Loss=1.2624 (C:1.2624, R:0.0105)
Batch 250/356: Loss=1.2112 (C:1.2112, R:0.0105)
Batch 275/356: Loss=1.2432 (C:1.2432, R:0.0105)
Batch 300/356: Loss=1.2193 (C:1.2193, R:0.0105)
Batch 325/356: Loss=1.2346 (C:1.2346, R:0.0105)
Batch 350/356: Loss=1.2141 (C:1.2141, R:0.0105)

============================================================
Epoch 4/300 completed in 26.5s
Train: Loss=1.2425 (C:1.2425, R:0.0105) Ratio=2.47x
Val:   Loss=1.2186 (C:1.2186, R:0.0104) Ratio=2.58x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2186)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/356: Loss=1.2282 (C:1.2282, R:0.0105)
Batch  25/356: Loss=1.2001 (C:1.2001, R:0.0106)
Batch  50/356: Loss=1.1889 (C:1.1889, R:0.0105)
Batch  75/356: Loss=1.2267 (C:1.2267, R:0.0105)
Batch 100/356: Loss=1.1959 (C:1.1959, R:0.0105)
Batch 125/356: Loss=1.2065 (C:1.2065, R:0.0105)
Batch 150/356: Loss=1.2194 (C:1.2194, R:0.0105)
Batch 175/356: Loss=1.2090 (C:1.2090, R:0.0105)
Batch 200/356: Loss=1.1787 (C:1.1787, R:0.0105)
Batch 225/356: Loss=1.2149 (C:1.2149, R:0.0105)
Batch 250/356: Loss=1.2040 (C:1.2040, R:0.0105)
Batch 275/356: Loss=1.1915 (C:1.1915, R:0.0105)
Batch 300/356: Loss=1.2501 (C:1.2501, R:0.0105)
Batch 325/356: Loss=1.1868 (C:1.1868, R:0.0105)
Batch 350/356: Loss=1.1823 (C:1.1823, R:0.0105)

============================================================
Epoch 5/300 completed in 21.3s
Train: Loss=1.2157 (C:1.2157, R:0.0105) Ratio=2.66x
Val:   Loss=1.2174 (C:1.2174, R:0.0104) Ratio=2.65x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2174)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/356: Loss=1.1938 (C:1.1938, R:0.0105)
Batch  25/356: Loss=1.1901 (C:1.1901, R:0.0105)
Batch  50/356: Loss=1.2062 (C:1.2062, R:0.0105)
Batch  75/356: Loss=1.2042 (C:1.2042, R:0.0105)
Batch 100/356: Loss=1.1892 (C:1.1892, R:0.0105)
Batch 125/356: Loss=1.1835 (C:1.1835, R:0.0105)
Batch 150/356: Loss=1.2031 (C:1.2031, R:0.0105)
Batch 175/356: Loss=1.2389 (C:1.2389, R:0.0105)
Batch 200/356: Loss=1.1727 (C:1.1727, R:0.0105)
Batch 225/356: Loss=1.1747 (C:1.1747, R:0.0105)
Batch 250/356: Loss=1.2094 (C:1.2094, R:0.0105)
Batch 275/356: Loss=1.2249 (C:1.2249, R:0.0105)
Batch 300/356: Loss=1.2039 (C:1.2039, R:0.0105)
Batch 325/356: Loss=1.2026 (C:1.2026, R:0.0105)
Batch 350/356: Loss=1.1981 (C:1.1981, R:0.0105)

============================================================
Epoch 6/300 completed in 21.3s
Train: Loss=1.2001 (C:1.2001, R:0.0105) Ratio=2.77x
Val:   Loss=1.2137 (C:1.2137, R:0.0104) Ratio=2.72x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2137)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.492 ± 0.550
    Neg distances: 1.602 ± 0.833
    Separation ratio: 3.25x
    Gap: -3.075
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/356: Loss=1.1377 (C:1.1377, R:0.0105)
Batch  25/356: Loss=1.1292 (C:1.1292, R:0.0106)
Batch  50/356: Loss=1.1365 (C:1.1365, R:0.0105)
Batch  75/356: Loss=1.1478 (C:1.1478, R:0.0105)
Batch 100/356: Loss=1.1050 (C:1.1050, R:0.0105)
Batch 125/356: Loss=1.1142 (C:1.1142, R:0.0106)
Batch 150/356: Loss=1.1249 (C:1.1249, R:0.0105)
Batch 175/356: Loss=1.1263 (C:1.1263, R:0.0105)
Batch 200/356: Loss=1.1295 (C:1.1295, R:0.0105)
Batch 225/356: Loss=1.1104 (C:1.1104, R:0.0105)
Batch 250/356: Loss=1.1118 (C:1.1118, R:0.0105)
Batch 275/356: Loss=1.1203 (C:1.1203, R:0.0105)
Batch 300/356: Loss=1.1233 (C:1.1233, R:0.0105)
Batch 325/356: Loss=1.1322 (C:1.1322, R:0.0105)
Batch 350/356: Loss=1.1042 (C:1.1042, R:0.0105)

============================================================
Epoch 7/300 completed in 27.2s
Train: Loss=1.1195 (C:1.1195, R:0.0105) Ratio=2.86x
Val:   Loss=1.1291 (C:1.1291, R:0.0104) Ratio=2.77x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1291)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/356: Loss=1.1025 (C:1.1025, R:0.0105)
Batch  25/356: Loss=1.1136 (C:1.1136, R:0.0105)
Batch  50/356: Loss=1.0950 (C:1.0950, R:0.0105)
Batch  75/356: Loss=1.0917 (C:1.0917, R:0.0105)
Batch 100/356: Loss=1.0899 (C:1.0899, R:0.0105)
Batch 125/356: Loss=1.0994 (C:1.0994, R:0.0105)
Batch 150/356: Loss=1.1012 (C:1.1012, R:0.0105)
Batch 175/356: Loss=1.1238 (C:1.1238, R:0.0105)
Batch 200/356: Loss=1.1324 (C:1.1324, R:0.0105)
Batch 225/356: Loss=1.0998 (C:1.0998, R:0.0105)
Batch 250/356: Loss=1.1136 (C:1.1136, R:0.0105)
Batch 275/356: Loss=1.0889 (C:1.0889, R:0.0105)
Batch 300/356: Loss=1.1006 (C:1.1006, R:0.0105)
Batch 325/356: Loss=1.1218 (C:1.1218, R:0.0105)
Batch 350/356: Loss=1.1110 (C:1.1110, R:0.0105)

============================================================
Epoch 8/300 completed in 20.2s
Train: Loss=1.1077 (C:1.1077, R:0.0105) Ratio=2.97x
Val:   Loss=1.1253 (C:1.1253, R:0.0104) Ratio=2.85x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1253)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/356: Loss=1.0912 (C:1.0912, R:0.0105)
Batch  25/356: Loss=1.1043 (C:1.1043, R:0.0105)
Batch  50/356: Loss=1.0636 (C:1.0636, R:0.0105)
Batch  75/356: Loss=1.0806 (C:1.0806, R:0.0105)
Batch 100/356: Loss=1.0726 (C:1.0726, R:0.0105)
Batch 125/356: Loss=1.1292 (C:1.1292, R:0.0105)
Batch 150/356: Loss=1.0969 (C:1.0969, R:0.0105)
Batch 175/356: Loss=1.1052 (C:1.1052, R:0.0105)
Batch 200/356: Loss=1.1136 (C:1.1136, R:0.0105)
Batch 225/356: Loss=1.1120 (C:1.1120, R:0.0105)
Batch 250/356: Loss=1.1338 (C:1.1338, R:0.0105)
Batch 275/356: Loss=1.1477 (C:1.1477, R:0.0105)
Batch 300/356: Loss=1.0849 (C:1.0849, R:0.0105)
Batch 325/356: Loss=1.1080 (C:1.1080, R:0.0105)
Batch 350/356: Loss=1.1174 (C:1.1174, R:0.0105)

============================================================
Epoch 9/300 completed in 20.2s
Train: Loss=1.0983 (C:1.0983, R:0.0105) Ratio=3.03x
Val:   Loss=1.1217 (C:1.1217, R:0.0104) Ratio=2.86x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1217)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.444 ± 0.521
    Neg distances: 1.717 ± 0.852
    Separation ratio: 3.87x
    Gap: -3.094
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/356: Loss=1.0111 (C:1.0111, R:0.0105)
Batch  25/356: Loss=1.0154 (C:1.0154, R:0.0105)
Batch  50/356: Loss=1.0444 (C:1.0444, R:0.0105)
Batch  75/356: Loss=1.0087 (C:1.0087, R:0.0105)
Batch 100/356: Loss=1.0205 (C:1.0205, R:0.0105)
Batch 125/356: Loss=1.0167 (C:1.0167, R:0.0105)
Batch 150/356: Loss=1.0267 (C:1.0267, R:0.0105)
Batch 175/356: Loss=1.0536 (C:1.0536, R:0.0105)
Batch 200/356: Loss=1.0163 (C:1.0163, R:0.0105)
Batch 225/356: Loss=1.0401 (C:1.0401, R:0.0105)
Batch 250/356: Loss=1.0228 (C:1.0228, R:0.0105)
Batch 275/356: Loss=1.0384 (C:1.0384, R:0.0105)
Batch 300/356: Loss=1.0356 (C:1.0356, R:0.0105)
Batch 325/356: Loss=1.0561 (C:1.0561, R:0.0105)
Batch 350/356: Loss=1.0690 (C:1.0690, R:0.0105)

============================================================
Epoch 10/300 completed in 26.0s
Train: Loss=1.0337 (C:1.0337, R:0.0105) Ratio=3.13x
Val:   Loss=1.0677 (C:1.0677, R:0.0104) Ratio=2.90x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0677)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/356: Loss=1.0061 (C:1.0061, R:0.0105)
Batch  25/356: Loss=1.0385 (C:1.0385, R:0.0105)
Batch  50/356: Loss=1.0150 (C:1.0150, R:0.0105)
Batch  75/356: Loss=1.0058 (C:1.0058, R:0.0105)
Batch 100/356: Loss=1.0120 (C:1.0120, R:0.0105)
Batch 125/356: Loss=1.0178 (C:1.0178, R:0.0105)
Batch 150/356: Loss=1.0166 (C:1.0166, R:0.0105)
Batch 175/356: Loss=1.0288 (C:1.0288, R:0.0105)
Batch 200/356: Loss=1.0140 (C:1.0140, R:0.0105)
Batch 225/356: Loss=1.0144 (C:1.0144, R:0.0105)
Batch 250/356: Loss=1.0336 (C:1.0336, R:0.0105)
Batch 275/356: Loss=1.0401 (C:1.0401, R:0.0105)
Batch 300/356: Loss=1.0340 (C:1.0340, R:0.0105)
Batch 325/356: Loss=1.0283 (C:1.0283, R:0.0105)
Batch 350/356: Loss=1.0518 (C:1.0518, R:0.0105)

============================================================
Epoch 11/300 completed in 20.7s
Train: Loss=1.0264 (C:1.0264, R:0.0105) Ratio=3.22x
Val:   Loss=1.0663 (C:1.0663, R:0.0104) Ratio=2.91x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0663)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/356: Loss=1.0378 (C:1.0378, R:0.0105)
Batch  25/356: Loss=1.0221 (C:1.0221, R:0.0105)
Batch  50/356: Loss=1.0218 (C:1.0218, R:0.0105)
Batch  75/356: Loss=1.0032 (C:1.0032, R:0.0105)
Batch 100/356: Loss=1.0366 (C:1.0366, R:0.0105)
Batch 125/356: Loss=1.0086 (C:1.0086, R:0.0105)
Batch 150/356: Loss=1.0245 (C:1.0245, R:0.0105)
Batch 175/356: Loss=1.0140 (C:1.0140, R:0.0105)
Batch 200/356: Loss=1.0136 (C:1.0136, R:0.0105)
Batch 225/356: Loss=1.0265 (C:1.0265, R:0.0105)
Batch 250/356: Loss=1.0011 (C:1.0011, R:0.0105)
Batch 275/356: Loss=1.0175 (C:1.0175, R:0.0105)
Batch 300/356: Loss=1.0171 (C:1.0171, R:0.0105)
Batch 325/356: Loss=1.0146 (C:1.0146, R:0.0105)
Batch 350/356: Loss=1.0314 (C:1.0314, R:0.0105)

============================================================
Epoch 12/300 completed in 21.1s
Train: Loss=1.0192 (C:1.0192, R:0.0105) Ratio=3.26x
Val:   Loss=1.0668 (C:1.0668, R:0.0104) Ratio=2.90x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.435 ± 0.537
    Neg distances: 1.788 ± 0.864
    Separation ratio: 4.11x
    Gap: -3.175
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/356: Loss=0.9987 (C:0.9987, R:0.0105)
Batch  25/356: Loss=0.9586 (C:0.9586, R:0.0105)
Batch  50/356: Loss=1.0030 (C:1.0030, R:0.0106)
Batch  75/356: Loss=0.9803 (C:0.9803, R:0.0105)
Batch 100/356: Loss=0.9830 (C:0.9830, R:0.0105)
Batch 125/356: Loss=0.9768 (C:0.9768, R:0.0105)
Batch 150/356: Loss=0.9647 (C:0.9647, R:0.0105)
Batch 175/356: Loss=0.9754 (C:0.9754, R:0.0105)
Batch 200/356: Loss=1.0094 (C:1.0094, R:0.0105)
Batch 225/356: Loss=1.0237 (C:1.0237, R:0.0105)
Batch 250/356: Loss=0.9921 (C:0.9921, R:0.0105)
Batch 275/356: Loss=0.9754 (C:0.9754, R:0.0105)
Batch 300/356: Loss=0.9818 (C:0.9818, R:0.0105)
Batch 325/356: Loss=1.0326 (C:1.0326, R:0.0105)
Batch 350/356: Loss=0.9699 (C:0.9699, R:0.0105)

============================================================
Epoch 13/300 completed in 26.1s
Train: Loss=0.9880 (C:0.9880, R:0.0105) Ratio=3.29x
Val:   Loss=1.0336 (C:1.0336, R:0.0104) Ratio=2.92x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0336)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/356: Loss=1.0125 (C:1.0125, R:0.0105)
Batch  25/356: Loss=0.9403 (C:0.9403, R:0.0105)
Batch  50/356: Loss=0.9745 (C:0.9745, R:0.0105)
Batch  75/356: Loss=0.9837 (C:0.9837, R:0.0105)
Batch 100/356: Loss=0.9668 (C:0.9668, R:0.0105)
Batch 125/356: Loss=0.9674 (C:0.9674, R:0.0105)
Batch 150/356: Loss=0.9710 (C:0.9710, R:0.0105)
Batch 175/356: Loss=0.9500 (C:0.9500, R:0.0105)
Batch 200/356: Loss=0.9962 (C:0.9962, R:0.0105)
Batch 225/356: Loss=0.9921 (C:0.9921, R:0.0105)
Batch 250/356: Loss=0.9876 (C:0.9876, R:0.0105)
Batch 275/356: Loss=0.9972 (C:0.9972, R:0.0105)
Batch 300/356: Loss=0.9578 (C:0.9578, R:0.0105)
Batch 325/356: Loss=0.9666 (C:0.9666, R:0.0105)
Batch 350/356: Loss=1.0100 (C:1.0100, R:0.0105)

============================================================
Epoch 14/300 completed in 20.3s
Train: Loss=0.9828 (C:0.9828, R:0.0105) Ratio=3.41x
Val:   Loss=1.0373 (C:1.0373, R:0.0104) Ratio=2.94x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/356: Loss=0.9693 (C:0.9693, R:0.0105)
Batch  25/356: Loss=0.9955 (C:0.9955, R:0.0105)
Batch  50/356: Loss=0.9498 (C:0.9498, R:0.0105)
Batch  75/356: Loss=0.9713 (C:0.9713, R:0.0105)
Batch 100/356: Loss=0.9923 (C:0.9923, R:0.0105)
Batch 125/356: Loss=0.9725 (C:0.9725, R:0.0105)
Batch 150/356: Loss=0.9672 (C:0.9672, R:0.0105)
Batch 175/356: Loss=0.9604 (C:0.9604, R:0.0105)
Batch 200/356: Loss=0.9836 (C:0.9836, R:0.0105)
Batch 225/356: Loss=0.9844 (C:0.9844, R:0.0105)
Batch 250/356: Loss=0.9830 (C:0.9830, R:0.0105)
Batch 275/356: Loss=0.9890 (C:0.9890, R:0.0105)
Batch 300/356: Loss=0.9722 (C:0.9722, R:0.0105)
Batch 325/356: Loss=0.9798 (C:0.9798, R:0.0105)
Batch 350/356: Loss=0.9757 (C:0.9757, R:0.0105)

============================================================
Epoch 15/300 completed in 20.4s
Train: Loss=0.9768 (C:0.9768, R:0.0105) Ratio=3.44x
Val:   Loss=1.0426 (C:1.0426, R:0.0104) Ratio=2.92x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.448 ± 0.569
    Neg distances: 1.867 ± 0.899
    Separation ratio: 4.16x
    Gap: -3.314
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/356: Loss=0.9473 (C:0.9473, R:0.0105)
Batch  25/356: Loss=0.9546 (C:0.9546, R:0.0105)
Batch  50/356: Loss=0.9562 (C:0.9562, R:0.0105)
Batch  75/356: Loss=0.9248 (C:0.9248, R:0.0105)
Batch 100/356: Loss=0.9990 (C:0.9990, R:0.0105)
Batch 125/356: Loss=0.9808 (C:0.9808, R:0.0105)
Batch 150/356: Loss=0.9600 (C:0.9600, R:0.0105)
Batch 175/356: Loss=0.9474 (C:0.9474, R:0.0105)
Batch 200/356: Loss=0.9446 (C:0.9446, R:0.0105)
Batch 225/356: Loss=0.9700 (C:0.9700, R:0.0105)
Batch 250/356: Loss=0.9566 (C:0.9566, R:0.0105)
Batch 275/356: Loss=0.9473 (C:0.9473, R:0.0105)
Batch 300/356: Loss=0.9593 (C:0.9593, R:0.0105)
Batch 325/356: Loss=0.9428 (C:0.9428, R:0.0106)
Batch 350/356: Loss=0.9552 (C:0.9552, R:0.0105)

============================================================
Epoch 16/300 completed in 26.2s
Train: Loss=0.9604 (C:0.9604, R:0.0105) Ratio=3.49x
Val:   Loss=1.0124 (C:1.0124, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0124)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/356: Loss=0.9439 (C:0.9439, R:0.0105)
Batch  25/356: Loss=0.9211 (C:0.9211, R:0.0105)
Batch  50/356: Loss=0.9339 (C:0.9339, R:0.0105)
Batch  75/356: Loss=0.9426 (C:0.9426, R:0.0105)
Batch 100/356: Loss=0.9730 (C:0.9730, R:0.0105)
Batch 125/356: Loss=0.9604 (C:0.9604, R:0.0105)
Batch 150/356: Loss=0.9633 (C:0.9633, R:0.0105)
Batch 175/356: Loss=0.9413 (C:0.9413, R:0.0105)
Batch 200/356: Loss=0.9394 (C:0.9394, R:0.0105)
Batch 225/356: Loss=0.9365 (C:0.9365, R:0.0105)
Batch 250/356: Loss=0.9169 (C:0.9169, R:0.0105)
Batch 275/356: Loss=0.9638 (C:0.9638, R:0.0105)
Batch 300/356: Loss=1.0101 (C:1.0101, R:0.0105)
Batch 325/356: Loss=0.9421 (C:0.9421, R:0.0105)
Batch 350/356: Loss=0.9719 (C:0.9719, R:0.0105)

============================================================
Epoch 17/300 completed in 20.2s
Train: Loss=0.9558 (C:0.9558, R:0.0105) Ratio=3.57x
Val:   Loss=1.0264 (C:1.0264, R:0.0104) Ratio=2.94x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/356: Loss=0.9639 (C:0.9639, R:0.0105)
Batch  25/356: Loss=0.9246 (C:0.9246, R:0.0105)
Batch  50/356: Loss=0.9507 (C:0.9507, R:0.0105)
Batch  75/356: Loss=0.9746 (C:0.9746, R:0.0105)
Batch 100/356: Loss=0.9073 (C:0.9073, R:0.0105)
Batch 125/356: Loss=0.9680 (C:0.9680, R:0.0105)
Batch 150/356: Loss=0.9299 (C:0.9299, R:0.0105)
Batch 175/356: Loss=0.9417 (C:0.9417, R:0.0105)
Batch 200/356: Loss=0.9653 (C:0.9653, R:0.0105)
Batch 225/356: Loss=0.9334 (C:0.9334, R:0.0105)
Batch 250/356: Loss=0.9539 (C:0.9539, R:0.0105)
Batch 275/356: Loss=0.9796 (C:0.9796, R:0.0105)
Batch 300/356: Loss=0.9393 (C:0.9393, R:0.0105)
Batch 325/356: Loss=0.9303 (C:0.9303, R:0.0105)
Batch 350/356: Loss=1.0081 (C:1.0081, R:0.0105)

============================================================
Epoch 18/300 completed in 20.5s
Train: Loss=0.9497 (C:0.9497, R:0.0105) Ratio=3.58x
Val:   Loss=1.0303 (C:1.0303, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.439 ± 0.579
    Neg distances: 1.925 ± 0.901
    Separation ratio: 4.39x
    Gap: -3.465
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/356: Loss=0.9193 (C:0.9193, R:0.0105)
Batch  25/356: Loss=0.9369 (C:0.9369, R:0.0105)
Batch  50/356: Loss=0.9420 (C:0.9420, R:0.0105)
Batch  75/356: Loss=0.9227 (C:0.9227, R:0.0105)
Batch 100/356: Loss=0.9301 (C:0.9301, R:0.0105)
Batch 125/356: Loss=0.9426 (C:0.9426, R:0.0105)
Batch 150/356: Loss=0.9240 (C:0.9240, R:0.0105)
Batch 175/356: Loss=0.9221 (C:0.9221, R:0.0105)
Batch 200/356: Loss=0.9254 (C:0.9254, R:0.0105)
Batch 225/356: Loss=0.9140 (C:0.9140, R:0.0105)
Batch 250/356: Loss=0.9137 (C:0.9137, R:0.0105)
Batch 275/356: Loss=0.9316 (C:0.9316, R:0.0105)
Batch 300/356: Loss=0.9301 (C:0.9301, R:0.0105)
Batch 325/356: Loss=0.9154 (C:0.9154, R:0.0105)
Batch 350/356: Loss=0.9454 (C:0.9454, R:0.0105)

============================================================
Epoch 19/300 completed in 26.9s
Train: Loss=0.9216 (C:0.9216, R:0.0105) Ratio=3.58x
Val:   Loss=1.0047 (C:1.0047, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0047)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/356: Loss=0.9017 (C:0.9017, R:0.0105)
Batch  25/356: Loss=0.8924 (C:0.8924, R:0.0105)
Batch  50/356: Loss=0.9041 (C:0.9041, R:0.0105)
Batch  75/356: Loss=0.9095 (C:0.9095, R:0.0105)
Batch 100/356: Loss=0.9093 (C:0.9093, R:0.0105)
Batch 125/356: Loss=0.9371 (C:0.9371, R:0.0105)
Batch 150/356: Loss=0.9504 (C:0.9504, R:0.0105)
Batch 175/356: Loss=0.9389 (C:0.9389, R:0.0105)
Batch 200/356: Loss=0.9109 (C:0.9109, R:0.0105)
Batch 225/356: Loss=0.9359 (C:0.9359, R:0.0105)
Batch 250/356: Loss=0.9296 (C:0.9296, R:0.0105)
Batch 275/356: Loss=0.9209 (C:0.9209, R:0.0105)
Batch 300/356: Loss=0.9208 (C:0.9208, R:0.0105)
Batch 325/356: Loss=0.9129 (C:0.9129, R:0.0105)
Batch 350/356: Loss=0.9229 (C:0.9229, R:0.0105)

============================================================
Epoch 20/300 completed in 20.6s
Train: Loss=0.9183 (C:0.9183, R:0.0105) Ratio=3.63x
Val:   Loss=1.0049 (C:1.0049, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/356: Loss=0.8850 (C:0.8850, R:0.0105)
Batch  25/356: Loss=0.8996 (C:0.8996, R:0.0105)
Batch  50/356: Loss=0.8966 (C:0.8966, R:0.0105)
Batch  75/356: Loss=0.9079 (C:0.9079, R:0.0105)
Batch 100/356: Loss=0.9099 (C:0.9099, R:0.0105)
Batch 125/356: Loss=0.9100 (C:0.9100, R:0.0105)
Batch 150/356: Loss=0.9253 (C:0.9253, R:0.0105)
Batch 175/356: Loss=0.8926 (C:0.8926, R:0.0105)
Batch 200/356: Loss=0.8908 (C:0.8908, R:0.0105)
Batch 225/356: Loss=0.9296 (C:0.9296, R:0.0105)
Batch 250/356: Loss=0.8988 (C:0.8988, R:0.0105)
Batch 275/356: Loss=0.9007 (C:0.9007, R:0.0105)
Batch 300/356: Loss=0.9195 (C:0.9195, R:0.0105)
Batch 325/356: Loss=0.9181 (C:0.9181, R:0.0105)
Batch 350/356: Loss=0.9187 (C:0.9187, R:0.0105)

============================================================
Epoch 21/300 completed in 21.0s
Train: Loss=0.9115 (C:0.9115, R:0.0105) Ratio=3.74x
Val:   Loss=0.9952 (C:0.9952, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9952)
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.444 ± 0.581
    Neg distances: 2.025 ± 0.947
    Separation ratio: 4.57x
    Gap: -3.596
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/356: Loss=0.8903 (C:0.8903, R:0.0105)
Batch  25/356: Loss=0.8379 (C:0.8379, R:0.0105)
Batch  50/356: Loss=0.8717 (C:0.8717, R:0.0105)
Batch  75/356: Loss=0.8869 (C:0.8869, R:0.0105)
Batch 100/356: Loss=0.8622 (C:0.8622, R:0.0105)
Batch 125/356: Loss=0.8879 (C:0.8879, R:0.0105)
Batch 150/356: Loss=0.9065 (C:0.9065, R:0.0105)
Batch 175/356: Loss=0.8777 (C:0.8777, R:0.0105)
Batch 200/356: Loss=0.8883 (C:0.8883, R:0.0105)
Batch 225/356: Loss=0.8858 (C:0.8858, R:0.0105)
Batch 250/356: Loss=0.8997 (C:0.8997, R:0.0105)
Batch 275/356: Loss=0.8927 (C:0.8927, R:0.0105)
Batch 300/356: Loss=0.8955 (C:0.8955, R:0.0105)
Batch 325/356: Loss=0.9113 (C:0.9113, R:0.0105)
Batch 350/356: Loss=0.8816 (C:0.8816, R:0.0105)

============================================================
Epoch 22/300 completed in 26.5s
Train: Loss=0.8889 (C:0.8889, R:0.0105) Ratio=3.75x
Val:   Loss=0.9827 (C:0.9827, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9827)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/356: Loss=0.8593 (C:0.8593, R:0.0105)
Batch  25/356: Loss=0.8857 (C:0.8857, R:0.0105)
Batch  50/356: Loss=0.8665 (C:0.8665, R:0.0105)
Batch  75/356: Loss=0.8958 (C:0.8958, R:0.0105)
Batch 100/356: Loss=0.8950 (C:0.8950, R:0.0105)
Batch 125/356: Loss=0.8809 (C:0.8809, R:0.0105)
Batch 150/356: Loss=0.9158 (C:0.9158, R:0.0105)
Batch 175/356: Loss=0.9166 (C:0.9166, R:0.0105)
Batch 200/356: Loss=0.8991 (C:0.8991, R:0.0105)
Batch 225/356: Loss=0.9074 (C:0.9074, R:0.0105)
Batch 250/356: Loss=0.8954 (C:0.8954, R:0.0105)
Batch 275/356: Loss=0.9035 (C:0.9035, R:0.0105)
Batch 300/356: Loss=0.8925 (C:0.8925, R:0.0105)
Batch 325/356: Loss=0.9031 (C:0.9031, R:0.0105)
Batch 350/356: Loss=0.8806 (C:0.8806, R:0.0105)

============================================================
Epoch 23/300 completed in 20.2s
Train: Loss=0.8839 (C:0.8839, R:0.0105) Ratio=3.78x
Val:   Loss=0.9804 (C:0.9804, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9804)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/356: Loss=0.8668 (C:0.8668, R:0.0105)
Batch  25/356: Loss=0.8849 (C:0.8849, R:0.0105)
Batch  50/356: Loss=0.8324 (C:0.8324, R:0.0105)
Batch  75/356: Loss=0.8911 (C:0.8911, R:0.0105)
Batch 100/356: Loss=0.8817 (C:0.8817, R:0.0105)
Batch 125/356: Loss=0.8575 (C:0.8575, R:0.0105)
Batch 150/356: Loss=0.8895 (C:0.8895, R:0.0105)
Batch 175/356: Loss=0.9001 (C:0.9001, R:0.0105)
Batch 200/356: Loss=0.8753 (C:0.8753, R:0.0105)
Batch 225/356: Loss=0.8639 (C:0.8639, R:0.0105)
Batch 250/356: Loss=0.8903 (C:0.8903, R:0.0106)
Batch 275/356: Loss=0.8939 (C:0.8939, R:0.0105)
Batch 300/356: Loss=0.9003 (C:0.9003, R:0.0105)
Batch 325/356: Loss=0.9034 (C:0.9034, R:0.0105)
Batch 350/356: Loss=0.9176 (C:0.9176, R:0.0105)

============================================================
Epoch 24/300 completed in 20.3s
Train: Loss=0.8793 (C:0.8793, R:0.0105) Ratio=3.84x
Val:   Loss=0.9817 (C:0.9817, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.430 ± 0.591
    Neg distances: 2.156 ± 0.992
    Separation ratio: 5.02x
    Gap: -3.676
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/356: Loss=0.8414 (C:0.8414, R:0.0105)
Batch  25/356: Loss=0.8433 (C:0.8433, R:0.0105)
Batch  50/356: Loss=0.8629 (C:0.8629, R:0.0105)
Batch  75/356: Loss=0.8089 (C:0.8089, R:0.0105)
Batch 100/356: Loss=0.8685 (C:0.8685, R:0.0105)
Batch 125/356: Loss=0.8526 (C:0.8526, R:0.0105)
Batch 150/356: Loss=0.8388 (C:0.8388, R:0.0105)
Batch 175/356: Loss=0.8375 (C:0.8375, R:0.0105)
Batch 200/356: Loss=0.8879 (C:0.8879, R:0.0105)
Batch 225/356: Loss=0.8423 (C:0.8423, R:0.0105)
Batch 250/356: Loss=0.8840 (C:0.8840, R:0.0105)
Batch 275/356: Loss=0.8572 (C:0.8572, R:0.0105)
Batch 300/356: Loss=0.8593 (C:0.8593, R:0.0105)
Batch 325/356: Loss=0.8446 (C:0.8446, R:0.0105)
Batch 350/356: Loss=0.8115 (C:0.8115, R:0.0105)

============================================================
Epoch 25/300 completed in 26.3s
Train: Loss=0.8431 (C:0.8431, R:0.0105) Ratio=3.84x
Val:   Loss=0.9497 (C:0.9497, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9497)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/356: Loss=0.8170 (C:0.8170, R:0.0105)
Batch  25/356: Loss=0.8467 (C:0.8467, R:0.0105)
Batch  50/356: Loss=0.8488 (C:0.8488, R:0.0105)
Batch  75/356: Loss=0.8279 (C:0.8279, R:0.0105)
Batch 100/356: Loss=0.8371 (C:0.8371, R:0.0105)
Batch 125/356: Loss=0.8101 (C:0.8101, R:0.0106)
Batch 150/356: Loss=0.8286 (C:0.8286, R:0.0105)
Batch 175/356: Loss=0.8428 (C:0.8428, R:0.0105)
Batch 200/356: Loss=0.8384 (C:0.8384, R:0.0105)
Batch 225/356: Loss=0.8199 (C:0.8199, R:0.0105)
Batch 250/356: Loss=0.8285 (C:0.8285, R:0.0105)
Batch 275/356: Loss=0.8387 (C:0.8387, R:0.0105)
Batch 300/356: Loss=0.8497 (C:0.8497, R:0.0105)
Batch 325/356: Loss=0.8618 (C:0.8618, R:0.0105)
Batch 350/356: Loss=0.8658 (C:0.8658, R:0.0105)

============================================================
Epoch 26/300 completed in 20.7s
Train: Loss=0.8389 (C:0.8389, R:0.0105) Ratio=3.89x
Val:   Loss=0.9595 (C:0.9595, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/356: Loss=0.8083 (C:0.8083, R:0.0105)
Batch  25/356: Loss=0.8330 (C:0.8330, R:0.0105)
Batch  50/356: Loss=0.8137 (C:0.8137, R:0.0105)
Batch  75/356: Loss=0.8089 (C:0.8089, R:0.0105)
Batch 100/356: Loss=0.8200 (C:0.8200, R:0.0105)
Batch 125/356: Loss=0.8271 (C:0.8271, R:0.0105)
Batch 150/356: Loss=0.8089 (C:0.8089, R:0.0105)
Batch 175/356: Loss=0.8425 (C:0.8425, R:0.0105)
Batch 200/356: Loss=0.8370 (C:0.8370, R:0.0105)
Batch 225/356: Loss=0.8356 (C:0.8356, R:0.0105)
Batch 250/356: Loss=0.8216 (C:0.8216, R:0.0105)
Batch 275/356: Loss=0.8152 (C:0.8152, R:0.0105)
Batch 300/356: Loss=0.8335 (C:0.8335, R:0.0105)
Batch 325/356: Loss=0.8251 (C:0.8251, R:0.0105)
Batch 350/356: Loss=0.8458 (C:0.8458, R:0.0105)

============================================================
Epoch 27/300 completed in 20.3s
Train: Loss=0.8340 (C:0.8340, R:0.0105) Ratio=4.05x
Val:   Loss=0.9413 (C:0.9413, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9413)
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.417 ± 0.577
    Neg distances: 2.188 ± 0.983
    Separation ratio: 5.24x
    Gap: -3.795
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/356: Loss=0.7989 (C:0.7989, R:0.0105)
Batch  25/356: Loss=0.8179 (C:0.8179, R:0.0105)
Batch  50/356: Loss=0.8125 (C:0.8125, R:0.0105)
Batch  75/356: Loss=0.7851 (C:0.7851, R:0.0105)
Batch 100/356: Loss=0.8087 (C:0.8087, R:0.0105)
Batch 125/356: Loss=0.7941 (C:0.7941, R:0.0105)
Batch 150/356: Loss=0.8352 (C:0.8352, R:0.0105)
Batch 175/356: Loss=0.8266 (C:0.8266, R:0.0105)
Batch 200/356: Loss=0.8131 (C:0.8131, R:0.0105)
Batch 225/356: Loss=0.8263 (C:0.8263, R:0.0105)
Batch 250/356: Loss=0.7931 (C:0.7931, R:0.0105)
Batch 275/356: Loss=0.8240 (C:0.8240, R:0.0105)
Batch 300/356: Loss=0.8195 (C:0.8195, R:0.0105)
Batch 325/356: Loss=0.8269 (C:0.8269, R:0.0105)
Batch 350/356: Loss=0.8076 (C:0.8076, R:0.0105)

============================================================
Epoch 28/300 completed in 26.2s
Train: Loss=0.8092 (C:0.8092, R:0.0105) Ratio=3.94x
Val:   Loss=0.9130 (C:0.9130, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9130)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/356: Loss=0.7889 (C:0.7889, R:0.0105)
Batch  25/356: Loss=0.7838 (C:0.7838, R:0.0105)
Batch  50/356: Loss=0.8147 (C:0.8147, R:0.0105)
Batch  75/356: Loss=0.7717 (C:0.7717, R:0.0105)
Batch 100/356: Loss=0.7897 (C:0.7897, R:0.0105)
Batch 125/356: Loss=0.7991 (C:0.7991, R:0.0105)
Batch 150/356: Loss=0.8391 (C:0.8391, R:0.0105)
Batch 175/356: Loss=0.7816 (C:0.7816, R:0.0105)
Batch 200/356: Loss=0.8137 (C:0.8137, R:0.0105)
Batch 225/356: Loss=0.8230 (C:0.8230, R:0.0105)
Batch 250/356: Loss=0.8068 (C:0.8068, R:0.0105)
Batch 275/356: Loss=0.7968 (C:0.7968, R:0.0105)
Batch 300/356: Loss=0.7886 (C:0.7886, R:0.0105)
Batch 325/356: Loss=0.8256 (C:0.8256, R:0.0105)
Batch 350/356: Loss=0.8018 (C:0.8018, R:0.0105)

============================================================
Epoch 29/300 completed in 20.6s
Train: Loss=0.8053 (C:0.8053, R:0.0105) Ratio=4.02x
Val:   Loss=0.9305 (C:0.9305, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/356: Loss=0.7863 (C:0.7863, R:0.0105)
Batch  25/356: Loss=0.8184 (C:0.8184, R:0.0105)
Batch  50/356: Loss=0.8419 (C:0.8419, R:0.0105)
Batch  75/356: Loss=0.7700 (C:0.7700, R:0.0105)
Batch 100/356: Loss=0.8092 (C:0.8092, R:0.0105)
Batch 125/356: Loss=0.8195 (C:0.8195, R:0.0105)
Batch 150/356: Loss=0.7726 (C:0.7726, R:0.0105)
Batch 175/356: Loss=0.8313 (C:0.8313, R:0.0105)
Batch 200/356: Loss=0.7860 (C:0.7860, R:0.0105)
Batch 225/356: Loss=0.8099 (C:0.8099, R:0.0105)
Batch 250/356: Loss=0.8417 (C:0.8417, R:0.0105)
Batch 275/356: Loss=0.8400 (C:0.8400, R:0.0105)
Batch 300/356: Loss=0.8180 (C:0.8180, R:0.0105)
Batch 325/356: Loss=0.8110 (C:0.8110, R:0.0105)
Batch 350/356: Loss=0.8269 (C:0.8269, R:0.0105)

============================================================
Epoch 30/300 completed in 20.3s
Train: Loss=0.8013 (C:0.8013, R:0.0105) Ratio=3.96x
Val:   Loss=0.9223 (C:0.9223, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.420 ± 0.609
    Neg distances: 2.276 ± 1.018
    Separation ratio: 5.43x
    Gap: -3.955
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/356: Loss=0.7443 (C:0.7443, R:0.0105)
Batch  25/356: Loss=0.7759 (C:0.7759, R:0.0105)
Batch  50/356: Loss=0.7787 (C:0.7787, R:0.0105)
Batch  75/356: Loss=0.7818 (C:0.7818, R:0.0105)
Batch 100/356: Loss=0.7505 (C:0.7505, R:0.0105)
Batch 125/356: Loss=0.7559 (C:0.7559, R:0.0105)
Batch 150/356: Loss=0.8093 (C:0.8093, R:0.0105)
Batch 175/356: Loss=0.8034 (C:0.8034, R:0.0105)
Batch 200/356: Loss=0.7602 (C:0.7602, R:0.0105)
Batch 225/356: Loss=0.7877 (C:0.7877, R:0.0105)
Batch 250/356: Loss=0.8126 (C:0.8126, R:0.0105)
Batch 275/356: Loss=0.7978 (C:0.7978, R:0.0105)
Batch 300/356: Loss=0.7955 (C:0.7955, R:0.0105)
Batch 325/356: Loss=0.7680 (C:0.7680, R:0.0105)
Batch 350/356: Loss=0.7957 (C:0.7957, R:0.0105)

============================================================
Epoch 31/300 completed in 26.6s
Train: Loss=0.7835 (C:0.7835, R:0.0105) Ratio=4.12x
Val:   Loss=0.8922 (C:0.8922, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8922)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/356: Loss=0.7754 (C:0.7754, R:0.0105)
Batch  25/356: Loss=0.7667 (C:0.7667, R:0.0105)
Batch  50/356: Loss=0.7923 (C:0.7923, R:0.0105)
Batch  75/356: Loss=0.7696 (C:0.7696, R:0.0105)
Batch 100/356: Loss=0.8024 (C:0.8024, R:0.0105)
Batch 125/356: Loss=0.7374 (C:0.7374, R:0.0105)
Batch 150/356: Loss=0.7834 (C:0.7834, R:0.0105)
Batch 175/356: Loss=0.7971 (C:0.7971, R:0.0105)
Batch 200/356: Loss=0.8041 (C:0.8041, R:0.0105)
Batch 225/356: Loss=0.7848 (C:0.7848, R:0.0105)
Batch 250/356: Loss=0.7776 (C:0.7776, R:0.0105)
Batch 275/356: Loss=0.7876 (C:0.7876, R:0.0105)
Batch 300/356: Loss=0.7782 (C:0.7782, R:0.0105)
Batch 325/356: Loss=0.8036 (C:0.8036, R:0.0105)
Batch 350/356: Loss=0.7618 (C:0.7618, R:0.0105)

============================================================
Epoch 32/300 completed in 20.3s
Train: Loss=0.7794 (C:0.7794, R:0.0105) Ratio=4.08x
Val:   Loss=0.8997 (C:0.8997, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/356: Loss=0.7741 (C:0.7741, R:0.0105)
Batch  25/356: Loss=0.7516 (C:0.7516, R:0.0105)
Batch  50/356: Loss=0.7622 (C:0.7622, R:0.0105)
Batch  75/356: Loss=0.7922 (C:0.7922, R:0.0105)
Batch 100/356: Loss=0.7971 (C:0.7971, R:0.0105)
Batch 125/356: Loss=0.7641 (C:0.7641, R:0.0105)
Batch 150/356: Loss=0.7614 (C:0.7614, R:0.0105)
Batch 175/356: Loss=0.7931 (C:0.7931, R:0.0105)
Batch 200/356: Loss=0.7668 (C:0.7668, R:0.0105)
Batch 225/356: Loss=0.8067 (C:0.8067, R:0.0105)
Batch 250/356: Loss=0.8024 (C:0.8024, R:0.0105)
Batch 275/356: Loss=0.8068 (C:0.8068, R:0.0105)
Batch 300/356: Loss=0.7662 (C:0.7662, R:0.0105)
Batch 325/356: Loss=0.7804 (C:0.7804, R:0.0105)
Batch 350/356: Loss=0.7867 (C:0.7867, R:0.0105)

============================================================
Epoch 33/300 completed in 20.5s
Train: Loss=0.7770 (C:0.7770, R:0.0105) Ratio=4.13x
Val:   Loss=0.8976 (C:0.8976, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.045
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.406 ± 0.592
    Neg distances: 2.357 ± 1.038
    Separation ratio: 5.81x
    Gap: -4.057
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/356: Loss=0.7463 (C:0.7463, R:0.0105)
Batch  25/356: Loss=0.7625 (C:0.7625, R:0.0105)
Batch  50/356: Loss=0.7461 (C:0.7461, R:0.0105)
Batch  75/356: Loss=0.7576 (C:0.7576, R:0.0105)
Batch 100/356: Loss=0.7847 (C:0.7847, R:0.0105)
Batch 125/356: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 150/356: Loss=0.7373 (C:0.7373, R:0.0105)
Batch 175/356: Loss=0.7485 (C:0.7485, R:0.0105)
Batch 200/356: Loss=0.7486 (C:0.7486, R:0.0105)
Batch 225/356: Loss=0.7694 (C:0.7694, R:0.0105)
Batch 250/356: Loss=0.7366 (C:0.7366, R:0.0105)
Batch 275/356: Loss=0.7393 (C:0.7393, R:0.0105)
Batch 300/356: Loss=0.7502 (C:0.7502, R:0.0105)
Batch 325/356: Loss=0.7447 (C:0.7447, R:0.0105)
Batch 350/356: Loss=0.7825 (C:0.7825, R:0.0105)

============================================================
Epoch 34/300 completed in 26.5s
Train: Loss=0.7506 (C:0.7506, R:0.0105) Ratio=4.12x
Val:   Loss=0.8695 (C:0.8695, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8695)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/356: Loss=0.7085 (C:0.7085, R:0.0105)
Batch  25/356: Loss=0.7042 (C:0.7042, R:0.0105)
Batch  50/356: Loss=0.7282 (C:0.7282, R:0.0105)
Batch  75/356: Loss=0.7163 (C:0.7163, R:0.0105)
Batch 100/356: Loss=0.7429 (C:0.7429, R:0.0105)
Batch 125/356: Loss=0.7419 (C:0.7419, R:0.0105)
Batch 150/356: Loss=0.7670 (C:0.7670, R:0.0105)
Batch 175/356: Loss=0.7348 (C:0.7348, R:0.0105)
Batch 200/356: Loss=0.7508 (C:0.7508, R:0.0105)
Batch 225/356: Loss=0.7531 (C:0.7531, R:0.0105)
Batch 250/356: Loss=0.7756 (C:0.7756, R:0.0105)
Batch 275/356: Loss=0.7560 (C:0.7560, R:0.0105)
Batch 300/356: Loss=0.7853 (C:0.7853, R:0.0105)
Batch 325/356: Loss=0.7363 (C:0.7363, R:0.0105)
Batch 350/356: Loss=0.7292 (C:0.7292, R:0.0105)

============================================================
Epoch 35/300 completed in 20.8s
Train: Loss=0.7459 (C:0.7459, R:0.0105) Ratio=4.24x
Val:   Loss=0.8660 (C:0.8660, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 0.8660)
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/356: Loss=0.7069 (C:0.7069, R:0.0106)
Batch  25/356: Loss=0.7439 (C:0.7439, R:0.0105)
Batch  50/356: Loss=0.7541 (C:0.7541, R:0.0105)
Batch  75/356: Loss=0.7590 (C:0.7590, R:0.0105)
Batch 100/356: Loss=0.7078 (C:0.7078, R:0.0105)
Batch 125/356: Loss=0.7171 (C:0.7171, R:0.0106)
Batch 150/356: Loss=0.7348 (C:0.7348, R:0.0105)
Batch 175/356: Loss=0.7136 (C:0.7136, R:0.0105)
Batch 200/356: Loss=0.7683 (C:0.7683, R:0.0105)
Batch 225/356: Loss=0.7432 (C:0.7432, R:0.0105)
Batch 250/356: Loss=0.7458 (C:0.7458, R:0.0105)
Batch 275/356: Loss=0.7472 (C:0.7472, R:0.0105)
Batch 300/356: Loss=0.7481 (C:0.7481, R:0.0105)
Batch 325/356: Loss=0.7681 (C:0.7681, R:0.0105)
Batch 350/356: Loss=0.7457 (C:0.7457, R:0.0105)

============================================================
Epoch 36/300 completed in 20.9s
Train: Loss=0.7444 (C:0.7444, R:0.0105) Ratio=4.23x
Val:   Loss=0.8679 (C:0.8679, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.090
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.410 ± 0.620
    Neg distances: 2.391 ± 1.055
    Separation ratio: 5.84x
    Gap: -4.116
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/356: Loss=0.7093 (C:0.7093, R:0.0105)
Batch  25/356: Loss=0.7048 (C:0.7048, R:0.0105)
Batch  50/356: Loss=0.7433 (C:0.7433, R:0.0105)
Batch  75/356: Loss=0.7174 (C:0.7174, R:0.0105)
Batch 100/356: Loss=0.7538 (C:0.7538, R:0.0105)
Batch 125/356: Loss=0.7358 (C:0.7358, R:0.0105)
Batch 150/356: Loss=0.7047 (C:0.7047, R:0.0105)
Batch 175/356: Loss=0.7683 (C:0.7683, R:0.0105)
Batch 200/356: Loss=0.7292 (C:0.7292, R:0.0105)
Batch 225/356: Loss=0.7034 (C:0.7034, R:0.0105)
Batch 250/356: Loss=0.7310 (C:0.7310, R:0.0105)
Batch 275/356: Loss=0.7314 (C:0.7314, R:0.0105)
Batch 300/356: Loss=0.7727 (C:0.7727, R:0.0105)
Batch 325/356: Loss=0.7392 (C:0.7392, R:0.0106)
Batch 350/356: Loss=0.7557 (C:0.7557, R:0.0105)

============================================================
Epoch 37/300 completed in 25.9s
Train: Loss=0.7359 (C:0.7359, R:0.0105) Ratio=4.32x
Val:   Loss=0.8655 (C:0.8655, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8655)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/356: Loss=0.7233 (C:0.7233, R:0.0105)
Batch  25/356: Loss=0.7208 (C:0.7208, R:0.0105)
Batch  50/356: Loss=0.7050 (C:0.7050, R:0.0105)
Batch  75/356: Loss=0.7219 (C:0.7219, R:0.0105)
Batch 100/356: Loss=0.7130 (C:0.7130, R:0.0105)
Batch 125/356: Loss=0.7246 (C:0.7246, R:0.0105)
Batch 150/356: Loss=0.6965 (C:0.6965, R:0.0105)
Batch 175/356: Loss=0.7371 (C:0.7371, R:0.0106)
Batch 200/356: Loss=0.7257 (C:0.7257, R:0.0105)
Batch 225/356: Loss=0.7571 (C:0.7571, R:0.0105)
Batch 250/356: Loss=0.7434 (C:0.7434, R:0.0105)
Batch 275/356: Loss=0.7462 (C:0.7462, R:0.0105)
Batch 300/356: Loss=0.7537 (C:0.7537, R:0.0105)
Batch 325/356: Loss=0.7003 (C:0.7003, R:0.0105)
Batch 350/356: Loss=0.7484 (C:0.7484, R:0.0105)

============================================================
Epoch 38/300 completed in 20.4s
Train: Loss=0.7304 (C:0.7304, R:0.0105) Ratio=4.38x
Val:   Loss=0.8755 (C:0.8755, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/356: Loss=0.7076 (C:0.7076, R:0.0105)
Batch  25/356: Loss=0.6826 (C:0.6826, R:0.0105)
Batch  50/356: Loss=0.7170 (C:0.7170, R:0.0105)
Batch  75/356: Loss=0.7068 (C:0.7068, R:0.0105)
Batch 100/356: Loss=0.7342 (C:0.7342, R:0.0105)
Batch 125/356: Loss=0.7220 (C:0.7220, R:0.0105)
Batch 150/356: Loss=0.7436 (C:0.7436, R:0.0105)
Batch 175/356: Loss=0.7426 (C:0.7426, R:0.0105)
Batch 200/356: Loss=0.7357 (C:0.7357, R:0.0105)
Batch 225/356: Loss=0.7597 (C:0.7597, R:0.0105)
Batch 250/356: Loss=0.7194 (C:0.7194, R:0.0105)
Batch 275/356: Loss=0.7275 (C:0.7275, R:0.0105)
Batch 300/356: Loss=0.7476 (C:0.7476, R:0.0105)
Batch 325/356: Loss=0.7112 (C:0.7112, R:0.0105)
Batch 350/356: Loss=0.7557 (C:0.7557, R:0.0105)

============================================================
Epoch 39/300 completed in 20.5s
Train: Loss=0.7284 (C:0.7284, R:0.0105) Ratio=4.41x
Val:   Loss=0.8674 (C:0.8674, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.385 ± 0.595
    Neg distances: 2.431 ± 1.050
    Separation ratio: 6.32x
    Gap: -4.339
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/356: Loss=0.6899 (C:0.6899, R:0.0105)
Batch  25/356: Loss=0.7113 (C:0.7113, R:0.0105)
Batch  50/356: Loss=0.6814 (C:0.6814, R:0.0105)
Batch  75/356: Loss=0.6728 (C:0.6728, R:0.0105)
Batch 100/356: Loss=0.6898 (C:0.6898, R:0.0105)
Batch 125/356: Loss=0.6973 (C:0.6973, R:0.0105)
Batch 150/356: Loss=0.7240 (C:0.7240, R:0.0105)
Batch 175/356: Loss=0.6892 (C:0.6892, R:0.0105)
Batch 200/356: Loss=0.7131 (C:0.7131, R:0.0105)
Batch 225/356: Loss=0.7129 (C:0.7129, R:0.0105)
Batch 250/356: Loss=0.6686 (C:0.6686, R:0.0105)
Batch 275/356: Loss=0.7215 (C:0.7215, R:0.0105)
Batch 300/356: Loss=0.6948 (C:0.6948, R:0.0105)
Batch 325/356: Loss=0.6929 (C:0.6929, R:0.0105)
Batch 350/356: Loss=0.7361 (C:0.7361, R:0.0105)

============================================================
Epoch 40/300 completed in 26.3s
Train: Loss=0.7014 (C:0.7014, R:0.0105) Ratio=4.40x
Val:   Loss=0.8365 (C:0.8365, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.8365)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/356: Loss=0.7114 (C:0.7114, R:0.0105)
Batch  25/356: Loss=0.6951 (C:0.6951, R:0.0105)
Batch  50/356: Loss=0.7126 (C:0.7126, R:0.0105)
Batch  75/356: Loss=0.6887 (C:0.6887, R:0.0105)
Batch 100/356: Loss=0.6962 (C:0.6962, R:0.0105)
Batch 125/356: Loss=0.6947 (C:0.6947, R:0.0105)
Batch 150/356: Loss=0.6958 (C:0.6958, R:0.0105)
Batch 175/356: Loss=0.7040 (C:0.7040, R:0.0105)
Batch 200/356: Loss=0.6819 (C:0.6819, R:0.0105)
Batch 225/356: Loss=0.7006 (C:0.7006, R:0.0105)
Batch 250/356: Loss=0.7046 (C:0.7046, R:0.0105)
Batch 275/356: Loss=0.7271 (C:0.7271, R:0.0105)
Batch 300/356: Loss=0.7097 (C:0.7097, R:0.0105)
Batch 325/356: Loss=0.7124 (C:0.7124, R:0.0105)
Batch 350/356: Loss=0.6992 (C:0.6992, R:0.0105)

============================================================
Epoch 41/300 completed in 20.1s
Train: Loss=0.6984 (C:0.6984, R:0.0105) Ratio=4.36x
Val:   Loss=0.8449 (C:0.8449, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.165
No improvement for 1 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/356: Loss=0.7159 (C:0.7159, R:0.0105)
Batch  25/356: Loss=0.7092 (C:0.7092, R:0.0105)
Batch  50/356: Loss=0.6814 (C:0.6814, R:0.0105)
Batch  75/356: Loss=0.7329 (C:0.7329, R:0.0105)
Batch 100/356: Loss=0.7085 (C:0.7085, R:0.0105)
Batch 125/356: Loss=0.6924 (C:0.6924, R:0.0105)
Batch 150/356: Loss=0.6995 (C:0.6995, R:0.0105)
Batch 175/356: Loss=0.6908 (C:0.6908, R:0.0105)
Batch 200/356: Loss=0.7013 (C:0.7013, R:0.0105)
Batch 225/356: Loss=0.7079 (C:0.7079, R:0.0105)
Batch 250/356: Loss=0.6744 (C:0.6744, R:0.0105)
Batch 275/356: Loss=0.6693 (C:0.6693, R:0.0105)
Batch 300/356: Loss=0.7077 (C:0.7077, R:0.0105)
Batch 325/356: Loss=0.7260 (C:0.7260, R:0.0105)
Batch 350/356: Loss=0.6982 (C:0.6982, R:0.0105)

============================================================
Epoch 42/300 completed in 20.1s
Train: Loss=0.6948 (C:0.6948, R:0.0105) Ratio=4.38x
Val:   Loss=0.8397 (C:0.8397, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.180
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.369 ± 0.587
    Neg distances: 2.492 ± 1.068
    Separation ratio: 6.76x
    Gap: -4.198
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/356: Loss=0.6498 (C:0.6498, R:0.0105)
Batch  25/356: Loss=0.6996 (C:0.6996, R:0.0105)
Batch  50/356: Loss=0.6721 (C:0.6721, R:0.0105)
Batch  75/356: Loss=0.6571 (C:0.6571, R:0.0106)
Batch 100/356: Loss=0.6952 (C:0.6952, R:0.0105)
Batch 125/356: Loss=0.7113 (C:0.7113, R:0.0105)
Batch 150/356: Loss=0.7143 (C:0.7143, R:0.0105)
Batch 175/356: Loss=0.6807 (C:0.6807, R:0.0105)
Batch 200/356: Loss=0.6524 (C:0.6524, R:0.0105)
Batch 225/356: Loss=0.6966 (C:0.6966, R:0.0105)
Batch 250/356: Loss=0.6498 (C:0.6498, R:0.0105)
Batch 275/356: Loss=0.6888 (C:0.6888, R:0.0105)
Batch 300/356: Loss=0.6809 (C:0.6809, R:0.0105)
Batch 325/356: Loss=0.6782 (C:0.6782, R:0.0105)
Batch 350/356: Loss=0.6751 (C:0.6751, R:0.0105)

============================================================
Epoch 43/300 completed in 26.1s
Train: Loss=0.6769 (C:0.6769, R:0.0105) Ratio=4.47x
Val:   Loss=0.8269 (C:0.8269, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.8269)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/356: Loss=0.6757 (C:0.6757, R:0.0105)
Batch  25/356: Loss=0.6727 (C:0.6727, R:0.0105)
Batch  50/356: Loss=0.6301 (C:0.6301, R:0.0105)
Batch  75/356: Loss=0.6357 (C:0.6357, R:0.0105)
Batch 100/356: Loss=0.6757 (C:0.6757, R:0.0105)
Batch 125/356: Loss=0.6791 (C:0.6791, R:0.0105)
Batch 150/356: Loss=0.6520 (C:0.6520, R:0.0105)
Batch 175/356: Loss=0.6716 (C:0.6716, R:0.0105)
Batch 200/356: Loss=0.6691 (C:0.6691, R:0.0105)
Batch 225/356: Loss=0.6766 (C:0.6766, R:0.0105)
Batch 250/356: Loss=0.7223 (C:0.7223, R:0.0105)
Batch 275/356: Loss=0.6770 (C:0.6770, R:0.0105)
Batch 300/356: Loss=0.6742 (C:0.6742, R:0.0105)
Batch 325/356: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 350/356: Loss=0.6795 (C:0.6795, R:0.0105)

============================================================
Epoch 44/300 completed in 20.2s
Train: Loss=0.6736 (C:0.6736, R:0.0105) Ratio=4.53x
Val:   Loss=0.8230 (C:0.8230, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.210
✅ New best model saved (Val Loss: 0.8230)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/356: Loss=0.6134 (C:0.6134, R:0.0105)
Batch  25/356: Loss=0.6653 (C:0.6653, R:0.0105)
Batch  50/356: Loss=0.6619 (C:0.6619, R:0.0105)
Batch  75/356: Loss=0.6692 (C:0.6692, R:0.0105)
Batch 100/356: Loss=0.6329 (C:0.6329, R:0.0105)
Batch 125/356: Loss=0.6633 (C:0.6633, R:0.0105)
Batch 150/356: Loss=0.6500 (C:0.6500, R:0.0105)
Batch 175/356: Loss=0.6500 (C:0.6500, R:0.0105)
Batch 200/356: Loss=0.6739 (C:0.6739, R:0.0105)
Batch 225/356: Loss=0.6950 (C:0.6950, R:0.0105)
Batch 250/356: Loss=0.6638 (C:0.6638, R:0.0105)
Batch 275/356: Loss=0.6883 (C:0.6883, R:0.0105)
Batch 300/356: Loss=0.6940 (C:0.6940, R:0.0105)
Batch 325/356: Loss=0.6771 (C:0.6771, R:0.0105)
Batch 350/356: Loss=0.6634 (C:0.6634, R:0.0105)

============================================================
Epoch 45/300 completed in 20.4s
Train: Loss=0.6715 (C:0.6715, R:0.0105) Ratio=4.54x
Val:   Loss=0.8308 (C:0.8308, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.225
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.371 ± 0.594
    Neg distances: 2.546 ± 1.083
    Separation ratio: 6.86x
    Gap: -4.299
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/356: Loss=0.6648 (C:0.6648, R:0.0105)
Batch  25/356: Loss=0.6655 (C:0.6655, R:0.0105)
Batch  50/356: Loss=0.6478 (C:0.6478, R:0.0105)
Batch  75/356: Loss=0.6722 (C:0.6722, R:0.0105)
Batch 100/356: Loss=0.6594 (C:0.6594, R:0.0106)
Batch 125/356: Loss=0.6699 (C:0.6699, R:0.0105)
Batch 150/356: Loss=0.6564 (C:0.6564, R:0.0105)
Batch 175/356: Loss=0.6886 (C:0.6886, R:0.0105)
Batch 200/356: Loss=0.6662 (C:0.6662, R:0.0105)
Batch 225/356: Loss=0.6597 (C:0.6597, R:0.0105)
Batch 250/356: Loss=0.6533 (C:0.6533, R:0.0105)
Batch 275/356: Loss=0.6929 (C:0.6929, R:0.0105)
Batch 300/356: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 325/356: Loss=0.6756 (C:0.6756, R:0.0105)
Batch 350/356: Loss=0.6644 (C:0.6644, R:0.0105)

============================================================
Epoch 46/300 completed in 26.6s
Train: Loss=0.6650 (C:0.6650, R:0.0105) Ratio=4.52x
Val:   Loss=0.8283 (C:0.8283, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.240
No improvement for 2 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/356: Loss=0.6557 (C:0.6557, R:0.0105)
Batch  25/356: Loss=0.6531 (C:0.6531, R:0.0105)
Batch  50/356: Loss=0.6705 (C:0.6705, R:0.0105)
Batch  75/356: Loss=0.6930 (C:0.6930, R:0.0105)
Batch 100/356: Loss=0.6825 (C:0.6825, R:0.0105)
Batch 125/356: Loss=0.6462 (C:0.6462, R:0.0105)
Batch 150/356: Loss=0.6176 (C:0.6176, R:0.0105)
Batch 175/356: Loss=0.6651 (C:0.6651, R:0.0105)
Batch 200/356: Loss=0.6579 (C:0.6579, R:0.0105)
Batch 225/356: Loss=0.6581 (C:0.6581, R:0.0105)
Batch 250/356: Loss=0.6784 (C:0.6784, R:0.0106)
Batch 275/356: Loss=0.6969 (C:0.6969, R:0.0105)
Batch 300/356: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 325/356: Loss=0.6797 (C:0.6797, R:0.0105)
Batch 350/356: Loss=0.6926 (C:0.6926, R:0.0105)

============================================================
Epoch 47/300 completed in 20.4s
Train: Loss=0.6619 (C:0.6619, R:0.0105) Ratio=4.54x
Val:   Loss=0.8233 (C:0.8233, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.255
No improvement for 3 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/356: Loss=0.6422 (C:0.6422, R:0.0105)
Batch  25/356: Loss=0.6250 (C:0.6250, R:0.0105)
Batch  50/356: Loss=0.6762 (C:0.6762, R:0.0105)
Batch  75/356: Loss=0.6558 (C:0.6558, R:0.0105)
Batch 100/356: Loss=0.6635 (C:0.6635, R:0.0105)
Batch 125/356: Loss=0.6529 (C:0.6529, R:0.0105)
Batch 150/356: Loss=0.6620 (C:0.6620, R:0.0105)
Batch 175/356: Loss=0.6748 (C:0.6748, R:0.0105)
Batch 200/356: Loss=0.6615 (C:0.6615, R:0.0105)
Batch 225/356: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 250/356: Loss=0.6545 (C:0.6545, R:0.0105)
Batch 275/356: Loss=0.6627 (C:0.6627, R:0.0105)
Batch 300/356: Loss=0.6584 (C:0.6584, R:0.0105)
Batch 325/356: Loss=0.6862 (C:0.6862, R:0.0105)
Batch 350/356: Loss=0.6780 (C:0.6780, R:0.0105)

============================================================
Epoch 48/300 completed in 20.4s
Train: Loss=0.6598 (C:0.6598, R:0.0105) Ratio=4.58x
Val:   Loss=0.8235 (C:0.8235, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.270
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.361 ± 0.596
    Neg distances: 2.584 ± 1.102
    Separation ratio: 7.15x
    Gap: -4.396
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/356: Loss=0.6171 (C:0.6171, R:0.0105)
Batch  25/356: Loss=0.6219 (C:0.6219, R:0.0105)
Batch  50/356: Loss=0.6206 (C:0.6206, R:0.0105)
Batch  75/356: Loss=0.6721 (C:0.6721, R:0.0105)
Batch 100/356: Loss=0.6211 (C:0.6211, R:0.0105)
Batch 125/356: Loss=0.6441 (C:0.6441, R:0.0105)
Batch 150/356: Loss=0.6208 (C:0.6208, R:0.0106)
Batch 175/356: Loss=0.6503 (C:0.6503, R:0.0105)
Batch 200/356: Loss=0.6161 (C:0.6161, R:0.0105)
Batch 225/356: Loss=0.6051 (C:0.6051, R:0.0105)
Batch 250/356: Loss=0.6309 (C:0.6309, R:0.0105)
Batch 275/356: Loss=0.6632 (C:0.6632, R:0.0105)
Batch 300/356: Loss=0.6513 (C:0.6513, R:0.0105)
Batch 325/356: Loss=0.6569 (C:0.6569, R:0.0105)
Batch 350/356: Loss=0.6458 (C:0.6458, R:0.0105)

============================================================
Epoch 49/300 completed in 26.3s
Train: Loss=0.6493 (C:0.6493, R:0.0105) Ratio=4.70x
Val:   Loss=0.8199 (C:0.8199, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.8199)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/356: Loss=0.6218 (C:0.6218, R:0.0105)
Batch  25/356: Loss=0.6409 (C:0.6409, R:0.0105)
Batch  50/356: Loss=0.6637 (C:0.6637, R:0.0105)
Batch  75/356: Loss=0.6772 (C:0.6772, R:0.0105)
Batch 100/356: Loss=0.6393 (C:0.6393, R:0.0105)
Batch 125/356: Loss=0.6659 (C:0.6659, R:0.0105)
Batch 150/356: Loss=0.6332 (C:0.6332, R:0.0105)
Batch 175/356: Loss=0.6444 (C:0.6444, R:0.0105)
Batch 200/356: Loss=0.6457 (C:0.6457, R:0.0105)
Batch 225/356: Loss=0.6597 (C:0.6597, R:0.0105)
Batch 250/356: Loss=0.6223 (C:0.6223, R:0.0105)
Batch 275/356: Loss=0.6610 (C:0.6610, R:0.0105)
Batch 300/356: Loss=0.6687 (C:0.6687, R:0.0105)
Batch 325/356: Loss=0.6302 (C:0.6302, R:0.0105)
Batch 350/356: Loss=0.6833 (C:0.6833, R:0.0105)

============================================================
Epoch 50/300 completed in 20.4s
Train: Loss=0.6482 (C:0.6482, R:0.0105) Ratio=4.65x
Val:   Loss=0.8141 (C:0.8141, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8141)
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/356: Loss=0.6507 (C:0.6507, R:0.0105)
Batch  25/356: Loss=0.6069 (C:0.6069, R:0.0105)
Batch  50/356: Loss=0.6543 (C:0.6543, R:0.0105)
Batch  75/356: Loss=0.6599 (C:0.6599, R:0.0105)
Batch 100/356: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 125/356: Loss=0.6502 (C:0.6502, R:0.0105)
Batch 150/356: Loss=0.6174 (C:0.6174, R:0.0105)
Batch 175/356: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 200/356: Loss=0.6419 (C:0.6419, R:0.0105)
Batch 225/356: Loss=0.6478 (C:0.6478, R:0.0105)
Batch 250/356: Loss=0.6459 (C:0.6459, R:0.0105)
Batch 275/356: Loss=0.6881 (C:0.6881, R:0.0105)
Batch 300/356: Loss=0.6738 (C:0.6738, R:0.0105)
Batch 325/356: Loss=0.6666 (C:0.6666, R:0.0105)
Batch 350/356: Loss=0.6430 (C:0.6430, R:0.0105)

============================================================
Epoch 51/300 completed in 20.4s
Train: Loss=0.6460 (C:0.6460, R:0.0105) Ratio=4.66x
Val:   Loss=0.8244 (C:0.8244, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.364 ± 0.618
    Neg distances: 2.562 ± 1.095
    Separation ratio: 7.04x
    Gap: -4.317
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/356: Loss=0.6445 (C:0.6445, R:0.0105)
Batch  25/356: Loss=0.6357 (C:0.6357, R:0.0105)
Batch  50/356: Loss=0.6489 (C:0.6489, R:0.0105)
Batch  75/356: Loss=0.6446 (C:0.6446, R:0.0105)
Batch 100/356: Loss=0.6692 (C:0.6692, R:0.0105)
Batch 125/356: Loss=0.6260 (C:0.6260, R:0.0105)
Batch 150/356: Loss=0.6181 (C:0.6181, R:0.0105)
Batch 175/356: Loss=0.6319 (C:0.6319, R:0.0105)
Batch 200/356: Loss=0.6687 (C:0.6687, R:0.0105)
Batch 225/356: Loss=0.6653 (C:0.6653, R:0.0105)
Batch 250/356: Loss=0.6338 (C:0.6338, R:0.0105)
Batch 275/356: Loss=0.6601 (C:0.6601, R:0.0105)
Batch 300/356: Loss=0.6438 (C:0.6438, R:0.0105)
Batch 325/356: Loss=0.6500 (C:0.6500, R:0.0105)
Batch 350/356: Loss=0.6667 (C:0.6667, R:0.0105)

============================================================
Epoch 52/300 completed in 26.3s
Train: Loss=0.6418 (C:0.6418, R:0.0105) Ratio=4.63x
Val:   Loss=0.8183 (C:0.8183, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/356: Loss=0.6359 (C:0.6359, R:0.0105)
Batch  25/356: Loss=0.6578 (C:0.6578, R:0.0105)
Batch  50/356: Loss=0.6433 (C:0.6433, R:0.0105)
Batch  75/356: Loss=0.6434 (C:0.6434, R:0.0105)
Batch 100/356: Loss=0.6147 (C:0.6147, R:0.0105)
Batch 125/356: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 150/356: Loss=0.6183 (C:0.6183, R:0.0105)
Batch 175/356: Loss=0.6742 (C:0.6742, R:0.0105)
Batch 200/356: Loss=0.6309 (C:0.6309, R:0.0105)
Batch 225/356: Loss=0.6351 (C:0.6351, R:0.0105)
Batch 250/356: Loss=0.6626 (C:0.6626, R:0.0105)
Batch 275/356: Loss=0.6410 (C:0.6410, R:0.0105)
Batch 300/356: Loss=0.6156 (C:0.6156, R:0.0105)
Batch 325/356: Loss=0.6682 (C:0.6682, R:0.0105)
Batch 350/356: Loss=0.6475 (C:0.6475, R:0.0105)

============================================================
Epoch 53/300 completed in 20.2s
Train: Loss=0.6427 (C:0.6427, R:0.0105) Ratio=4.68x
Val:   Loss=0.8080 (C:0.8080, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8080)
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/356: Loss=0.6121 (C:0.6121, R:0.0105)
Batch  25/356: Loss=0.6389 (C:0.6389, R:0.0105)
Batch  50/356: Loss=0.6529 (C:0.6529, R:0.0105)
Batch  75/356: Loss=0.6468 (C:0.6468, R:0.0105)
Batch 100/356: Loss=0.6168 (C:0.6168, R:0.0105)
Batch 125/356: Loss=0.6456 (C:0.6456, R:0.0105)
Batch 150/356: Loss=0.6521 (C:0.6521, R:0.0105)
Batch 175/356: Loss=0.6406 (C:0.6406, R:0.0105)
Batch 200/356: Loss=0.6368 (C:0.6368, R:0.0105)
Batch 225/356: Loss=0.6939 (C:0.6939, R:0.0105)
Batch 250/356: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 275/356: Loss=0.6137 (C:0.6137, R:0.0105)
Batch 300/356: Loss=0.6667 (C:0.6667, R:0.0105)
Batch 325/356: Loss=0.6624 (C:0.6624, R:0.0105)
Batch 350/356: Loss=0.6670 (C:0.6670, R:0.0105)

============================================================
Epoch 54/300 completed in 20.1s
Train: Loss=0.6401 (C:0.6401, R:0.0105) Ratio=4.73x
Val:   Loss=0.8182 (C:0.8182, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.331 ± 0.563
    Neg distances: 2.608 ± 1.091
    Separation ratio: 7.88x
    Gap: -4.398
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/356: Loss=0.5988 (C:0.5988, R:0.0105)
Batch  25/356: Loss=0.6126 (C:0.6126, R:0.0105)
Batch  50/356: Loss=0.5918 (C:0.5918, R:0.0105)
Batch  75/356: Loss=0.5973 (C:0.5973, R:0.0105)
Batch 100/356: Loss=0.6143 (C:0.6143, R:0.0105)
Batch 125/356: Loss=0.6309 (C:0.6309, R:0.0105)
Batch 150/356: Loss=0.5945 (C:0.5945, R:0.0105)
Batch 175/356: Loss=0.6049 (C:0.6049, R:0.0105)
Batch 200/356: Loss=0.6215 (C:0.6215, R:0.0105)
Batch 225/356: Loss=0.6504 (C:0.6504, R:0.0105)
Batch 250/356: Loss=0.5965 (C:0.5965, R:0.0105)
Batch 275/356: Loss=0.6226 (C:0.6226, R:0.0105)
Batch 300/356: Loss=0.6344 (C:0.6344, R:0.0105)
Batch 325/356: Loss=0.6614 (C:0.6614, R:0.0105)
Batch 350/356: Loss=0.6418 (C:0.6418, R:0.0105)

============================================================
Epoch 55/300 completed in 25.6s
Train: Loss=0.6146 (C:0.6146, R:0.0105) Ratio=4.67x
Val:   Loss=0.7977 (C:0.7977, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7977)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/356: Loss=0.6057 (C:0.6057, R:0.0105)
Batch  25/356: Loss=0.5917 (C:0.5917, R:0.0105)
Batch  50/356: Loss=0.5848 (C:0.5848, R:0.0105)
Batch  75/356: Loss=0.6015 (C:0.6015, R:0.0105)
Batch 100/356: Loss=0.6002 (C:0.6002, R:0.0105)
Batch 125/356: Loss=0.6084 (C:0.6084, R:0.0105)
Batch 150/356: Loss=0.6352 (C:0.6352, R:0.0105)
Batch 175/356: Loss=0.6030 (C:0.6030, R:0.0105)
Batch 200/356: Loss=0.5851 (C:0.5851, R:0.0105)
Batch 225/356: Loss=0.6376 (C:0.6376, R:0.0105)
Batch 250/356: Loss=0.6049 (C:0.6049, R:0.0105)
Batch 275/356: Loss=0.6082 (C:0.6082, R:0.0105)
Batch 300/356: Loss=0.6436 (C:0.6436, R:0.0105)
Batch 325/356: Loss=0.6164 (C:0.6164, R:0.0105)
Batch 350/356: Loss=0.6300 (C:0.6300, R:0.0105)

============================================================
Epoch 56/300 completed in 20.3s
Train: Loss=0.6141 (C:0.6141, R:0.0105) Ratio=4.83x
Val:   Loss=0.7947 (C:0.7947, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7947)
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/356: Loss=0.6184 (C:0.6184, R:0.0105)
Batch  25/356: Loss=0.6173 (C:0.6173, R:0.0105)
Batch  50/356: Loss=0.6141 (C:0.6141, R:0.0105)
Batch  75/356: Loss=0.5726 (C:0.5726, R:0.0105)
Batch 100/356: Loss=0.5901 (C:0.5901, R:0.0105)
Batch 125/356: Loss=0.6283 (C:0.6283, R:0.0105)
Batch 150/356: Loss=0.6329 (C:0.6329, R:0.0105)
Batch 175/356: Loss=0.6002 (C:0.6002, R:0.0105)
Batch 200/356: Loss=0.6253 (C:0.6253, R:0.0105)
Batch 225/356: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 250/356: Loss=0.6043 (C:0.6043, R:0.0106)
Batch 275/356: Loss=0.6154 (C:0.6154, R:0.0105)
Batch 300/356: Loss=0.6339 (C:0.6339, R:0.0105)
Batch 325/356: Loss=0.6154 (C:0.6154, R:0.0105)
Batch 350/356: Loss=0.6171 (C:0.6171, R:0.0105)

============================================================
Epoch 57/300 completed in 20.1s
Train: Loss=0.6108 (C:0.6108, R:0.0105) Ratio=4.77x
Val:   Loss=0.8008 (C:0.8008, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.333 ± 0.597
    Neg distances: 2.610 ± 1.094
    Separation ratio: 7.84x
    Gap: -4.373
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/356: Loss=0.5803 (C:0.5803, R:0.0105)
Batch  25/356: Loss=0.5540 (C:0.5540, R:0.0105)
Batch  50/356: Loss=0.6105 (C:0.6105, R:0.0105)
Batch  75/356: Loss=0.6163 (C:0.6163, R:0.0105)
Batch 100/356: Loss=0.6399 (C:0.6399, R:0.0105)
Batch 125/356: Loss=0.6073 (C:0.6073, R:0.0105)
Batch 150/356: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 175/356: Loss=0.5928 (C:0.5928, R:0.0105)
Batch 200/356: Loss=0.6372 (C:0.6372, R:0.0105)
Batch 225/356: Loss=0.6424 (C:0.6424, R:0.0105)
Batch 250/356: Loss=0.6142 (C:0.6142, R:0.0105)
Batch 275/356: Loss=0.6011 (C:0.6011, R:0.0105)
Batch 300/356: Loss=0.6347 (C:0.6347, R:0.0105)
Batch 325/356: Loss=0.6171 (C:0.6171, R:0.0105)
Batch 350/356: Loss=0.6286 (C:0.6286, R:0.0105)

============================================================
Epoch 58/300 completed in 26.0s
Train: Loss=0.6098 (C:0.6098, R:0.0105) Ratio=4.75x
Val:   Loss=0.8073 (C:0.8073, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/356: Loss=0.6304 (C:0.6304, R:0.0105)
Batch  25/356: Loss=0.5605 (C:0.5605, R:0.0105)
Batch  50/356: Loss=0.5927 (C:0.5927, R:0.0105)
Batch  75/356: Loss=0.6068 (C:0.6068, R:0.0105)
Batch 100/356: Loss=0.5493 (C:0.5493, R:0.0105)
Batch 125/356: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 150/356: Loss=0.5898 (C:0.5898, R:0.0105)
Batch 175/356: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 200/356: Loss=0.6028 (C:0.6028, R:0.0105)
Batch 225/356: Loss=0.6146 (C:0.6146, R:0.0105)
Batch 250/356: Loss=0.6165 (C:0.6165, R:0.0105)
Batch 275/356: Loss=0.6456 (C:0.6456, R:0.0105)
Batch 300/356: Loss=0.6168 (C:0.6168, R:0.0105)
Batch 325/356: Loss=0.6471 (C:0.6471, R:0.0105)
Batch 350/356: Loss=0.5968 (C:0.5968, R:0.0105)

============================================================
Epoch 59/300 completed in 20.4s
Train: Loss=0.6085 (C:0.6085, R:0.0105) Ratio=4.93x
Val:   Loss=0.8044 (C:0.8044, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/356: Loss=0.5912 (C:0.5912, R:0.0105)
Batch  25/356: Loss=0.6222 (C:0.6222, R:0.0105)
Batch  50/356: Loss=0.5756 (C:0.5756, R:0.0105)
Batch  75/356: Loss=0.6032 (C:0.6032, R:0.0105)
Batch 100/356: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 125/356: Loss=0.5975 (C:0.5975, R:0.0105)
Batch 150/356: Loss=0.6106 (C:0.6106, R:0.0105)
Batch 175/356: Loss=0.5687 (C:0.5687, R:0.0105)
Batch 200/356: Loss=0.6219 (C:0.6219, R:0.0105)
Batch 225/356: Loss=0.6300 (C:0.6300, R:0.0105)
Batch 250/356: Loss=0.5718 (C:0.5718, R:0.0106)
Batch 275/356: Loss=0.5629 (C:0.5629, R:0.0105)
Batch 300/356: Loss=0.5879 (C:0.5879, R:0.0105)
Batch 325/356: Loss=0.6176 (C:0.6176, R:0.0105)
Batch 350/356: Loss=0.6314 (C:0.6314, R:0.0105)

============================================================
Epoch 60/300 completed in 20.9s
Train: Loss=0.6052 (C:0.6052, R:0.0105) Ratio=4.90x
Val:   Loss=0.8071 (C:0.8071, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 4 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.318 ± 0.551
    Neg distances: 2.663 ± 1.105
    Separation ratio: 8.37x
    Gap: -4.434
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/356: Loss=0.5882 (C:0.5882, R:0.0105)
Batch  25/356: Loss=0.6120 (C:0.6120, R:0.0105)
Batch  50/356: Loss=0.5905 (C:0.5905, R:0.0105)
Batch  75/356: Loss=0.6250 (C:0.6250, R:0.0105)
Batch 100/356: Loss=0.5660 (C:0.5660, R:0.0105)
Batch 125/356: Loss=0.5951 (C:0.5951, R:0.0105)
Batch 150/356: Loss=0.6009 (C:0.6009, R:0.0105)
Batch 175/356: Loss=0.5977 (C:0.5977, R:0.0105)
Batch 200/356: Loss=0.5877 (C:0.5877, R:0.0105)
Batch 225/356: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 250/356: Loss=0.6136 (C:0.6136, R:0.0106)
Batch 275/356: Loss=0.5965 (C:0.5965, R:0.0105)
Batch 300/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 325/356: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 350/356: Loss=0.5971 (C:0.5971, R:0.0105)

============================================================
Epoch 61/300 completed in 26.4s
Train: Loss=0.5924 (C:0.5924, R:0.0105) Ratio=4.88x
Val:   Loss=0.7912 (C:0.7912, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7912)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/356: Loss=0.5826 (C:0.5826, R:0.0105)
Batch  25/356: Loss=0.5969 (C:0.5969, R:0.0105)
Batch  50/356: Loss=0.5984 (C:0.5984, R:0.0105)
Batch  75/356: Loss=0.5905 (C:0.5905, R:0.0105)
Batch 100/356: Loss=0.5695 (C:0.5695, R:0.0105)
Batch 125/356: Loss=0.5883 (C:0.5883, R:0.0105)
Batch 150/356: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 175/356: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 200/356: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 225/356: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 250/356: Loss=0.5976 (C:0.5976, R:0.0105)
Batch 275/356: Loss=0.6124 (C:0.6124, R:0.0105)
Batch 300/356: Loss=0.5870 (C:0.5870, R:0.0105)
Batch 325/356: Loss=0.5761 (C:0.5761, R:0.0105)
Batch 350/356: Loss=0.6034 (C:0.6034, R:0.0105)

============================================================
Epoch 62/300 completed in 20.3s
Train: Loss=0.5910 (C:0.5910, R:0.0105) Ratio=4.90x
Val:   Loss=0.7910 (C:0.7910, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7910)
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/356: Loss=0.5683 (C:0.5683, R:0.0105)
Batch  25/356: Loss=0.5675 (C:0.5675, R:0.0105)
Batch  50/356: Loss=0.5979 (C:0.5979, R:0.0105)
Batch  75/356: Loss=0.5876 (C:0.5876, R:0.0105)
Batch 100/356: Loss=0.5763 (C:0.5763, R:0.0105)
Batch 125/356: Loss=0.5855 (C:0.5855, R:0.0105)
Batch 150/356: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 175/356: Loss=0.6104 (C:0.6104, R:0.0105)
Batch 200/356: Loss=0.5648 (C:0.5648, R:0.0105)
Batch 225/356: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 250/356: Loss=0.6021 (C:0.6021, R:0.0105)
Batch 275/356: Loss=0.5831 (C:0.5831, R:0.0105)
Batch 300/356: Loss=0.5686 (C:0.5686, R:0.0105)
Batch 325/356: Loss=0.5993 (C:0.5993, R:0.0105)
Batch 350/356: Loss=0.5993 (C:0.5993, R:0.0105)

============================================================
Epoch 63/300 completed in 20.2s
Train: Loss=0.5894 (C:0.5894, R:0.0105) Ratio=4.99x
Val:   Loss=0.7946 (C:0.7946, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.328 ± 0.590
    Neg distances: 2.654 ± 1.102
    Separation ratio: 8.10x
    Gap: -4.541
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/356: Loss=0.5704 (C:0.5704, R:0.0105)
Batch  25/356: Loss=0.5637 (C:0.5637, R:0.0105)
Batch  50/356: Loss=0.5976 (C:0.5976, R:0.0105)
Batch  75/356: Loss=0.5703 (C:0.5703, R:0.0105)
Batch 100/356: Loss=0.5513 (C:0.5513, R:0.0105)
Batch 125/356: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 150/356: Loss=0.5913 (C:0.5913, R:0.0105)
Batch 175/356: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 200/356: Loss=0.5954 (C:0.5954, R:0.0105)
Batch 225/356: Loss=0.6434 (C:0.6434, R:0.0105)
Batch 250/356: Loss=0.5934 (C:0.5934, R:0.0105)
Batch 275/356: Loss=0.5625 (C:0.5625, R:0.0105)
Batch 300/356: Loss=0.6072 (C:0.6072, R:0.0105)
Batch 325/356: Loss=0.5834 (C:0.5834, R:0.0105)
Batch 350/356: Loss=0.5619 (C:0.5619, R:0.0105)

============================================================
Epoch 64/300 completed in 26.4s
Train: Loss=0.5907 (C:0.5907, R:0.0105) Ratio=5.04x
Val:   Loss=0.7939 (C:0.7939, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/356: Loss=0.5543 (C:0.5543, R:0.0105)
Batch  25/356: Loss=0.5640 (C:0.5640, R:0.0105)
Batch  50/356: Loss=0.5807 (C:0.5807, R:0.0105)
Batch  75/356: Loss=0.6556 (C:0.6556, R:0.0105)
Batch 100/356: Loss=0.6054 (C:0.6054, R:0.0105)
Batch 125/356: Loss=0.5836 (C:0.5836, R:0.0105)
Batch 150/356: Loss=0.5879 (C:0.5879, R:0.0105)
Batch 175/356: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 200/356: Loss=0.5744 (C:0.5744, R:0.0105)
Batch 225/356: Loss=0.5495 (C:0.5495, R:0.0105)
Batch 250/356: Loss=0.5732 (C:0.5732, R:0.0105)
Batch 275/356: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 300/356: Loss=0.5913 (C:0.5913, R:0.0105)
Batch 325/356: Loss=0.6025 (C:0.6025, R:0.0105)
Batch 350/356: Loss=0.6065 (C:0.6065, R:0.0105)

============================================================
Epoch 65/300 completed in 20.5s
Train: Loss=0.5913 (C:0.5913, R:0.0105) Ratio=5.05x
Val:   Loss=0.8036 (C:0.8036, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/356: Loss=0.5818 (C:0.5818, R:0.0105)
Batch  25/356: Loss=0.5576 (C:0.5576, R:0.0105)
Batch  50/356: Loss=0.5844 (C:0.5844, R:0.0105)
Batch  75/356: Loss=0.5715 (C:0.5715, R:0.0105)
Batch 100/356: Loss=0.5869 (C:0.5869, R:0.0105)
Batch 125/356: Loss=0.5838 (C:0.5838, R:0.0105)
Batch 150/356: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 175/356: Loss=0.5980 (C:0.5980, R:0.0105)
Batch 200/356: Loss=0.5636 (C:0.5636, R:0.0105)
Batch 225/356: Loss=0.6127 (C:0.6127, R:0.0105)
Batch 250/356: Loss=0.6005 (C:0.6005, R:0.0105)
Batch 275/356: Loss=0.6084 (C:0.6084, R:0.0105)
Batch 300/356: Loss=0.6465 (C:0.6465, R:0.0105)
Batch 325/356: Loss=0.6277 (C:0.6277, R:0.0105)
Batch 350/356: Loss=0.6021 (C:0.6021, R:0.0105)

============================================================
Epoch 66/300 completed in 20.6s
Train: Loss=0.5884 (C:0.5884, R:0.0105) Ratio=4.96x
Val:   Loss=0.7953 (C:0.7953, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.316 ± 0.571
    Neg distances: 2.650 ± 1.101
    Separation ratio: 8.40x
    Gap: -4.442
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/356: Loss=0.5784 (C:0.5784, R:0.0105)
Batch  25/356: Loss=0.5572 (C:0.5572, R:0.0105)
Batch  50/356: Loss=0.5627 (C:0.5627, R:0.0105)
Batch  75/356: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 100/356: Loss=0.5553 (C:0.5553, R:0.0105)
Batch 125/356: Loss=0.5461 (C:0.5461, R:0.0105)
Batch 150/356: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 175/356: Loss=0.6246 (C:0.6246, R:0.0105)
Batch 200/356: Loss=0.5497 (C:0.5497, R:0.0105)
Batch 225/356: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 250/356: Loss=0.5815 (C:0.5815, R:0.0105)
Batch 275/356: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 300/356: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 325/356: Loss=0.5484 (C:0.5484, R:0.0105)
Batch 350/356: Loss=0.5890 (C:0.5890, R:0.0105)

============================================================
Epoch 67/300 completed in 25.9s
Train: Loss=0.5808 (C:0.5808, R:0.0105) Ratio=5.10x
Val:   Loss=0.7832 (C:0.7832, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7832)
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch  25/356: Loss=0.5691 (C:0.5691, R:0.0105)
Batch  50/356: Loss=0.5661 (C:0.5661, R:0.0105)
Batch  75/356: Loss=0.6008 (C:0.6008, R:0.0105)
Batch 100/356: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 125/356: Loss=0.5782 (C:0.5782, R:0.0105)
Batch 150/356: Loss=0.5890 (C:0.5890, R:0.0105)
Batch 175/356: Loss=0.5656 (C:0.5656, R:0.0105)
Batch 200/356: Loss=0.6321 (C:0.6321, R:0.0105)
Batch 225/356: Loss=0.6125 (C:0.6125, R:0.0105)
Batch 250/356: Loss=0.5343 (C:0.5343, R:0.0105)
Batch 275/356: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 300/356: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 325/356: Loss=0.5924 (C:0.5924, R:0.0105)
Batch 350/356: Loss=0.5736 (C:0.5736, R:0.0105)

============================================================
Epoch 68/300 completed in 20.3s
Train: Loss=0.5798 (C:0.5798, R:0.0105) Ratio=5.03x
Val:   Loss=0.7930 (C:0.7930, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/356: Loss=0.5701 (C:0.5701, R:0.0105)
Batch  25/356: Loss=0.5727 (C:0.5727, R:0.0105)
Batch  50/356: Loss=0.5771 (C:0.5771, R:0.0105)
Batch  75/356: Loss=0.5905 (C:0.5905, R:0.0105)
Batch 100/356: Loss=0.5298 (C:0.5298, R:0.0105)
Batch 125/356: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 150/356: Loss=0.5765 (C:0.5765, R:0.0105)
Batch 175/356: Loss=0.5631 (C:0.5631, R:0.0105)
Batch 200/356: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 225/356: Loss=0.5604 (C:0.5604, R:0.0105)
Batch 250/356: Loss=0.5923 (C:0.5923, R:0.0105)
Batch 275/356: Loss=0.5960 (C:0.5960, R:0.0105)
Batch 300/356: Loss=0.5788 (C:0.5788, R:0.0105)
Batch 325/356: Loss=0.5725 (C:0.5725, R:0.0105)
Batch 350/356: Loss=0.5914 (C:0.5914, R:0.0105)

============================================================
Epoch 69/300 completed in 20.3s
Train: Loss=0.5795 (C:0.5795, R:0.0105) Ratio=5.08x
Val:   Loss=0.7946 (C:0.7946, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.314 ± 0.575
    Neg distances: 2.657 ± 1.100
    Separation ratio: 8.46x
    Gap: -4.471
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/356: Loss=0.5539 (C:0.5539, R:0.0105)
Batch  25/356: Loss=0.5649 (C:0.5649, R:0.0105)
Batch  50/356: Loss=0.5568 (C:0.5568, R:0.0105)
Batch  75/356: Loss=0.5487 (C:0.5487, R:0.0105)
Batch 100/356: Loss=0.5640 (C:0.5640, R:0.0105)
Batch 125/356: Loss=0.5800 (C:0.5800, R:0.0105)
Batch 150/356: Loss=0.5859 (C:0.5859, R:0.0105)
Batch 175/356: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 200/356: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 225/356: Loss=0.5789 (C:0.5789, R:0.0105)
Batch 250/356: Loss=0.5720 (C:0.5720, R:0.0105)
Batch 275/356: Loss=0.5901 (C:0.5901, R:0.0105)
Batch 300/356: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 325/356: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 350/356: Loss=0.5733 (C:0.5733, R:0.0105)

============================================================
Epoch 70/300 completed in 26.0s
Train: Loss=0.5756 (C:0.5756, R:0.0105) Ratio=5.11x
Val:   Loss=0.7917 (C:0.7917, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/356: Loss=0.5482 (C:0.5482, R:0.0105)
Batch  25/356: Loss=0.5582 (C:0.5582, R:0.0105)
Batch  50/356: Loss=0.5797 (C:0.5797, R:0.0105)
Batch  75/356: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 100/356: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 125/356: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 150/356: Loss=0.5954 (C:0.5954, R:0.0105)
Batch 175/356: Loss=0.5554 (C:0.5554, R:0.0105)
Batch 200/356: Loss=0.5824 (C:0.5824, R:0.0105)
Batch 225/356: Loss=0.5831 (C:0.5831, R:0.0105)
Batch 250/356: Loss=0.5540 (C:0.5540, R:0.0105)
Batch 275/356: Loss=0.5690 (C:0.5690, R:0.0105)
Batch 300/356: Loss=0.5665 (C:0.5665, R:0.0105)
Batch 325/356: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 350/356: Loss=0.6144 (C:0.6144, R:0.0105)

============================================================
Epoch 71/300 completed in 20.3s
Train: Loss=0.5746 (C:0.5746, R:0.0105) Ratio=5.13x
Val:   Loss=0.8006 (C:0.8006, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/356: Loss=0.5398 (C:0.5398, R:0.0105)
Batch  25/356: Loss=0.5286 (C:0.5286, R:0.0105)
Batch  50/356: Loss=0.5737 (C:0.5737, R:0.0105)
Batch  75/356: Loss=0.5519 (C:0.5519, R:0.0106)
Batch 100/356: Loss=0.5952 (C:0.5952, R:0.0105)
Batch 125/356: Loss=0.5476 (C:0.5476, R:0.0105)
Batch 150/356: Loss=0.5971 (C:0.5971, R:0.0105)
Batch 175/356: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 200/356: Loss=0.5479 (C:0.5479, R:0.0105)
Batch 225/356: Loss=0.5737 (C:0.5737, R:0.0105)
Batch 250/356: Loss=0.5731 (C:0.5731, R:0.0105)
Batch 275/356: Loss=0.5984 (C:0.5984, R:0.0105)
Batch 300/356: Loss=0.5928 (C:0.5928, R:0.0105)
Batch 325/356: Loss=0.5586 (C:0.5586, R:0.0106)
Batch 350/356: Loss=0.5793 (C:0.5793, R:0.0105)

============================================================
Epoch 72/300 completed in 20.3s
Train: Loss=0.5721 (C:0.5721, R:0.0105) Ratio=5.08x
Val:   Loss=0.7870 (C:0.7870, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.288 ± 0.560
    Neg distances: 2.692 ± 1.102
    Separation ratio: 9.36x
    Gap: -4.481
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/356: Loss=0.5582 (C:0.5582, R:0.0105)
Batch  25/356: Loss=0.5346 (C:0.5346, R:0.0105)
Batch  50/356: Loss=0.5277 (C:0.5277, R:0.0105)
Batch  75/356: Loss=0.5540 (C:0.5540, R:0.0105)
Batch 100/356: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 125/356: Loss=0.5469 (C:0.5469, R:0.0105)
Batch 150/356: Loss=0.5547 (C:0.5547, R:0.0105)
Batch 175/356: Loss=0.5188 (C:0.5188, R:0.0105)
Batch 200/356: Loss=0.5702 (C:0.5702, R:0.0105)
Batch 225/356: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 250/356: Loss=0.5563 (C:0.5563, R:0.0105)
Batch 275/356: Loss=0.5632 (C:0.5632, R:0.0105)
Batch 300/356: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 325/356: Loss=0.5597 (C:0.5597, R:0.0105)
Batch 350/356: Loss=0.5616 (C:0.5616, R:0.0105)

============================================================
Epoch 73/300 completed in 25.9s
Train: Loss=0.5539 (C:0.5539, R:0.0105) Ratio=5.17x
Val:   Loss=0.7757 (C:0.7757, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7757)
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/356: Loss=0.5564 (C:0.5564, R:0.0105)
Batch  25/356: Loss=0.5259 (C:0.5259, R:0.0105)
Batch  50/356: Loss=0.5302 (C:0.5302, R:0.0105)
Batch  75/356: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 100/356: Loss=0.5311 (C:0.5311, R:0.0105)
Batch 125/356: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 150/356: Loss=0.5426 (C:0.5426, R:0.0105)
Batch 175/356: Loss=0.5359 (C:0.5359, R:0.0105)
Batch 200/356: Loss=0.5512 (C:0.5512, R:0.0105)
Batch 225/356: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 250/356: Loss=0.5509 (C:0.5509, R:0.0105)
Batch 275/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 300/356: Loss=0.5251 (C:0.5251, R:0.0105)
Batch 325/356: Loss=0.5320 (C:0.5320, R:0.0105)
Batch 350/356: Loss=0.5689 (C:0.5689, R:0.0105)

============================================================
Epoch 74/300 completed in 20.4s
Train: Loss=0.5501 (C:0.5501, R:0.0105) Ratio=5.20x
Val:   Loss=0.7819 (C:0.7819, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/356: Loss=0.5611 (C:0.5611, R:0.0105)
Batch  25/356: Loss=0.5429 (C:0.5429, R:0.0105)
Batch  50/356: Loss=0.5405 (C:0.5405, R:0.0105)
Batch  75/356: Loss=0.5656 (C:0.5656, R:0.0105)
Batch 100/356: Loss=0.5584 (C:0.5584, R:0.0105)
Batch 125/356: Loss=0.5342 (C:0.5342, R:0.0105)
Batch 150/356: Loss=0.5466 (C:0.5466, R:0.0105)
Batch 175/356: Loss=0.5577 (C:0.5577, R:0.0105)
Batch 200/356: Loss=0.5663 (C:0.5663, R:0.0105)
Batch 225/356: Loss=0.5556 (C:0.5556, R:0.0105)
Batch 250/356: Loss=0.5658 (C:0.5658, R:0.0105)
Batch 275/356: Loss=0.5603 (C:0.5603, R:0.0105)
Batch 300/356: Loss=0.5654 (C:0.5654, R:0.0105)
Batch 325/356: Loss=0.5509 (C:0.5509, R:0.0105)
Batch 350/356: Loss=0.5472 (C:0.5472, R:0.0105)

============================================================
Epoch 75/300 completed in 20.1s
Train: Loss=0.5500 (C:0.5500, R:0.0105) Ratio=5.13x
Val:   Loss=0.7691 (C:0.7691, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7691)
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.287 ± 0.541
    Neg distances: 2.684 ± 1.099
    Separation ratio: 9.34x
    Gap: -4.485
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/356: Loss=0.5653 (C:0.5653, R:0.0105)
Batch  25/356: Loss=0.5642 (C:0.5642, R:0.0105)
Batch  50/356: Loss=0.5290 (C:0.5290, R:0.0105)
Batch  75/356: Loss=0.5738 (C:0.5738, R:0.0105)
Batch 100/356: Loss=0.5402 (C:0.5402, R:0.0105)
Batch 125/356: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 150/356: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 175/356: Loss=0.5280 (C:0.5280, R:0.0105)
Batch 200/356: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 225/356: Loss=0.5221 (C:0.5221, R:0.0105)
Batch 250/356: Loss=0.5367 (C:0.5367, R:0.0105)
Batch 275/356: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 300/356: Loss=0.5530 (C:0.5530, R:0.0105)
Batch 325/356: Loss=0.5656 (C:0.5656, R:0.0105)
Batch 350/356: Loss=0.5510 (C:0.5510, R:0.0105)

============================================================
Epoch 76/300 completed in 26.1s
Train: Loss=0.5487 (C:0.5487, R:0.0105) Ratio=5.28x
Val:   Loss=0.7710 (C:0.7710, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/356: Loss=0.5397 (C:0.5397, R:0.0105)
Batch  25/356: Loss=0.5611 (C:0.5611, R:0.0105)
Batch  50/356: Loss=0.5069 (C:0.5069, R:0.0105)
Batch  75/356: Loss=0.5074 (C:0.5074, R:0.0105)
Batch 100/356: Loss=0.5227 (C:0.5227, R:0.0105)
Batch 125/356: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 150/356: Loss=0.5506 (C:0.5506, R:0.0105)
Batch 175/356: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 200/356: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 225/356: Loss=0.5767 (C:0.5767, R:0.0105)
Batch 250/356: Loss=0.5965 (C:0.5965, R:0.0105)
Batch 275/356: Loss=0.5947 (C:0.5947, R:0.0105)
Batch 300/356: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 325/356: Loss=0.5494 (C:0.5494, R:0.0105)
Batch 350/356: Loss=0.5609 (C:0.5609, R:0.0105)

============================================================
Epoch 77/300 completed in 20.7s
Train: Loss=0.5469 (C:0.5469, R:0.0105) Ratio=5.16x
Val:   Loss=0.7643 (C:0.7643, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7643)
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/356: Loss=0.5383 (C:0.5383, R:0.0105)
Batch  25/356: Loss=0.5558 (C:0.5558, R:0.0105)
Batch  50/356: Loss=0.5158 (C:0.5158, R:0.0105)
Batch  75/356: Loss=0.5212 (C:0.5212, R:0.0105)
Batch 100/356: Loss=0.5458 (C:0.5458, R:0.0105)
Batch 125/356: Loss=0.5256 (C:0.5256, R:0.0105)
Batch 150/356: Loss=0.5213 (C:0.5213, R:0.0105)
Batch 175/356: Loss=0.5284 (C:0.5284, R:0.0105)
Batch 200/356: Loss=0.5732 (C:0.5732, R:0.0105)
Batch 225/356: Loss=0.5438 (C:0.5438, R:0.0105)
Batch 250/356: Loss=0.5831 (C:0.5831, R:0.0105)
Batch 275/356: Loss=0.5405 (C:0.5405, R:0.0106)
Batch 300/356: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 325/356: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 350/356: Loss=0.5284 (C:0.5284, R:0.0106)

============================================================
Epoch 78/300 completed in 20.4s
Train: Loss=0.5467 (C:0.5467, R:0.0105) Ratio=5.41x
Val:   Loss=0.7778 (C:0.7778, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.278 ± 0.529
    Neg distances: 2.689 ± 1.088
    Separation ratio: 9.66x
    Gap: -4.524
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/356: Loss=0.5179 (C:0.5179, R:0.0105)
Batch  25/356: Loss=0.5459 (C:0.5459, R:0.0105)
Batch  50/356: Loss=0.5427 (C:0.5427, R:0.0105)
Batch  75/356: Loss=0.5257 (C:0.5257, R:0.0105)
Batch 100/356: Loss=0.5364 (C:0.5364, R:0.0105)
Batch 125/356: Loss=0.4996 (C:0.4996, R:0.0105)
Batch 150/356: Loss=0.5522 (C:0.5522, R:0.0105)
Batch 175/356: Loss=0.5443 (C:0.5443, R:0.0105)
Batch 200/356: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 225/356: Loss=0.5323 (C:0.5323, R:0.0105)
Batch 250/356: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 275/356: Loss=0.5230 (C:0.5230, R:0.0105)
Batch 300/356: Loss=0.5464 (C:0.5464, R:0.0105)
Batch 325/356: Loss=0.5611 (C:0.5611, R:0.0105)
Batch 350/356: Loss=0.5372 (C:0.5372, R:0.0105)

============================================================
Epoch 79/300 completed in 26.0s
Train: Loss=0.5391 (C:0.5391, R:0.0105) Ratio=5.21x
Val:   Loss=0.7733 (C:0.7733, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/356: Loss=0.5111 (C:0.5111, R:0.0105)
Batch  25/356: Loss=0.5133 (C:0.5133, R:0.0105)
Batch  50/356: Loss=0.5593 (C:0.5593, R:0.0105)
Batch  75/356: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 100/356: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 125/356: Loss=0.5751 (C:0.5751, R:0.0105)
Batch 150/356: Loss=0.5465 (C:0.5465, R:0.0105)
Batch 175/356: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 200/356: Loss=0.5247 (C:0.5247, R:0.0105)
Batch 225/356: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 250/356: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 275/356: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 300/356: Loss=0.5883 (C:0.5883, R:0.0105)
Batch 325/356: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 350/356: Loss=0.5350 (C:0.5350, R:0.0105)

============================================================
Epoch 80/300 completed in 20.3s
Train: Loss=0.5378 (C:0.5378, R:0.0105) Ratio=5.26x
Val:   Loss=0.7606 (C:0.7606, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7606)
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/356: Loss=0.5020 (C:0.5020, R:0.0105)
Batch  25/356: Loss=0.5060 (C:0.5060, R:0.0105)
Batch  50/356: Loss=0.5506 (C:0.5506, R:0.0105)
Batch  75/356: Loss=0.5345 (C:0.5345, R:0.0105)
Batch 100/356: Loss=0.5023 (C:0.5023, R:0.0106)
Batch 125/356: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 150/356: Loss=0.5467 (C:0.5467, R:0.0105)
Batch 175/356: Loss=0.5066 (C:0.5066, R:0.0105)
Batch 200/356: Loss=0.5406 (C:0.5406, R:0.0105)
Batch 225/356: Loss=0.5383 (C:0.5383, R:0.0105)
Batch 250/356: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 275/356: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 300/356: Loss=0.5233 (C:0.5233, R:0.0106)
Batch 325/356: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 350/356: Loss=0.5230 (C:0.5230, R:0.0105)

============================================================
Epoch 81/300 completed in 20.4s
Train: Loss=0.5342 (C:0.5342, R:0.0105) Ratio=5.40x
Val:   Loss=0.7735 (C:0.7735, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.288 ± 0.543
    Neg distances: 2.752 ± 1.124
    Separation ratio: 9.56x
    Gap: -4.547
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/356: Loss=0.5205 (C:0.5205, R:0.0105)
Batch  25/356: Loss=0.5423 (C:0.5423, R:0.0105)
Batch  50/356: Loss=0.4836 (C:0.4836, R:0.0105)
Batch  75/356: Loss=0.5263 (C:0.5263, R:0.0105)
Batch 100/356: Loss=0.5280 (C:0.5280, R:0.0105)
Batch 125/356: Loss=0.5336 (C:0.5336, R:0.0105)
Batch 150/356: Loss=0.5512 (C:0.5512, R:0.0105)
Batch 175/356: Loss=0.5287 (C:0.5287, R:0.0105)
Batch 200/356: Loss=0.5280 (C:0.5280, R:0.0105)
Batch 225/356: Loss=0.5581 (C:0.5581, R:0.0105)
Batch 250/356: Loss=0.5575 (C:0.5575, R:0.0106)
Batch 275/356: Loss=0.5633 (C:0.5633, R:0.0105)
Batch 300/356: Loss=0.5464 (C:0.5464, R:0.0105)
Batch 325/356: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 350/356: Loss=0.5424 (C:0.5424, R:0.0105)

============================================================
Epoch 82/300 completed in 26.7s
Train: Loss=0.5394 (C:0.5394, R:0.0105) Ratio=5.35x
Val:   Loss=0.7756 (C:0.7756, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/356: Loss=0.5180 (C:0.5180, R:0.0105)
Batch  25/356: Loss=0.5253 (C:0.5253, R:0.0105)
Batch  50/356: Loss=0.5590 (C:0.5590, R:0.0105)
Batch  75/356: Loss=0.5443 (C:0.5443, R:0.0105)
Batch 100/356: Loss=0.5204 (C:0.5204, R:0.0105)
Batch 125/356: Loss=0.5704 (C:0.5704, R:0.0105)
Batch 150/356: Loss=0.5160 (C:0.5160, R:0.0105)
Batch 175/356: Loss=0.5702 (C:0.5702, R:0.0105)
Batch 200/356: Loss=0.5360 (C:0.5360, R:0.0105)
Batch 225/356: Loss=0.5628 (C:0.5628, R:0.0105)
Batch 250/356: Loss=0.5596 (C:0.5596, R:0.0105)
Batch 275/356: Loss=0.5833 (C:0.5833, R:0.0105)
Batch 300/356: Loss=0.5536 (C:0.5536, R:0.0105)
Batch 325/356: Loss=0.5421 (C:0.5421, R:0.0105)
Batch 350/356: Loss=0.5465 (C:0.5465, R:0.0105)

============================================================
Epoch 83/300 completed in 20.8s
Train: Loss=0.5372 (C:0.5372, R:0.0105) Ratio=5.27x
Val:   Loss=0.7775 (C:0.7775, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/356: Loss=0.5202 (C:0.5202, R:0.0105)
Batch  25/356: Loss=0.5246 (C:0.5246, R:0.0105)
Batch  50/356: Loss=0.5246 (C:0.5246, R:0.0105)
Batch  75/356: Loss=0.4977 (C:0.4977, R:0.0105)
Batch 100/356: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 125/356: Loss=0.5252 (C:0.5252, R:0.0105)
Batch 150/356: Loss=0.5004 (C:0.5004, R:0.0105)
Batch 175/356: Loss=0.5476 (C:0.5476, R:0.0105)
Batch 200/356: Loss=0.5448 (C:0.5448, R:0.0105)
Batch 225/356: Loss=0.5449 (C:0.5449, R:0.0105)
Batch 250/356: Loss=0.5394 (C:0.5394, R:0.0105)
Batch 275/356: Loss=0.5597 (C:0.5597, R:0.0105)
Batch 300/356: Loss=0.5163 (C:0.5163, R:0.0105)
Batch 325/356: Loss=0.5567 (C:0.5567, R:0.0105)
Batch 350/356: Loss=0.5433 (C:0.5433, R:0.0105)

============================================================
Epoch 84/300 completed in 20.7s
Train: Loss=0.5381 (C:0.5381, R:0.0105) Ratio=5.42x
Val:   Loss=0.7775 (C:0.7775, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 85
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.286 ± 0.547
    Neg distances: 2.760 ± 1.122
    Separation ratio: 9.65x
    Gap: -4.622
    ✅ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/356: Loss=0.5418 (C:0.5418, R:0.0105)
Batch  25/356: Loss=0.5442 (C:0.5442, R:0.0105)
Batch  50/356: Loss=0.5672 (C:0.5672, R:0.0105)
Batch  75/356: Loss=0.5363 (C:0.5363, R:0.0105)
Batch 100/356: Loss=0.5647 (C:0.5647, R:0.0105)
Batch 125/356: Loss=0.5323 (C:0.5323, R:0.0105)
Batch 150/356: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 175/356: Loss=0.5419 (C:0.5419, R:0.0105)
Batch 200/356: Loss=0.5573 (C:0.5573, R:0.0105)
Batch 225/356: Loss=0.5588 (C:0.5588, R:0.0105)
Batch 250/356: Loss=0.5460 (C:0.5460, R:0.0105)
Batch 275/356: Loss=0.5341 (C:0.5341, R:0.0105)
Batch 300/356: Loss=0.5302 (C:0.5302, R:0.0105)
Batch 325/356: Loss=0.5269 (C:0.5269, R:0.0105)
Batch 350/356: Loss=0.5298 (C:0.5298, R:0.0105)

============================================================
Epoch 85/300 completed in 26.5s
Train: Loss=0.5324 (C:0.5324, R:0.0105) Ratio=5.26x
Val:   Loss=0.7756 (C:0.7756, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/356: Loss=0.5626 (C:0.5626, R:0.0105)
Batch  25/356: Loss=0.4970 (C:0.4970, R:0.0105)
Batch  50/356: Loss=0.5037 (C:0.5037, R:0.0105)
Batch  75/356: Loss=0.5329 (C:0.5329, R:0.0105)
Batch 100/356: Loss=0.5535 (C:0.5535, R:0.0105)
Batch 125/356: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 150/356: Loss=0.5673 (C:0.5673, R:0.0105)
Batch 175/356: Loss=0.5667 (C:0.5667, R:0.0105)
Batch 200/356: Loss=0.5280 (C:0.5280, R:0.0105)
Batch 225/356: Loss=0.5260 (C:0.5260, R:0.0105)
Batch 250/356: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 275/356: Loss=0.5392 (C:0.5392, R:0.0105)
Batch 300/356: Loss=0.5305 (C:0.5305, R:0.0105)
Batch 325/356: Loss=0.5086 (C:0.5086, R:0.0105)
Batch 350/356: Loss=0.5065 (C:0.5065, R:0.0105)

============================================================
Epoch 86/300 completed in 20.3s
Train: Loss=0.5318 (C:0.5318, R:0.0105) Ratio=5.42x
Val:   Loss=0.7764 (C:0.7764, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/356: Loss=0.5049 (C:0.5049, R:0.0105)
Batch  25/356: Loss=0.5034 (C:0.5034, R:0.0105)
Batch  50/356: Loss=0.5414 (C:0.5414, R:0.0105)
Batch  75/356: Loss=0.5501 (C:0.5501, R:0.0105)
Batch 100/356: Loss=0.5472 (C:0.5472, R:0.0105)
Batch 125/356: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 150/356: Loss=0.5269 (C:0.5269, R:0.0105)
Batch 175/356: Loss=0.5130 (C:0.5130, R:0.0105)
Batch 200/356: Loss=0.5041 (C:0.5041, R:0.0105)
Batch 225/356: Loss=0.5253 (C:0.5253, R:0.0105)
Batch 250/356: Loss=0.5249 (C:0.5249, R:0.0106)
Batch 275/356: Loss=0.5323 (C:0.5323, R:0.0105)
Batch 300/356: Loss=0.5269 (C:0.5269, R:0.0105)
Batch 325/356: Loss=0.5424 (C:0.5424, R:0.0105)
Batch 350/356: Loss=0.5326 (C:0.5326, R:0.0105)

============================================================
Epoch 87/300 completed in 20.8s
Train: Loss=0.5305 (C:0.5305, R:0.0105) Ratio=5.42x
Val:   Loss=0.7733 (C:0.7733, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

🌍 Updating global dataset at epoch 88
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.264 ± 0.521
    Neg distances: 2.735 ± 1.098
    Separation ratio: 10.36x
    Gap: -4.486
    ✅ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/356: Loss=0.5127 (C:0.5127, R:0.0105)
Batch  25/356: Loss=0.5256 (C:0.5256, R:0.0105)
Batch  50/356: Loss=0.5144 (C:0.5144, R:0.0105)
Batch  75/356: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 100/356: Loss=0.4900 (C:0.4900, R:0.0105)
Batch 125/356: Loss=0.4867 (C:0.4867, R:0.0105)
Batch 150/356: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 175/356: Loss=0.5117 (C:0.5117, R:0.0105)
Batch 200/356: Loss=0.5225 (C:0.5225, R:0.0105)
Batch 225/356: Loss=0.5497 (C:0.5497, R:0.0105)
Batch 250/356: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 275/356: Loss=0.5362 (C:0.5362, R:0.0105)
Batch 300/356: Loss=0.5082 (C:0.5082, R:0.0105)
Batch 325/356: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 350/356: Loss=0.5258 (C:0.5258, R:0.0105)

============================================================
Epoch 88/300 completed in 26.8s
Train: Loss=0.5174 (C:0.5174, R:0.0105) Ratio=5.41x
Val:   Loss=0.7673 (C:0.7673, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 88 epochs
Best model was at epoch 80 with Val Loss: 0.7606

Global Dataset Training Completed!
Best epoch: 80
Best validation loss: 0.7606
Final separation ratios: Train=5.41x, Val=3.02x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/7 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4601
  Adjusted Rand Score: 0.5340
  Clustering Accuracy: 0.8166
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
Extracted representations: torch.Size([546816, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/6 batches
Extracted representations: torch.Size([9216, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9216 samples
Classification Results:
  Accuracy: 0.8145
  Per-class F1: [0.8355065195586759, 0.7531941414770957, 0.8587533156498673]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.768 ± 0.900
  Negative distances: 2.341 ± 1.247
  Separation ratio: 3.05x
  Gap: -4.478
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4601
  Clustering Accuracy: 0.8166
  Adjusted Rand Score: 0.5340

Classification Performance:
  Accuracy: 0.8145

Separation Quality:
  Separation Ratio: 3.05x
  Gap: -4.478
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935/results/evaluation_results_20250714_183251.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935/results/evaluation_results_20250714_183251.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat75_bs1536_20250714_175935/final_results.json

Key Results:
  Separation ratio: 3.05x
  Perfect separation: False
  Classification accuracy: 0.8145
  Result: 0.8145% (improvement: +-80.86%)
  Cleaning up: coarse_lr1e-04_lat75_bs1536_20250714_175935

[5/12] Testing: coarse_lr1e-04_lat100_bs1020
  Learning rate: 0.0001
  Latent dim: 100
  Batch size: 1020
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 18:32:51.691149
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 100
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,889,380
Model created with 1,889,380 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,889,380
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.108 ± 0.011
    Neg distances: 0.108 ± 0.011
    Separation ratio: 1.00x
    Gap: -0.159
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9999 (C:1.9999, R:0.0115)
Batch  25/537: Loss=1.9945 (C:1.9945, R:0.0113)
Batch  50/537: Loss=1.9793 (C:1.9793, R:0.0112)
Batch  75/537: Loss=1.9754 (C:1.9754, R:0.0111)
Batch 100/537: Loss=1.9699 (C:1.9699, R:0.0109)
Batch 125/537: Loss=1.9598 (C:1.9598, R:0.0109)
Batch 150/537: Loss=1.9490 (C:1.9490, R:0.0108)
Batch 175/537: Loss=1.9409 (C:1.9409, R:0.0108)
Batch 200/537: Loss=1.9357 (C:1.9357, R:0.0107)
Batch 225/537: Loss=1.9272 (C:1.9272, R:0.0106)
Batch 250/537: Loss=1.9281 (C:1.9281, R:0.0106)
Batch 275/537: Loss=1.9154 (C:1.9154, R:0.0106)
Batch 300/537: Loss=1.9207 (C:1.9207, R:0.0106)
Batch 325/537: Loss=1.9169 (C:1.9169, R:0.0106)
Batch 350/537: Loss=1.9112 (C:1.9112, R:0.0105)
Batch 375/537: Loss=1.9035 (C:1.9035, R:0.0106)
Batch 400/537: Loss=1.9045 (C:1.9045, R:0.0105)
Batch 425/537: Loss=1.8980 (C:1.8980, R:0.0106)
Batch 450/537: Loss=1.9007 (C:1.9007, R:0.0105)
Batch 475/537: Loss=1.9089 (C:1.9089, R:0.0105)
Batch 500/537: Loss=1.9155 (C:1.9155, R:0.0105)
Batch 525/537: Loss=1.8993 (C:1.8993, R:0.0105)

============================================================
Epoch 1/300 completed in 27.4s
Train: Loss=1.9330 (C:1.9330, R:0.0107) Ratio=1.66x
Val:   Loss=1.8970 (C:1.8970, R:0.0104) Ratio=2.13x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8970)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.9009 (C:1.9009, R:0.0106)
Batch  25/537: Loss=1.8960 (C:1.8960, R:0.0105)
Batch  50/537: Loss=1.8913 (C:1.8913, R:0.0105)
Batch  75/537: Loss=1.8946 (C:1.8946, R:0.0105)
Batch 100/537: Loss=1.8965 (C:1.8965, R:0.0105)
Batch 125/537: Loss=1.9083 (C:1.9083, R:0.0105)
Batch 150/537: Loss=1.8980 (C:1.8980, R:0.0105)
Batch 175/537: Loss=1.9007 (C:1.9007, R:0.0105)
Batch 200/537: Loss=1.8914 (C:1.8914, R:0.0105)
Batch 225/537: Loss=1.9016 (C:1.9016, R:0.0105)
Batch 250/537: Loss=1.8906 (C:1.8906, R:0.0105)
Batch 275/537: Loss=1.8813 (C:1.8813, R:0.0105)
Batch 300/537: Loss=1.8974 (C:1.8974, R:0.0105)
Batch 325/537: Loss=1.8905 (C:1.8905, R:0.0105)
Batch 350/537: Loss=1.8964 (C:1.8964, R:0.0105)
Batch 375/537: Loss=1.8975 (C:1.8975, R:0.0105)
Batch 400/537: Loss=1.8889 (C:1.8889, R:0.0105)
Batch 425/537: Loss=1.8901 (C:1.8901, R:0.0105)
Batch 450/537: Loss=1.8976 (C:1.8976, R:0.0105)
Batch 475/537: Loss=1.8957 (C:1.8957, R:0.0105)
Batch 500/537: Loss=1.8854 (C:1.8854, R:0.0105)
Batch 525/537: Loss=1.8675 (C:1.8675, R:0.0105)

============================================================
Epoch 2/300 completed in 21.3s
Train: Loss=1.8925 (C:1.8925, R:0.0105) Ratio=2.18x
Val:   Loss=1.8840 (C:1.8840, R:0.0104) Ratio=2.36x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8840)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8916 (C:1.8916, R:0.0105)
Batch  25/537: Loss=1.8841 (C:1.8841, R:0.0105)
Batch  50/537: Loss=1.8683 (C:1.8683, R:0.0105)
Batch  75/537: Loss=1.8933 (C:1.8933, R:0.0105)
Batch 100/537: Loss=1.8834 (C:1.8834, R:0.0105)
Batch 125/537: Loss=1.8890 (C:1.8890, R:0.0105)
Batch 150/537: Loss=1.8854 (C:1.8854, R:0.0105)
Batch 175/537: Loss=1.8861 (C:1.8861, R:0.0105)
Batch 200/537: Loss=1.8853 (C:1.8853, R:0.0105)
Batch 225/537: Loss=1.8766 (C:1.8766, R:0.0105)
Batch 250/537: Loss=1.8798 (C:1.8798, R:0.0105)
Batch 275/537: Loss=1.8907 (C:1.8907, R:0.0106)
Batch 300/537: Loss=1.8813 (C:1.8813, R:0.0105)
Batch 325/537: Loss=1.8819 (C:1.8819, R:0.0105)
Batch 350/537: Loss=1.8899 (C:1.8899, R:0.0105)
Batch 375/537: Loss=1.8790 (C:1.8790, R:0.0105)
Batch 400/537: Loss=1.8744 (C:1.8744, R:0.0105)
Batch 425/537: Loss=1.8882 (C:1.8882, R:0.0105)
Batch 450/537: Loss=1.8722 (C:1.8722, R:0.0105)
Batch 475/537: Loss=1.8874 (C:1.8874, R:0.0105)
Batch 500/537: Loss=1.8787 (C:1.8787, R:0.0105)
Batch 525/537: Loss=1.8885 (C:1.8885, R:0.0105)

============================================================
Epoch 3/300 completed in 21.4s
Train: Loss=1.8815 (C:1.8815, R:0.0105) Ratio=2.37x
Val:   Loss=1.8796 (C:1.8796, R:0.0104) Ratio=2.47x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8796)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.571 ± 0.571
    Neg distances: 1.515 ± 0.840
    Separation ratio: 2.65x
    Gap: -3.291
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.2261 (C:1.2261, R:0.0105)
Batch  25/537: Loss=1.2268 (C:1.2268, R:0.0105)
Batch  50/537: Loss=1.2145 (C:1.2145, R:0.0105)
Batch  75/537: Loss=1.2312 (C:1.2312, R:0.0105)
Batch 100/537: Loss=1.2840 (C:1.2840, R:0.0105)
Batch 125/537: Loss=1.2186 (C:1.2186, R:0.0105)
Batch 150/537: Loss=1.2180 (C:1.2180, R:0.0105)
Batch 175/537: Loss=1.2104 (C:1.2104, R:0.0105)
Batch 200/537: Loss=1.1807 (C:1.1807, R:0.0105)
Batch 225/537: Loss=1.2542 (C:1.2542, R:0.0105)
Batch 250/537: Loss=1.2522 (C:1.2522, R:0.0105)
Batch 275/537: Loss=1.2131 (C:1.2131, R:0.0105)
Batch 300/537: Loss=1.2274 (C:1.2274, R:0.0105)
Batch 325/537: Loss=1.2161 (C:1.2161, R:0.0105)
Batch 350/537: Loss=1.1997 (C:1.1997, R:0.0105)
Batch 375/537: Loss=1.2309 (C:1.2309, R:0.0105)
Batch 400/537: Loss=1.2394 (C:1.2394, R:0.0105)
Batch 425/537: Loss=1.2150 (C:1.2150, R:0.0105)
Batch 450/537: Loss=1.2357 (C:1.2357, R:0.0105)
Batch 475/537: Loss=1.1992 (C:1.1992, R:0.0105)
Batch 500/537: Loss=1.2148 (C:1.2148, R:0.0105)
Batch 525/537: Loss=1.2066 (C:1.2066, R:0.0105)

============================================================
Epoch 4/300 completed in 28.0s
Train: Loss=1.2296 (C:1.2296, R:0.0105) Ratio=2.51x
Val:   Loss=1.2256 (C:1.2256, R:0.0104) Ratio=2.57x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2256)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.2570 (C:1.2570, R:0.0105)
Batch  25/537: Loss=1.2028 (C:1.2028, R:0.0105)
Batch  50/537: Loss=1.2045 (C:1.2045, R:0.0105)
Batch  75/537: Loss=1.2053 (C:1.2053, R:0.0105)
Batch 100/537: Loss=1.2178 (C:1.2178, R:0.0106)
Batch 125/537: Loss=1.2332 (C:1.2332, R:0.0105)
Batch 150/537: Loss=1.1886 (C:1.1886, R:0.0105)
Batch 175/537: Loss=1.2121 (C:1.2121, R:0.0105)
Batch 200/537: Loss=1.1995 (C:1.1995, R:0.0105)
Batch 225/537: Loss=1.1603 (C:1.1603, R:0.0105)
Batch 250/537: Loss=1.2103 (C:1.2103, R:0.0105)
Batch 275/537: Loss=1.2372 (C:1.2372, R:0.0105)
Batch 300/537: Loss=1.2284 (C:1.2284, R:0.0105)
Batch 325/537: Loss=1.2057 (C:1.2057, R:0.0105)
Batch 350/537: Loss=1.1980 (C:1.1980, R:0.0105)
Batch 375/537: Loss=1.2042 (C:1.2042, R:0.0105)
Batch 400/537: Loss=1.2176 (C:1.2176, R:0.0105)
Batch 425/537: Loss=1.1875 (C:1.1875, R:0.0105)
Batch 450/537: Loss=1.2173 (C:1.2173, R:0.0106)
Batch 475/537: Loss=1.2072 (C:1.2072, R:0.0105)
Batch 500/537: Loss=1.2257 (C:1.2257, R:0.0105)
Batch 525/537: Loss=1.2105 (C:1.2105, R:0.0105)

============================================================
Epoch 5/300 completed in 21.4s
Train: Loss=1.2081 (C:1.2081, R:0.0105) Ratio=2.66x
Val:   Loss=1.2020 (C:1.2020, R:0.0104) Ratio=2.67x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2020)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.1709 (C:1.1709, R:0.0105)
Batch  25/537: Loss=1.1673 (C:1.1673, R:0.0105)
Batch  50/537: Loss=1.1653 (C:1.1653, R:0.0105)
Batch  75/537: Loss=1.1697 (C:1.1697, R:0.0105)
Batch 100/537: Loss=1.1560 (C:1.1560, R:0.0105)
Batch 125/537: Loss=1.1780 (C:1.1780, R:0.0105)
Batch 150/537: Loss=1.1989 (C:1.1989, R:0.0105)
Batch 175/537: Loss=1.2299 (C:1.2299, R:0.0105)
Batch 200/537: Loss=1.1715 (C:1.1715, R:0.0105)
Batch 225/537: Loss=1.2146 (C:1.2146, R:0.0105)
Batch 250/537: Loss=1.1741 (C:1.1741, R:0.0105)
Batch 275/537: Loss=1.1700 (C:1.1700, R:0.0105)
Batch 300/537: Loss=1.1549 (C:1.1549, R:0.0105)
Batch 325/537: Loss=1.1668 (C:1.1668, R:0.0105)
Batch 350/537: Loss=1.1921 (C:1.1921, R:0.0105)
Batch 375/537: Loss=1.2055 (C:1.2055, R:0.0105)
Batch 400/537: Loss=1.2114 (C:1.2114, R:0.0105)
Batch 425/537: Loss=1.2169 (C:1.2169, R:0.0105)
Batch 450/537: Loss=1.2000 (C:1.2000, R:0.0105)
Batch 475/537: Loss=1.1751 (C:1.1751, R:0.0105)
Batch 500/537: Loss=1.1896 (C:1.1896, R:0.0105)
Batch 525/537: Loss=1.2054 (C:1.2054, R:0.0105)

============================================================
Epoch 6/300 completed in 21.2s
Train: Loss=1.1935 (C:1.1935, R:0.0105) Ratio=2.86x
Val:   Loss=1.2077 (C:1.2077, R:0.0104) Ratio=2.73x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.487 ± 0.557
    Neg distances: 1.642 ± 0.857
    Separation ratio: 3.37x
    Gap: -3.181
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.0916 (C:1.0916, R:0.0105)
Batch  25/537: Loss=1.0790 (C:1.0790, R:0.0105)
Batch  50/537: Loss=1.0833 (C:1.0833, R:0.0105)
Batch  75/537: Loss=1.0822 (C:1.0822, R:0.0105)
Batch 100/537: Loss=1.1048 (C:1.1048, R:0.0105)
Batch 125/537: Loss=1.0670 (C:1.0670, R:0.0105)
Batch 150/537: Loss=1.1406 (C:1.1406, R:0.0105)
Batch 175/537: Loss=1.0995 (C:1.0995, R:0.0105)
Batch 200/537: Loss=1.0993 (C:1.0993, R:0.0105)
Batch 225/537: Loss=1.0815 (C:1.0815, R:0.0105)
Batch 250/537: Loss=1.0887 (C:1.0887, R:0.0105)
Batch 275/537: Loss=1.0965 (C:1.0965, R:0.0105)
Batch 300/537: Loss=1.0861 (C:1.0861, R:0.0105)
Batch 325/537: Loss=1.0896 (C:1.0896, R:0.0105)
Batch 350/537: Loss=1.0844 (C:1.0844, R:0.0105)
Batch 375/537: Loss=1.0782 (C:1.0782, R:0.0105)
Batch 400/537: Loss=1.0979 (C:1.0979, R:0.0105)
Batch 425/537: Loss=1.0861 (C:1.0861, R:0.0105)
Batch 450/537: Loss=1.0647 (C:1.0647, R:0.0105)
Batch 475/537: Loss=1.0796 (C:1.0796, R:0.0105)
Batch 500/537: Loss=1.0940 (C:1.0940, R:0.0105)
Batch 525/537: Loss=1.0785 (C:1.0785, R:0.0105)

============================================================
Epoch 7/300 completed in 27.3s
Train: Loss=1.0895 (C:1.0895, R:0.0105) Ratio=2.95x
Val:   Loss=1.1102 (C:1.1102, R:0.0104) Ratio=2.79x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1102)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.0761 (C:1.0761, R:0.0105)
Batch  25/537: Loss=1.0732 (C:1.0732, R:0.0105)
Batch  50/537: Loss=1.0352 (C:1.0352, R:0.0105)
Batch  75/537: Loss=1.0848 (C:1.0848, R:0.0105)
Batch 100/537: Loss=1.0890 (C:1.0890, R:0.0105)
Batch 125/537: Loss=1.0745 (C:1.0745, R:0.0105)
Batch 150/537: Loss=1.0580 (C:1.0580, R:0.0105)
Batch 175/537: Loss=1.0656 (C:1.0656, R:0.0105)
Batch 200/537: Loss=1.0858 (C:1.0858, R:0.0105)
Batch 225/537: Loss=1.0769 (C:1.0769, R:0.0105)
Batch 250/537: Loss=1.0699 (C:1.0699, R:0.0105)
Batch 275/537: Loss=1.0865 (C:1.0865, R:0.0105)
Batch 300/537: Loss=1.0643 (C:1.0643, R:0.0105)
Batch 325/537: Loss=1.1135 (C:1.1135, R:0.0105)
Batch 350/537: Loss=1.0508 (C:1.0508, R:0.0105)
Batch 375/537: Loss=1.1015 (C:1.1015, R:0.0105)
Batch 400/537: Loss=1.0889 (C:1.0889, R:0.0105)
Batch 425/537: Loss=1.0821 (C:1.0821, R:0.0105)
Batch 450/537: Loss=1.0716 (C:1.0716, R:0.0105)
Batch 475/537: Loss=1.0954 (C:1.0954, R:0.0105)
Batch 500/537: Loss=1.0526 (C:1.0526, R:0.0105)
Batch 525/537: Loss=1.0773 (C:1.0773, R:0.0106)

============================================================
Epoch 8/300 completed in 21.6s
Train: Loss=1.0793 (C:1.0793, R:0.0105) Ratio=3.09x
Val:   Loss=1.1161 (C:1.1161, R:0.0104) Ratio=2.84x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0790 (C:1.0790, R:0.0105)
Batch  25/537: Loss=1.0826 (C:1.0826, R:0.0105)
Batch  50/537: Loss=1.0819 (C:1.0819, R:0.0105)
Batch  75/537: Loss=1.0227 (C:1.0227, R:0.0105)
Batch 100/537: Loss=1.0433 (C:1.0433, R:0.0105)
Batch 125/537: Loss=1.0619 (C:1.0619, R:0.0105)
Batch 150/537: Loss=1.0437 (C:1.0437, R:0.0105)
Batch 175/537: Loss=1.0326 (C:1.0326, R:0.0105)
Batch 200/537: Loss=1.0819 (C:1.0819, R:0.0105)
Batch 225/537: Loss=1.0586 (C:1.0586, R:0.0105)
Batch 250/537: Loss=1.0801 (C:1.0801, R:0.0105)
Batch 275/537: Loss=1.0851 (C:1.0851, R:0.0105)
Batch 300/537: Loss=1.1033 (C:1.1033, R:0.0105)
Batch 325/537: Loss=1.0650 (C:1.0650, R:0.0105)
Batch 350/537: Loss=1.0630 (C:1.0630, R:0.0105)
Batch 375/537: Loss=1.0873 (C:1.0873, R:0.0105)
Batch 400/537: Loss=1.0754 (C:1.0754, R:0.0105)
Batch 425/537: Loss=1.0795 (C:1.0795, R:0.0105)
Batch 450/537: Loss=1.0865 (C:1.0865, R:0.0105)
Batch 475/537: Loss=1.0982 (C:1.0982, R:0.0105)
Batch 500/537: Loss=1.0756 (C:1.0756, R:0.0105)
Batch 525/537: Loss=1.0589 (C:1.0589, R:0.0105)

============================================================
Epoch 9/300 completed in 21.3s
Train: Loss=1.0710 (C:1.0710, R:0.0105) Ratio=3.15x
Val:   Loss=1.1014 (C:1.1014, R:0.0104) Ratio=2.86x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1014)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.457 ± 0.561
    Neg distances: 1.698 ± 0.857
    Separation ratio: 3.71x
    Gap: -3.145
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=0.9984 (C:0.9984, R:0.0105)
Batch  25/537: Loss=1.0207 (C:1.0207, R:0.0105)
Batch  50/537: Loss=1.0182 (C:1.0182, R:0.0105)
Batch  75/537: Loss=1.0402 (C:1.0402, R:0.0105)
Batch 100/537: Loss=1.0072 (C:1.0072, R:0.0105)
Batch 125/537: Loss=1.0270 (C:1.0270, R:0.0105)
Batch 150/537: Loss=1.0117 (C:1.0117, R:0.0105)
Batch 175/537: Loss=1.0251 (C:1.0251, R:0.0105)
Batch 200/537: Loss=1.0547 (C:1.0547, R:0.0105)
Batch 225/537: Loss=1.0725 (C:1.0725, R:0.0105)
Batch 250/537: Loss=1.0290 (C:1.0290, R:0.0105)
Batch 275/537: Loss=1.0277 (C:1.0277, R:0.0105)
Batch 300/537: Loss=1.0518 (C:1.0518, R:0.0106)
Batch 325/537: Loss=1.0656 (C:1.0656, R:0.0105)
Batch 350/537: Loss=1.0120 (C:1.0120, R:0.0105)
Batch 375/537: Loss=1.0275 (C:1.0275, R:0.0105)
Batch 400/537: Loss=1.0440 (C:1.0440, R:0.0106)
Batch 425/537: Loss=1.0429 (C:1.0429, R:0.0105)
Batch 450/537: Loss=1.0161 (C:1.0161, R:0.0105)
Batch 475/537: Loss=1.0496 (C:1.0496, R:0.0105)
Batch 500/537: Loss=1.0618 (C:1.0618, R:0.0105)
Batch 525/537: Loss=1.0514 (C:1.0514, R:0.0105)

============================================================
Epoch 10/300 completed in 26.9s
Train: Loss=1.0287 (C:1.0287, R:0.0105) Ratio=3.23x
Val:   Loss=1.0785 (C:1.0785, R:0.0104) Ratio=2.85x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0785)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=0.9955 (C:0.9955, R:0.0105)
Batch  25/537: Loss=1.0363 (C:1.0363, R:0.0105)
Batch  50/537: Loss=1.0203 (C:1.0203, R:0.0105)
Batch  75/537: Loss=1.0216 (C:1.0216, R:0.0105)
Batch 100/537: Loss=0.9901 (C:0.9901, R:0.0106)
Batch 125/537: Loss=1.0141 (C:1.0141, R:0.0105)
Batch 150/537: Loss=1.0242 (C:1.0242, R:0.0105)
Batch 175/537: Loss=1.0439 (C:1.0439, R:0.0105)
Batch 200/537: Loss=1.0119 (C:1.0119, R:0.0105)
Batch 225/537: Loss=1.0123 (C:1.0123, R:0.0105)
Batch 250/537: Loss=1.0199 (C:1.0199, R:0.0105)
Batch 275/537: Loss=1.0225 (C:1.0225, R:0.0105)
Batch 300/537: Loss=0.9877 (C:0.9877, R:0.0105)
Batch 325/537: Loss=1.0111 (C:1.0111, R:0.0105)
Batch 350/537: Loss=1.0180 (C:1.0180, R:0.0105)
Batch 375/537: Loss=0.9996 (C:0.9996, R:0.0105)
Batch 400/537: Loss=0.9705 (C:0.9705, R:0.0105)
Batch 425/537: Loss=0.9968 (C:0.9968, R:0.0105)
Batch 450/537: Loss=1.0312 (C:1.0312, R:0.0105)
Batch 475/537: Loss=1.0782 (C:1.0782, R:0.0105)
Batch 500/537: Loss=1.0264 (C:1.0264, R:0.0105)
Batch 525/537: Loss=1.0346 (C:1.0346, R:0.0105)

============================================================
Epoch 11/300 completed in 21.2s
Train: Loss=1.0217 (C:1.0217, R:0.0105) Ratio=3.31x
Val:   Loss=1.0606 (C:1.0606, R:0.0104) Ratio=2.90x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0606)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.0046 (C:1.0046, R:0.0105)
Batch  25/537: Loss=1.0112 (C:1.0112, R:0.0105)
Batch  50/537: Loss=1.0232 (C:1.0232, R:0.0105)
Batch  75/537: Loss=1.0329 (C:1.0329, R:0.0105)
Batch 100/537: Loss=1.0318 (C:1.0318, R:0.0105)
Batch 125/537: Loss=1.0176 (C:1.0176, R:0.0105)
Batch 150/537: Loss=0.9988 (C:0.9988, R:0.0105)
Batch 175/537: Loss=1.0274 (C:1.0274, R:0.0105)
Batch 200/537: Loss=1.0124 (C:1.0124, R:0.0105)
Batch 225/537: Loss=0.9999 (C:0.9999, R:0.0105)
Batch 250/537: Loss=1.0027 (C:1.0027, R:0.0106)
Batch 275/537: Loss=0.9960 (C:0.9960, R:0.0105)
Batch 300/537: Loss=1.0173 (C:1.0173, R:0.0105)
Batch 325/537: Loss=1.0516 (C:1.0516, R:0.0105)
Batch 350/537: Loss=1.0055 (C:1.0055, R:0.0105)
Batch 375/537: Loss=0.9965 (C:0.9965, R:0.0105)
Batch 400/537: Loss=0.9952 (C:0.9952, R:0.0105)
Batch 425/537: Loss=0.9974 (C:0.9974, R:0.0105)
Batch 450/537: Loss=1.0109 (C:1.0109, R:0.0106)
Batch 475/537: Loss=1.0324 (C:1.0324, R:0.0105)
Batch 500/537: Loss=1.0216 (C:1.0216, R:0.0105)
Batch 525/537: Loss=1.0086 (C:1.0086, R:0.0105)

============================================================
Epoch 12/300 completed in 21.4s
Train: Loss=1.0153 (C:1.0153, R:0.0105) Ratio=3.39x
Val:   Loss=1.0619 (C:1.0619, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.416 ± 0.549
    Neg distances: 1.799 ± 0.869
    Separation ratio: 4.33x
    Gap: -3.136
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9479 (C:0.9479, R:0.0105)
Batch  25/537: Loss=0.9367 (C:0.9367, R:0.0105)
Batch  50/537: Loss=0.9611 (C:0.9611, R:0.0105)
Batch  75/537: Loss=0.9735 (C:0.9735, R:0.0105)
Batch 100/537: Loss=0.9419 (C:0.9419, R:0.0105)
Batch 125/537: Loss=0.9534 (C:0.9534, R:0.0105)
Batch 150/537: Loss=0.9726 (C:0.9726, R:0.0105)
Batch 175/537: Loss=0.9649 (C:0.9649, R:0.0105)
Batch 200/537: Loss=0.9259 (C:0.9259, R:0.0105)
Batch 225/537: Loss=0.9306 (C:0.9306, R:0.0105)
Batch 250/537: Loss=0.9309 (C:0.9309, R:0.0105)
Batch 275/537: Loss=0.9424 (C:0.9424, R:0.0105)
Batch 300/537: Loss=0.9505 (C:0.9505, R:0.0105)
Batch 325/537: Loss=0.9520 (C:0.9520, R:0.0105)
Batch 350/537: Loss=0.9618 (C:0.9618, R:0.0105)
Batch 375/537: Loss=0.9755 (C:0.9755, R:0.0105)
Batch 400/537: Loss=0.9533 (C:0.9533, R:0.0105)
Batch 425/537: Loss=0.9616 (C:0.9616, R:0.0105)
Batch 450/537: Loss=0.9442 (C:0.9442, R:0.0105)
Batch 475/537: Loss=0.9655 (C:0.9655, R:0.0105)
Batch 500/537: Loss=1.0079 (C:1.0079, R:0.0105)
Batch 525/537: Loss=0.9342 (C:0.9342, R:0.0106)

============================================================
Epoch 13/300 completed in 27.1s
Train: Loss=0.9534 (C:0.9534, R:0.0105) Ratio=3.39x
Val:   Loss=1.0049 (C:1.0049, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0049)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9598 (C:0.9598, R:0.0105)
Batch  25/537: Loss=0.9338 (C:0.9338, R:0.0105)
Batch  50/537: Loss=0.9429 (C:0.9429, R:0.0105)
Batch  75/537: Loss=0.9613 (C:0.9613, R:0.0105)
Batch 100/537: Loss=0.9786 (C:0.9786, R:0.0105)
Batch 125/537: Loss=0.9165 (C:0.9165, R:0.0105)
Batch 150/537: Loss=0.9442 (C:0.9442, R:0.0105)
Batch 175/537: Loss=0.9624 (C:0.9624, R:0.0105)
Batch 200/537: Loss=0.9519 (C:0.9519, R:0.0105)
Batch 225/537: Loss=0.9716 (C:0.9716, R:0.0105)
Batch 250/537: Loss=0.9440 (C:0.9440, R:0.0105)
Batch 275/537: Loss=0.9403 (C:0.9403, R:0.0105)
Batch 300/537: Loss=0.9395 (C:0.9395, R:0.0105)
Batch 325/537: Loss=0.9532 (C:0.9532, R:0.0105)
Batch 350/537: Loss=0.9459 (C:0.9459, R:0.0105)
Batch 375/537: Loss=0.9745 (C:0.9745, R:0.0105)
Batch 400/537: Loss=0.9803 (C:0.9803, R:0.0105)
Batch 425/537: Loss=0.9338 (C:0.9338, R:0.0105)
Batch 450/537: Loss=0.9348 (C:0.9348, R:0.0105)
Batch 475/537: Loss=0.9574 (C:0.9574, R:0.0105)
Batch 500/537: Loss=0.9767 (C:0.9767, R:0.0105)
Batch 525/537: Loss=0.9588 (C:0.9588, R:0.0105)

============================================================
Epoch 14/300 completed in 21.5s
Train: Loss=0.9491 (C:0.9491, R:0.0105) Ratio=3.49x
Val:   Loss=1.0053 (C:1.0053, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.9224 (C:0.9224, R:0.0105)
Batch  25/537: Loss=0.9429 (C:0.9429, R:0.0105)
Batch  50/537: Loss=0.9448 (C:0.9448, R:0.0105)
Batch  75/537: Loss=0.9337 (C:0.9337, R:0.0105)
Batch 100/537: Loss=0.9761 (C:0.9761, R:0.0105)
Batch 125/537: Loss=0.9357 (C:0.9357, R:0.0105)
Batch 150/537: Loss=0.9509 (C:0.9509, R:0.0105)
Batch 175/537: Loss=0.9414 (C:0.9414, R:0.0105)
Batch 200/537: Loss=0.9355 (C:0.9355, R:0.0105)
Batch 225/537: Loss=0.9461 (C:0.9461, R:0.0105)
Batch 250/537: Loss=0.9417 (C:0.9417, R:0.0105)
Batch 275/537: Loss=0.9233 (C:0.9233, R:0.0106)
Batch 300/537: Loss=0.9089 (C:0.9089, R:0.0105)
Batch 325/537: Loss=0.9550 (C:0.9550, R:0.0105)
Batch 350/537: Loss=0.9384 (C:0.9384, R:0.0105)
Batch 375/537: Loss=0.9502 (C:0.9502, R:0.0105)
Batch 400/537: Loss=0.9476 (C:0.9476, R:0.0105)
Batch 425/537: Loss=0.9229 (C:0.9229, R:0.0105)
Batch 450/537: Loss=0.9455 (C:0.9455, R:0.0105)
Batch 475/537: Loss=0.9573 (C:0.9573, R:0.0105)
Batch 500/537: Loss=0.9681 (C:0.9681, R:0.0105)
Batch 525/537: Loss=0.9454 (C:0.9454, R:0.0105)

============================================================
Epoch 15/300 completed in 21.8s
Train: Loss=0.9439 (C:0.9439, R:0.0105) Ratio=3.56x
Val:   Loss=1.0006 (C:1.0006, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0006)
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.428 ± 0.566
    Neg distances: 1.832 ± 0.881
    Separation ratio: 4.28x
    Gap: -3.209
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.9605 (C:0.9605, R:0.0105)
Batch  25/537: Loss=0.9508 (C:0.9508, R:0.0105)
Batch  50/537: Loss=0.9268 (C:0.9268, R:0.0105)
Batch  75/537: Loss=0.9299 (C:0.9299, R:0.0105)
Batch 100/537: Loss=0.9579 (C:0.9579, R:0.0105)
Batch 125/537: Loss=0.9385 (C:0.9385, R:0.0105)
Batch 150/537: Loss=0.9207 (C:0.9207, R:0.0105)
Batch 175/537: Loss=0.9309 (C:0.9309, R:0.0105)
Batch 200/537: Loss=0.9480 (C:0.9480, R:0.0105)
Batch 225/537: Loss=0.9293 (C:0.9293, R:0.0105)
Batch 250/537: Loss=0.9654 (C:0.9654, R:0.0105)
Batch 275/537: Loss=0.9523 (C:0.9523, R:0.0105)
Batch 300/537: Loss=0.9568 (C:0.9568, R:0.0105)
Batch 325/537: Loss=0.9270 (C:0.9270, R:0.0105)
Batch 350/537: Loss=0.9261 (C:0.9261, R:0.0105)
Batch 375/537: Loss=0.9466 (C:0.9466, R:0.0105)
Batch 400/537: Loss=0.9253 (C:0.9253, R:0.0105)
Batch 425/537: Loss=0.9484 (C:0.9484, R:0.0105)
Batch 450/537: Loss=0.9313 (C:0.9313, R:0.0105)
Batch 475/537: Loss=0.9641 (C:0.9641, R:0.0105)
Batch 500/537: Loss=0.9078 (C:0.9078, R:0.0105)
Batch 525/537: Loss=0.9501 (C:0.9501, R:0.0105)

============================================================
Epoch 16/300 completed in 27.5s
Train: Loss=0.9355 (C:0.9355, R:0.0105) Ratio=3.60x
Val:   Loss=0.9971 (C:0.9971, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9971)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.9100 (C:0.9100, R:0.0105)
Batch  25/537: Loss=0.9235 (C:0.9235, R:0.0105)
Batch  50/537: Loss=0.9365 (C:0.9365, R:0.0105)
Batch  75/537: Loss=0.9301 (C:0.9301, R:0.0105)
Batch 100/537: Loss=0.9047 (C:0.9047, R:0.0106)
Batch 125/537: Loss=0.9157 (C:0.9157, R:0.0105)
Batch 150/537: Loss=0.9502 (C:0.9502, R:0.0105)
Batch 175/537: Loss=0.9228 (C:0.9228, R:0.0106)
Batch 200/537: Loss=0.9496 (C:0.9496, R:0.0105)
Batch 225/537: Loss=0.9183 (C:0.9183, R:0.0105)
Batch 250/537: Loss=0.9411 (C:0.9411, R:0.0105)
Batch 275/537: Loss=0.9056 (C:0.9056, R:0.0105)
Batch 300/537: Loss=0.9250 (C:0.9250, R:0.0106)
Batch 325/537: Loss=0.9093 (C:0.9093, R:0.0105)
Batch 350/537: Loss=0.9522 (C:0.9522, R:0.0105)
Batch 375/537: Loss=0.9533 (C:0.9533, R:0.0105)
Batch 400/537: Loss=0.9253 (C:0.9253, R:0.0105)
Batch 425/537: Loss=0.9508 (C:0.9508, R:0.0105)
Batch 450/537: Loss=0.9002 (C:0.9002, R:0.0105)
Batch 475/537: Loss=0.9374 (C:0.9374, R:0.0105)
Batch 500/537: Loss=0.9513 (C:0.9513, R:0.0105)
Batch 525/537: Loss=0.9445 (C:0.9445, R:0.0106)

============================================================
Epoch 17/300 completed in 21.2s
Train: Loss=0.9324 (C:0.9324, R:0.0105) Ratio=3.67x
Val:   Loss=0.9954 (C:0.9954, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9954)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.9380 (C:0.9380, R:0.0105)
Batch  25/537: Loss=0.9186 (C:0.9186, R:0.0105)
Batch  50/537: Loss=0.9140 (C:0.9140, R:0.0105)
Batch  75/537: Loss=0.9293 (C:0.9293, R:0.0105)
Batch 100/537: Loss=0.9360 (C:0.9360, R:0.0105)
Batch 125/537: Loss=0.9396 (C:0.9396, R:0.0105)
Batch 150/537: Loss=0.9516 (C:0.9516, R:0.0105)
Batch 175/537: Loss=0.8893 (C:0.8893, R:0.0105)
Batch 200/537: Loss=0.9301 (C:0.9301, R:0.0105)
Batch 225/537: Loss=0.9277 (C:0.9277, R:0.0105)
Batch 250/537: Loss=0.9394 (C:0.9394, R:0.0105)
Batch 275/537: Loss=0.9172 (C:0.9172, R:0.0105)
Batch 300/537: Loss=0.9587 (C:0.9587, R:0.0105)
Batch 325/537: Loss=0.9469 (C:0.9469, R:0.0105)
Batch 350/537: Loss=0.9641 (C:0.9641, R:0.0105)
Batch 375/537: Loss=0.9379 (C:0.9379, R:0.0105)
Batch 400/537: Loss=0.9177 (C:0.9177, R:0.0105)
Batch 425/537: Loss=0.9764 (C:0.9764, R:0.0105)
Batch 450/537: Loss=0.9527 (C:0.9527, R:0.0105)
Batch 475/537: Loss=0.9311 (C:0.9311, R:0.0105)
Batch 500/537: Loss=0.9577 (C:0.9577, R:0.0105)
Batch 525/537: Loss=0.9071 (C:0.9071, R:0.0105)

============================================================
Epoch 18/300 completed in 21.6s
Train: Loss=0.9276 (C:0.9276, R:0.0105) Ratio=3.71x
Val:   Loss=1.0059 (C:1.0059, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.386 ± 0.544
    Neg distances: 1.888 ± 0.876
    Separation ratio: 4.89x
    Gap: -3.230
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.9012 (C:0.9012, R:0.0105)
Batch  25/537: Loss=0.8882 (C:0.8882, R:0.0105)
Batch  50/537: Loss=0.8940 (C:0.8940, R:0.0105)
Batch  75/537: Loss=0.8607 (C:0.8607, R:0.0105)
Batch 100/537: Loss=0.9055 (C:0.9055, R:0.0105)
Batch 125/537: Loss=0.8773 (C:0.8773, R:0.0105)
Batch 150/537: Loss=0.8934 (C:0.8934, R:0.0105)
Batch 175/537: Loss=0.8744 (C:0.8744, R:0.0105)
Batch 200/537: Loss=0.8774 (C:0.8774, R:0.0105)
Batch 225/537: Loss=0.8931 (C:0.8931, R:0.0106)
Batch 250/537: Loss=0.8823 (C:0.8823, R:0.0105)
Batch 275/537: Loss=0.8752 (C:0.8752, R:0.0105)
Batch 300/537: Loss=0.9158 (C:0.9158, R:0.0105)
Batch 325/537: Loss=0.8591 (C:0.8591, R:0.0105)
Batch 350/537: Loss=0.8658 (C:0.8658, R:0.0105)
Batch 375/537: Loss=0.8871 (C:0.8871, R:0.0105)
Batch 400/537: Loss=0.9117 (C:0.9117, R:0.0105)
Batch 425/537: Loss=0.8918 (C:0.8918, R:0.0105)
Batch 450/537: Loss=0.9037 (C:0.9037, R:0.0105)
Batch 475/537: Loss=0.8651 (C:0.8651, R:0.0105)
Batch 500/537: Loss=0.9033 (C:0.9033, R:0.0105)
Batch 525/537: Loss=0.8971 (C:0.8971, R:0.0105)

============================================================
Epoch 19/300 completed in 27.7s
Train: Loss=0.8796 (C:0.8796, R:0.0105) Ratio=3.77x
Val:   Loss=0.9523 (C:0.9523, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9523)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.8682 (C:0.8682, R:0.0106)
Batch  25/537: Loss=0.8661 (C:0.8661, R:0.0105)
Batch  50/537: Loss=0.8954 (C:0.8954, R:0.0105)
Batch  75/537: Loss=0.8667 (C:0.8667, R:0.0105)
Batch 100/537: Loss=0.8429 (C:0.8429, R:0.0105)
Batch 125/537: Loss=0.8868 (C:0.8868, R:0.0105)
Batch 150/537: Loss=0.8331 (C:0.8331, R:0.0105)
Batch 175/537: Loss=0.8841 (C:0.8841, R:0.0105)
Batch 200/537: Loss=0.8897 (C:0.8897, R:0.0105)
Batch 225/537: Loss=0.8794 (C:0.8794, R:0.0105)
Batch 250/537: Loss=0.9224 (C:0.9224, R:0.0105)
Batch 275/537: Loss=0.8864 (C:0.8864, R:0.0105)
Batch 300/537: Loss=0.8616 (C:0.8616, R:0.0105)
Batch 325/537: Loss=0.8675 (C:0.8675, R:0.0105)
Batch 350/537: Loss=0.8678 (C:0.8678, R:0.0105)
Batch 375/537: Loss=0.8317 (C:0.8317, R:0.0105)
Batch 400/537: Loss=0.8865 (C:0.8865, R:0.0105)
Batch 425/537: Loss=0.9145 (C:0.9145, R:0.0105)
Batch 450/537: Loss=0.8808 (C:0.8808, R:0.0105)
Batch 475/537: Loss=0.8590 (C:0.8590, R:0.0105)
Batch 500/537: Loss=0.8975 (C:0.8975, R:0.0105)
Batch 525/537: Loss=0.8956 (C:0.8956, R:0.0105)

============================================================
Epoch 20/300 completed in 21.1s
Train: Loss=0.8764 (C:0.8764, R:0.0105) Ratio=3.89x
Val:   Loss=0.9426 (C:0.9426, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9426)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8760 (C:0.8760, R:0.0106)
Batch  25/537: Loss=0.8343 (C:0.8343, R:0.0105)
Batch  50/537: Loss=0.8561 (C:0.8561, R:0.0105)
Batch  75/537: Loss=0.8361 (C:0.8361, R:0.0106)
Batch 100/537: Loss=0.8523 (C:0.8523, R:0.0105)
Batch 125/537: Loss=0.8604 (C:0.8604, R:0.0105)
Batch 150/537: Loss=0.8895 (C:0.8895, R:0.0105)
Batch 175/537: Loss=0.8896 (C:0.8896, R:0.0105)
Batch 200/537: Loss=0.8553 (C:0.8553, R:0.0105)
Batch 225/537: Loss=0.8582 (C:0.8582, R:0.0105)
Batch 250/537: Loss=0.8717 (C:0.8717, R:0.0105)
Batch 275/537: Loss=0.8783 (C:0.8783, R:0.0105)
Batch 300/537: Loss=0.8864 (C:0.8864, R:0.0105)
Batch 325/537: Loss=0.8846 (C:0.8846, R:0.0105)
Batch 350/537: Loss=0.8760 (C:0.8760, R:0.0105)
Batch 375/537: Loss=0.8982 (C:0.8982, R:0.0105)
Batch 400/537: Loss=0.8639 (C:0.8639, R:0.0105)
Batch 425/537: Loss=0.8919 (C:0.8919, R:0.0105)
Batch 450/537: Loss=0.9229 (C:0.9229, R:0.0105)
Batch 475/537: Loss=0.9057 (C:0.9057, R:0.0105)
Batch 500/537: Loss=0.8848 (C:0.8848, R:0.0105)
Batch 525/537: Loss=0.8881 (C:0.8881, R:0.0105)

============================================================
Epoch 21/300 completed in 21.3s
Train: Loss=0.8732 (C:0.8732, R:0.0105) Ratio=3.87x
Val:   Loss=0.9491 (C:0.9491, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.381 ± 0.538
    Neg distances: 1.925 ± 0.887
    Separation ratio: 5.06x
    Gap: -3.267
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8574 (C:0.8574, R:0.0105)
Batch  25/537: Loss=0.8249 (C:0.8249, R:0.0105)
Batch  50/537: Loss=0.8144 (C:0.8144, R:0.0105)
Batch  75/537: Loss=0.8599 (C:0.8599, R:0.0105)
Batch 100/537: Loss=0.8645 (C:0.8645, R:0.0105)
Batch 125/537: Loss=0.8500 (C:0.8500, R:0.0105)
Batch 150/537: Loss=0.8454 (C:0.8454, R:0.0105)
Batch 175/537: Loss=0.8853 (C:0.8853, R:0.0105)
Batch 200/537: Loss=0.8620 (C:0.8620, R:0.0105)
Batch 225/537: Loss=0.8581 (C:0.8581, R:0.0105)
Batch 250/537: Loss=0.8481 (C:0.8481, R:0.0105)
Batch 275/537: Loss=0.8484 (C:0.8484, R:0.0105)
Batch 300/537: Loss=0.8504 (C:0.8504, R:0.0105)
Batch 325/537: Loss=0.8712 (C:0.8712, R:0.0105)
Batch 350/537: Loss=0.8491 (C:0.8491, R:0.0105)
Batch 375/537: Loss=0.8206 (C:0.8206, R:0.0105)
Batch 400/537: Loss=0.8563 (C:0.8563, R:0.0105)
Batch 425/537: Loss=0.8487 (C:0.8487, R:0.0105)
Batch 450/537: Loss=0.8753 (C:0.8753, R:0.0105)
Batch 475/537: Loss=0.8949 (C:0.8949, R:0.0105)
Batch 500/537: Loss=0.8838 (C:0.8838, R:0.0105)
Batch 525/537: Loss=0.8452 (C:0.8452, R:0.0105)

============================================================
Epoch 22/300 completed in 27.7s
Train: Loss=0.8533 (C:0.8533, R:0.0105) Ratio=3.92x
Val:   Loss=0.9360 (C:0.9360, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9360)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.8512 (C:0.8512, R:0.0105)
Batch  25/537: Loss=0.8700 (C:0.8700, R:0.0105)
Batch  50/537: Loss=0.8515 (C:0.8515, R:0.0105)
Batch  75/537: Loss=0.8326 (C:0.8326, R:0.0105)
Batch 100/537: Loss=0.8487 (C:0.8487, R:0.0105)
Batch 125/537: Loss=0.8612 (C:0.8612, R:0.0105)
Batch 150/537: Loss=0.8579 (C:0.8579, R:0.0105)
Batch 175/537: Loss=0.8247 (C:0.8247, R:0.0105)
Batch 200/537: Loss=0.8483 (C:0.8483, R:0.0105)
Batch 225/537: Loss=0.8678 (C:0.8678, R:0.0105)
Batch 250/537: Loss=0.8853 (C:0.8853, R:0.0105)
Batch 275/537: Loss=0.8509 (C:0.8509, R:0.0105)
Batch 300/537: Loss=0.8168 (C:0.8168, R:0.0105)
Batch 325/537: Loss=0.8614 (C:0.8614, R:0.0105)
Batch 350/537: Loss=0.8958 (C:0.8958, R:0.0105)
Batch 375/537: Loss=0.8288 (C:0.8288, R:0.0105)
Batch 400/537: Loss=0.8397 (C:0.8397, R:0.0105)
Batch 425/537: Loss=0.8689 (C:0.8689, R:0.0105)
Batch 450/537: Loss=0.8501 (C:0.8501, R:0.0105)
Batch 475/537: Loss=0.8437 (C:0.8437, R:0.0105)
Batch 500/537: Loss=0.8491 (C:0.8491, R:0.0105)
Batch 525/537: Loss=0.8175 (C:0.8175, R:0.0105)

============================================================
Epoch 23/300 completed in 21.5s
Train: Loss=0.8512 (C:0.8512, R:0.0105) Ratio=4.01x
Val:   Loss=0.9350 (C:0.9350, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9350)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8083 (C:0.8083, R:0.0105)
Batch  25/537: Loss=0.8681 (C:0.8681, R:0.0105)
Batch  50/537: Loss=0.8889 (C:0.8889, R:0.0105)
Batch  75/537: Loss=0.8615 (C:0.8615, R:0.0105)
Batch 100/537: Loss=0.8668 (C:0.8668, R:0.0105)
Batch 125/537: Loss=0.8751 (C:0.8751, R:0.0106)
Batch 150/537: Loss=0.8370 (C:0.8370, R:0.0105)
Batch 175/537: Loss=0.8712 (C:0.8712, R:0.0105)
Batch 200/537: Loss=0.8168 (C:0.8168, R:0.0105)
Batch 225/537: Loss=0.8559 (C:0.8559, R:0.0105)
Batch 250/537: Loss=0.8628 (C:0.8628, R:0.0105)
Batch 275/537: Loss=0.8233 (C:0.8233, R:0.0105)
Batch 300/537: Loss=0.8373 (C:0.8373, R:0.0105)
Batch 325/537: Loss=0.8825 (C:0.8825, R:0.0105)
Batch 350/537: Loss=0.8455 (C:0.8455, R:0.0105)
Batch 375/537: Loss=0.8136 (C:0.8136, R:0.0105)
Batch 400/537: Loss=0.8343 (C:0.8343, R:0.0105)
Batch 425/537: Loss=0.8860 (C:0.8860, R:0.0105)
Batch 450/537: Loss=0.8488 (C:0.8488, R:0.0105)
Batch 475/537: Loss=0.8482 (C:0.8482, R:0.0105)
Batch 500/537: Loss=0.8605 (C:0.8605, R:0.0105)
Batch 525/537: Loss=0.8432 (C:0.8432, R:0.0105)

============================================================
Epoch 24/300 completed in 21.2s
Train: Loss=0.8482 (C:0.8482, R:0.0105) Ratio=4.04x
Val:   Loss=0.9331 (C:0.9331, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9331)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.369 ± 0.525
    Neg distances: 1.997 ± 0.901
    Separation ratio: 5.41x
    Gap: -3.380
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.8295 (C:0.8295, R:0.0105)
Batch  25/537: Loss=0.7720 (C:0.7720, R:0.0105)
Batch  50/537: Loss=0.7854 (C:0.7854, R:0.0105)
Batch  75/537: Loss=0.8165 (C:0.8165, R:0.0105)
Batch 100/537: Loss=0.8055 (C:0.8055, R:0.0105)
Batch 125/537: Loss=0.8156 (C:0.8156, R:0.0105)
Batch 150/537: Loss=0.8181 (C:0.8181, R:0.0105)
Batch 175/537: Loss=0.8333 (C:0.8333, R:0.0105)
Batch 200/537: Loss=0.8120 (C:0.8120, R:0.0105)
Batch 225/537: Loss=0.8446 (C:0.8446, R:0.0105)
Batch 250/537: Loss=0.8067 (C:0.8067, R:0.0105)
Batch 275/537: Loss=0.8330 (C:0.8330, R:0.0105)
Batch 300/537: Loss=0.8250 (C:0.8250, R:0.0105)
Batch 325/537: Loss=0.8015 (C:0.8015, R:0.0105)
Batch 350/537: Loss=0.8269 (C:0.8269, R:0.0105)
Batch 375/537: Loss=0.8474 (C:0.8474, R:0.0106)
Batch 400/537: Loss=0.8086 (C:0.8086, R:0.0105)
Batch 425/537: Loss=0.8511 (C:0.8511, R:0.0105)
Batch 450/537: Loss=0.8072 (C:0.8072, R:0.0105)
Batch 475/537: Loss=0.8032 (C:0.8032, R:0.0105)
Batch 500/537: Loss=0.8314 (C:0.8314, R:0.0105)
Batch 525/537: Loss=0.8051 (C:0.8051, R:0.0105)

============================================================
Epoch 25/300 completed in 27.1s
Train: Loss=0.8159 (C:0.8159, R:0.0105) Ratio=4.08x
Val:   Loss=0.9133 (C:0.9133, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9133)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.8100 (C:0.8100, R:0.0105)
Batch  25/537: Loss=0.7911 (C:0.7911, R:0.0105)
Batch  50/537: Loss=0.8241 (C:0.8241, R:0.0105)
Batch  75/537: Loss=0.8215 (C:0.8215, R:0.0105)
Batch 100/537: Loss=0.8177 (C:0.8177, R:0.0105)
Batch 125/537: Loss=0.8083 (C:0.8083, R:0.0105)
Batch 150/537: Loss=0.8432 (C:0.8432, R:0.0105)
Batch 175/537: Loss=0.8217 (C:0.8217, R:0.0105)
Batch 200/537: Loss=0.8305 (C:0.8305, R:0.0105)
Batch 225/537: Loss=0.8170 (C:0.8170, R:0.0105)
Batch 250/537: Loss=0.8107 (C:0.8107, R:0.0105)
Batch 275/537: Loss=0.8238 (C:0.8238, R:0.0105)
Batch 300/537: Loss=0.8079 (C:0.8079, R:0.0105)
Batch 325/537: Loss=0.8039 (C:0.8039, R:0.0105)
Batch 350/537: Loss=0.8406 (C:0.8406, R:0.0105)
Batch 375/537: Loss=0.8050 (C:0.8050, R:0.0105)
Batch 400/537: Loss=0.8363 (C:0.8363, R:0.0105)
Batch 425/537: Loss=0.7668 (C:0.7668, R:0.0105)
Batch 450/537: Loss=0.8249 (C:0.8249, R:0.0105)
Batch 475/537: Loss=0.8251 (C:0.8251, R:0.0105)
Batch 500/537: Loss=0.8118 (C:0.8118, R:0.0105)
Batch 525/537: Loss=0.7874 (C:0.7874, R:0.0105)

============================================================
Epoch 26/300 completed in 21.3s
Train: Loss=0.8129 (C:0.8129, R:0.0105) Ratio=4.14x
Val:   Loss=0.8919 (C:0.8919, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8919)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.7928 (C:0.7928, R:0.0105)
Batch  25/537: Loss=0.7727 (C:0.7727, R:0.0105)
Batch  50/537: Loss=0.8137 (C:0.8137, R:0.0105)
Batch  75/537: Loss=0.7818 (C:0.7818, R:0.0105)
Batch 100/537: Loss=0.8166 (C:0.8166, R:0.0105)
Batch 125/537: Loss=0.8020 (C:0.8020, R:0.0105)
Batch 150/537: Loss=0.8317 (C:0.8317, R:0.0105)
Batch 175/537: Loss=0.8327 (C:0.8327, R:0.0105)
Batch 200/537: Loss=0.7933 (C:0.7933, R:0.0106)
Batch 225/537: Loss=0.7818 (C:0.7818, R:0.0105)
Batch 250/537: Loss=0.7974 (C:0.7974, R:0.0106)
Batch 275/537: Loss=0.8146 (C:0.8146, R:0.0106)
Batch 300/537: Loss=0.7936 (C:0.7936, R:0.0105)
Batch 325/537: Loss=0.8130 (C:0.8130, R:0.0105)
Batch 350/537: Loss=0.8033 (C:0.8033, R:0.0105)
Batch 375/537: Loss=0.8461 (C:0.8461, R:0.0105)
Batch 400/537: Loss=0.7898 (C:0.7898, R:0.0105)
Batch 425/537: Loss=0.7848 (C:0.7848, R:0.0105)
Batch 450/537: Loss=0.8510 (C:0.8510, R:0.0105)
Batch 475/537: Loss=0.8346 (C:0.8346, R:0.0105)
Batch 500/537: Loss=0.8223 (C:0.8223, R:0.0105)
Batch 525/537: Loss=0.8310 (C:0.8310, R:0.0105)

============================================================
Epoch 27/300 completed in 21.5s
Train: Loss=0.8095 (C:0.8095, R:0.0105) Ratio=4.18x
Val:   Loss=0.9038 (C:0.9038, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.363 ± 0.546
    Neg distances: 2.076 ± 0.927
    Separation ratio: 5.71x
    Gap: -3.514
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.7978 (C:0.7978, R:0.0105)
Batch  25/537: Loss=0.7638 (C:0.7638, R:0.0105)
Batch  50/537: Loss=0.7906 (C:0.7906, R:0.0105)
Batch  75/537: Loss=0.7485 (C:0.7485, R:0.0105)
Batch 100/537: Loss=0.7480 (C:0.7480, R:0.0105)
Batch 125/537: Loss=0.7652 (C:0.7652, R:0.0105)
Batch 150/537: Loss=0.7973 (C:0.7973, R:0.0105)
Batch 175/537: Loss=0.8103 (C:0.8103, R:0.0105)
Batch 200/537: Loss=0.7667 (C:0.7667, R:0.0105)
Batch 225/537: Loss=0.7732 (C:0.7732, R:0.0105)
Batch 250/537: Loss=0.8216 (C:0.8216, R:0.0105)
Batch 275/537: Loss=0.7952 (C:0.7952, R:0.0105)
Batch 300/537: Loss=0.7900 (C:0.7900, R:0.0105)
Batch 325/537: Loss=0.7836 (C:0.7836, R:0.0105)
Batch 350/537: Loss=0.7870 (C:0.7870, R:0.0105)
Batch 375/537: Loss=0.8094 (C:0.8094, R:0.0105)
Batch 400/537: Loss=0.7814 (C:0.7814, R:0.0105)
Batch 425/537: Loss=0.7723 (C:0.7723, R:0.0105)
Batch 450/537: Loss=0.7869 (C:0.7869, R:0.0105)
Batch 475/537: Loss=0.8136 (C:0.8136, R:0.0105)
Batch 500/537: Loss=0.8096 (C:0.8096, R:0.0105)
Batch 525/537: Loss=0.7538 (C:0.7538, R:0.0105)

============================================================
Epoch 28/300 completed in 27.8s
Train: Loss=0.7813 (C:0.7813, R:0.0105) Ratio=4.15x
Val:   Loss=0.8871 (C:0.8871, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8871)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.7555 (C:0.7555, R:0.0105)
Batch  25/537: Loss=0.7810 (C:0.7810, R:0.0105)
Batch  50/537: Loss=0.7691 (C:0.7691, R:0.0105)
Batch  75/537: Loss=0.7741 (C:0.7741, R:0.0105)
Batch 100/537: Loss=0.7412 (C:0.7412, R:0.0105)
Batch 125/537: Loss=0.8046 (C:0.8046, R:0.0105)
Batch 150/537: Loss=0.7890 (C:0.7890, R:0.0105)
Batch 175/537: Loss=0.8057 (C:0.8057, R:0.0106)
Batch 200/537: Loss=0.7891 (C:0.7891, R:0.0105)
Batch 225/537: Loss=0.7849 (C:0.7849, R:0.0105)
Batch 250/537: Loss=0.7478 (C:0.7478, R:0.0105)
Batch 275/537: Loss=0.7839 (C:0.7839, R:0.0105)
Batch 300/537: Loss=0.7829 (C:0.7829, R:0.0105)
Batch 325/537: Loss=0.7666 (C:0.7666, R:0.0105)
Batch 350/537: Loss=0.7744 (C:0.7744, R:0.0105)
Batch 375/537: Loss=0.8036 (C:0.8036, R:0.0105)
Batch 400/537: Loss=0.7726 (C:0.7726, R:0.0105)
Batch 425/537: Loss=0.8101 (C:0.8101, R:0.0105)
Batch 450/537: Loss=0.7838 (C:0.7838, R:0.0105)
Batch 475/537: Loss=0.7685 (C:0.7685, R:0.0105)
Batch 500/537: Loss=0.8186 (C:0.8186, R:0.0105)
Batch 525/537: Loss=0.7769 (C:0.7769, R:0.0105)

============================================================
Epoch 29/300 completed in 21.3s
Train: Loss=0.7802 (C:0.7802, R:0.0105) Ratio=4.31x
Val:   Loss=0.8770 (C:0.8770, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8770)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7744 (C:0.7744, R:0.0106)
Batch  25/537: Loss=0.7756 (C:0.7756, R:0.0105)
Batch  50/537: Loss=0.7502 (C:0.7502, R:0.0105)
Batch  75/537: Loss=0.7849 (C:0.7849, R:0.0105)
Batch 100/537: Loss=0.7964 (C:0.7964, R:0.0105)
Batch 125/537: Loss=0.7683 (C:0.7683, R:0.0105)
Batch 150/537: Loss=0.7429 (C:0.7429, R:0.0105)
Batch 175/537: Loss=0.7757 (C:0.7757, R:0.0105)
Batch 200/537: Loss=0.7726 (C:0.7726, R:0.0105)
Batch 225/537: Loss=0.7838 (C:0.7838, R:0.0105)
Batch 250/537: Loss=0.7887 (C:0.7887, R:0.0105)
Batch 275/537: Loss=0.7798 (C:0.7798, R:0.0105)
Batch 300/537: Loss=0.7839 (C:0.7839, R:0.0105)
Batch 325/537: Loss=0.7714 (C:0.7714, R:0.0105)
Batch 350/537: Loss=0.7912 (C:0.7912, R:0.0105)
Batch 375/537: Loss=0.8164 (C:0.8164, R:0.0105)
Batch 400/537: Loss=0.7636 (C:0.7636, R:0.0105)
Batch 425/537: Loss=0.7934 (C:0.7934, R:0.0105)
Batch 450/537: Loss=0.7534 (C:0.7534, R:0.0105)
Batch 475/537: Loss=0.7752 (C:0.7752, R:0.0105)
Batch 500/537: Loss=0.7968 (C:0.7968, R:0.0105)
Batch 525/537: Loss=0.7977 (C:0.7977, R:0.0105)

============================================================
Epoch 30/300 completed in 21.2s
Train: Loss=0.7752 (C:0.7752, R:0.0105) Ratio=4.28x
Val:   Loss=0.8798 (C:0.8798, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.362 ± 0.557
    Neg distances: 2.127 ± 0.942
    Separation ratio: 5.87x
    Gap: -3.581
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.7760 (C:0.7760, R:0.0105)
Batch  25/537: Loss=0.7515 (C:0.7515, R:0.0105)
Batch  50/537: Loss=0.7579 (C:0.7579, R:0.0105)
Batch  75/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 100/537: Loss=0.7849 (C:0.7849, R:0.0105)
Batch 125/537: Loss=0.7415 (C:0.7415, R:0.0105)
Batch 150/537: Loss=0.7651 (C:0.7651, R:0.0105)
Batch 175/537: Loss=0.7805 (C:0.7805, R:0.0105)
Batch 200/537: Loss=0.7497 (C:0.7497, R:0.0105)
Batch 225/537: Loss=0.7720 (C:0.7720, R:0.0105)
Batch 250/537: Loss=0.7733 (C:0.7733, R:0.0105)
Batch 275/537: Loss=0.7662 (C:0.7662, R:0.0106)
Batch 300/537: Loss=0.7632 (C:0.7632, R:0.0105)
Batch 325/537: Loss=0.7732 (C:0.7732, R:0.0105)
Batch 350/537: Loss=0.7911 (C:0.7911, R:0.0105)
Batch 375/537: Loss=0.7754 (C:0.7754, R:0.0105)
Batch 400/537: Loss=0.7511 (C:0.7511, R:0.0105)
Batch 425/537: Loss=0.7658 (C:0.7658, R:0.0106)
Batch 450/537: Loss=0.7611 (C:0.7611, R:0.0105)
Batch 475/537: Loss=0.7877 (C:0.7877, R:0.0105)
Batch 500/537: Loss=0.7309 (C:0.7309, R:0.0105)
Batch 525/537: Loss=0.7720 (C:0.7720, R:0.0105)

============================================================
Epoch 31/300 completed in 27.3s
Train: Loss=0.7604 (C:0.7604, R:0.0105) Ratio=4.36x
Val:   Loss=0.8640 (C:0.8640, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8640)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.7591 (C:0.7591, R:0.0105)
Batch  25/537: Loss=0.7556 (C:0.7556, R:0.0105)
Batch  50/537: Loss=0.7772 (C:0.7772, R:0.0105)
Batch  75/537: Loss=0.7800 (C:0.7800, R:0.0105)
Batch 100/537: Loss=0.7307 (C:0.7307, R:0.0105)
Batch 125/537: Loss=0.7245 (C:0.7245, R:0.0105)
Batch 150/537: Loss=0.7477 (C:0.7477, R:0.0105)
Batch 175/537: Loss=0.7816 (C:0.7816, R:0.0105)
Batch 200/537: Loss=0.7528 (C:0.7528, R:0.0105)
Batch 225/537: Loss=0.7547 (C:0.7547, R:0.0105)
Batch 250/537: Loss=0.7415 (C:0.7415, R:0.0104)
Batch 275/537: Loss=0.7463 (C:0.7463, R:0.0105)
Batch 300/537: Loss=0.7659 (C:0.7659, R:0.0105)
Batch 325/537: Loss=0.7814 (C:0.7814, R:0.0105)
Batch 350/537: Loss=0.7441 (C:0.7441, R:0.0105)
Batch 375/537: Loss=0.7725 (C:0.7725, R:0.0105)
Batch 400/537: Loss=0.7476 (C:0.7476, R:0.0105)
Batch 425/537: Loss=0.7171 (C:0.7171, R:0.0105)
Batch 450/537: Loss=0.7321 (C:0.7321, R:0.0106)
Batch 475/537: Loss=0.7186 (C:0.7186, R:0.0105)
Batch 500/537: Loss=0.7810 (C:0.7810, R:0.0105)
Batch 525/537: Loss=0.7418 (C:0.7418, R:0.0105)

============================================================
Epoch 32/300 completed in 21.6s
Train: Loss=0.7576 (C:0.7576, R:0.0105) Ratio=4.43x
Val:   Loss=0.8612 (C:0.8612, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.030
✅ New best model saved (Val Loss: 0.8612)
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.7222 (C:0.7222, R:0.0105)
Batch  25/537: Loss=0.7764 (C:0.7764, R:0.0105)
Batch  50/537: Loss=0.7198 (C:0.7198, R:0.0105)
Batch  75/537: Loss=0.7493 (C:0.7493, R:0.0105)
Batch 100/537: Loss=0.7432 (C:0.7432, R:0.0105)
Batch 125/537: Loss=0.7577 (C:0.7577, R:0.0105)
Batch 150/537: Loss=0.7619 (C:0.7619, R:0.0105)
Batch 175/537: Loss=0.7408 (C:0.7408, R:0.0105)
Batch 200/537: Loss=0.7644 (C:0.7644, R:0.0105)
Batch 225/537: Loss=0.7709 (C:0.7709, R:0.0105)
Batch 250/537: Loss=0.7479 (C:0.7479, R:0.0105)
Batch 275/537: Loss=0.7445 (C:0.7445, R:0.0105)
Batch 300/537: Loss=0.7500 (C:0.7500, R:0.0105)
Batch 325/537: Loss=0.7757 (C:0.7757, R:0.0105)
Batch 350/537: Loss=0.7321 (C:0.7321, R:0.0105)
Batch 375/537: Loss=0.7501 (C:0.7501, R:0.0105)
Batch 400/537: Loss=0.7724 (C:0.7724, R:0.0105)
Batch 425/537: Loss=0.7586 (C:0.7586, R:0.0105)
Batch 450/537: Loss=0.7371 (C:0.7371, R:0.0105)
Batch 475/537: Loss=0.7257 (C:0.7257, R:0.0105)
Batch 500/537: Loss=0.7618 (C:0.7618, R:0.0105)
Batch 525/537: Loss=0.7672 (C:0.7672, R:0.0105)

============================================================
Epoch 33/300 completed in 21.5s
Train: Loss=0.7545 (C:0.7545, R:0.0105) Ratio=4.39x
Val:   Loss=0.8621 (C:0.8621, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.045
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.339 ± 0.540
    Neg distances: 2.171 ± 0.949
    Separation ratio: 6.41x
    Gap: -3.634
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.7388 (C:0.7388, R:0.0105)
Batch  25/537: Loss=0.7129 (C:0.7129, R:0.0105)
Batch  50/537: Loss=0.7675 (C:0.7675, R:0.0105)
Batch  75/537: Loss=0.7232 (C:0.7232, R:0.0105)
Batch 100/537: Loss=0.7344 (C:0.7344, R:0.0105)
Batch 125/537: Loss=0.7276 (C:0.7276, R:0.0105)
Batch 150/537: Loss=0.7227 (C:0.7227, R:0.0106)
Batch 175/537: Loss=0.7356 (C:0.7356, R:0.0105)
Batch 200/537: Loss=0.7235 (C:0.7235, R:0.0105)
Batch 225/537: Loss=0.7203 (C:0.7203, R:0.0105)
Batch 250/537: Loss=0.7252 (C:0.7252, R:0.0105)
Batch 275/537: Loss=0.7001 (C:0.7001, R:0.0105)
Batch 300/537: Loss=0.6977 (C:0.6977, R:0.0105)
Batch 325/537: Loss=0.7184 (C:0.7184, R:0.0105)
Batch 350/537: Loss=0.7377 (C:0.7377, R:0.0105)
Batch 375/537: Loss=0.7266 (C:0.7266, R:0.0105)
Batch 400/537: Loss=0.6915 (C:0.6915, R:0.0105)
Batch 425/537: Loss=0.7255 (C:0.7255, R:0.0105)
Batch 450/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 475/537: Loss=0.7467 (C:0.7467, R:0.0105)
Batch 500/537: Loss=0.7196 (C:0.7196, R:0.0105)
Batch 525/537: Loss=0.7142 (C:0.7142, R:0.0105)

============================================================
Epoch 34/300 completed in 27.2s
Train: Loss=0.7239 (C:0.7239, R:0.0105) Ratio=4.45x
Val:   Loss=0.8304 (C:0.8304, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8304)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.7451 (C:0.7451, R:0.0105)
Batch  25/537: Loss=0.7218 (C:0.7218, R:0.0105)
Batch  50/537: Loss=0.6793 (C:0.6793, R:0.0105)
Batch  75/537: Loss=0.7226 (C:0.7226, R:0.0105)
Batch 100/537: Loss=0.7206 (C:0.7206, R:0.0105)
Batch 125/537: Loss=0.6963 (C:0.6963, R:0.0105)
Batch 150/537: Loss=0.7025 (C:0.7025, R:0.0105)
Batch 175/537: Loss=0.6986 (C:0.6986, R:0.0105)
Batch 200/537: Loss=0.7155 (C:0.7155, R:0.0105)
Batch 225/537: Loss=0.7275 (C:0.7275, R:0.0105)
Batch 250/537: Loss=0.6975 (C:0.6975, R:0.0105)
Batch 275/537: Loss=0.7080 (C:0.7080, R:0.0105)
Batch 300/537: Loss=0.7056 (C:0.7056, R:0.0106)
Batch 325/537: Loss=0.7326 (C:0.7326, R:0.0105)
Batch 350/537: Loss=0.7124 (C:0.7124, R:0.0105)
Batch 375/537: Loss=0.7434 (C:0.7434, R:0.0105)
Batch 400/537: Loss=0.7234 (C:0.7234, R:0.0105)
Batch 425/537: Loss=0.7057 (C:0.7057, R:0.0105)
Batch 450/537: Loss=0.7101 (C:0.7101, R:0.0105)
Batch 475/537: Loss=0.7008 (C:0.7008, R:0.0105)
Batch 500/537: Loss=0.7523 (C:0.7523, R:0.0105)
Batch 525/537: Loss=0.7559 (C:0.7559, R:0.0105)

============================================================
Epoch 35/300 completed in 21.4s
Train: Loss=0.7219 (C:0.7219, R:0.0105) Ratio=4.56x
Val:   Loss=0.8254 (C:0.8254, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 0.8254)
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.6687 (C:0.6687, R:0.0105)
Batch  25/537: Loss=0.7414 (C:0.7414, R:0.0105)
Batch  50/537: Loss=0.7143 (C:0.7143, R:0.0105)
Batch  75/537: Loss=0.7015 (C:0.7015, R:0.0105)
Batch 100/537: Loss=0.7228 (C:0.7228, R:0.0105)
Batch 125/537: Loss=0.7208 (C:0.7208, R:0.0105)
Batch 150/537: Loss=0.7029 (C:0.7029, R:0.0105)
Batch 175/537: Loss=0.7320 (C:0.7320, R:0.0105)
Batch 200/537: Loss=0.7503 (C:0.7503, R:0.0104)
Batch 225/537: Loss=0.7375 (C:0.7375, R:0.0105)
Batch 250/537: Loss=0.7136 (C:0.7136, R:0.0105)
Batch 275/537: Loss=0.7361 (C:0.7361, R:0.0105)
Batch 300/537: Loss=0.7476 (C:0.7476, R:0.0105)
Batch 325/537: Loss=0.7553 (C:0.7553, R:0.0105)
Batch 350/537: Loss=0.7193 (C:0.7193, R:0.0105)
Batch 375/537: Loss=0.7354 (C:0.7354, R:0.0105)
Batch 400/537: Loss=0.6989 (C:0.6989, R:0.0105)
Batch 425/537: Loss=0.7433 (C:0.7433, R:0.0105)
Batch 450/537: Loss=0.7499 (C:0.7499, R:0.0105)
Batch 475/537: Loss=0.7727 (C:0.7727, R:0.0105)
Batch 500/537: Loss=0.7291 (C:0.7291, R:0.0105)
Batch 525/537: Loss=0.7407 (C:0.7407, R:0.0105)

============================================================
Epoch 36/300 completed in 21.6s
Train: Loss=0.7193 (C:0.7193, R:0.0105) Ratio=4.48x
Val:   Loss=0.8287 (C:0.8287, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.090
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.335 ± 0.556
    Neg distances: 2.239 ± 0.975
    Separation ratio: 6.69x
    Gap: -3.783
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.6967 (C:0.6967, R:0.0105)
Batch  25/537: Loss=0.6877 (C:0.6877, R:0.0105)
Batch  50/537: Loss=0.7014 (C:0.7014, R:0.0105)
Batch  75/537: Loss=0.7016 (C:0.7016, R:0.0105)
Batch 100/537: Loss=0.7140 (C:0.7140, R:0.0105)
Batch 125/537: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 150/537: Loss=0.7127 (C:0.7127, R:0.0105)
Batch 175/537: Loss=0.7027 (C:0.7027, R:0.0105)
Batch 200/537: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 225/537: Loss=0.6740 (C:0.6740, R:0.0105)
Batch 250/537: Loss=0.6723 (C:0.6723, R:0.0105)
Batch 275/537: Loss=0.7052 (C:0.7052, R:0.0105)
Batch 300/537: Loss=0.6711 (C:0.6711, R:0.0105)
Batch 325/537: Loss=0.6958 (C:0.6958, R:0.0105)
Batch 350/537: Loss=0.6993 (C:0.6993, R:0.0105)
Batch 375/537: Loss=0.7047 (C:0.7047, R:0.0105)
Batch 400/537: Loss=0.6819 (C:0.6819, R:0.0105)
Batch 425/537: Loss=0.6791 (C:0.6791, R:0.0105)
Batch 450/537: Loss=0.6662 (C:0.6662, R:0.0105)
Batch 475/537: Loss=0.7053 (C:0.7053, R:0.0105)
Batch 500/537: Loss=0.6917 (C:0.6917, R:0.0105)
Batch 525/537: Loss=0.6972 (C:0.6972, R:0.0105)

============================================================
Epoch 37/300 completed in 27.8s
Train: Loss=0.6990 (C:0.6990, R:0.0105) Ratio=4.63x
Val:   Loss=0.8187 (C:0.8187, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8187)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.6793 (C:0.6793, R:0.0105)
Batch  25/537: Loss=0.6944 (C:0.6944, R:0.0105)
Batch  50/537: Loss=0.6831 (C:0.6831, R:0.0105)
Batch  75/537: Loss=0.6968 (C:0.6968, R:0.0105)
Batch 100/537: Loss=0.7086 (C:0.7086, R:0.0105)
Batch 125/537: Loss=0.6843 (C:0.6843, R:0.0106)
Batch 150/537: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 175/537: Loss=0.7012 (C:0.7012, R:0.0105)
Batch 200/537: Loss=0.6984 (C:0.6984, R:0.0105)
Batch 225/537: Loss=0.7085 (C:0.7085, R:0.0105)
Batch 250/537: Loss=0.6830 (C:0.6830, R:0.0105)
Batch 275/537: Loss=0.7353 (C:0.7353, R:0.0105)
Batch 300/537: Loss=0.7178 (C:0.7178, R:0.0105)
Batch 325/537: Loss=0.6722 (C:0.6722, R:0.0105)
Batch 350/537: Loss=0.7135 (C:0.7135, R:0.0105)
Batch 375/537: Loss=0.6880 (C:0.6880, R:0.0105)
Batch 400/537: Loss=0.7205 (C:0.7205, R:0.0105)
Batch 425/537: Loss=0.7074 (C:0.7074, R:0.0105)
Batch 450/537: Loss=0.7096 (C:0.7096, R:0.0105)
Batch 475/537: Loss=0.6897 (C:0.6897, R:0.0105)
Batch 500/537: Loss=0.6973 (C:0.6973, R:0.0105)
Batch 525/537: Loss=0.6750 (C:0.6750, R:0.0105)

============================================================
Epoch 38/300 completed in 21.1s
Train: Loss=0.6966 (C:0.6966, R:0.0105) Ratio=4.59x
Val:   Loss=0.8166 (C:0.8166, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.120
✅ New best model saved (Val Loss: 0.8166)
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.6975 (C:0.6975, R:0.0105)
Batch  25/537: Loss=0.6625 (C:0.6625, R:0.0105)
Batch  50/537: Loss=0.6563 (C:0.6563, R:0.0105)
Batch  75/537: Loss=0.6949 (C:0.6949, R:0.0105)
Batch 100/537: Loss=0.6980 (C:0.6980, R:0.0105)
Batch 125/537: Loss=0.6999 (C:0.6999, R:0.0105)
Batch 150/537: Loss=0.6830 (C:0.6830, R:0.0105)
Batch 175/537: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 200/537: Loss=0.7202 (C:0.7202, R:0.0105)
Batch 225/537: Loss=0.6765 (C:0.6765, R:0.0105)
Batch 250/537: Loss=0.6997 (C:0.6997, R:0.0105)
Batch 275/537: Loss=0.7015 (C:0.7015, R:0.0105)
Batch 300/537: Loss=0.7236 (C:0.7236, R:0.0105)
Batch 325/537: Loss=0.6655 (C:0.6655, R:0.0105)
Batch 350/537: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 375/537: Loss=0.6950 (C:0.6950, R:0.0105)
Batch 400/537: Loss=0.6956 (C:0.6956, R:0.0106)
Batch 425/537: Loss=0.7230 (C:0.7230, R:0.0106)
Batch 450/537: Loss=0.7366 (C:0.7366, R:0.0105)
Batch 475/537: Loss=0.6809 (C:0.6809, R:0.0105)
Batch 500/537: Loss=0.6890 (C:0.6890, R:0.0105)
Batch 525/537: Loss=0.6928 (C:0.6928, R:0.0105)

============================================================
Epoch 39/300 completed in 21.0s
Train: Loss=0.6948 (C:0.6948, R:0.0105) Ratio=4.64x
Val:   Loss=0.8167 (C:0.8167, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.135
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.314 ± 0.524
    Neg distances: 2.299 ± 0.986
    Separation ratio: 7.32x
    Gap: -3.777
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.6824 (C:0.6824, R:0.0105)
Batch  25/537: Loss=0.6456 (C:0.6456, R:0.0105)
Batch  50/537: Loss=0.6669 (C:0.6669, R:0.0105)
Batch  75/537: Loss=0.6356 (C:0.6356, R:0.0106)
Batch 100/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 125/537: Loss=0.6562 (C:0.6562, R:0.0105)
Batch 150/537: Loss=0.6512 (C:0.6512, R:0.0105)
Batch 175/537: Loss=0.6676 (C:0.6676, R:0.0105)
Batch 200/537: Loss=0.6387 (C:0.6387, R:0.0105)
Batch 225/537: Loss=0.6743 (C:0.6743, R:0.0105)
Batch 250/537: Loss=0.6354 (C:0.6354, R:0.0105)
Batch 275/537: Loss=0.6700 (C:0.6700, R:0.0105)
Batch 300/537: Loss=0.6558 (C:0.6558, R:0.0105)
Batch 325/537: Loss=0.6702 (C:0.6702, R:0.0105)
Batch 350/537: Loss=0.6480 (C:0.6480, R:0.0105)
Batch 375/537: Loss=0.6504 (C:0.6504, R:0.0105)
Batch 400/537: Loss=0.7010 (C:0.7010, R:0.0105)
Batch 425/537: Loss=0.6333 (C:0.6333, R:0.0105)
Batch 450/537: Loss=0.6649 (C:0.6649, R:0.0105)
Batch 475/537: Loss=0.6482 (C:0.6482, R:0.0105)
Batch 500/537: Loss=0.6939 (C:0.6939, R:0.0105)
Batch 525/537: Loss=0.6891 (C:0.6891, R:0.0105)

============================================================
Epoch 40/300 completed in 27.0s
Train: Loss=0.6637 (C:0.6637, R:0.0105) Ratio=4.64x
Val:   Loss=0.7793 (C:0.7793, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.7793)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6585 (C:0.6585, R:0.0105)
Batch  25/537: Loss=0.6643 (C:0.6643, R:0.0105)
Batch  50/537: Loss=0.6995 (C:0.6995, R:0.0105)
Batch  75/537: Loss=0.6504 (C:0.6504, R:0.0105)
Batch 100/537: Loss=0.6437 (C:0.6437, R:0.0105)
Batch 125/537: Loss=0.6506 (C:0.6506, R:0.0105)
Batch 150/537: Loss=0.6684 (C:0.6684, R:0.0105)
Batch 175/537: Loss=0.6722 (C:0.6722, R:0.0105)
Batch 200/537: Loss=0.6777 (C:0.6777, R:0.0105)
Batch 225/537: Loss=0.6592 (C:0.6592, R:0.0105)
Batch 250/537: Loss=0.6748 (C:0.6748, R:0.0105)
Batch 275/537: Loss=0.6781 (C:0.6781, R:0.0105)
Batch 300/537: Loss=0.7047 (C:0.7047, R:0.0105)
Batch 325/537: Loss=0.6687 (C:0.6687, R:0.0105)
Batch 350/537: Loss=0.7005 (C:0.7005, R:0.0105)
Batch 375/537: Loss=0.6726 (C:0.6726, R:0.0105)
Batch 400/537: Loss=0.6411 (C:0.6411, R:0.0105)
Batch 425/537: Loss=0.6697 (C:0.6697, R:0.0105)
Batch 450/537: Loss=0.6358 (C:0.6358, R:0.0105)
Batch 475/537: Loss=0.6835 (C:0.6835, R:0.0105)
Batch 500/537: Loss=0.6650 (C:0.6650, R:0.0105)
Batch 525/537: Loss=0.6521 (C:0.6521, R:0.0105)

============================================================
Epoch 41/300 completed in 20.9s
Train: Loss=0.6625 (C:0.6625, R:0.0105) Ratio=4.64x
Val:   Loss=0.7909 (C:0.7909, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.165
No improvement for 1 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.6792 (C:0.6792, R:0.0105)
Batch  25/537: Loss=0.6639 (C:0.6639, R:0.0105)
Batch  50/537: Loss=0.6513 (C:0.6513, R:0.0105)
Batch  75/537: Loss=0.6335 (C:0.6335, R:0.0105)
Batch 100/537: Loss=0.6247 (C:0.6247, R:0.0105)
Batch 125/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 150/537: Loss=0.6614 (C:0.6614, R:0.0105)
Batch 175/537: Loss=0.6497 (C:0.6497, R:0.0105)
Batch 200/537: Loss=0.6354 (C:0.6354, R:0.0105)
Batch 225/537: Loss=0.6605 (C:0.6605, R:0.0106)
Batch 250/537: Loss=0.6894 (C:0.6894, R:0.0105)
Batch 275/537: Loss=0.6631 (C:0.6631, R:0.0105)
Batch 300/537: Loss=0.6731 (C:0.6731, R:0.0105)
Batch 325/537: Loss=0.6758 (C:0.6758, R:0.0106)
Batch 350/537: Loss=0.6793 (C:0.6793, R:0.0105)
Batch 375/537: Loss=0.6899 (C:0.6899, R:0.0105)
Batch 400/537: Loss=0.6525 (C:0.6525, R:0.0105)
Batch 425/537: Loss=0.6985 (C:0.6985, R:0.0105)
Batch 450/537: Loss=0.7048 (C:0.7048, R:0.0105)
Batch 475/537: Loss=0.6747 (C:0.6747, R:0.0105)
Batch 500/537: Loss=0.6492 (C:0.6492, R:0.0105)
Batch 525/537: Loss=0.6591 (C:0.6591, R:0.0105)

============================================================
Epoch 42/300 completed in 21.1s
Train: Loss=0.6606 (C:0.6606, R:0.0105) Ratio=4.65x
Val:   Loss=0.7857 (C:0.7857, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.180
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.326 ± 0.553
    Neg distances: 2.308 ± 0.990
    Separation ratio: 7.08x
    Gap: -3.940
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.6198 (C:0.6198, R:0.0105)
Batch  25/537: Loss=0.6293 (C:0.6293, R:0.0105)
Batch  50/537: Loss=0.6138 (C:0.6138, R:0.0105)
Batch  75/537: Loss=0.6466 (C:0.6466, R:0.0105)
Batch 100/537: Loss=0.6618 (C:0.6618, R:0.0105)
Batch 125/537: Loss=0.6438 (C:0.6438, R:0.0105)
Batch 150/537: Loss=0.6796 (C:0.6796, R:0.0105)
Batch 175/537: Loss=0.6787 (C:0.6787, R:0.0105)
Batch 200/537: Loss=0.6742 (C:0.6742, R:0.0105)
Batch 225/537: Loss=0.6887 (C:0.6887, R:0.0105)
Batch 250/537: Loss=0.6931 (C:0.6931, R:0.0106)
Batch 275/537: Loss=0.6566 (C:0.6566, R:0.0105)
Batch 300/537: Loss=0.6716 (C:0.6716, R:0.0105)
Batch 325/537: Loss=0.6825 (C:0.6825, R:0.0105)
Batch 350/537: Loss=0.6726 (C:0.6726, R:0.0105)
Batch 375/537: Loss=0.6872 (C:0.6872, R:0.0105)
Batch 400/537: Loss=0.6553 (C:0.6553, R:0.0105)
Batch 425/537: Loss=0.6314 (C:0.6314, R:0.0105)
Batch 450/537: Loss=0.6491 (C:0.6491, R:0.0105)
Batch 475/537: Loss=0.6711 (C:0.6711, R:0.0105)
Batch 500/537: Loss=0.7014 (C:0.7014, R:0.0105)
Batch 525/537: Loss=0.6401 (C:0.6401, R:0.0105)

============================================================
Epoch 43/300 completed in 27.1s
Train: Loss=0.6590 (C:0.6590, R:0.0105) Ratio=4.72x
Val:   Loss=0.7925 (C:0.7925, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.195
No improvement for 3 epochs
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.6535 (C:0.6535, R:0.0105)
Batch  25/537: Loss=0.6728 (C:0.6728, R:0.0105)
Batch  50/537: Loss=0.6712 (C:0.6712, R:0.0105)
Batch  75/537: Loss=0.6830 (C:0.6830, R:0.0105)
Batch 100/537: Loss=0.6874 (C:0.6874, R:0.0105)
Batch 125/537: Loss=0.6261 (C:0.6261, R:0.0105)
Batch 150/537: Loss=0.6504 (C:0.6504, R:0.0105)
Batch 175/537: Loss=0.6668 (C:0.6668, R:0.0105)
Batch 200/537: Loss=0.6322 (C:0.6322, R:0.0105)
Batch 225/537: Loss=0.6395 (C:0.6395, R:0.0105)
Batch 250/537: Loss=0.6462 (C:0.6462, R:0.0105)
Batch 275/537: Loss=0.6291 (C:0.6291, R:0.0105)
Batch 300/537: Loss=0.6538 (C:0.6538, R:0.0106)
Batch 325/537: Loss=0.6725 (C:0.6725, R:0.0105)
Batch 350/537: Loss=0.6955 (C:0.6955, R:0.0105)
Batch 375/537: Loss=0.6605 (C:0.6605, R:0.0106)
Batch 400/537: Loss=0.6756 (C:0.6756, R:0.0105)
Batch 425/537: Loss=0.6679 (C:0.6679, R:0.0105)
Batch 450/537: Loss=0.6529 (C:0.6529, R:0.0105)
Batch 475/537: Loss=0.6529 (C:0.6529, R:0.0105)
Batch 500/537: Loss=0.6693 (C:0.6693, R:0.0105)
Batch 525/537: Loss=0.6561 (C:0.6561, R:0.0105)

============================================================
Epoch 44/300 completed in 21.2s
Train: Loss=0.6575 (C:0.6575, R:0.0105) Ratio=4.74x
Val:   Loss=0.7769 (C:0.7769, R:0.0104) Ratio=3.19x
Reconstruction weight: 0.210
✅ New best model saved (Val Loss: 0.7769)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.6477 (C:0.6477, R:0.0105)
Batch  25/537: Loss=0.6592 (C:0.6592, R:0.0105)
Batch  50/537: Loss=0.6649 (C:0.6649, R:0.0105)
Batch  75/537: Loss=0.6479 (C:0.6479, R:0.0105)
Batch 100/537: Loss=0.6404 (C:0.6404, R:0.0105)
Batch 125/537: Loss=0.6456 (C:0.6456, R:0.0105)
Batch 150/537: Loss=0.6743 (C:0.6743, R:0.0104)
Batch 175/537: Loss=0.6568 (C:0.6568, R:0.0105)
Batch 200/537: Loss=0.6420 (C:0.6420, R:0.0105)
Batch 225/537: Loss=0.6593 (C:0.6593, R:0.0105)
Batch 250/537: Loss=0.7026 (C:0.7026, R:0.0105)
Batch 275/537: Loss=0.6380 (C:0.6380, R:0.0105)
Batch 300/537: Loss=0.6563 (C:0.6563, R:0.0105)
Batch 325/537: Loss=0.6387 (C:0.6387, R:0.0105)
Batch 350/537: Loss=0.6539 (C:0.6539, R:0.0105)
Batch 375/537: Loss=0.6331 (C:0.6331, R:0.0105)
Batch 400/537: Loss=0.6550 (C:0.6550, R:0.0105)
Batch 425/537: Loss=0.6557 (C:0.6557, R:0.0105)
Batch 450/537: Loss=0.6058 (C:0.6058, R:0.0105)
Batch 475/537: Loss=0.6975 (C:0.6975, R:0.0105)
Batch 500/537: Loss=0.6493 (C:0.6493, R:0.0105)
Batch 525/537: Loss=0.6664 (C:0.6664, R:0.0105)

============================================================
Epoch 45/300 completed in 21.2s
Train: Loss=0.6552 (C:0.6552, R:0.0105) Ratio=4.81x
Val:   Loss=0.7767 (C:0.7767, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.225
✅ New best model saved (Val Loss: 0.7767)
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.316 ± 0.554
    Neg distances: 2.356 ± 1.005
    Separation ratio: 7.46x
    Gap: -3.874
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.6350 (C:0.6350, R:0.0105)
Batch  25/537: Loss=0.6312 (C:0.6312, R:0.0105)
Batch  50/537: Loss=0.6397 (C:0.6397, R:0.0105)
Batch  75/537: Loss=0.6431 (C:0.6431, R:0.0105)
Batch 100/537: Loss=0.6152 (C:0.6152, R:0.0105)
Batch 125/537: Loss=0.6103 (C:0.6103, R:0.0105)
Batch 150/537: Loss=0.6490 (C:0.6490, R:0.0105)
Batch 175/537: Loss=0.6409 (C:0.6409, R:0.0105)
Batch 200/537: Loss=0.6419 (C:0.6419, R:0.0105)
Batch 225/537: Loss=0.6275 (C:0.6275, R:0.0105)
Batch 250/537: Loss=0.6116 (C:0.6116, R:0.0105)
Batch 275/537: Loss=0.6226 (C:0.6226, R:0.0105)
Batch 300/537: Loss=0.6376 (C:0.6376, R:0.0105)
Batch 325/537: Loss=0.6142 (C:0.6142, R:0.0105)
Batch 350/537: Loss=0.5979 (C:0.5979, R:0.0105)
Batch 375/537: Loss=0.6777 (C:0.6777, R:0.0105)
Batch 400/537: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 425/537: Loss=0.6572 (C:0.6572, R:0.0105)
Batch 450/537: Loss=0.6190 (C:0.6190, R:0.0105)
Batch 475/537: Loss=0.6550 (C:0.6550, R:0.0105)
Batch 500/537: Loss=0.6250 (C:0.6250, R:0.0105)
Batch 525/537: Loss=0.6674 (C:0.6674, R:0.0105)

============================================================
Epoch 46/300 completed in 26.8s
Train: Loss=0.6356 (C:0.6356, R:0.0105) Ratio=4.82x
Val:   Loss=0.7799 (C:0.7799, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.240
No improvement for 1 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.6360 (C:0.6360, R:0.0105)
Batch  25/537: Loss=0.6069 (C:0.6069, R:0.0105)
Batch  50/537: Loss=0.6243 (C:0.6243, R:0.0105)
Batch  75/537: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 100/537: Loss=0.6404 (C:0.6404, R:0.0105)
Batch 125/537: Loss=0.6191 (C:0.6191, R:0.0105)
Batch 150/537: Loss=0.6495 (C:0.6495, R:0.0105)
Batch 175/537: Loss=0.6267 (C:0.6267, R:0.0105)
Batch 200/537: Loss=0.6510 (C:0.6510, R:0.0105)
Batch 225/537: Loss=0.6354 (C:0.6354, R:0.0105)
Batch 250/537: Loss=0.6127 (C:0.6127, R:0.0105)
Batch 275/537: Loss=0.6395 (C:0.6395, R:0.0105)
Batch 300/537: Loss=0.6525 (C:0.6525, R:0.0105)
Batch 325/537: Loss=0.6073 (C:0.6073, R:0.0105)
Batch 350/537: Loss=0.6086 (C:0.6086, R:0.0105)
Batch 375/537: Loss=0.6225 (C:0.6225, R:0.0105)
Batch 400/537: Loss=0.5939 (C:0.5939, R:0.0105)
Batch 425/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch 450/537: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 475/537: Loss=0.6335 (C:0.6335, R:0.0105)
Batch 500/537: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 525/537: Loss=0.6034 (C:0.6034, R:0.0105)

============================================================
Epoch 47/300 completed in 20.8s
Train: Loss=0.6344 (C:0.6344, R:0.0105) Ratio=4.95x
Val:   Loss=0.7673 (C:0.7673, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.255
✅ New best model saved (Val Loss: 0.7673)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.6325 (C:0.6325, R:0.0105)
Batch  25/537: Loss=0.6022 (C:0.6022, R:0.0105)
Batch  50/537: Loss=0.6181 (C:0.6181, R:0.0105)
Batch  75/537: Loss=0.6796 (C:0.6796, R:0.0105)
Batch 100/537: Loss=0.5931 (C:0.5931, R:0.0105)
Batch 125/537: Loss=0.6009 (C:0.6009, R:0.0105)
Batch 150/537: Loss=0.6365 (C:0.6365, R:0.0105)
Batch 175/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 200/537: Loss=0.6371 (C:0.6371, R:0.0105)
Batch 225/537: Loss=0.6036 (C:0.6036, R:0.0105)
Batch 250/537: Loss=0.6070 (C:0.6070, R:0.0105)
Batch 275/537: Loss=0.6170 (C:0.6170, R:0.0105)
Batch 300/537: Loss=0.6210 (C:0.6210, R:0.0106)
Batch 325/537: Loss=0.6668 (C:0.6668, R:0.0105)
Batch 350/537: Loss=0.6339 (C:0.6339, R:0.0105)
Batch 375/537: Loss=0.6122 (C:0.6122, R:0.0105)
Batch 400/537: Loss=0.6358 (C:0.6358, R:0.0105)
Batch 425/537: Loss=0.6460 (C:0.6460, R:0.0105)
Batch 450/537: Loss=0.6597 (C:0.6597, R:0.0105)
Batch 475/537: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 500/537: Loss=0.6450 (C:0.6450, R:0.0105)
Batch 525/537: Loss=0.6329 (C:0.6329, R:0.0105)

============================================================
Epoch 48/300 completed in 21.0s
Train: Loss=0.6321 (C:0.6321, R:0.0105) Ratio=4.98x
Val:   Loss=0.7699 (C:0.7699, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.270
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.317 ± 0.556
    Neg distances: 2.403 ± 1.021
    Separation ratio: 7.58x
    Gap: -3.987
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.6083 (C:0.6083, R:0.0105)
Batch  25/537: Loss=0.6010 (C:0.6010, R:0.0105)
Batch  50/537: Loss=0.6346 (C:0.6346, R:0.0105)
Batch  75/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 100/537: Loss=0.6056 (C:0.6056, R:0.0105)
Batch 125/537: Loss=0.6589 (C:0.6589, R:0.0105)
Batch 150/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 175/537: Loss=0.5695 (C:0.5695, R:0.0105)
Batch 200/537: Loss=0.6718 (C:0.6718, R:0.0105)
Batch 225/537: Loss=0.6678 (C:0.6678, R:0.0105)
Batch 250/537: Loss=0.6074 (C:0.6074, R:0.0105)
Batch 275/537: Loss=0.6204 (C:0.6204, R:0.0105)
Batch 300/537: Loss=0.6427 (C:0.6427, R:0.0105)
Batch 325/537: Loss=0.6247 (C:0.6247, R:0.0105)
Batch 350/537: Loss=0.6248 (C:0.6248, R:0.0105)
Batch 375/537: Loss=0.6457 (C:0.6457, R:0.0105)
Batch 400/537: Loss=0.6386 (C:0.6386, R:0.0105)
Batch 425/537: Loss=0.6525 (C:0.6525, R:0.0106)
Batch 450/537: Loss=0.6180 (C:0.6180, R:0.0105)
Batch 475/537: Loss=0.6275 (C:0.6275, R:0.0105)
Batch 500/537: Loss=0.6346 (C:0.6346, R:0.0105)
Batch 525/537: Loss=0.6274 (C:0.6274, R:0.0105)

============================================================
Epoch 49/300 completed in 27.1s
Train: Loss=0.6203 (C:0.6203, R:0.0105) Ratio=4.83x
Val:   Loss=0.7627 (C:0.7627, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.7627)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.5946 (C:0.5946, R:0.0105)
Batch  25/537: Loss=0.6053 (C:0.6053, R:0.0105)
Batch  50/537: Loss=0.6109 (C:0.6109, R:0.0105)
Batch  75/537: Loss=0.6202 (C:0.6202, R:0.0105)
Batch 100/537: Loss=0.6028 (C:0.6028, R:0.0105)
Batch 125/537: Loss=0.6086 (C:0.6086, R:0.0105)
Batch 150/537: Loss=0.6462 (C:0.6462, R:0.0105)
Batch 175/537: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 200/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch 225/537: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 250/537: Loss=0.6496 (C:0.6496, R:0.0105)
Batch 275/537: Loss=0.6217 (C:0.6217, R:0.0105)
Batch 300/537: Loss=0.6621 (C:0.6621, R:0.0105)
Batch 325/537: Loss=0.6268 (C:0.6268, R:0.0105)
Batch 350/537: Loss=0.6274 (C:0.6274, R:0.0105)
Batch 375/537: Loss=0.6131 (C:0.6131, R:0.0105)
Batch 400/537: Loss=0.6311 (C:0.6311, R:0.0105)
Batch 425/537: Loss=0.6115 (C:0.6115, R:0.0105)
Batch 450/537: Loss=0.6259 (C:0.6259, R:0.0105)
Batch 475/537: Loss=0.6435 (C:0.6435, R:0.0105)
Batch 500/537: Loss=0.6290 (C:0.6290, R:0.0105)
Batch 525/537: Loss=0.6417 (C:0.6417, R:0.0105)

============================================================
Epoch 50/300 completed in 21.3s
Train: Loss=0.6177 (C:0.6177, R:0.0105) Ratio=4.93x
Val:   Loss=0.7685 (C:0.7685, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.6209 (C:0.6209, R:0.0106)
Batch  25/537: Loss=0.5884 (C:0.5884, R:0.0105)
Batch  50/537: Loss=0.6104 (C:0.6104, R:0.0105)
Batch  75/537: Loss=0.6264 (C:0.6264, R:0.0105)
Batch 100/537: Loss=0.6177 (C:0.6177, R:0.0105)
Batch 125/537: Loss=0.5985 (C:0.5985, R:0.0105)
Batch 150/537: Loss=0.5919 (C:0.5919, R:0.0105)
Batch 175/537: Loss=0.6108 (C:0.6108, R:0.0105)
Batch 200/537: Loss=0.6366 (C:0.6366, R:0.0105)
Batch 225/537: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 250/537: Loss=0.5835 (C:0.5835, R:0.0105)
Batch 275/537: Loss=0.5975 (C:0.5975, R:0.0105)
Batch 300/537: Loss=0.6140 (C:0.6140, R:0.0105)
Batch 325/537: Loss=0.6461 (C:0.6461, R:0.0105)
Batch 350/537: Loss=0.6185 (C:0.6185, R:0.0105)
Batch 375/537: Loss=0.6340 (C:0.6340, R:0.0105)
Batch 400/537: Loss=0.6132 (C:0.6132, R:0.0105)
Batch 425/537: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 450/537: Loss=0.6030 (C:0.6030, R:0.0105)
Batch 475/537: Loss=0.6357 (C:0.6357, R:0.0105)
Batch 500/537: Loss=0.6298 (C:0.6298, R:0.0105)
Batch 525/537: Loss=0.6254 (C:0.6254, R:0.0105)

============================================================
Epoch 51/300 completed in 21.0s
Train: Loss=0.6168 (C:0.6168, R:0.0105) Ratio=4.96x
Val:   Loss=0.7549 (C:0.7549, R:0.0104) Ratio=3.19x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7549)
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.341 ± 0.618
    Neg distances: 2.451 ± 1.053
    Separation ratio: 7.19x
    Gap: -4.095
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.5769 (C:0.5769, R:0.0105)
Batch  25/537: Loss=0.6192 (C:0.6192, R:0.0105)
Batch  50/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch  75/537: Loss=0.6119 (C:0.6119, R:0.0105)
Batch 100/537: Loss=0.6293 (C:0.6293, R:0.0105)
Batch 125/537: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 150/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 175/537: Loss=0.6399 (C:0.6399, R:0.0105)
Batch 200/537: Loss=0.6174 (C:0.6174, R:0.0105)
Batch 225/537: Loss=0.6403 (C:0.6403, R:0.0104)
Batch 250/537: Loss=0.6060 (C:0.6060, R:0.0105)
Batch 275/537: Loss=0.6125 (C:0.6125, R:0.0105)
Batch 300/537: Loss=0.6478 (C:0.6478, R:0.0105)
Batch 325/537: Loss=0.6291 (C:0.6291, R:0.0106)
Batch 350/537: Loss=0.6373 (C:0.6373, R:0.0105)
Batch 375/537: Loss=0.6323 (C:0.6323, R:0.0105)
Batch 400/537: Loss=0.6429 (C:0.6429, R:0.0105)
Batch 425/537: Loss=0.6421 (C:0.6421, R:0.0105)
Batch 450/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 475/537: Loss=0.6547 (C:0.6547, R:0.0105)
Batch 500/537: Loss=0.6651 (C:0.6651, R:0.0105)
Batch 525/537: Loss=0.6478 (C:0.6478, R:0.0105)

============================================================
Epoch 52/300 completed in 26.7s
Train: Loss=0.6206 (C:0.6206, R:0.0105) Ratio=4.88x
Val:   Loss=0.7619 (C:0.7619, R:0.0104) Ratio=3.19x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.6231 (C:0.6231, R:0.0105)
Batch  25/537: Loss=0.6048 (C:0.6048, R:0.0105)
Batch  50/537: Loss=0.5925 (C:0.5925, R:0.0105)
Batch  75/537: Loss=0.6936 (C:0.6936, R:0.0105)
Batch 100/537: Loss=0.6626 (C:0.6626, R:0.0105)
Batch 125/537: Loss=0.6173 (C:0.6173, R:0.0105)
Batch 150/537: Loss=0.6049 (C:0.6049, R:0.0106)
Batch 175/537: Loss=0.6172 (C:0.6172, R:0.0105)
Batch 200/537: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 225/537: Loss=0.6448 (C:0.6448, R:0.0105)
Batch 250/537: Loss=0.6329 (C:0.6329, R:0.0105)
Batch 275/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch 300/537: Loss=0.6093 (C:0.6093, R:0.0105)
Batch 325/537: Loss=0.5893 (C:0.5893, R:0.0105)
Batch 350/537: Loss=0.6683 (C:0.6683, R:0.0106)
Batch 375/537: Loss=0.6388 (C:0.6388, R:0.0105)
Batch 400/537: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 425/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 450/537: Loss=0.6109 (C:0.6109, R:0.0105)
Batch 475/537: Loss=0.6378 (C:0.6378, R:0.0105)
Batch 500/537: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 525/537: Loss=0.6473 (C:0.6473, R:0.0105)

============================================================
Epoch 53/300 completed in 20.9s
Train: Loss=0.6187 (C:0.6187, R:0.0105) Ratio=4.90x
Val:   Loss=0.7502 (C:0.7502, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7502)
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.6161 (C:0.6161, R:0.0105)
Batch  25/537: Loss=0.6023 (C:0.6023, R:0.0105)
Batch  50/537: Loss=0.5766 (C:0.5766, R:0.0105)
Batch  75/537: Loss=0.6347 (C:0.6347, R:0.0105)
Batch 100/537: Loss=0.5954 (C:0.5954, R:0.0105)
Batch 125/537: Loss=0.6095 (C:0.6095, R:0.0105)
Batch 150/537: Loss=0.6202 (C:0.6202, R:0.0105)
Batch 175/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 200/537: Loss=0.6417 (C:0.6417, R:0.0105)
Batch 225/537: Loss=0.6507 (C:0.6507, R:0.0106)
Batch 250/537: Loss=0.6181 (C:0.6181, R:0.0105)
Batch 275/537: Loss=0.6275 (C:0.6275, R:0.0105)
Batch 300/537: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 325/537: Loss=0.6560 (C:0.6560, R:0.0105)
Batch 350/537: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 375/537: Loss=0.6027 (C:0.6027, R:0.0105)
Batch 400/537: Loss=0.6593 (C:0.6593, R:0.0105)
Batch 425/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 450/537: Loss=0.6319 (C:0.6319, R:0.0105)
Batch 475/537: Loss=0.6042 (C:0.6042, R:0.0105)
Batch 500/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 525/537: Loss=0.6361 (C:0.6361, R:0.0105)

============================================================
Epoch 54/300 completed in 21.1s
Train: Loss=0.6183 (C:0.6183, R:0.0105) Ratio=5.02x
Val:   Loss=0.7523 (C:0.7523, R:0.0104) Ratio=3.19x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.317 ± 0.555
    Neg distances: 2.478 ± 1.045
    Separation ratio: 7.81x
    Gap: -4.054
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.5652 (C:0.5652, R:0.0105)
Batch  25/537: Loss=0.5903 (C:0.5903, R:0.0105)
Batch  50/537: Loss=0.5809 (C:0.5809, R:0.0105)
Batch  75/537: Loss=0.5997 (C:0.5997, R:0.0105)
Batch 100/537: Loss=0.5818 (C:0.5818, R:0.0105)
Batch 125/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 150/537: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 175/537: Loss=0.5932 (C:0.5932, R:0.0106)
Batch 200/537: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 225/537: Loss=0.5804 (C:0.5804, R:0.0105)
Batch 250/537: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 275/537: Loss=0.6190 (C:0.6190, R:0.0105)
Batch 300/537: Loss=0.6167 (C:0.6167, R:0.0105)
Batch 325/537: Loss=0.5986 (C:0.5986, R:0.0105)
Batch 350/537: Loss=0.5846 (C:0.5846, R:0.0105)
Batch 375/537: Loss=0.5798 (C:0.5798, R:0.0105)
Batch 400/537: Loss=0.5784 (C:0.5784, R:0.0105)
Batch 425/537: Loss=0.6015 (C:0.6015, R:0.0105)
Batch 450/537: Loss=0.5822 (C:0.5822, R:0.0105)
Batch 475/537: Loss=0.6194 (C:0.6194, R:0.0105)
Batch 500/537: Loss=0.6111 (C:0.6111, R:0.0105)
Batch 525/537: Loss=0.6133 (C:0.6133, R:0.0106)

============================================================
Epoch 55/300 completed in 26.8s
Train: Loss=0.5909 (C:0.5909, R:0.0105) Ratio=5.02x
Val:   Loss=0.7376 (C:0.7376, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7376)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.5650 (C:0.5650, R:0.0105)
Batch  25/537: Loss=0.5712 (C:0.5712, R:0.0105)
Batch  50/537: Loss=0.5702 (C:0.5702, R:0.0105)
Batch  75/537: Loss=0.6161 (C:0.6161, R:0.0105)
Batch 100/537: Loss=0.6099 (C:0.6099, R:0.0105)
Batch 125/537: Loss=0.5612 (C:0.5612, R:0.0105)
Batch 150/537: Loss=0.5838 (C:0.5838, R:0.0105)
Batch 175/537: Loss=0.6054 (C:0.6054, R:0.0105)
Batch 200/537: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 225/537: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 250/537: Loss=0.5920 (C:0.5920, R:0.0105)
Batch 275/537: Loss=0.5791 (C:0.5791, R:0.0105)
Batch 300/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 325/537: Loss=0.5970 (C:0.5970, R:0.0105)
Batch 350/537: Loss=0.6018 (C:0.6018, R:0.0105)
Batch 375/537: Loss=0.5907 (C:0.5907, R:0.0105)
Batch 400/537: Loss=0.5784 (C:0.5784, R:0.0105)
Batch 425/537: Loss=0.5869 (C:0.5869, R:0.0105)
Batch 450/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch 475/537: Loss=0.6157 (C:0.6157, R:0.0105)
Batch 500/537: Loss=0.5862 (C:0.5862, R:0.0105)
Batch 525/537: Loss=0.5866 (C:0.5866, R:0.0105)

============================================================
Epoch 56/300 completed in 21.1s
Train: Loss=0.5898 (C:0.5898, R:0.0105) Ratio=5.09x
Val:   Loss=0.7395 (C:0.7395, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.5792 (C:0.5792, R:0.0105)
Batch  25/537: Loss=0.5823 (C:0.5823, R:0.0105)
Batch  50/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch  75/537: Loss=0.5765 (C:0.5765, R:0.0105)
Batch 100/537: Loss=0.5576 (C:0.5576, R:0.0105)
Batch 125/537: Loss=0.5760 (C:0.5760, R:0.0105)
Batch 150/537: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 175/537: Loss=0.5894 (C:0.5894, R:0.0105)
Batch 200/537: Loss=0.5596 (C:0.5596, R:0.0105)
Batch 225/537: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 250/537: Loss=0.5492 (C:0.5492, R:0.0105)
Batch 275/537: Loss=0.5508 (C:0.5508, R:0.0105)
Batch 300/537: Loss=0.5613 (C:0.5613, R:0.0106)
Batch 325/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 350/537: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 375/537: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 400/537: Loss=0.5704 (C:0.5704, R:0.0105)
Batch 425/537: Loss=0.5998 (C:0.5998, R:0.0105)
Batch 450/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 475/537: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 500/537: Loss=0.6139 (C:0.6139, R:0.0105)
Batch 525/537: Loss=0.6038 (C:0.6038, R:0.0105)

============================================================
Epoch 57/300 completed in 21.0s
Train: Loss=0.5875 (C:0.5875, R:0.0105) Ratio=5.20x
Val:   Loss=0.7359 (C:0.7359, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7359)
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.309 ± 0.555
    Neg distances: 2.534 ± 1.063
    Separation ratio: 8.19x
    Gap: -4.175
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.5969 (C:0.5969, R:0.0105)
Batch  25/537: Loss=0.5975 (C:0.5975, R:0.0106)
Batch  50/537: Loss=0.5737 (C:0.5737, R:0.0105)
Batch  75/537: Loss=0.5456 (C:0.5456, R:0.0105)
Batch 100/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 125/537: Loss=0.5941 (C:0.5941, R:0.0105)
Batch 150/537: Loss=0.5825 (C:0.5825, R:0.0105)
Batch 175/537: Loss=0.5798 (C:0.5798, R:0.0105)
Batch 200/537: Loss=0.5478 (C:0.5478, R:0.0105)
Batch 225/537: Loss=0.5826 (C:0.5826, R:0.0105)
Batch 250/537: Loss=0.5818 (C:0.5818, R:0.0105)
Batch 275/537: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 300/537: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 325/537: Loss=0.5649 (C:0.5649, R:0.0105)
Batch 350/537: Loss=0.5985 (C:0.5985, R:0.0105)
Batch 375/537: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 400/537: Loss=0.5902 (C:0.5902, R:0.0105)
Batch 425/537: Loss=0.5688 (C:0.5688, R:0.0105)
Batch 450/537: Loss=0.5724 (C:0.5724, R:0.0105)
Batch 475/537: Loss=0.5784 (C:0.5784, R:0.0105)
Batch 500/537: Loss=0.5678 (C:0.5678, R:0.0105)
Batch 525/537: Loss=0.5701 (C:0.5701, R:0.0105)

============================================================
Epoch 58/300 completed in 26.7s
Train: Loss=0.5731 (C:0.5731, R:0.0105) Ratio=5.04x
Val:   Loss=0.7259 (C:0.7259, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7259)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.5383 (C:0.5383, R:0.0105)
Batch  25/537: Loss=0.5812 (C:0.5812, R:0.0105)
Batch  50/537: Loss=0.5559 (C:0.5559, R:0.0105)
Batch  75/537: Loss=0.5498 (C:0.5498, R:0.0105)
Batch 100/537: Loss=0.5717 (C:0.5717, R:0.0105)
Batch 125/537: Loss=0.5778 (C:0.5778, R:0.0105)
Batch 150/537: Loss=0.5904 (C:0.5904, R:0.0105)
Batch 175/537: Loss=0.5524 (C:0.5524, R:0.0105)
Batch 200/537: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 225/537: Loss=0.5915 (C:0.5915, R:0.0106)
Batch 250/537: Loss=0.5691 (C:0.5691, R:0.0105)
Batch 275/537: Loss=0.6213 (C:0.6213, R:0.0105)
Batch 300/537: Loss=0.5384 (C:0.5384, R:0.0105)
Batch 325/537: Loss=0.5167 (C:0.5167, R:0.0105)
Batch 350/537: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 375/537: Loss=0.5563 (C:0.5563, R:0.0105)
Batch 400/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 425/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 450/537: Loss=0.5841 (C:0.5841, R:0.0105)
Batch 475/537: Loss=0.5664 (C:0.5664, R:0.0105)
Batch 500/537: Loss=0.5828 (C:0.5828, R:0.0105)
Batch 525/537: Loss=0.5799 (C:0.5799, R:0.0105)

============================================================
Epoch 59/300 completed in 21.0s
Train: Loss=0.5710 (C:0.5710, R:0.0105) Ratio=5.17x
Val:   Loss=0.7213 (C:0.7213, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7213)
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.5835 (C:0.5835, R:0.0105)
Batch  25/537: Loss=0.5841 (C:0.5841, R:0.0105)
Batch  50/537: Loss=0.6028 (C:0.6028, R:0.0105)
Batch  75/537: Loss=0.5657 (C:0.5657, R:0.0105)
Batch 100/537: Loss=0.5830 (C:0.5830, R:0.0105)
Batch 125/537: Loss=0.5519 (C:0.5519, R:0.0105)
Batch 150/537: Loss=0.5540 (C:0.5540, R:0.0106)
Batch 175/537: Loss=0.5419 (C:0.5419, R:0.0105)
Batch 200/537: Loss=0.5911 (C:0.5911, R:0.0105)
Batch 225/537: Loss=0.5681 (C:0.5681, R:0.0105)
Batch 250/537: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 275/537: Loss=0.5896 (C:0.5896, R:0.0105)
Batch 300/537: Loss=0.5706 (C:0.5706, R:0.0105)
Batch 325/537: Loss=0.6374 (C:0.6374, R:0.0105)
Batch 350/537: Loss=0.5726 (C:0.5726, R:0.0105)
Batch 375/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 400/537: Loss=0.5933 (C:0.5933, R:0.0105)
Batch 425/537: Loss=0.6144 (C:0.6144, R:0.0105)
Batch 450/537: Loss=0.5827 (C:0.5827, R:0.0105)
Batch 475/537: Loss=0.5730 (C:0.5730, R:0.0105)
Batch 500/537: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 525/537: Loss=0.5859 (C:0.5859, R:0.0105)

============================================================
Epoch 60/300 completed in 21.1s
Train: Loss=0.5711 (C:0.5711, R:0.0105) Ratio=5.07x
Val:   Loss=0.7324 (C:0.7324, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.326 ± 0.595
    Neg distances: 2.542 ± 1.073
    Separation ratio: 7.79x
    Gap: -4.331
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.5799 (C:0.5799, R:0.0105)
Batch  25/537: Loss=0.5893 (C:0.5893, R:0.0105)
Batch  50/537: Loss=0.5617 (C:0.5617, R:0.0105)
Batch  75/537: Loss=0.5863 (C:0.5863, R:0.0105)
Batch 100/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 125/537: Loss=0.6003 (C:0.6003, R:0.0105)
Batch 150/537: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 175/537: Loss=0.5757 (C:0.5757, R:0.0105)
Batch 200/537: Loss=0.5593 (C:0.5593, R:0.0105)
Batch 225/537: Loss=0.6109 (C:0.6109, R:0.0105)
Batch 250/537: Loss=0.5885 (C:0.5885, R:0.0105)
Batch 275/537: Loss=0.5656 (C:0.5656, R:0.0105)
Batch 300/537: Loss=0.5963 (C:0.5963, R:0.0105)
Batch 325/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 350/537: Loss=0.6106 (C:0.6106, R:0.0105)
Batch 375/537: Loss=0.5605 (C:0.5605, R:0.0105)
Batch 400/537: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 425/537: Loss=0.5838 (C:0.5838, R:0.0105)
Batch 450/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 475/537: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 500/537: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 525/537: Loss=0.5437 (C:0.5437, R:0.0105)

============================================================
Epoch 61/300 completed in 27.3s
Train: Loss=0.5790 (C:0.5790, R:0.0105) Ratio=5.17x
Val:   Loss=0.7292 (C:0.7292, R:0.0104) Ratio=3.18x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.5637 (C:0.5637, R:0.0105)
Batch  25/537: Loss=0.5523 (C:0.5523, R:0.0105)
Batch  50/537: Loss=0.5717 (C:0.5717, R:0.0105)
Batch  75/537: Loss=0.5902 (C:0.5902, R:0.0105)
Batch 100/537: Loss=0.5968 (C:0.5968, R:0.0105)
Batch 125/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 150/537: Loss=0.5763 (C:0.5763, R:0.0105)
Batch 175/537: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 200/537: Loss=0.6007 (C:0.6007, R:0.0105)
Batch 225/537: Loss=0.5668 (C:0.5668, R:0.0106)
Batch 250/537: Loss=0.5576 (C:0.5576, R:0.0105)
Batch 275/537: Loss=0.6143 (C:0.6143, R:0.0105)
Batch 300/537: Loss=0.5563 (C:0.5563, R:0.0105)
Batch 325/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 350/537: Loss=0.5687 (C:0.5687, R:0.0105)
Batch 375/537: Loss=0.5934 (C:0.5934, R:0.0105)
Batch 400/537: Loss=0.5952 (C:0.5952, R:0.0105)
Batch 425/537: Loss=0.6164 (C:0.6164, R:0.0105)
Batch 450/537: Loss=0.5877 (C:0.5877, R:0.0105)
Batch 475/537: Loss=0.5706 (C:0.5706, R:0.0105)
Batch 500/537: Loss=0.6069 (C:0.6069, R:0.0105)
Batch 525/537: Loss=0.5727 (C:0.5727, R:0.0105)

============================================================
Epoch 62/300 completed in 21.1s
Train: Loss=0.5798 (C:0.5798, R:0.0105) Ratio=5.12x
Val:   Loss=0.7400 (C:0.7400, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.5663 (C:0.5663, R:0.0105)
Batch  25/537: Loss=0.5361 (C:0.5361, R:0.0105)
Batch  50/537: Loss=0.6041 (C:0.6041, R:0.0105)
Batch  75/537: Loss=0.6029 (C:0.6029, R:0.0105)
Batch 100/537: Loss=0.5888 (C:0.5888, R:0.0105)
Batch 125/537: Loss=0.5409 (C:0.5409, R:0.0105)
Batch 150/537: Loss=0.5791 (C:0.5791, R:0.0105)
Batch 175/537: Loss=0.5870 (C:0.5870, R:0.0105)
Batch 200/537: Loss=0.5701 (C:0.5701, R:0.0105)
Batch 225/537: Loss=0.5856 (C:0.5856, R:0.0105)
Batch 250/537: Loss=0.5744 (C:0.5744, R:0.0106)
Batch 275/537: Loss=0.6189 (C:0.6189, R:0.0105)
Batch 300/537: Loss=0.6174 (C:0.6174, R:0.0106)
Batch 325/537: Loss=0.5670 (C:0.5670, R:0.0105)
Batch 350/537: Loss=0.5892 (C:0.5892, R:0.0105)
Batch 375/537: Loss=0.5470 (C:0.5470, R:0.0105)
Batch 400/537: Loss=0.6049 (C:0.6049, R:0.0105)
Batch 425/537: Loss=0.5870 (C:0.5870, R:0.0105)
Batch 450/537: Loss=0.5691 (C:0.5691, R:0.0105)
Batch 475/537: Loss=0.5856 (C:0.5856, R:0.0105)
Batch 500/537: Loss=0.5756 (C:0.5756, R:0.0105)
Batch 525/537: Loss=0.5979 (C:0.5979, R:0.0105)

============================================================
Epoch 63/300 completed in 21.1s
Train: Loss=0.5761 (C:0.5761, R:0.0105) Ratio=5.08x
Val:   Loss=0.7375 (C:0.7375, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.306 ± 0.571
    Neg distances: 2.596 ± 1.084
    Separation ratio: 8.48x
    Gap: -4.299
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.5367 (C:0.5367, R:0.0105)
Batch  25/537: Loss=0.5345 (C:0.5345, R:0.0105)
Batch  50/537: Loss=0.5353 (C:0.5353, R:0.0105)
Batch  75/537: Loss=0.5403 (C:0.5403, R:0.0105)
Batch 100/537: Loss=0.5828 (C:0.5828, R:0.0105)
Batch 125/537: Loss=0.5687 (C:0.5687, R:0.0105)
Batch 150/537: Loss=0.5166 (C:0.5166, R:0.0105)
Batch 175/537: Loss=0.5439 (C:0.5439, R:0.0105)
Batch 200/537: Loss=0.5390 (C:0.5390, R:0.0105)
Batch 225/537: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 250/537: Loss=0.5587 (C:0.5587, R:0.0105)
Batch 275/537: Loss=0.5839 (C:0.5839, R:0.0105)
Batch 300/537: Loss=0.5597 (C:0.5597, R:0.0105)
Batch 325/537: Loss=0.5694 (C:0.5694, R:0.0105)
Batch 350/537: Loss=0.5778 (C:0.5778, R:0.0105)
Batch 375/537: Loss=0.6085 (C:0.6085, R:0.0105)
Batch 400/537: Loss=0.5448 (C:0.5448, R:0.0105)
Batch 425/537: Loss=0.5785 (C:0.5785, R:0.0106)
Batch 450/537: Loss=0.5302 (C:0.5302, R:0.0106)
Batch 475/537: Loss=0.5984 (C:0.5984, R:0.0105)
Batch 500/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 525/537: Loss=0.5363 (C:0.5363, R:0.0105)

============================================================
Epoch 64/300 completed in 27.0s
Train: Loss=0.5555 (C:0.5555, R:0.0105) Ratio=5.14x
Val:   Loss=0.7304 (C:0.7304, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.5163 (C:0.5163, R:0.0105)
Batch  25/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch  50/537: Loss=0.5473 (C:0.5473, R:0.0105)
Batch  75/537: Loss=0.5640 (C:0.5640, R:0.0105)
Batch 100/537: Loss=0.5841 (C:0.5841, R:0.0105)
Batch 125/537: Loss=0.5358 (C:0.5358, R:0.0105)
Batch 150/537: Loss=0.5524 (C:0.5524, R:0.0106)
Batch 175/537: Loss=0.5569 (C:0.5569, R:0.0105)
Batch 200/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 225/537: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 250/537: Loss=0.5319 (C:0.5319, R:0.0105)
Batch 275/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 300/537: Loss=0.5727 (C:0.5727, R:0.0105)
Batch 325/537: Loss=0.5730 (C:0.5730, R:0.0105)
Batch 350/537: Loss=0.5441 (C:0.5441, R:0.0105)
Batch 375/537: Loss=0.5682 (C:0.5682, R:0.0105)
Batch 400/537: Loss=0.5595 (C:0.5595, R:0.0105)
Batch 425/537: Loss=0.5944 (C:0.5944, R:0.0106)
Batch 450/537: Loss=0.5568 (C:0.5568, R:0.0105)
Batch 475/537: Loss=0.5343 (C:0.5343, R:0.0105)
Batch 500/537: Loss=0.5799 (C:0.5799, R:0.0105)
Batch 525/537: Loss=0.5530 (C:0.5530, R:0.0105)

============================================================
Epoch 65/300 completed in 21.2s
Train: Loss=0.5545 (C:0.5545, R:0.0105) Ratio=5.27x
Val:   Loss=0.7151 (C:0.7151, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7151)
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.5812 (C:0.5812, R:0.0105)
Batch  25/537: Loss=0.5281 (C:0.5281, R:0.0105)
Batch  50/537: Loss=0.5353 (C:0.5353, R:0.0105)
Batch  75/537: Loss=0.5395 (C:0.5395, R:0.0105)
Batch 100/537: Loss=0.5441 (C:0.5441, R:0.0105)
Batch 125/537: Loss=0.5161 (C:0.5161, R:0.0106)
Batch 150/537: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 175/537: Loss=0.5343 (C:0.5343, R:0.0105)
Batch 200/537: Loss=0.5094 (C:0.5094, R:0.0105)
Batch 225/537: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 250/537: Loss=0.5693 (C:0.5693, R:0.0105)
Batch 275/537: Loss=0.5281 (C:0.5281, R:0.0105)
Batch 300/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 325/537: Loss=0.5993 (C:0.5993, R:0.0105)
Batch 350/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 375/537: Loss=0.5265 (C:0.5265, R:0.0105)
Batch 400/537: Loss=0.5608 (C:0.5608, R:0.0105)
Batch 425/537: Loss=0.5274 (C:0.5274, R:0.0105)
Batch 450/537: Loss=0.5291 (C:0.5291, R:0.0105)
Batch 475/537: Loss=0.5484 (C:0.5484, R:0.0105)
Batch 500/537: Loss=0.5720 (C:0.5720, R:0.0105)
Batch 525/537: Loss=0.5461 (C:0.5461, R:0.0105)

============================================================
Epoch 66/300 completed in 21.2s
Train: Loss=0.5530 (C:0.5530, R:0.0105) Ratio=5.32x
Val:   Loss=0.7146 (C:0.7146, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7146)
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.276 ± 0.559
    Neg distances: 2.633 ± 1.078
    Separation ratio: 9.53x
    Gap: -4.359
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.5534 (C:0.5534, R:0.0105)
Batch  25/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch  50/537: Loss=0.5300 (C:0.5300, R:0.0105)
Batch  75/537: Loss=0.5366 (C:0.5366, R:0.0104)
Batch 100/537: Loss=0.4968 (C:0.4968, R:0.0105)
Batch 125/537: Loss=0.5364 (C:0.5364, R:0.0105)
Batch 150/537: Loss=0.5274 (C:0.5274, R:0.0105)
Batch 175/537: Loss=0.5210 (C:0.5210, R:0.0105)
Batch 200/537: Loss=0.5261 (C:0.5261, R:0.0105)
Batch 225/537: Loss=0.5727 (C:0.5727, R:0.0105)
Batch 250/537: Loss=0.5020 (C:0.5020, R:0.0105)
Batch 275/537: Loss=0.5498 (C:0.5498, R:0.0105)
Batch 300/537: Loss=0.5333 (C:0.5333, R:0.0105)
Batch 325/537: Loss=0.4926 (C:0.4926, R:0.0105)
Batch 350/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 375/537: Loss=0.5287 (C:0.5287, R:0.0105)
Batch 400/537: Loss=0.4954 (C:0.4954, R:0.0105)
Batch 425/537: Loss=0.5272 (C:0.5272, R:0.0105)
Batch 450/537: Loss=0.5386 (C:0.5386, R:0.0106)
Batch 475/537: Loss=0.5092 (C:0.5092, R:0.0105)
Batch 500/537: Loss=0.5392 (C:0.5392, R:0.0105)
Batch 525/537: Loss=0.5299 (C:0.5299, R:0.0105)

============================================================
Epoch 67/300 completed in 27.6s
Train: Loss=0.5238 (C:0.5238, R:0.0105) Ratio=5.19x
Val:   Loss=0.6915 (C:0.6915, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6915)
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch  25/537: Loss=0.5063 (C:0.5063, R:0.0105)
Batch  50/537: Loss=0.5004 (C:0.5004, R:0.0105)
Batch  75/537: Loss=0.5163 (C:0.5163, R:0.0105)
Batch 100/537: Loss=0.4964 (C:0.4964, R:0.0105)
Batch 125/537: Loss=0.5679 (C:0.5679, R:0.0105)
Batch 150/537: Loss=0.4971 (C:0.4971, R:0.0105)
Batch 175/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 200/537: Loss=0.5463 (C:0.5463, R:0.0105)
Batch 225/537: Loss=0.5346 (C:0.5346, R:0.0106)
Batch 250/537: Loss=0.4815 (C:0.4815, R:0.0105)
Batch 275/537: Loss=0.5263 (C:0.5263, R:0.0105)
Batch 300/537: Loss=0.4985 (C:0.4985, R:0.0105)
Batch 325/537: Loss=0.5302 (C:0.5302, R:0.0105)
Batch 350/537: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 375/537: Loss=0.5165 (C:0.5165, R:0.0105)
Batch 400/537: Loss=0.4987 (C:0.4987, R:0.0105)
Batch 425/537: Loss=0.5243 (C:0.5243, R:0.0105)
Batch 450/537: Loss=0.5258 (C:0.5258, R:0.0105)
Batch 475/537: Loss=0.5341 (C:0.5341, R:0.0105)
Batch 500/537: Loss=0.5168 (C:0.5168, R:0.0105)
Batch 525/537: Loss=0.5447 (C:0.5447, R:0.0105)

============================================================
Epoch 68/300 completed in 21.5s
Train: Loss=0.5237 (C:0.5237, R:0.0105) Ratio=5.39x
Val:   Loss=0.6916 (C:0.6916, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.4939 (C:0.4939, R:0.0105)
Batch  25/537: Loss=0.4885 (C:0.4885, R:0.0105)
Batch  50/537: Loss=0.5464 (C:0.5464, R:0.0105)
Batch  75/537: Loss=0.5239 (C:0.5239, R:0.0105)
Batch 100/537: Loss=0.5207 (C:0.5207, R:0.0105)
Batch 125/537: Loss=0.4881 (C:0.4881, R:0.0105)
Batch 150/537: Loss=0.5120 (C:0.5120, R:0.0105)
Batch 175/537: Loss=0.5186 (C:0.5186, R:0.0105)
Batch 200/537: Loss=0.5709 (C:0.5709, R:0.0105)
Batch 225/537: Loss=0.5194 (C:0.5194, R:0.0104)
Batch 250/537: Loss=0.5325 (C:0.5325, R:0.0105)
Batch 275/537: Loss=0.5179 (C:0.5179, R:0.0105)
Batch 300/537: Loss=0.5102 (C:0.5102, R:0.0105)
Batch 325/537: Loss=0.5060 (C:0.5060, R:0.0105)
Batch 350/537: Loss=0.5238 (C:0.5238, R:0.0105)
Batch 375/537: Loss=0.5093 (C:0.5093, R:0.0105)
Batch 400/537: Loss=0.5327 (C:0.5327, R:0.0105)
Batch 425/537: Loss=0.5119 (C:0.5119, R:0.0105)
Batch 450/537: Loss=0.5133 (C:0.5133, R:0.0105)
Batch 475/537: Loss=0.5109 (C:0.5109, R:0.0106)
Batch 500/537: Loss=0.5302 (C:0.5302, R:0.0105)
Batch 525/537: Loss=0.4886 (C:0.4886, R:0.0105)

============================================================
Epoch 69/300 completed in 21.0s
Train: Loss=0.5223 (C:0.5223, R:0.0105) Ratio=5.36x
Val:   Loss=0.7000 (C:0.7000, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.279 ± 0.543
    Neg distances: 2.632 ± 1.082
    Separation ratio: 9.43x
    Gap: -4.319
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.5046 (C:0.5046, R:0.0105)
Batch  25/537: Loss=0.5135 (C:0.5135, R:0.0105)
Batch  50/537: Loss=0.5155 (C:0.5155, R:0.0105)
Batch  75/537: Loss=0.5335 (C:0.5335, R:0.0105)
Batch 100/537: Loss=0.5295 (C:0.5295, R:0.0105)
Batch 125/537: Loss=0.5075 (C:0.5075, R:0.0105)
Batch 150/537: Loss=0.4817 (C:0.4817, R:0.0105)
Batch 175/537: Loss=0.5213 (C:0.5213, R:0.0105)
Batch 200/537: Loss=0.5656 (C:0.5656, R:0.0105)
Batch 225/537: Loss=0.4713 (C:0.4713, R:0.0105)
Batch 250/537: Loss=0.5464 (C:0.5464, R:0.0105)
Batch 275/537: Loss=0.5014 (C:0.5014, R:0.0105)
Batch 300/537: Loss=0.4958 (C:0.4958, R:0.0105)
Batch 325/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 350/537: Loss=0.5255 (C:0.5255, R:0.0105)
Batch 375/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 400/537: Loss=0.5459 (C:0.5459, R:0.0105)
Batch 425/537: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 450/537: Loss=0.4987 (C:0.4987, R:0.0106)
Batch 475/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 500/537: Loss=0.5324 (C:0.5324, R:0.0105)
Batch 525/537: Loss=0.5581 (C:0.5581, R:0.0105)

============================================================
Epoch 70/300 completed in 26.9s
Train: Loss=0.5231 (C:0.5231, R:0.0105) Ratio=5.36x
Val:   Loss=0.6967 (C:0.6967, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.5137 (C:0.5137, R:0.0105)
Batch  25/537: Loss=0.5379 (C:0.5379, R:0.0105)
Batch  50/537: Loss=0.5128 (C:0.5128, R:0.0105)
Batch  75/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 100/537: Loss=0.5239 (C:0.5239, R:0.0105)
Batch 125/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch 150/537: Loss=0.5166 (C:0.5166, R:0.0105)
Batch 175/537: Loss=0.5083 (C:0.5083, R:0.0105)
Batch 200/537: Loss=0.5030 (C:0.5030, R:0.0105)
Batch 225/537: Loss=0.5133 (C:0.5133, R:0.0105)
Batch 250/537: Loss=0.4994 (C:0.4994, R:0.0105)
Batch 275/537: Loss=0.5049 (C:0.5049, R:0.0106)
Batch 300/537: Loss=0.5157 (C:0.5157, R:0.0105)
Batch 325/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 350/537: Loss=0.5021 (C:0.5021, R:0.0106)
Batch 375/537: Loss=0.5217 (C:0.5217, R:0.0105)
Batch 400/537: Loss=0.5119 (C:0.5119, R:0.0105)
Batch 425/537: Loss=0.5368 (C:0.5368, R:0.0105)
Batch 450/537: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 475/537: Loss=0.5110 (C:0.5110, R:0.0105)
Batch 500/537: Loss=0.5180 (C:0.5180, R:0.0105)
Batch 525/537: Loss=0.5120 (C:0.5120, R:0.0105)

============================================================
Epoch 71/300 completed in 21.2s
Train: Loss=0.5206 (C:0.5206, R:0.0105) Ratio=5.36x
Val:   Loss=0.6995 (C:0.6995, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.5205 (C:0.5205, R:0.0105)
Batch  25/537: Loss=0.5281 (C:0.5281, R:0.0105)
Batch  50/537: Loss=0.5244 (C:0.5244, R:0.0105)
Batch  75/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch 100/537: Loss=0.5170 (C:0.5170, R:0.0105)
Batch 125/537: Loss=0.4888 (C:0.4888, R:0.0105)
Batch 150/537: Loss=0.5402 (C:0.5402, R:0.0105)
Batch 175/537: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 200/537: Loss=0.5070 (C:0.5070, R:0.0105)
Batch 225/537: Loss=0.5168 (C:0.5168, R:0.0105)
Batch 250/537: Loss=0.5113 (C:0.5113, R:0.0105)
Batch 275/537: Loss=0.5542 (C:0.5542, R:0.0106)
Batch 300/537: Loss=0.5327 (C:0.5327, R:0.0105)
Batch 325/537: Loss=0.5091 (C:0.5091, R:0.0105)
Batch 350/537: Loss=0.5103 (C:0.5103, R:0.0105)
Batch 375/537: Loss=0.5034 (C:0.5034, R:0.0105)
Batch 400/537: Loss=0.4949 (C:0.4949, R:0.0105)
Batch 425/537: Loss=0.5102 (C:0.5102, R:0.0105)
Batch 450/537: Loss=0.4869 (C:0.4869, R:0.0105)
Batch 475/537: Loss=0.5257 (C:0.5257, R:0.0105)
Batch 500/537: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 525/537: Loss=0.5509 (C:0.5509, R:0.0105)

============================================================
Epoch 72/300 completed in 21.2s
Train: Loss=0.5213 (C:0.5213, R:0.0105) Ratio=5.48x
Val:   Loss=0.6916 (C:0.6916, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.300 ± 0.567
    Neg distances: 2.625 ± 1.088
    Separation ratio: 8.74x
    Gap: -4.293
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=0.5254 (C:0.5254, R:0.0105)
Batch  25/537: Loss=0.5039 (C:0.5039, R:0.0105)
Batch  50/537: Loss=0.5132 (C:0.5132, R:0.0105)
Batch  75/537: Loss=0.5443 (C:0.5443, R:0.0105)
Batch 100/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 125/537: Loss=0.5329 (C:0.5329, R:0.0105)
Batch 150/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch 175/537: Loss=0.5499 (C:0.5499, R:0.0105)
Batch 200/537: Loss=0.5254 (C:0.5254, R:0.0105)
Batch 225/537: Loss=0.5100 (C:0.5100, R:0.0105)
Batch 250/537: Loss=0.5376 (C:0.5376, R:0.0105)
Batch 275/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 300/537: Loss=0.5105 (C:0.5105, R:0.0105)
Batch 325/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 350/537: Loss=0.5557 (C:0.5557, R:0.0105)
Batch 375/537: Loss=0.4971 (C:0.4971, R:0.0106)
Batch 400/537: Loss=0.5719 (C:0.5719, R:0.0105)
Batch 425/537: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 450/537: Loss=0.5355 (C:0.5355, R:0.0105)
Batch 475/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 500/537: Loss=0.4994 (C:0.4994, R:0.0105)
Batch 525/537: Loss=0.5297 (C:0.5297, R:0.0105)

============================================================
Epoch 73/300 completed in 27.4s
Train: Loss=0.5340 (C:0.5340, R:0.0105) Ratio=5.49x
Val:   Loss=0.7123 (C:0.7123, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=0.5270 (C:0.5270, R:0.0105)
Batch  25/537: Loss=0.5392 (C:0.5392, R:0.0105)
Batch  50/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch  75/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 100/537: Loss=0.5565 (C:0.5565, R:0.0105)
Batch 125/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 150/537: Loss=0.4974 (C:0.4974, R:0.0105)
Batch 175/537: Loss=0.5360 (C:0.5360, R:0.0105)
Batch 200/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 225/537: Loss=0.5186 (C:0.5186, R:0.0105)
Batch 250/537: Loss=0.5379 (C:0.5379, R:0.0105)
Batch 275/537: Loss=0.5366 (C:0.5366, R:0.0105)
Batch 300/537: Loss=0.5279 (C:0.5279, R:0.0105)
Batch 325/537: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 350/537: Loss=0.5513 (C:0.5513, R:0.0105)
Batch 375/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 400/537: Loss=0.5336 (C:0.5336, R:0.0105)
Batch 425/537: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 450/537: Loss=0.5358 (C:0.5358, R:0.0105)
Batch 475/537: Loss=0.5236 (C:0.5236, R:0.0106)
Batch 500/537: Loss=0.5625 (C:0.5625, R:0.0105)
Batch 525/537: Loss=0.5180 (C:0.5180, R:0.0105)

============================================================
Epoch 74/300 completed in 21.1s
Train: Loss=0.5327 (C:0.5327, R:0.0105) Ratio=5.50x
Val:   Loss=0.7043 (C:0.7043, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=0.5308 (C:0.5308, R:0.0105)
Batch  25/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch  50/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch  75/537: Loss=0.5178 (C:0.5178, R:0.0105)
Batch 100/537: Loss=0.5028 (C:0.5028, R:0.0105)
Batch 125/537: Loss=0.5503 (C:0.5503, R:0.0105)
Batch 150/537: Loss=0.5088 (C:0.5088, R:0.0105)
Batch 175/537: Loss=0.4909 (C:0.4909, R:0.0105)
Batch 200/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 225/537: Loss=0.5030 (C:0.5030, R:0.0105)
Batch 250/537: Loss=0.5390 (C:0.5390, R:0.0105)
Batch 275/537: Loss=0.5459 (C:0.5459, R:0.0105)
Batch 300/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 325/537: Loss=0.4943 (C:0.4943, R:0.0105)
Batch 350/537: Loss=0.5569 (C:0.5569, R:0.0105)
Batch 375/537: Loss=0.5659 (C:0.5659, R:0.0105)
Batch 400/537: Loss=0.5151 (C:0.5151, R:0.0105)
Batch 425/537: Loss=0.5156 (C:0.5156, R:0.0105)
Batch 450/537: Loss=0.5427 (C:0.5427, R:0.0105)
Batch 475/537: Loss=0.5369 (C:0.5369, R:0.0106)
Batch 500/537: Loss=0.5448 (C:0.5448, R:0.0105)
Batch 525/537: Loss=0.5794 (C:0.5794, R:0.0105)

============================================================
Epoch 75/300 completed in 21.3s
Train: Loss=0.5308 (C:0.5308, R:0.0105) Ratio=5.40x
Val:   Loss=0.6993 (C:0.6993, R:0.0104) Ratio=3.21x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 75 epochs
Best model was at epoch 67 with Val Loss: 0.6915

Global Dataset Training Completed!
Best epoch: 67
Best validation loss: 0.6915
Final separation ratios: Train=5.40x, Val=3.21x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 100])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4615
  Adjusted Rand Score: 0.5286
  Clustering Accuracy: 0.8121
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 100])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 100])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8154
  Per-class F1: [0.8365660313712263, 0.7500801025312401, 0.8608822103732429]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.765 ± 0.936
  Negative distances: 2.344 ± 1.245
  Separation ratio: 3.06x
  Gap: -4.388
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4615
  Clustering Accuracy: 0.8121
  Adjusted Rand Score: 0.5286

Classification Performance:
  Accuracy: 0.8154

Separation Quality:
  Separation Ratio: 3.06x
  Gap: -4.388
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251/results/evaluation_results_20250714_190215.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251/results/evaluation_results_20250714_190215.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1020_20250714_183251/final_results.json

Key Results:
  Separation ratio: 3.06x
  Perfect separation: False
  Classification accuracy: 0.8154
  Result: 0.8154% (improvement: +-80.85%)
  Cleaning up: coarse_lr1e-04_lat100_bs1020_20250714_183251

[6/12] Testing: coarse_lr1e-04_lat100_bs1536
  Learning rate: 0.0001
  Latent dim: 100
  Batch size: 1536
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 19:02:16.032791
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1536
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 356
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 6
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1536
  Balanced sampling: True
  Train batches: 356
  Val batches: 6
  Test batches: 7
Data loading completed!
  Train: 549367 samples, 356 batches
  Val: 9842 samples, 6 batches
  Test: 9824 samples, 7 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 100
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,889,380
Model created with 1,889,380 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,889,380
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.104 ± 0.011
    Neg distances: 0.104 ± 0.011
    Separation ratio: 1.00x
    Gap: -0.140
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/356: Loss=1.9999 (C:1.9999, R:0.0116)
Batch  25/356: Loss=1.9875 (C:1.9875, R:0.0114)
Batch  50/356: Loss=1.9594 (C:1.9594, R:0.0113)
Batch  75/356: Loss=1.9437 (C:1.9437, R:0.0112)
Batch 100/356: Loss=1.9410 (C:1.9410, R:0.0110)
Batch 125/356: Loss=1.9113 (C:1.9113, R:0.0109)
Batch 150/356: Loss=1.9123 (C:1.9123, R:0.0109)
Batch 175/356: Loss=1.8931 (C:1.8931, R:0.0108)
Batch 200/356: Loss=1.8809 (C:1.8809, R:0.0107)
Batch 225/356: Loss=1.8655 (C:1.8655, R:0.0107)
Batch 250/356: Loss=1.8685 (C:1.8685, R:0.0107)
Batch 275/356: Loss=1.8725 (C:1.8725, R:0.0107)
Batch 300/356: Loss=1.8613 (C:1.8613, R:0.0106)
Batch 325/356: Loss=1.8475 (C:1.8475, R:0.0106)
Batch 350/356: Loss=1.8447 (C:1.8447, R:0.0106)

============================================================
Epoch 1/300 completed in 26.9s
Train: Loss=1.9040 (C:1.9040, R:0.0109) Ratio=1.55x
Val:   Loss=1.8357 (C:1.8357, R:0.0105) Ratio=2.07x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8357)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/356: Loss=1.8578 (C:1.8578, R:0.0106)
Batch  25/356: Loss=1.8364 (C:1.8364, R:0.0105)
Batch  50/356: Loss=1.8402 (C:1.8402, R:0.0105)
Batch  75/356: Loss=1.8354 (C:1.8354, R:0.0106)
Batch 100/356: Loss=1.8313 (C:1.8313, R:0.0106)
Batch 125/356: Loss=1.8306 (C:1.8306, R:0.0105)
Batch 150/356: Loss=1.8436 (C:1.8436, R:0.0105)
Batch 175/356: Loss=1.8448 (C:1.8448, R:0.0106)
Batch 200/356: Loss=1.8338 (C:1.8338, R:0.0105)
Batch 225/356: Loss=1.8346 (C:1.8346, R:0.0105)
Batch 250/356: Loss=1.8166 (C:1.8166, R:0.0105)
Batch 275/356: Loss=1.8139 (C:1.8139, R:0.0105)
Batch 300/356: Loss=1.8269 (C:1.8269, R:0.0105)
Batch 325/356: Loss=1.8349 (C:1.8349, R:0.0105)
Batch 350/356: Loss=1.8331 (C:1.8331, R:0.0105)

============================================================
Epoch 2/300 completed in 20.2s
Train: Loss=1.8323 (C:1.8323, R:0.0105) Ratio=2.10x
Val:   Loss=1.8121 (C:1.8121, R:0.0104) Ratio=2.36x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8121)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/356: Loss=1.8305 (C:1.8305, R:0.0105)
Batch  25/356: Loss=1.8181 (C:1.8181, R:0.0105)
Batch  50/356: Loss=1.8282 (C:1.8282, R:0.0105)
Batch  75/356: Loss=1.8211 (C:1.8211, R:0.0105)
Batch 100/356: Loss=1.8083 (C:1.8083, R:0.0105)
Batch 125/356: Loss=1.8233 (C:1.8233, R:0.0105)
Batch 150/356: Loss=1.8117 (C:1.8117, R:0.0105)
Batch 175/356: Loss=1.8000 (C:1.8000, R:0.0105)
Batch 200/356: Loss=1.8029 (C:1.8029, R:0.0105)
Batch 225/356: Loss=1.8045 (C:1.8045, R:0.0105)
Batch 250/356: Loss=1.8160 (C:1.8160, R:0.0105)
Batch 275/356: Loss=1.8090 (C:1.8090, R:0.0105)
Batch 300/356: Loss=1.8087 (C:1.8087, R:0.0105)
Batch 325/356: Loss=1.8149 (C:1.8149, R:0.0105)
Batch 350/356: Loss=1.8234 (C:1.8234, R:0.0105)

============================================================
Epoch 3/300 completed in 20.5s
Train: Loss=1.8141 (C:1.8141, R:0.0105) Ratio=2.36x
Val:   Loss=1.8036 (C:1.8036, R:0.0104) Ratio=2.46x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8036)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.531 ± 0.556
    Neg distances: 1.409 ± 0.806
    Separation ratio: 2.65x
    Gap: -3.053
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/356: Loss=1.2581 (C:1.2581, R:0.0105)
Batch  25/356: Loss=1.2314 (C:1.2314, R:0.0105)
Batch  50/356: Loss=1.2304 (C:1.2304, R:0.0105)
Batch  75/356: Loss=1.2398 (C:1.2398, R:0.0105)
Batch 100/356: Loss=1.2695 (C:1.2695, R:0.0105)
Batch 125/356: Loss=1.2401 (C:1.2401, R:0.0105)
Batch 150/356: Loss=1.2494 (C:1.2494, R:0.0105)
Batch 175/356: Loss=1.2391 (C:1.2391, R:0.0105)
Batch 200/356: Loss=1.2175 (C:1.2175, R:0.0105)
Batch 225/356: Loss=1.2642 (C:1.2642, R:0.0105)
Batch 250/356: Loss=1.2306 (C:1.2306, R:0.0105)
Batch 275/356: Loss=1.2146 (C:1.2146, R:0.0105)
Batch 300/356: Loss=1.2849 (C:1.2849, R:0.0105)
Batch 325/356: Loss=1.2613 (C:1.2613, R:0.0105)
Batch 350/356: Loss=1.2661 (C:1.2661, R:0.0105)

============================================================
Epoch 4/300 completed in 27.5s
Train: Loss=1.2479 (C:1.2479, R:0.0105) Ratio=2.48x
Val:   Loss=1.2393 (C:1.2393, R:0.0104) Ratio=2.54x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2393)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/356: Loss=1.2348 (C:1.2348, R:0.0105)
Batch  25/356: Loss=1.2032 (C:1.2032, R:0.0105)
Batch  50/356: Loss=1.2237 (C:1.2237, R:0.0105)
Batch  75/356: Loss=1.2105 (C:1.2105, R:0.0105)
Batch 100/356: Loss=1.2079 (C:1.2079, R:0.0105)
Batch 125/356: Loss=1.2275 (C:1.2275, R:0.0105)
Batch 150/356: Loss=1.1913 (C:1.1913, R:0.0105)
Batch 175/356: Loss=1.2483 (C:1.2483, R:0.0105)
Batch 200/356: Loss=1.2317 (C:1.2317, R:0.0105)
Batch 225/356: Loss=1.1984 (C:1.1984, R:0.0105)
Batch 250/356: Loss=1.2087 (C:1.2087, R:0.0105)
Batch 275/356: Loss=1.2121 (C:1.2121, R:0.0105)
Batch 300/356: Loss=1.2371 (C:1.2371, R:0.0105)
Batch 325/356: Loss=1.2138 (C:1.2138, R:0.0105)
Batch 350/356: Loss=1.2207 (C:1.2207, R:0.0106)

============================================================
Epoch 5/300 completed in 20.6s
Train: Loss=1.2242 (C:1.2242, R:0.0105) Ratio=2.66x
Val:   Loss=1.2271 (C:1.2271, R:0.0104) Ratio=2.61x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2271)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/356: Loss=1.2058 (C:1.2058, R:0.0105)
Batch  25/356: Loss=1.1904 (C:1.1904, R:0.0105)
Batch  50/356: Loss=1.2028 (C:1.2028, R:0.0105)
Batch  75/356: Loss=1.2103 (C:1.2103, R:0.0105)
Batch 100/356: Loss=1.1979 (C:1.1979, R:0.0105)
Batch 125/356: Loss=1.2183 (C:1.2183, R:0.0105)
Batch 150/356: Loss=1.1995 (C:1.1995, R:0.0105)
Batch 175/356: Loss=1.2129 (C:1.2129, R:0.0105)
Batch 200/356: Loss=1.2233 (C:1.2233, R:0.0105)
Batch 225/356: Loss=1.2046 (C:1.2046, R:0.0105)
Batch 250/356: Loss=1.1961 (C:1.1961, R:0.0105)
Batch 275/356: Loss=1.2037 (C:1.2037, R:0.0105)
Batch 300/356: Loss=1.2094 (C:1.2094, R:0.0105)
Batch 325/356: Loss=1.1943 (C:1.1943, R:0.0105)
Batch 350/356: Loss=1.2154 (C:1.2154, R:0.0105)

============================================================
Epoch 6/300 completed in 20.5s
Train: Loss=1.2093 (C:1.2093, R:0.0105) Ratio=2.80x
Val:   Loss=1.2228 (C:1.2228, R:0.0104) Ratio=2.66x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.2228)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.488 ± 0.552
    Neg distances: 1.542 ± 0.830
    Separation ratio: 3.16x
    Gap: -3.308
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/356: Loss=1.1289 (C:1.1289, R:0.0105)
Batch  25/356: Loss=1.0968 (C:1.0968, R:0.0105)
Batch  50/356: Loss=1.1289 (C:1.1289, R:0.0105)
Batch  75/356: Loss=1.0855 (C:1.0855, R:0.0105)
Batch 100/356: Loss=1.1286 (C:1.1286, R:0.0105)
Batch 125/356: Loss=1.0798 (C:1.0798, R:0.0105)
Batch 150/356: Loss=1.1079 (C:1.1079, R:0.0105)
Batch 175/356: Loss=1.1173 (C:1.1173, R:0.0105)
Batch 200/356: Loss=1.1420 (C:1.1420, R:0.0105)
Batch 225/356: Loss=1.1770 (C:1.1770, R:0.0105)
Batch 250/356: Loss=1.1497 (C:1.1497, R:0.0105)
Batch 275/356: Loss=1.1295 (C:1.1295, R:0.0105)
Batch 300/356: Loss=1.1302 (C:1.1302, R:0.0105)
Batch 325/356: Loss=1.0901 (C:1.0901, R:0.0105)
Batch 350/356: Loss=1.1116 (C:1.1116, R:0.0105)

============================================================
Epoch 7/300 completed in 26.3s
Train: Loss=1.1328 (C:1.1328, R:0.0105) Ratio=2.92x
Val:   Loss=1.1540 (C:1.1540, R:0.0104) Ratio=2.70x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1540)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/356: Loss=1.1286 (C:1.1286, R:0.0105)
Batch  25/356: Loss=1.0968 (C:1.0968, R:0.0105)
Batch  50/356: Loss=1.0940 (C:1.0940, R:0.0105)
Batch  75/356: Loss=1.1242 (C:1.1242, R:0.0105)
Batch 100/356: Loss=1.1254 (C:1.1254, R:0.0105)
Batch 125/356: Loss=1.1051 (C:1.1051, R:0.0105)
Batch 150/356: Loss=1.1308 (C:1.1308, R:0.0105)
Batch 175/356: Loss=1.1429 (C:1.1429, R:0.0105)
Batch 200/356: Loss=1.1238 (C:1.1238, R:0.0105)
Batch 225/356: Loss=1.1221 (C:1.1221, R:0.0105)
Batch 250/356: Loss=1.1285 (C:1.1285, R:0.0105)
Batch 275/356: Loss=1.1297 (C:1.1297, R:0.0105)
Batch 300/356: Loss=1.1234 (C:1.1234, R:0.0105)
Batch 325/356: Loss=1.1291 (C:1.1291, R:0.0105)
Batch 350/356: Loss=1.1667 (C:1.1667, R:0.0105)

============================================================
Epoch 8/300 completed in 20.4s
Train: Loss=1.1217 (C:1.1217, R:0.0105) Ratio=2.97x
Val:   Loss=1.1446 (C:1.1446, R:0.0104) Ratio=2.76x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1446)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/356: Loss=1.0893 (C:1.0893, R:0.0105)
Batch  25/356: Loss=1.1201 (C:1.1201, R:0.0105)
Batch  50/356: Loss=1.0910 (C:1.0910, R:0.0105)
Batch  75/356: Loss=1.0821 (C:1.0821, R:0.0105)
Batch 100/356: Loss=1.0975 (C:1.0975, R:0.0105)
Batch 125/356: Loss=1.0908 (C:1.0908, R:0.0105)
Batch 150/356: Loss=1.1041 (C:1.1041, R:0.0105)
Batch 175/356: Loss=1.1066 (C:1.1066, R:0.0105)
Batch 200/356: Loss=1.0926 (C:1.0926, R:0.0105)
Batch 225/356: Loss=1.0914 (C:1.0914, R:0.0105)
Batch 250/356: Loss=1.1332 (C:1.1332, R:0.0105)
Batch 275/356: Loss=1.1310 (C:1.1310, R:0.0105)
Batch 300/356: Loss=1.0940 (C:1.0940, R:0.0105)
Batch 325/356: Loss=1.1108 (C:1.1108, R:0.0105)
Batch 350/356: Loss=1.0908 (C:1.0908, R:0.0105)

============================================================
Epoch 9/300 completed in 20.6s
Train: Loss=1.1103 (C:1.1103, R:0.0105) Ratio=3.10x
Val:   Loss=1.1502 (C:1.1502, R:0.0104) Ratio=2.80x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.447 ± 0.533
    Neg distances: 1.627 ± 0.841
    Separation ratio: 3.64x
    Gap: -3.199
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/356: Loss=1.0402 (C:1.0402, R:0.0105)
Batch  25/356: Loss=1.0645 (C:1.0645, R:0.0106)
Batch  50/356: Loss=1.0729 (C:1.0729, R:0.0105)
Batch  75/356: Loss=1.0460 (C:1.0460, R:0.0105)
Batch 100/356: Loss=1.0637 (C:1.0637, R:0.0105)
Batch 125/356: Loss=1.0841 (C:1.0841, R:0.0105)
Batch 150/356: Loss=1.0487 (C:1.0487, R:0.0105)
Batch 175/356: Loss=1.0975 (C:1.0975, R:0.0105)
Batch 200/356: Loss=1.0710 (C:1.0710, R:0.0105)
Batch 225/356: Loss=1.0520 (C:1.0520, R:0.0105)
Batch 250/356: Loss=1.0309 (C:1.0309, R:0.0105)
Batch 275/356: Loss=1.0670 (C:1.0670, R:0.0105)
Batch 300/356: Loss=1.0454 (C:1.0454, R:0.0105)
Batch 325/356: Loss=1.0859 (C:1.0859, R:0.0105)
Batch 350/356: Loss=1.0444 (C:1.0444, R:0.0105)

============================================================
Epoch 10/300 completed in 26.9s
Train: Loss=1.0581 (C:1.0581, R:0.0105) Ratio=3.13x
Val:   Loss=1.1059 (C:1.1059, R:0.0104) Ratio=2.81x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1059)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/356: Loss=1.0504 (C:1.0504, R:0.0105)
Batch  25/356: Loss=1.0451 (C:1.0451, R:0.0105)
Batch  50/356: Loss=1.0049 (C:1.0049, R:0.0105)
Batch  75/356: Loss=1.0748 (C:1.0748, R:0.0105)
Batch 100/356: Loss=1.0656 (C:1.0656, R:0.0105)
Batch 125/356: Loss=1.0250 (C:1.0250, R:0.0105)
Batch 150/356: Loss=1.0659 (C:1.0659, R:0.0105)
Batch 175/356: Loss=1.0406 (C:1.0406, R:0.0105)
Batch 200/356: Loss=1.0657 (C:1.0657, R:0.0105)
Batch 225/356: Loss=1.0544 (C:1.0544, R:0.0105)
Batch 250/356: Loss=1.0650 (C:1.0650, R:0.0105)
Batch 275/356: Loss=1.0416 (C:1.0416, R:0.0105)
Batch 300/356: Loss=1.0368 (C:1.0368, R:0.0105)
Batch 325/356: Loss=1.0181 (C:1.0181, R:0.0105)
Batch 350/356: Loss=1.0516 (C:1.0516, R:0.0105)

============================================================
Epoch 11/300 completed in 20.5s
Train: Loss=1.0482 (C:1.0482, R:0.0105) Ratio=3.22x
Val:   Loss=1.0872 (C:1.0872, R:0.0104) Ratio=2.88x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0872)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/356: Loss=1.0602 (C:1.0602, R:0.0105)
Batch  25/356: Loss=1.0174 (C:1.0174, R:0.0105)
Batch  50/356: Loss=1.0803 (C:1.0803, R:0.0106)
Batch  75/356: Loss=1.0292 (C:1.0292, R:0.0105)
Batch 100/356: Loss=1.0465 (C:1.0465, R:0.0105)
Batch 125/356: Loss=1.0310 (C:1.0310, R:0.0105)
Batch 150/356: Loss=1.0657 (C:1.0657, R:0.0105)
Batch 175/356: Loss=1.0437 (C:1.0437, R:0.0105)
Batch 200/356: Loss=1.0457 (C:1.0457, R:0.0105)
Batch 225/356: Loss=1.0406 (C:1.0406, R:0.0105)
Batch 250/356: Loss=1.0482 (C:1.0482, R:0.0105)
Batch 275/356: Loss=1.0426 (C:1.0426, R:0.0105)
Batch 300/356: Loss=1.0042 (C:1.0042, R:0.0105)
Batch 325/356: Loss=1.0593 (C:1.0593, R:0.0105)
Batch 350/356: Loss=1.0218 (C:1.0218, R:0.0105)

============================================================
Epoch 12/300 completed in 20.5s
Train: Loss=1.0405 (C:1.0405, R:0.0105) Ratio=3.37x
Val:   Loss=1.0790 (C:1.0790, R:0.0104) Ratio=2.90x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0790)
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.434 ± 0.556
    Neg distances: 1.720 ± 0.866
    Separation ratio: 3.96x
    Gap: -3.167
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/356: Loss=0.9991 (C:0.9991, R:0.0105)
Batch  25/356: Loss=1.0153 (C:1.0153, R:0.0105)
Batch  50/356: Loss=0.9707 (C:0.9707, R:0.0105)
Batch  75/356: Loss=0.9975 (C:0.9975, R:0.0105)
Batch 100/356: Loss=1.0071 (C:1.0071, R:0.0105)
Batch 125/356: Loss=1.0075 (C:1.0075, R:0.0105)
Batch 150/356: Loss=1.0254 (C:1.0254, R:0.0105)
Batch 175/356: Loss=1.0267 (C:1.0267, R:0.0105)
Batch 200/356: Loss=1.0114 (C:1.0114, R:0.0105)
Batch 225/356: Loss=1.0181 (C:1.0181, R:0.0105)
Batch 250/356: Loss=1.0041 (C:1.0041, R:0.0105)
Batch 275/356: Loss=1.0231 (C:1.0231, R:0.0105)
Batch 300/356: Loss=1.0019 (C:1.0019, R:0.0105)
Batch 325/356: Loss=0.9801 (C:0.9801, R:0.0105)
Batch 350/356: Loss=1.0196 (C:1.0196, R:0.0105)

============================================================
Epoch 13/300 completed in 26.4s
Train: Loss=1.0050 (C:1.0050, R:0.0105) Ratio=3.41x
Val:   Loss=1.0514 (C:1.0514, R:0.0104) Ratio=2.92x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0514)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/356: Loss=0.9763 (C:0.9763, R:0.0105)
Batch  25/356: Loss=1.0016 (C:1.0016, R:0.0105)
Batch  50/356: Loss=0.9634 (C:0.9634, R:0.0105)
Batch  75/356: Loss=0.9887 (C:0.9887, R:0.0105)
Batch 100/356: Loss=0.9907 (C:0.9907, R:0.0105)
Batch 125/356: Loss=0.9919 (C:0.9919, R:0.0105)
Batch 150/356: Loss=0.9861 (C:0.9861, R:0.0105)
Batch 175/356: Loss=1.0170 (C:1.0170, R:0.0106)
Batch 200/356: Loss=1.0125 (C:1.0125, R:0.0105)
Batch 225/356: Loss=1.0095 (C:1.0095, R:0.0105)
Batch 250/356: Loss=1.0040 (C:1.0040, R:0.0105)
Batch 275/356: Loss=0.9574 (C:0.9574, R:0.0105)
Batch 300/356: Loss=0.9981 (C:0.9981, R:0.0106)
Batch 325/356: Loss=1.0061 (C:1.0061, R:0.0105)
Batch 350/356: Loss=1.0137 (C:1.0137, R:0.0105)

============================================================
Epoch 14/300 completed in 20.5s
Train: Loss=0.9972 (C:0.9972, R:0.0105) Ratio=3.51x
Val:   Loss=1.0661 (C:1.0661, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/356: Loss=0.9658 (C:0.9658, R:0.0105)
Batch  25/356: Loss=0.9853 (C:0.9853, R:0.0105)
Batch  50/356: Loss=0.9825 (C:0.9825, R:0.0105)
Batch  75/356: Loss=0.9608 (C:0.9608, R:0.0105)
Batch 100/356: Loss=1.0008 (C:1.0008, R:0.0105)
Batch 125/356: Loss=0.9813 (C:0.9813, R:0.0105)
Batch 150/356: Loss=1.0163 (C:1.0163, R:0.0105)
Batch 175/356: Loss=0.9897 (C:0.9897, R:0.0105)
Batch 200/356: Loss=0.9774 (C:0.9774, R:0.0106)
Batch 225/356: Loss=1.0130 (C:1.0130, R:0.0105)
Batch 250/356: Loss=0.9999 (C:0.9999, R:0.0105)
Batch 275/356: Loss=0.9915 (C:0.9915, R:0.0105)
Batch 300/356: Loss=0.9676 (C:0.9676, R:0.0105)
Batch 325/356: Loss=0.9916 (C:0.9916, R:0.0105)
Batch 350/356: Loss=1.0083 (C:1.0083, R:0.0105)

============================================================
Epoch 15/300 completed in 20.4s
Train: Loss=0.9896 (C:0.9896, R:0.0105) Ratio=3.58x
Val:   Loss=1.0520 (C:1.0520, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.416 ± 0.543
    Neg distances: 1.764 ± 0.873
    Separation ratio: 4.24x
    Gap: -3.114
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/356: Loss=0.9488 (C:0.9488, R:0.0105)
Batch  25/356: Loss=0.9414 (C:0.9414, R:0.0105)
Batch  50/356: Loss=0.9796 (C:0.9796, R:0.0105)
Batch  75/356: Loss=0.9317 (C:0.9317, R:0.0105)
Batch 100/356: Loss=0.9705 (C:0.9705, R:0.0105)
Batch 125/356: Loss=0.9205 (C:0.9205, R:0.0105)
Batch 150/356: Loss=0.9527 (C:0.9527, R:0.0105)
Batch 175/356: Loss=0.9734 (C:0.9734, R:0.0105)
Batch 200/356: Loss=0.9608 (C:0.9608, R:0.0105)
Batch 225/356: Loss=0.9779 (C:0.9779, R:0.0105)
Batch 250/356: Loss=0.9794 (C:0.9794, R:0.0105)
Batch 275/356: Loss=0.9329 (C:0.9329, R:0.0106)
Batch 300/356: Loss=0.9858 (C:0.9858, R:0.0105)
Batch 325/356: Loss=1.0045 (C:1.0045, R:0.0105)
Batch 350/356: Loss=0.9635 (C:0.9635, R:0.0105)

============================================================
Epoch 16/300 completed in 26.7s
Train: Loss=0.9647 (C:0.9647, R:0.0105) Ratio=3.59x
Val:   Loss=1.0344 (C:1.0344, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0344)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/356: Loss=0.9346 (C:0.9346, R:0.0105)
Batch  25/356: Loss=0.9247 (C:0.9247, R:0.0105)
Batch  50/356: Loss=0.9555 (C:0.9555, R:0.0105)
Batch  75/356: Loss=0.9545 (C:0.9545, R:0.0105)
Batch 100/356: Loss=0.9600 (C:0.9600, R:0.0105)
Batch 125/356: Loss=0.9657 (C:0.9657, R:0.0105)
Batch 150/356: Loss=0.9583 (C:0.9583, R:0.0105)
Batch 175/356: Loss=0.9267 (C:0.9267, R:0.0105)
Batch 200/356: Loss=0.9711 (C:0.9711, R:0.0105)
Batch 225/356: Loss=0.9502 (C:0.9502, R:0.0105)
Batch 250/356: Loss=0.9653 (C:0.9653, R:0.0105)
Batch 275/356: Loss=0.9737 (C:0.9737, R:0.0105)
Batch 300/356: Loss=0.9592 (C:0.9592, R:0.0105)
Batch 325/356: Loss=0.9733 (C:0.9733, R:0.0105)
Batch 350/356: Loss=0.9547 (C:0.9547, R:0.0105)

============================================================
Epoch 17/300 completed in 20.9s
Train: Loss=0.9598 (C:0.9598, R:0.0105) Ratio=3.70x
Val:   Loss=1.0319 (C:1.0319, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0319)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/356: Loss=0.9512 (C:0.9512, R:0.0105)
Batch  25/356: Loss=0.9318 (C:0.9318, R:0.0105)
Batch  50/356: Loss=0.9581 (C:0.9581, R:0.0105)
Batch  75/356: Loss=0.9557 (C:0.9557, R:0.0105)
Batch 100/356: Loss=0.9357 (C:0.9357, R:0.0105)
Batch 125/356: Loss=0.9305 (C:0.9305, R:0.0105)
Batch 150/356: Loss=0.9454 (C:0.9454, R:0.0105)
Batch 175/356: Loss=0.9257 (C:0.9257, R:0.0105)
Batch 200/356: Loss=0.9760 (C:0.9760, R:0.0105)
Batch 225/356: Loss=0.9336 (C:0.9336, R:0.0105)
Batch 250/356: Loss=0.9270 (C:0.9270, R:0.0105)
Batch 275/356: Loss=0.9589 (C:0.9589, R:0.0105)
Batch 300/356: Loss=0.9489 (C:0.9489, R:0.0105)
Batch 325/356: Loss=0.9674 (C:0.9674, R:0.0105)
Batch 350/356: Loss=0.9177 (C:0.9177, R:0.0105)

============================================================
Epoch 18/300 completed in 20.9s
Train: Loss=0.9545 (C:0.9545, R:0.0105) Ratio=3.81x
Val:   Loss=1.0254 (C:1.0254, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0254)
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.397 ± 0.547
    Neg distances: 1.843 ± 0.892
    Separation ratio: 4.64x
    Gap: -3.214
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/356: Loss=0.9318 (C:0.9318, R:0.0105)
Batch  25/356: Loss=0.9171 (C:0.9171, R:0.0105)
Batch  50/356: Loss=0.9090 (C:0.9090, R:0.0105)
Batch  75/356: Loss=0.9049 (C:0.9049, R:0.0105)
Batch 100/356: Loss=0.8972 (C:0.8972, R:0.0105)
Batch 125/356: Loss=0.9357 (C:0.9357, R:0.0105)
Batch 150/356: Loss=0.9549 (C:0.9549, R:0.0105)
Batch 175/356: Loss=0.9080 (C:0.9080, R:0.0105)
Batch 200/356: Loss=0.9260 (C:0.9260, R:0.0105)
Batch 225/356: Loss=0.8959 (C:0.8959, R:0.0105)
Batch 250/356: Loss=0.9358 (C:0.9358, R:0.0105)
Batch 275/356: Loss=0.9570 (C:0.9570, R:0.0105)
Batch 300/356: Loss=0.9116 (C:0.9116, R:0.0106)
Batch 325/356: Loss=0.9330 (C:0.9330, R:0.0105)
Batch 350/356: Loss=0.9194 (C:0.9194, R:0.0105)

============================================================
Epoch 19/300 completed in 27.1s
Train: Loss=0.9220 (C:0.9220, R:0.0105) Ratio=3.73x
Val:   Loss=1.0067 (C:1.0067, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0067)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/356: Loss=0.9218 (C:0.9218, R:0.0105)
Batch  25/356: Loss=0.9161 (C:0.9161, R:0.0105)
Batch  50/356: Loss=0.9370 (C:0.9370, R:0.0105)
Batch  75/356: Loss=0.8679 (C:0.8679, R:0.0105)
Batch 100/356: Loss=0.9175 (C:0.9175, R:0.0105)
Batch 125/356: Loss=0.8989 (C:0.8989, R:0.0105)
Batch 150/356: Loss=0.9579 (C:0.9579, R:0.0105)
Batch 175/356: Loss=0.9219 (C:0.9219, R:0.0105)
Batch 200/356: Loss=0.9086 (C:0.9086, R:0.0105)
Batch 225/356: Loss=0.9114 (C:0.9114, R:0.0105)
Batch 250/356: Loss=0.9288 (C:0.9288, R:0.0105)
Batch 275/356: Loss=0.8904 (C:0.8904, R:0.0105)
Batch 300/356: Loss=0.9014 (C:0.9014, R:0.0105)
Batch 325/356: Loss=0.8813 (C:0.8813, R:0.0105)
Batch 350/356: Loss=0.9194 (C:0.9194, R:0.0105)

============================================================
Epoch 20/300 completed in 20.9s
Train: Loss=0.9164 (C:0.9164, R:0.0105) Ratio=3.85x
Val:   Loss=1.0097 (C:1.0097, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/356: Loss=0.8984 (C:0.8984, R:0.0105)
Batch  25/356: Loss=0.8961 (C:0.8961, R:0.0105)
Batch  50/356: Loss=0.9208 (C:0.9208, R:0.0105)
Batch  75/356: Loss=0.8945 (C:0.8945, R:0.0105)
Batch 100/356: Loss=0.9212 (C:0.9212, R:0.0105)
Batch 125/356: Loss=0.9137 (C:0.9137, R:0.0105)
Batch 150/356: Loss=0.8994 (C:0.8994, R:0.0105)
Batch 175/356: Loss=0.9003 (C:0.9003, R:0.0105)
Batch 200/356: Loss=0.9051 (C:0.9051, R:0.0105)
Batch 225/356: Loss=0.9070 (C:0.9070, R:0.0105)
Batch 250/356: Loss=0.9566 (C:0.9566, R:0.0105)
Batch 275/356: Loss=0.8983 (C:0.8983, R:0.0105)
Batch 300/356: Loss=0.9136 (C:0.9136, R:0.0105)
Batch 325/356: Loss=0.9133 (C:0.9133, R:0.0105)
Batch 350/356: Loss=0.8843 (C:0.8843, R:0.0105)

============================================================
Epoch 21/300 completed in 20.9s
Train: Loss=0.9135 (C:0.9135, R:0.0105) Ratio=3.88x
Val:   Loss=1.0005 (C:1.0005, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0005)
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.378 ± 0.553
    Neg distances: 1.911 ± 0.899
    Separation ratio: 5.06x
    Gap: -3.206
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/356: Loss=0.8809 (C:0.8809, R:0.0105)
Batch  25/356: Loss=0.8936 (C:0.8936, R:0.0105)
Batch  50/356: Loss=0.8568 (C:0.8568, R:0.0105)
Batch  75/356: Loss=0.8774 (C:0.8774, R:0.0105)
Batch 100/356: Loss=0.8805 (C:0.8805, R:0.0105)
Batch 125/356: Loss=0.9104 (C:0.9104, R:0.0105)
Batch 150/356: Loss=0.8548 (C:0.8548, R:0.0105)
Batch 175/356: Loss=0.8724 (C:0.8724, R:0.0105)
Batch 200/356: Loss=0.8951 (C:0.8951, R:0.0105)
Batch 225/356: Loss=0.9137 (C:0.9137, R:0.0105)
Batch 250/356: Loss=0.8840 (C:0.8840, R:0.0105)
Batch 275/356: Loss=0.8809 (C:0.8809, R:0.0105)
Batch 300/356: Loss=0.8703 (C:0.8703, R:0.0105)
Batch 325/356: Loss=0.8873 (C:0.8873, R:0.0105)
Batch 350/356: Loss=0.8872 (C:0.8872, R:0.0105)

============================================================
Epoch 22/300 completed in 27.0s
Train: Loss=0.8807 (C:0.8807, R:0.0105) Ratio=3.95x
Val:   Loss=0.9764 (C:0.9764, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9764)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/356: Loss=0.9029 (C:0.9029, R:0.0105)
Batch  25/356: Loss=0.8753 (C:0.8753, R:0.0105)
Batch  50/356: Loss=0.8812 (C:0.8812, R:0.0105)
Batch  75/356: Loss=0.8754 (C:0.8754, R:0.0105)
Batch 100/356: Loss=0.8349 (C:0.8349, R:0.0105)
Batch 125/356: Loss=0.8781 (C:0.8781, R:0.0105)
Batch 150/356: Loss=0.8743 (C:0.8743, R:0.0105)
Batch 175/356: Loss=0.8801 (C:0.8801, R:0.0105)
Batch 200/356: Loss=0.9060 (C:0.9060, R:0.0105)
Batch 225/356: Loss=0.8803 (C:0.8803, R:0.0105)
Batch 250/356: Loss=0.8921 (C:0.8921, R:0.0105)
Batch 275/356: Loss=0.9091 (C:0.9091, R:0.0105)
Batch 300/356: Loss=0.8701 (C:0.8701, R:0.0105)
Batch 325/356: Loss=0.8746 (C:0.8746, R:0.0105)
Batch 350/356: Loss=0.8681 (C:0.8681, R:0.0105)

============================================================
Epoch 23/300 completed in 20.8s
Train: Loss=0.8764 (C:0.8764, R:0.0105) Ratio=3.92x
Val:   Loss=0.9692 (C:0.9692, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9692)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/356: Loss=0.8803 (C:0.8803, R:0.0105)
Batch  25/356: Loss=0.8661 (C:0.8661, R:0.0105)
Batch  50/356: Loss=0.8618 (C:0.8618, R:0.0105)
Batch  75/356: Loss=0.8654 (C:0.8654, R:0.0105)
Batch 100/356: Loss=0.8711 (C:0.8711, R:0.0105)
Batch 125/356: Loss=0.8613 (C:0.8613, R:0.0105)
Batch 150/356: Loss=0.8682 (C:0.8682, R:0.0105)
Batch 175/356: Loss=0.8773 (C:0.8773, R:0.0105)
Batch 200/356: Loss=0.8672 (C:0.8672, R:0.0105)
Batch 225/356: Loss=0.9011 (C:0.9011, R:0.0105)
Batch 250/356: Loss=0.8956 (C:0.8956, R:0.0105)
Batch 275/356: Loss=0.8720 (C:0.8720, R:0.0105)
Batch 300/356: Loss=0.8925 (C:0.8925, R:0.0105)
Batch 325/356: Loss=0.8941 (C:0.8941, R:0.0105)
Batch 350/356: Loss=0.8571 (C:0.8571, R:0.0105)

============================================================
Epoch 24/300 completed in 20.5s
Train: Loss=0.8729 (C:0.8729, R:0.0105) Ratio=3.93x
Val:   Loss=0.9690 (C:0.9690, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9690)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.371 ± 0.527
    Neg distances: 1.972 ± 0.911
    Separation ratio: 5.32x
    Gap: -3.377
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/356: Loss=0.8162 (C:0.8162, R:0.0105)
Batch  25/356: Loss=0.8357 (C:0.8357, R:0.0105)
Batch  50/356: Loss=0.8485 (C:0.8485, R:0.0105)
Batch  75/356: Loss=0.8539 (C:0.8539, R:0.0105)
Batch 100/356: Loss=0.8328 (C:0.8328, R:0.0105)
Batch 125/356: Loss=0.7962 (C:0.7962, R:0.0105)
Batch 150/356: Loss=0.8318 (C:0.8318, R:0.0105)
Batch 175/356: Loss=0.8314 (C:0.8314, R:0.0105)
Batch 200/356: Loss=0.8446 (C:0.8446, R:0.0105)
Batch 225/356: Loss=0.8553 (C:0.8553, R:0.0105)
Batch 250/356: Loss=0.9037 (C:0.9037, R:0.0105)
Batch 275/356: Loss=0.8317 (C:0.8317, R:0.0105)
Batch 300/356: Loss=0.8536 (C:0.8536, R:0.0105)
Batch 325/356: Loss=0.8748 (C:0.8748, R:0.0105)
Batch 350/356: Loss=0.8570 (C:0.8570, R:0.0105)

============================================================
Epoch 25/300 completed in 26.1s
Train: Loss=0.8473 (C:0.8473, R:0.0105) Ratio=4.06x
Val:   Loss=0.9564 (C:0.9564, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9564)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/356: Loss=0.8090 (C:0.8090, R:0.0105)
Batch  25/356: Loss=0.8233 (C:0.8233, R:0.0105)
Batch  50/356: Loss=0.8266 (C:0.8266, R:0.0105)
Batch  75/356: Loss=0.8382 (C:0.8382, R:0.0105)
Batch 100/356: Loss=0.8317 (C:0.8317, R:0.0105)
Batch 125/356: Loss=0.8629 (C:0.8629, R:0.0105)
Batch 150/356: Loss=0.8490 (C:0.8490, R:0.0105)
Batch 175/356: Loss=0.8591 (C:0.8591, R:0.0105)
Batch 200/356: Loss=0.8583 (C:0.8583, R:0.0105)
Batch 225/356: Loss=0.8133 (C:0.8133, R:0.0105)
Batch 250/356: Loss=0.8465 (C:0.8465, R:0.0105)
Batch 275/356: Loss=0.8543 (C:0.8543, R:0.0105)
Batch 300/356: Loss=0.8372 (C:0.8372, R:0.0105)
Batch 325/356: Loss=0.8537 (C:0.8537, R:0.0105)
Batch 350/356: Loss=0.8892 (C:0.8892, R:0.0105)

============================================================
Epoch 26/300 completed in 20.5s
Train: Loss=0.8435 (C:0.8435, R:0.0105) Ratio=4.05x
Val:   Loss=0.9665 (C:0.9665, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/356: Loss=0.8076 (C:0.8076, R:0.0105)
Batch  25/356: Loss=0.8058 (C:0.8058, R:0.0105)
Batch  50/356: Loss=0.8209 (C:0.8209, R:0.0105)
Batch  75/356: Loss=0.8417 (C:0.8417, R:0.0105)
Batch 100/356: Loss=0.8560 (C:0.8560, R:0.0105)
Batch 125/356: Loss=0.8242 (C:0.8242, R:0.0105)
Batch 150/356: Loss=0.8396 (C:0.8396, R:0.0105)
Batch 175/356: Loss=0.8556 (C:0.8556, R:0.0105)
Batch 200/356: Loss=0.8339 (C:0.8339, R:0.0105)
Batch 225/356: Loss=0.8056 (C:0.8056, R:0.0105)
Batch 250/356: Loss=0.8405 (C:0.8405, R:0.0105)
Batch 275/356: Loss=0.8256 (C:0.8256, R:0.0105)
Batch 300/356: Loss=0.8543 (C:0.8543, R:0.0105)
Batch 325/356: Loss=0.8618 (C:0.8618, R:0.0105)
Batch 350/356: Loss=0.8343 (C:0.8343, R:0.0105)

============================================================
Epoch 27/300 completed in 20.3s
Train: Loss=0.8405 (C:0.8405, R:0.0105) Ratio=4.21x
Val:   Loss=0.9465 (C:0.9465, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9465)
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.373 ± 0.533
    Neg distances: 2.026 ± 0.918
    Separation ratio: 5.43x
    Gap: -3.467
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/356: Loss=0.8121 (C:0.8121, R:0.0105)
Batch  25/356: Loss=0.8288 (C:0.8288, R:0.0105)
Batch  50/356: Loss=0.8069 (C:0.8069, R:0.0105)
Batch  75/356: Loss=0.8201 (C:0.8201, R:0.0105)
Batch 100/356: Loss=0.7943 (C:0.7943, R:0.0105)
Batch 125/356: Loss=0.8099 (C:0.8099, R:0.0105)
Batch 150/356: Loss=0.8507 (C:0.8507, R:0.0105)
Batch 175/356: Loss=0.7937 (C:0.7937, R:0.0105)
Batch 200/356: Loss=0.8168 (C:0.8168, R:0.0105)
Batch 225/356: Loss=0.8370 (C:0.8370, R:0.0105)
Batch 250/356: Loss=0.7965 (C:0.7965, R:0.0105)
Batch 275/356: Loss=0.8294 (C:0.8294, R:0.0105)
Batch 300/356: Loss=0.8266 (C:0.8266, R:0.0105)
Batch 325/356: Loss=0.8204 (C:0.8204, R:0.0105)
Batch 350/356: Loss=0.8108 (C:0.8108, R:0.0105)

============================================================
Epoch 28/300 completed in 26.1s
Train: Loss=0.8215 (C:0.8215, R:0.0105) Ratio=4.21x
Val:   Loss=0.9355 (C:0.9355, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9355)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/356: Loss=0.7868 (C:0.7868, R:0.0105)
Batch  25/356: Loss=0.7949 (C:0.7949, R:0.0105)
Batch  50/356: Loss=0.8105 (C:0.8105, R:0.0105)
Batch  75/356: Loss=0.8217 (C:0.8217, R:0.0105)
Batch 100/356: Loss=0.8343 (C:0.8343, R:0.0105)
Batch 125/356: Loss=0.8421 (C:0.8421, R:0.0105)
Batch 150/356: Loss=0.8265 (C:0.8265, R:0.0105)
Batch 175/356: Loss=0.8145 (C:0.8145, R:0.0105)
Batch 200/356: Loss=0.8302 (C:0.8302, R:0.0105)
Batch 225/356: Loss=0.7824 (C:0.7824, R:0.0106)
Batch 250/356: Loss=0.8220 (C:0.8220, R:0.0105)
Batch 275/356: Loss=0.8269 (C:0.8269, R:0.0105)
Batch 300/356: Loss=0.7792 (C:0.7792, R:0.0105)
Batch 325/356: Loss=0.8195 (C:0.8195, R:0.0105)
Batch 350/356: Loss=0.8333 (C:0.8333, R:0.0105)

============================================================
Epoch 29/300 completed in 20.6s
Train: Loss=0.8165 (C:0.8165, R:0.0105) Ratio=4.25x
Val:   Loss=0.9396 (C:0.9396, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/356: Loss=0.7669 (C:0.7669, R:0.0105)
Batch  25/356: Loss=0.8164 (C:0.8164, R:0.0105)
Batch  50/356: Loss=0.8174 (C:0.8174, R:0.0105)
Batch  75/356: Loss=0.8120 (C:0.8120, R:0.0105)
Batch 100/356: Loss=0.7770 (C:0.7770, R:0.0105)
Batch 125/356: Loss=0.8284 (C:0.8284, R:0.0105)
Batch 150/356: Loss=0.8125 (C:0.8125, R:0.0105)
Batch 175/356: Loss=0.8479 (C:0.8479, R:0.0106)
Batch 200/356: Loss=0.7926 (C:0.7926, R:0.0105)
Batch 225/356: Loss=0.7969 (C:0.7969, R:0.0105)
Batch 250/356: Loss=0.7986 (C:0.7986, R:0.0105)
Batch 275/356: Loss=0.8127 (C:0.8127, R:0.0105)
Batch 300/356: Loss=0.7971 (C:0.7971, R:0.0105)
Batch 325/356: Loss=0.8148 (C:0.8148, R:0.0105)
Batch 350/356: Loss=0.8008 (C:0.8008, R:0.0105)

============================================================
Epoch 30/300 completed in 20.3s
Train: Loss=0.8138 (C:0.8138, R:0.0105) Ratio=4.25x
Val:   Loss=0.9384 (C:0.9384, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.353 ± 0.540
    Neg distances: 2.117 ± 0.953
    Separation ratio: 5.99x
    Gap: -3.467
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/356: Loss=0.8175 (C:0.8175, R:0.0105)
Batch  25/356: Loss=0.7654 (C:0.7654, R:0.0105)
Batch  50/356: Loss=0.7875 (C:0.7875, R:0.0105)
Batch  75/356: Loss=0.7969 (C:0.7969, R:0.0105)
Batch 100/356: Loss=0.8120 (C:0.8120, R:0.0105)
Batch 125/356: Loss=0.7580 (C:0.7580, R:0.0105)
Batch 150/356: Loss=0.7705 (C:0.7705, R:0.0105)
Batch 175/356: Loss=0.7771 (C:0.7771, R:0.0105)
Batch 200/356: Loss=0.7768 (C:0.7768, R:0.0105)
Batch 225/356: Loss=0.7997 (C:0.7997, R:0.0105)
Batch 250/356: Loss=0.7843 (C:0.7843, R:0.0105)
Batch 275/356: Loss=0.7638 (C:0.7638, R:0.0105)
Batch 300/356: Loss=0.7969 (C:0.7969, R:0.0105)
Batch 325/356: Loss=0.8063 (C:0.8063, R:0.0105)
Batch 350/356: Loss=0.8015 (C:0.8015, R:0.0105)

============================================================
Epoch 31/300 completed in 26.5s
Train: Loss=0.7852 (C:0.7852, R:0.0105) Ratio=4.23x
Val:   Loss=0.9046 (C:0.9046, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.9046)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/356: Loss=0.7555 (C:0.7555, R:0.0105)
Batch  25/356: Loss=0.7640 (C:0.7640, R:0.0105)
Batch  50/356: Loss=0.7551 (C:0.7551, R:0.0105)
Batch  75/356: Loss=0.7676 (C:0.7676, R:0.0105)
Batch 100/356: Loss=0.7761 (C:0.7761, R:0.0105)
Batch 125/356: Loss=0.7842 (C:0.7842, R:0.0105)
Batch 150/356: Loss=0.7740 (C:0.7740, R:0.0105)
Batch 175/356: Loss=0.7794 (C:0.7794, R:0.0105)
Batch 200/356: Loss=0.7824 (C:0.7824, R:0.0105)
Batch 225/356: Loss=0.7897 (C:0.7897, R:0.0105)
Batch 250/356: Loss=0.7939 (C:0.7939, R:0.0105)
Batch 275/356: Loss=0.7728 (C:0.7728, R:0.0105)
Batch 300/356: Loss=0.8130 (C:0.8130, R:0.0105)
Batch 325/356: Loss=0.7953 (C:0.7953, R:0.0105)
Batch 350/356: Loss=0.7706 (C:0.7706, R:0.0105)

============================================================
Epoch 32/300 completed in 20.6s
Train: Loss=0.7825 (C:0.7825, R:0.0105) Ratio=4.34x
Val:   Loss=0.9009 (C:0.9009, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.030
✅ New best model saved (Val Loss: 0.9009)
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/356: Loss=0.7620 (C:0.7620, R:0.0105)
Batch  25/356: Loss=0.7905 (C:0.7905, R:0.0105)
Batch  50/356: Loss=0.8123 (C:0.8123, R:0.0105)
Batch  75/356: Loss=0.7647 (C:0.7647, R:0.0105)
Batch 100/356: Loss=0.7836 (C:0.7836, R:0.0105)
Batch 125/356: Loss=0.7816 (C:0.7816, R:0.0105)
Batch 150/356: Loss=0.7552 (C:0.7552, R:0.0105)
Batch 175/356: Loss=0.7725 (C:0.7725, R:0.0105)
Batch 200/356: Loss=0.7751 (C:0.7751, R:0.0105)
Batch 225/356: Loss=0.7829 (C:0.7829, R:0.0105)
Batch 250/356: Loss=0.7933 (C:0.7933, R:0.0105)
Batch 275/356: Loss=0.8031 (C:0.8031, R:0.0105)
Batch 300/356: Loss=0.7864 (C:0.7864, R:0.0105)
Batch 325/356: Loss=0.7374 (C:0.7374, R:0.0105)
Batch 350/356: Loss=0.8060 (C:0.8060, R:0.0105)

============================================================
Epoch 33/300 completed in 20.9s
Train: Loss=0.7788 (C:0.7788, R:0.0105) Ratio=4.37x
Val:   Loss=0.9188 (C:0.9188, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.045
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.360 ± 0.554
    Neg distances: 2.157 ± 0.963
    Separation ratio: 5.99x
    Gap: -3.597
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/356: Loss=0.7903 (C:0.7903, R:0.0105)
Batch  25/356: Loss=0.7867 (C:0.7867, R:0.0105)
Batch  50/356: Loss=0.7738 (C:0.7738, R:0.0105)
Batch  75/356: Loss=0.7457 (C:0.7457, R:0.0105)
Batch 100/356: Loss=0.7653 (C:0.7653, R:0.0105)
Batch 125/356: Loss=0.8113 (C:0.8113, R:0.0105)
Batch 150/356: Loss=0.7629 (C:0.7629, R:0.0106)
Batch 175/356: Loss=0.8249 (C:0.8249, R:0.0105)
Batch 200/356: Loss=0.7643 (C:0.7643, R:0.0105)
Batch 225/356: Loss=0.8197 (C:0.8197, R:0.0105)
Batch 250/356: Loss=0.7490 (C:0.7490, R:0.0105)
Batch 275/356: Loss=0.7930 (C:0.7930, R:0.0106)
Batch 300/356: Loss=0.7998 (C:0.7998, R:0.0105)
Batch 325/356: Loss=0.7908 (C:0.7908, R:0.0105)
Batch 350/356: Loss=0.7978 (C:0.7978, R:0.0105)

============================================================
Epoch 34/300 completed in 26.7s
Train: Loss=0.7687 (C:0.7687, R:0.0105) Ratio=4.25x
Val:   Loss=0.9074 (C:0.9074, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.060
No improvement for 2 epochs
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/356: Loss=0.7720 (C:0.7720, R:0.0105)
Batch  25/356: Loss=0.7541 (C:0.7541, R:0.0105)
Batch  50/356: Loss=0.7578 (C:0.7578, R:0.0105)
Batch  75/356: Loss=0.7846 (C:0.7846, R:0.0105)
Batch 100/356: Loss=0.7789 (C:0.7789, R:0.0105)
Batch 125/356: Loss=0.7687 (C:0.7687, R:0.0105)
Batch 150/356: Loss=0.7672 (C:0.7672, R:0.0105)
Batch 175/356: Loss=0.7761 (C:0.7761, R:0.0105)
Batch 200/356: Loss=0.7886 (C:0.7886, R:0.0105)
Batch 225/356: Loss=0.7799 (C:0.7799, R:0.0105)
Batch 250/356: Loss=0.7546 (C:0.7546, R:0.0105)
Batch 275/356: Loss=0.7735 (C:0.7735, R:0.0105)
Batch 300/356: Loss=0.7690 (C:0.7690, R:0.0105)
Batch 325/356: Loss=0.7483 (C:0.7483, R:0.0105)
Batch 350/356: Loss=0.7757 (C:0.7757, R:0.0105)

============================================================
Epoch 35/300 completed in 20.9s
Train: Loss=0.7667 (C:0.7667, R:0.0105) Ratio=4.38x
Val:   Loss=0.9046 (C:0.9046, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.075
No improvement for 3 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/356: Loss=0.7341 (C:0.7341, R:0.0105)
Batch  25/356: Loss=0.7665 (C:0.7665, R:0.0105)
Batch  50/356: Loss=0.7403 (C:0.7403, R:0.0105)
Batch  75/356: Loss=0.7156 (C:0.7156, R:0.0105)
Batch 100/356: Loss=0.7538 (C:0.7538, R:0.0105)
Batch 125/356: Loss=0.7519 (C:0.7519, R:0.0105)
Batch 150/356: Loss=0.7778 (C:0.7778, R:0.0105)
Batch 175/356: Loss=0.7764 (C:0.7764, R:0.0105)
Batch 200/356: Loss=0.7484 (C:0.7484, R:0.0105)
Batch 225/356: Loss=0.7832 (C:0.7832, R:0.0105)
Batch 250/356: Loss=0.7520 (C:0.7520, R:0.0105)
Batch 275/356: Loss=0.7809 (C:0.7809, R:0.0105)
Batch 300/356: Loss=0.7366 (C:0.7366, R:0.0105)
Batch 325/356: Loss=0.7970 (C:0.7970, R:0.0105)
Batch 350/356: Loss=0.7450 (C:0.7450, R:0.0105)

============================================================
Epoch 36/300 completed in 20.7s
Train: Loss=0.7622 (C:0.7622, R:0.0105) Ratio=4.47x
Val:   Loss=0.9003 (C:0.9003, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.090
✅ New best model saved (Val Loss: 0.9003)
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.345 ± 0.552
    Neg distances: 2.219 ± 0.974
    Separation ratio: 6.42x
    Gap: -3.711
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/356: Loss=0.7170 (C:0.7170, R:0.0105)
Batch  25/356: Loss=0.7496 (C:0.7496, R:0.0105)
Batch  50/356: Loss=0.7223 (C:0.7223, R:0.0105)
Batch  75/356: Loss=0.7493 (C:0.7493, R:0.0105)
Batch 100/356: Loss=0.7653 (C:0.7653, R:0.0105)
Batch 125/356: Loss=0.7045 (C:0.7045, R:0.0105)
Batch 150/356: Loss=0.7238 (C:0.7238, R:0.0105)
Batch 175/356: Loss=0.7228 (C:0.7228, R:0.0105)
Batch 200/356: Loss=0.7332 (C:0.7332, R:0.0105)
Batch 225/356: Loss=0.7114 (C:0.7114, R:0.0105)
Batch 250/356: Loss=0.7409 (C:0.7409, R:0.0105)
Batch 275/356: Loss=0.7332 (C:0.7332, R:0.0105)
Batch 300/356: Loss=0.7341 (C:0.7341, R:0.0105)
Batch 325/356: Loss=0.7698 (C:0.7698, R:0.0105)
Batch 350/356: Loss=0.7462 (C:0.7462, R:0.0105)

============================================================
Epoch 37/300 completed in 26.4s
Train: Loss=0.7381 (C:0.7381, R:0.0105) Ratio=4.47x
Val:   Loss=0.8889 (C:0.8889, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8889)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/356: Loss=0.7223 (C:0.7223, R:0.0105)
Batch  25/356: Loss=0.6949 (C:0.6949, R:0.0105)
Batch  50/356: Loss=0.7314 (C:0.7314, R:0.0105)
Batch  75/356: Loss=0.7252 (C:0.7252, R:0.0105)
Batch 100/356: Loss=0.7882 (C:0.7882, R:0.0105)
Batch 125/356: Loss=0.7565 (C:0.7565, R:0.0105)
Batch 150/356: Loss=0.7258 (C:0.7258, R:0.0105)
Batch 175/356: Loss=0.7380 (C:0.7380, R:0.0105)
Batch 200/356: Loss=0.7477 (C:0.7477, R:0.0105)
Batch 225/356: Loss=0.7442 (C:0.7442, R:0.0105)
Batch 250/356: Loss=0.6951 (C:0.6951, R:0.0105)
Batch 275/356: Loss=0.7570 (C:0.7570, R:0.0105)
Batch 300/356: Loss=0.7408 (C:0.7408, R:0.0105)
Batch 325/356: Loss=0.7258 (C:0.7258, R:0.0105)
Batch 350/356: Loss=0.7379 (C:0.7379, R:0.0105)

============================================================
Epoch 38/300 completed in 20.6s
Train: Loss=0.7353 (C:0.7353, R:0.0105) Ratio=4.48x
Val:   Loss=0.8844 (C:0.8844, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.120
✅ New best model saved (Val Loss: 0.8844)
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/356: Loss=0.7290 (C:0.7290, R:0.0105)
Batch  25/356: Loss=0.7092 (C:0.7092, R:0.0105)
Batch  50/356: Loss=0.7538 (C:0.7538, R:0.0105)
Batch  75/356: Loss=0.7310 (C:0.7310, R:0.0105)
Batch 100/356: Loss=0.7108 (C:0.7108, R:0.0105)
Batch 125/356: Loss=0.7218 (C:0.7218, R:0.0105)
Batch 150/356: Loss=0.7273 (C:0.7273, R:0.0105)
Batch 175/356: Loss=0.7241 (C:0.7241, R:0.0105)
Batch 200/356: Loss=0.7372 (C:0.7372, R:0.0105)
Batch 225/356: Loss=0.7135 (C:0.7135, R:0.0105)
Batch 250/356: Loss=0.6987 (C:0.6987, R:0.0105)
Batch 275/356: Loss=0.7370 (C:0.7370, R:0.0105)
Batch 300/356: Loss=0.7761 (C:0.7761, R:0.0105)
Batch 325/356: Loss=0.7293 (C:0.7293, R:0.0105)
Batch 350/356: Loss=0.7180 (C:0.7180, R:0.0105)

============================================================
Epoch 39/300 completed in 20.5s
Train: Loss=0.7324 (C:0.7324, R:0.0105) Ratio=4.56x
Val:   Loss=0.8805 (C:0.8805, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.135
✅ New best model saved (Val Loss: 0.8805)
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.351 ± 0.558
    Neg distances: 2.272 ± 0.992
    Separation ratio: 6.47x
    Gap: -3.823
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/356: Loss=0.7018 (C:0.7018, R:0.0105)
Batch  25/356: Loss=0.6935 (C:0.6935, R:0.0106)
Batch  50/356: Loss=0.7202 (C:0.7202, R:0.0105)
Batch  75/356: Loss=0.7434 (C:0.7434, R:0.0105)
Batch 100/356: Loss=0.6746 (C:0.6746, R:0.0105)
Batch 125/356: Loss=0.7375 (C:0.7375, R:0.0105)
Batch 150/356: Loss=0.7158 (C:0.7158, R:0.0105)
Batch 175/356: Loss=0.7182 (C:0.7182, R:0.0105)
Batch 200/356: Loss=0.7099 (C:0.7099, R:0.0105)
Batch 225/356: Loss=0.7199 (C:0.7199, R:0.0105)
Batch 250/356: Loss=0.7265 (C:0.7265, R:0.0105)
Batch 275/356: Loss=0.7414 (C:0.7414, R:0.0106)
Batch 300/356: Loss=0.6858 (C:0.6858, R:0.0105)
Batch 325/356: Loss=0.7488 (C:0.7488, R:0.0105)
Batch 350/356: Loss=0.7193 (C:0.7193, R:0.0105)

============================================================
Epoch 40/300 completed in 26.3s
Train: Loss=0.7207 (C:0.7207, R:0.0105) Ratio=4.56x
Val:   Loss=0.8773 (C:0.8773, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.8773)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/356: Loss=0.7046 (C:0.7046, R:0.0105)
Batch  25/356: Loss=0.6974 (C:0.6974, R:0.0105)
Batch  50/356: Loss=0.7145 (C:0.7145, R:0.0105)
Batch  75/356: Loss=0.7380 (C:0.7380, R:0.0105)
Batch 100/356: Loss=0.7351 (C:0.7351, R:0.0105)
Batch 125/356: Loss=0.7507 (C:0.7507, R:0.0105)
Batch 150/356: Loss=0.7227 (C:0.7227, R:0.0105)
Batch 175/356: Loss=0.7065 (C:0.7065, R:0.0105)
Batch 200/356: Loss=0.7380 (C:0.7380, R:0.0105)
Batch 225/356: Loss=0.7121 (C:0.7121, R:0.0105)
Batch 250/356: Loss=0.7425 (C:0.7425, R:0.0105)
Batch 275/356: Loss=0.7412 (C:0.7412, R:0.0105)
Batch 300/356: Loss=0.7251 (C:0.7251, R:0.0105)
Batch 325/356: Loss=0.7592 (C:0.7592, R:0.0105)
Batch 350/356: Loss=0.7400 (C:0.7400, R:0.0105)

============================================================
Epoch 41/300 completed in 20.7s
Train: Loss=0.7194 (C:0.7194, R:0.0105) Ratio=4.48x
Val:   Loss=0.8699 (C:0.8699, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.8699)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/356: Loss=0.6922 (C:0.6922, R:0.0105)
Batch  25/356: Loss=0.7485 (C:0.7485, R:0.0105)
Batch  50/356: Loss=0.7078 (C:0.7078, R:0.0105)
Batch  75/356: Loss=0.7079 (C:0.7079, R:0.0105)
Batch 100/356: Loss=0.7255 (C:0.7255, R:0.0105)
Batch 125/356: Loss=0.6970 (C:0.6970, R:0.0105)
Batch 150/356: Loss=0.6802 (C:0.6802, R:0.0105)
Batch 175/356: Loss=0.7313 (C:0.7313, R:0.0105)
Batch 200/356: Loss=0.7260 (C:0.7260, R:0.0105)
Batch 225/356: Loss=0.7244 (C:0.7244, R:0.0105)
Batch 250/356: Loss=0.7309 (C:0.7309, R:0.0105)
Batch 275/356: Loss=0.7058 (C:0.7058, R:0.0105)
Batch 300/356: Loss=0.7255 (C:0.7255, R:0.0105)
Batch 325/356: Loss=0.7489 (C:0.7489, R:0.0105)
Batch 350/356: Loss=0.7412 (C:0.7412, R:0.0105)

============================================================
Epoch 42/300 completed in 20.5s
Train: Loss=0.7148 (C:0.7148, R:0.0105) Ratio=4.56x
Val:   Loss=0.8707 (C:0.8707, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.180
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.342 ± 0.557
    Neg distances: 2.313 ± 1.001
    Separation ratio: 6.76x
    Gap: -3.841
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/356: Loss=0.6978 (C:0.6978, R:0.0105)
Batch  25/356: Loss=0.7241 (C:0.7241, R:0.0105)
Batch  50/356: Loss=0.6785 (C:0.6785, R:0.0105)
Batch  75/356: Loss=0.6675 (C:0.6675, R:0.0105)
Batch 100/356: Loss=0.6923 (C:0.6923, R:0.0105)
Batch 125/356: Loss=0.6836 (C:0.6836, R:0.0105)
Batch 150/356: Loss=0.6751 (C:0.6751, R:0.0105)
Batch 175/356: Loss=0.6735 (C:0.6735, R:0.0105)
Batch 200/356: Loss=0.6983 (C:0.6983, R:0.0105)
Batch 225/356: Loss=0.6835 (C:0.6835, R:0.0105)
Batch 250/356: Loss=0.7069 (C:0.7069, R:0.0105)
Batch 275/356: Loss=0.7137 (C:0.7137, R:0.0105)
Batch 300/356: Loss=0.7041 (C:0.7041, R:0.0105)
Batch 325/356: Loss=0.7038 (C:0.7038, R:0.0105)
Batch 350/356: Loss=0.7407 (C:0.7407, R:0.0105)

============================================================
Epoch 43/300 completed in 26.1s
Train: Loss=0.6960 (C:0.6960, R:0.0105) Ratio=4.62x
Val:   Loss=0.8494 (C:0.8494, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.8494)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/356: Loss=0.6901 (C:0.6901, R:0.0105)
Batch  25/356: Loss=0.6875 (C:0.6875, R:0.0105)
Batch  50/356: Loss=0.6972 (C:0.6972, R:0.0105)
Batch  75/356: Loss=0.6882 (C:0.6882, R:0.0105)
Batch 100/356: Loss=0.6712 (C:0.6712, R:0.0105)
Batch 125/356: Loss=0.6485 (C:0.6485, R:0.0105)
Batch 150/356: Loss=0.6992 (C:0.6992, R:0.0105)
Batch 175/356: Loss=0.7047 (C:0.7047, R:0.0105)
Batch 200/356: Loss=0.6803 (C:0.6803, R:0.0105)
Batch 225/356: Loss=0.7070 (C:0.7070, R:0.0105)
Batch 250/356: Loss=0.6924 (C:0.6924, R:0.0105)
Batch 275/356: Loss=0.7148 (C:0.7148, R:0.0105)
Batch 300/356: Loss=0.6782 (C:0.6782, R:0.0105)
Batch 325/356: Loss=0.7228 (C:0.7228, R:0.0105)
Batch 350/356: Loss=0.7127 (C:0.7127, R:0.0105)

============================================================
Epoch 44/300 completed in 20.4s
Train: Loss=0.6943 (C:0.6943, R:0.0105) Ratio=4.65x
Val:   Loss=0.8615 (C:0.8615, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/356: Loss=0.6890 (C:0.6890, R:0.0105)
Batch  25/356: Loss=0.6857 (C:0.6857, R:0.0105)
Batch  50/356: Loss=0.7142 (C:0.7142, R:0.0105)
Batch  75/356: Loss=0.6867 (C:0.6867, R:0.0105)
Batch 100/356: Loss=0.6927 (C:0.6927, R:0.0105)
Batch 125/356: Loss=0.6877 (C:0.6877, R:0.0105)
Batch 150/356: Loss=0.6820 (C:0.6820, R:0.0105)
Batch 175/356: Loss=0.6717 (C:0.6717, R:0.0105)
Batch 200/356: Loss=0.6931 (C:0.6931, R:0.0105)
Batch 225/356: Loss=0.6872 (C:0.6872, R:0.0105)
Batch 250/356: Loss=0.7025 (C:0.7025, R:0.0105)
Batch 275/356: Loss=0.6897 (C:0.6897, R:0.0105)
Batch 300/356: Loss=0.6952 (C:0.6952, R:0.0105)
Batch 325/356: Loss=0.6966 (C:0.6966, R:0.0105)
Batch 350/356: Loss=0.7022 (C:0.7022, R:0.0105)

============================================================
Epoch 45/300 completed in 20.6s
Train: Loss=0.6921 (C:0.6921, R:0.0105) Ratio=4.64x
Val:   Loss=0.8523 (C:0.8523, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.225
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.336 ± 0.579
    Neg distances: 2.381 ± 1.022
    Separation ratio: 7.09x
    Gap: -3.939
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/356: Loss=0.6523 (C:0.6523, R:0.0105)
Batch  25/356: Loss=0.6753 (C:0.6753, R:0.0105)
Batch  50/356: Loss=0.6741 (C:0.6741, R:0.0105)
Batch  75/356: Loss=0.6558 (C:0.6558, R:0.0105)
Batch 100/356: Loss=0.6699 (C:0.6699, R:0.0105)
Batch 125/356: Loss=0.6334 (C:0.6334, R:0.0105)
Batch 150/356: Loss=0.6758 (C:0.6758, R:0.0105)
Batch 175/356: Loss=0.7221 (C:0.7221, R:0.0105)
Batch 200/356: Loss=0.6530 (C:0.6530, R:0.0105)
Batch 225/356: Loss=0.6524 (C:0.6524, R:0.0105)
Batch 250/356: Loss=0.6720 (C:0.6720, R:0.0105)
Batch 275/356: Loss=0.6596 (C:0.6596, R:0.0105)
Batch 300/356: Loss=0.6657 (C:0.6657, R:0.0105)
Batch 325/356: Loss=0.6562 (C:0.6562, R:0.0105)
Batch 350/356: Loss=0.6666 (C:0.6666, R:0.0105)

============================================================
Epoch 46/300 completed in 26.3s
Train: Loss=0.6746 (C:0.6746, R:0.0105) Ratio=4.79x
Val:   Loss=0.8482 (C:0.8482, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.240
✅ New best model saved (Val Loss: 0.8482)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/356: Loss=0.6773 (C:0.6773, R:0.0106)
Batch  25/356: Loss=0.6827 (C:0.6827, R:0.0105)
Batch  50/356: Loss=0.6723 (C:0.6723, R:0.0105)
Batch  75/356: Loss=0.6636 (C:0.6636, R:0.0105)
Batch 100/356: Loss=0.6767 (C:0.6767, R:0.0105)
Batch 125/356: Loss=0.6714 (C:0.6714, R:0.0105)
Batch 150/356: Loss=0.6704 (C:0.6704, R:0.0105)
Batch 175/356: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 200/356: Loss=0.6635 (C:0.6635, R:0.0105)
Batch 225/356: Loss=0.6843 (C:0.6843, R:0.0105)
Batch 250/356: Loss=0.7238 (C:0.7238, R:0.0105)
Batch 275/356: Loss=0.6734 (C:0.6734, R:0.0105)
Batch 300/356: Loss=0.6917 (C:0.6917, R:0.0105)
Batch 325/356: Loss=0.6798 (C:0.6798, R:0.0105)
Batch 350/356: Loss=0.6559 (C:0.6559, R:0.0105)

============================================================
Epoch 47/300 completed in 20.3s
Train: Loss=0.6720 (C:0.6720, R:0.0105) Ratio=4.70x
Val:   Loss=0.8437 (C:0.8437, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.255
✅ New best model saved (Val Loss: 0.8437)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/356: Loss=0.6786 (C:0.6786, R:0.0105)
Batch  25/356: Loss=0.6621 (C:0.6621, R:0.0105)
Batch  50/356: Loss=0.6645 (C:0.6645, R:0.0105)
Batch  75/356: Loss=0.6665 (C:0.6665, R:0.0105)
Batch 100/356: Loss=0.6676 (C:0.6676, R:0.0105)
Batch 125/356: Loss=0.7064 (C:0.7064, R:0.0105)
Batch 150/356: Loss=0.6851 (C:0.6851, R:0.0105)
Batch 175/356: Loss=0.6707 (C:0.6707, R:0.0105)
Batch 200/356: Loss=0.6534 (C:0.6534, R:0.0105)
Batch 225/356: Loss=0.6503 (C:0.6503, R:0.0105)
Batch 250/356: Loss=0.6157 (C:0.6157, R:0.0105)
Batch 275/356: Loss=0.6602 (C:0.6602, R:0.0105)
Batch 300/356: Loss=0.6931 (C:0.6931, R:0.0105)
Batch 325/356: Loss=0.7120 (C:0.7120, R:0.0105)
Batch 350/356: Loss=0.6891 (C:0.6891, R:0.0105)

============================================================
Epoch 48/300 completed in 20.5s
Train: Loss=0.6689 (C:0.6689, R:0.0105) Ratio=4.74x
Val:   Loss=0.8450 (C:0.8450, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.270
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.344 ± 0.569
    Neg distances: 2.404 ± 1.037
    Separation ratio: 7.00x
    Gap: -4.005
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/356: Loss=0.6614 (C:0.6614, R:0.0105)
Batch  25/356: Loss=0.6643 (C:0.6643, R:0.0105)
Batch  50/356: Loss=0.6583 (C:0.6583, R:0.0105)
Batch  75/356: Loss=0.6493 (C:0.6493, R:0.0105)
Batch 100/356: Loss=0.6604 (C:0.6604, R:0.0105)
Batch 125/356: Loss=0.6576 (C:0.6576, R:0.0105)
Batch 150/356: Loss=0.6510 (C:0.6510, R:0.0105)
Batch 175/356: Loss=0.6932 (C:0.6932, R:0.0105)
Batch 200/356: Loss=0.6758 (C:0.6758, R:0.0105)
Batch 225/356: Loss=0.7056 (C:0.7056, R:0.0105)
Batch 250/356: Loss=0.7038 (C:0.7038, R:0.0105)
Batch 275/356: Loss=0.7051 (C:0.7051, R:0.0105)
Batch 300/356: Loss=0.6668 (C:0.6668, R:0.0105)
Batch 325/356: Loss=0.6758 (C:0.6758, R:0.0105)
Batch 350/356: Loss=0.6739 (C:0.6739, R:0.0105)

============================================================
Epoch 49/300 completed in 26.5s
Train: Loss=0.6659 (C:0.6659, R:0.0105) Ratio=4.69x
Val:   Loss=0.8424 (C:0.8424, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.8424)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/356: Loss=0.6447 (C:0.6447, R:0.0105)
Batch  25/356: Loss=0.6619 (C:0.6619, R:0.0105)
Batch  50/356: Loss=0.6561 (C:0.6561, R:0.0105)
Batch  75/356: Loss=0.6550 (C:0.6550, R:0.0105)
Batch 100/356: Loss=0.6424 (C:0.6424, R:0.0105)
Batch 125/356: Loss=0.7011 (C:0.7011, R:0.0105)
Batch 150/356: Loss=0.6720 (C:0.6720, R:0.0105)
Batch 175/356: Loss=0.6679 (C:0.6679, R:0.0105)
Batch 200/356: Loss=0.6624 (C:0.6624, R:0.0105)
Batch 225/356: Loss=0.6932 (C:0.6932, R:0.0105)
Batch 250/356: Loss=0.6511 (C:0.6511, R:0.0105)
Batch 275/356: Loss=0.6547 (C:0.6547, R:0.0105)
Batch 300/356: Loss=0.6975 (C:0.6975, R:0.0105)
Batch 325/356: Loss=0.6752 (C:0.6752, R:0.0105)
Batch 350/356: Loss=0.6485 (C:0.6485, R:0.0105)

============================================================
Epoch 50/300 completed in 20.4s
Train: Loss=0.6644 (C:0.6644, R:0.0105) Ratio=4.82x
Val:   Loss=0.8337 (C:0.8337, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8337)
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/356: Loss=0.6456 (C:0.6456, R:0.0105)
Batch  25/356: Loss=0.6693 (C:0.6693, R:0.0105)
Batch  50/356: Loss=0.6756 (C:0.6756, R:0.0105)
Batch  75/356: Loss=0.6453 (C:0.6453, R:0.0105)
Batch 100/356: Loss=0.6748 (C:0.6748, R:0.0105)
Batch 125/356: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 150/356: Loss=0.6576 (C:0.6576, R:0.0105)
Batch 175/356: Loss=0.6513 (C:0.6513, R:0.0105)
Batch 200/356: Loss=0.6812 (C:0.6812, R:0.0105)
Batch 225/356: Loss=0.6819 (C:0.6819, R:0.0105)
Batch 250/356: Loss=0.6431 (C:0.6431, R:0.0105)
Batch 275/356: Loss=0.6873 (C:0.6873, R:0.0105)
Batch 300/356: Loss=0.6487 (C:0.6487, R:0.0105)
Batch 325/356: Loss=0.6829 (C:0.6829, R:0.0105)
Batch 350/356: Loss=0.6801 (C:0.6801, R:0.0105)

============================================================
Epoch 51/300 completed in 20.5s
Train: Loss=0.6615 (C:0.6615, R:0.0105) Ratio=4.79x
Val:   Loss=0.8374 (C:0.8374, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.331 ± 0.584
    Neg distances: 2.483 ± 1.056
    Separation ratio: 7.50x
    Gap: -4.351
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/356: Loss=0.6218 (C:0.6218, R:0.0105)
Batch  25/356: Loss=0.6239 (C:0.6239, R:0.0105)
Batch  50/356: Loss=0.6484 (C:0.6484, R:0.0105)
Batch  75/356: Loss=0.6486 (C:0.6486, R:0.0105)
Batch 100/356: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 125/356: Loss=0.6798 (C:0.6798, R:0.0105)
Batch 150/356: Loss=0.6305 (C:0.6305, R:0.0105)
Batch 175/356: Loss=0.6712 (C:0.6712, R:0.0105)
Batch 200/356: Loss=0.6699 (C:0.6699, R:0.0105)
Batch 225/356: Loss=0.6296 (C:0.6296, R:0.0105)
Batch 250/356: Loss=0.6594 (C:0.6594, R:0.0105)
Batch 275/356: Loss=0.6511 (C:0.6511, R:0.0105)
Batch 300/356: Loss=0.6544 (C:0.6544, R:0.0105)
Batch 325/356: Loss=0.6531 (C:0.6531, R:0.0105)
Batch 350/356: Loss=0.6421 (C:0.6421, R:0.0106)

============================================================
Epoch 52/300 completed in 26.4s
Train: Loss=0.6420 (C:0.6420, R:0.0105) Ratio=4.78x
Val:   Loss=0.8064 (C:0.8064, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8064)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/356: Loss=0.6084 (C:0.6084, R:0.0105)
Batch  25/356: Loss=0.6338 (C:0.6338, R:0.0105)
Batch  50/356: Loss=0.6141 (C:0.6141, R:0.0105)
Batch  75/356: Loss=0.6371 (C:0.6371, R:0.0105)
Batch 100/356: Loss=0.6506 (C:0.6506, R:0.0105)
Batch 125/356: Loss=0.6304 (C:0.6304, R:0.0105)
Batch 150/356: Loss=0.6459 (C:0.6459, R:0.0105)
Batch 175/356: Loss=0.6612 (C:0.6612, R:0.0105)
Batch 200/356: Loss=0.6246 (C:0.6246, R:0.0105)
Batch 225/356: Loss=0.6535 (C:0.6535, R:0.0105)
Batch 250/356: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 275/356: Loss=0.6531 (C:0.6531, R:0.0106)
Batch 300/356: Loss=0.6357 (C:0.6357, R:0.0106)
Batch 325/356: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 350/356: Loss=0.6359 (C:0.6359, R:0.0105)

============================================================
Epoch 53/300 completed in 20.3s
Train: Loss=0.6394 (C:0.6394, R:0.0105) Ratio=4.88x
Val:   Loss=0.8183 (C:0.8183, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/356: Loss=0.6076 (C:0.6076, R:0.0105)
Batch  25/356: Loss=0.6321 (C:0.6321, R:0.0105)
Batch  50/356: Loss=0.6139 (C:0.6139, R:0.0105)
Batch  75/356: Loss=0.6159 (C:0.6159, R:0.0105)
Batch 100/356: Loss=0.6487 (C:0.6487, R:0.0105)
Batch 125/356: Loss=0.6255 (C:0.6255, R:0.0105)
Batch 150/356: Loss=0.6026 (C:0.6026, R:0.0105)
Batch 175/356: Loss=0.6256 (C:0.6256, R:0.0105)
Batch 200/356: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 225/356: Loss=0.6020 (C:0.6020, R:0.0105)
Batch 250/356: Loss=0.6546 (C:0.6546, R:0.0105)
Batch 275/356: Loss=0.6362 (C:0.6362, R:0.0105)
Batch 300/356: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 325/356: Loss=0.6615 (C:0.6615, R:0.0105)
Batch 350/356: Loss=0.6305 (C:0.6305, R:0.0105)

============================================================
Epoch 54/300 completed in 20.4s
Train: Loss=0.6356 (C:0.6356, R:0.0105) Ratio=4.91x
Val:   Loss=0.8280 (C:0.8280, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.348 ± 0.614
    Neg distances: 2.486 ± 1.060
    Separation ratio: 7.15x
    Gap: -4.135
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/356: Loss=0.6415 (C:0.6415, R:0.0106)
Batch  25/356: Loss=0.6340 (C:0.6340, R:0.0105)
Batch  50/356: Loss=0.6319 (C:0.6319, R:0.0105)
Batch  75/356: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 100/356: Loss=0.6605 (C:0.6605, R:0.0105)
Batch 125/356: Loss=0.6350 (C:0.6350, R:0.0105)
Batch 150/356: Loss=0.6650 (C:0.6650, R:0.0105)
Batch 175/356: Loss=0.6454 (C:0.6454, R:0.0105)
Batch 200/356: Loss=0.6612 (C:0.6612, R:0.0105)
Batch 225/356: Loss=0.6399 (C:0.6399, R:0.0105)
Batch 250/356: Loss=0.6719 (C:0.6719, R:0.0105)
Batch 275/356: Loss=0.6652 (C:0.6652, R:0.0105)
Batch 300/356: Loss=0.6596 (C:0.6596, R:0.0105)
Batch 325/356: Loss=0.6658 (C:0.6658, R:0.0105)
Batch 350/356: Loss=0.6396 (C:0.6396, R:0.0105)

============================================================
Epoch 55/300 completed in 26.7s
Train: Loss=0.6390 (C:0.6390, R:0.0105) Ratio=4.89x
Val:   Loss=0.8275 (C:0.8275, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/356: Loss=0.6507 (C:0.6507, R:0.0105)
Batch  25/356: Loss=0.6572 (C:0.6572, R:0.0105)
Batch  50/356: Loss=0.6066 (C:0.6066, R:0.0105)
Batch  75/356: Loss=0.6492 (C:0.6492, R:0.0105)
Batch 100/356: Loss=0.6380 (C:0.6380, R:0.0105)
Batch 125/356: Loss=0.6157 (C:0.6157, R:0.0105)
Batch 150/356: Loss=0.6495 (C:0.6495, R:0.0105)
Batch 175/356: Loss=0.6554 (C:0.6554, R:0.0105)
Batch 200/356: Loss=0.6095 (C:0.6095, R:0.0105)
Batch 225/356: Loss=0.6378 (C:0.6378, R:0.0105)
Batch 250/356: Loss=0.6278 (C:0.6278, R:0.0105)
Batch 275/356: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 300/356: Loss=0.6484 (C:0.6484, R:0.0105)
Batch 325/356: Loss=0.6856 (C:0.6856, R:0.0105)
Batch 350/356: Loss=0.6529 (C:0.6529, R:0.0105)

============================================================
Epoch 56/300 completed in 20.7s
Train: Loss=0.6402 (C:0.6402, R:0.0105) Ratio=4.92x
Val:   Loss=0.8216 (C:0.8216, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/356: Loss=0.6370 (C:0.6370, R:0.0105)
Batch  25/356: Loss=0.6297 (C:0.6297, R:0.0105)
Batch  50/356: Loss=0.6454 (C:0.6454, R:0.0105)
Batch  75/356: Loss=0.6434 (C:0.6434, R:0.0105)
Batch 100/356: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 125/356: Loss=0.6193 (C:0.6193, R:0.0105)
Batch 150/356: Loss=0.6204 (C:0.6204, R:0.0105)
Batch 175/356: Loss=0.6306 (C:0.6306, R:0.0105)
Batch 200/356: Loss=0.6390 (C:0.6390, R:0.0105)
Batch 225/356: Loss=0.6530 (C:0.6530, R:0.0105)
Batch 250/356: Loss=0.6392 (C:0.6392, R:0.0105)
Batch 275/356: Loss=0.6482 (C:0.6482, R:0.0105)
Batch 300/356: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 325/356: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 350/356: Loss=0.6222 (C:0.6222, R:0.0105)

============================================================
Epoch 57/300 completed in 21.0s
Train: Loss=0.6364 (C:0.6364, R:0.0105) Ratio=4.98x
Val:   Loss=0.8218 (C:0.8218, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.310 ± 0.539
    Neg distances: 2.546 ± 1.060
    Separation ratio: 8.22x
    Gap: -4.200
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/356: Loss=0.5707 (C:0.5707, R:0.0105)
Batch  25/356: Loss=0.5803 (C:0.5803, R:0.0105)
Batch  50/356: Loss=0.6236 (C:0.6236, R:0.0105)
Batch  75/356: Loss=0.5785 (C:0.5785, R:0.0105)
Batch 100/356: Loss=0.5976 (C:0.5976, R:0.0105)
Batch 125/356: Loss=0.6050 (C:0.6050, R:0.0105)
Batch 150/356: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 175/356: Loss=0.5947 (C:0.5947, R:0.0105)
Batch 200/356: Loss=0.6263 (C:0.6263, R:0.0105)
Batch 225/356: Loss=0.6194 (C:0.6194, R:0.0105)
Batch 250/356: Loss=0.5978 (C:0.5978, R:0.0105)
Batch 275/356: Loss=0.5902 (C:0.5902, R:0.0105)
Batch 300/356: Loss=0.6167 (C:0.6167, R:0.0105)
Batch 325/356: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 350/356: Loss=0.6435 (C:0.6435, R:0.0105)

============================================================
Epoch 58/300 completed in 26.2s
Train: Loss=0.6067 (C:0.6067, R:0.0105) Ratio=5.05x
Val:   Loss=0.8003 (C:0.8003, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8003)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/356: Loss=0.5847 (C:0.5847, R:0.0105)
Batch  25/356: Loss=0.6022 (C:0.6022, R:0.0105)
Batch  50/356: Loss=0.5908 (C:0.5908, R:0.0105)
Batch  75/356: Loss=0.6196 (C:0.6196, R:0.0105)
Batch 100/356: Loss=0.5900 (C:0.5900, R:0.0105)
Batch 125/356: Loss=0.6036 (C:0.6036, R:0.0105)
Batch 150/356: Loss=0.6197 (C:0.6197, R:0.0106)
Batch 175/356: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 200/356: Loss=0.6082 (C:0.6082, R:0.0105)
Batch 225/356: Loss=0.6023 (C:0.6023, R:0.0105)
Batch 250/356: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 275/356: Loss=0.5895 (C:0.5895, R:0.0105)
Batch 300/356: Loss=0.6240 (C:0.6240, R:0.0105)
Batch 325/356: Loss=0.6093 (C:0.6093, R:0.0105)
Batch 350/356: Loss=0.6315 (C:0.6315, R:0.0105)

============================================================
Epoch 59/300 completed in 20.4s
Train: Loss=0.6028 (C:0.6028, R:0.0105) Ratio=5.01x
Val:   Loss=0.7951 (C:0.7951, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7951)
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/356: Loss=0.5702 (C:0.5702, R:0.0105)
Batch  25/356: Loss=0.5919 (C:0.5919, R:0.0105)
Batch  50/356: Loss=0.6212 (C:0.6212, R:0.0105)
Batch  75/356: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 100/356: Loss=0.6131 (C:0.6131, R:0.0105)
Batch 125/356: Loss=0.5995 (C:0.5995, R:0.0105)
Batch 150/356: Loss=0.5978 (C:0.5978, R:0.0105)
Batch 175/356: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 200/356: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 225/356: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 250/356: Loss=0.6105 (C:0.6105, R:0.0105)
Batch 275/356: Loss=0.6156 (C:0.6156, R:0.0105)
Batch 300/356: Loss=0.5720 (C:0.5720, R:0.0105)
Batch 325/356: Loss=0.6351 (C:0.6351, R:0.0105)
Batch 350/356: Loss=0.6007 (C:0.6007, R:0.0105)

============================================================
Epoch 60/300 completed in 20.4s
Train: Loss=0.6027 (C:0.6027, R:0.0105) Ratio=5.05x
Val:   Loss=0.8030 (C:0.8030, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.336 ± 0.590
    Neg distances: 2.559 ± 1.084
    Separation ratio: 7.62x
    Gap: -4.237
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/356: Loss=0.6137 (C:0.6137, R:0.0105)
Batch  25/356: Loss=0.6028 (C:0.6028, R:0.0105)
Batch  50/356: Loss=0.6012 (C:0.6012, R:0.0105)
Batch  75/356: Loss=0.6119 (C:0.6119, R:0.0105)
Batch 100/356: Loss=0.6204 (C:0.6204, R:0.0105)
Batch 125/356: Loss=0.6062 (C:0.6062, R:0.0105)
Batch 150/356: Loss=0.6118 (C:0.6118, R:0.0105)
Batch 175/356: Loss=0.6166 (C:0.6166, R:0.0105)
Batch 200/356: Loss=0.6044 (C:0.6044, R:0.0105)
Batch 225/356: Loss=0.5965 (C:0.5965, R:0.0105)
Batch 250/356: Loss=0.6052 (C:0.6052, R:0.0105)
Batch 275/356: Loss=0.5918 (C:0.5918, R:0.0105)
Batch 300/356: Loss=0.6138 (C:0.6138, R:0.0105)
Batch 325/356: Loss=0.6282 (C:0.6282, R:0.0105)
Batch 350/356: Loss=0.6284 (C:0.6284, R:0.0105)

============================================================
Epoch 61/300 completed in 27.1s
Train: Loss=0.6129 (C:0.6129, R:0.0105) Ratio=5.10x
Val:   Loss=0.8018 (C:0.8018, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/356: Loss=0.6003 (C:0.6003, R:0.0105)
Batch  25/356: Loss=0.5943 (C:0.5943, R:0.0105)
Batch  50/356: Loss=0.6119 (C:0.6119, R:0.0105)
Batch  75/356: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 100/356: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 125/356: Loss=0.6491 (C:0.6491, R:0.0105)
Batch 150/356: Loss=0.5893 (C:0.5893, R:0.0105)
Batch 175/356: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 200/356: Loss=0.5792 (C:0.5792, R:0.0105)
Batch 225/356: Loss=0.6372 (C:0.6372, R:0.0105)
Batch 250/356: Loss=0.6242 (C:0.6242, R:0.0105)
Batch 275/356: Loss=0.6212 (C:0.6212, R:0.0105)
Batch 300/356: Loss=0.6069 (C:0.6069, R:0.0105)
Batch 325/356: Loss=0.6076 (C:0.6076, R:0.0105)
Batch 350/356: Loss=0.6007 (C:0.6007, R:0.0105)

============================================================
Epoch 62/300 completed in 20.9s
Train: Loss=0.6117 (C:0.6117, R:0.0105) Ratio=5.04x
Val:   Loss=0.8136 (C:0.8136, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/356: Loss=0.5892 (C:0.5892, R:0.0105)
Batch  25/356: Loss=0.5860 (C:0.5860, R:0.0105)
Batch  50/356: Loss=0.6009 (C:0.6009, R:0.0105)
Batch  75/356: Loss=0.5934 (C:0.5934, R:0.0105)
Batch 100/356: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 125/356: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 150/356: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 175/356: Loss=0.6095 (C:0.6095, R:0.0105)
Batch 200/356: Loss=0.6401 (C:0.6401, R:0.0105)
Batch 225/356: Loss=0.6017 (C:0.6017, R:0.0105)
Batch 250/356: Loss=0.6036 (C:0.6036, R:0.0105)
Batch 275/356: Loss=0.6327 (C:0.6327, R:0.0105)
Batch 300/356: Loss=0.6095 (C:0.6095, R:0.0105)
Batch 325/356: Loss=0.6253 (C:0.6253, R:0.0105)
Batch 350/356: Loss=0.6582 (C:0.6582, R:0.0105)

============================================================
Epoch 63/300 completed in 20.8s
Train: Loss=0.6095 (C:0.6095, R:0.0105) Ratio=5.12x
Val:   Loss=0.8177 (C:0.8177, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.343 ± 0.615
    Neg distances: 2.592 ± 1.096
    Separation ratio: 7.56x
    Gap: -4.293
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/356: Loss=0.6042 (C:0.6042, R:0.0105)
Batch  25/356: Loss=0.5865 (C:0.5865, R:0.0105)
Batch  50/356: Loss=0.5713 (C:0.5713, R:0.0105)
Batch  75/356: Loss=0.5948 (C:0.5948, R:0.0105)
Batch 100/356: Loss=0.6399 (C:0.6399, R:0.0105)
Batch 125/356: Loss=0.5998 (C:0.5998, R:0.0105)
Batch 150/356: Loss=0.6385 (C:0.6385, R:0.0105)
Batch 175/356: Loss=0.6102 (C:0.6102, R:0.0105)
Batch 200/356: Loss=0.5964 (C:0.5964, R:0.0105)
Batch 225/356: Loss=0.6137 (C:0.6137, R:0.0105)
Batch 250/356: Loss=0.5981 (C:0.5981, R:0.0105)
Batch 275/356: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 300/356: Loss=0.5889 (C:0.5889, R:0.0105)
Batch 325/356: Loss=0.6336 (C:0.6336, R:0.0105)
Batch 350/356: Loss=0.6080 (C:0.6080, R:0.0105)

============================================================
Epoch 64/300 completed in 26.3s
Train: Loss=0.6100 (C:0.6100, R:0.0105) Ratio=5.13x
Val:   Loss=0.8141 (C:0.8141, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/356: Loss=0.5896 (C:0.5896, R:0.0105)
Batch  25/356: Loss=0.6134 (C:0.6134, R:0.0105)
Batch  50/356: Loss=0.6311 (C:0.6311, R:0.0105)
Batch  75/356: Loss=0.6106 (C:0.6106, R:0.0105)
Batch 100/356: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 125/356: Loss=0.6034 (C:0.6034, R:0.0105)
Batch 150/356: Loss=0.6315 (C:0.6315, R:0.0105)
Batch 175/356: Loss=0.6311 (C:0.6311, R:0.0105)
Batch 200/356: Loss=0.5892 (C:0.5892, R:0.0105)
Batch 225/356: Loss=0.5785 (C:0.5785, R:0.0105)
Batch 250/356: Loss=0.6006 (C:0.6006, R:0.0105)
Batch 275/356: Loss=0.6376 (C:0.6376, R:0.0105)
Batch 300/356: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 325/356: Loss=0.5927 (C:0.5927, R:0.0105)
Batch 350/356: Loss=0.6066 (C:0.6066, R:0.0105)

============================================================
Epoch 65/300 completed in 20.9s
Train: Loss=0.6088 (C:0.6088, R:0.0105) Ratio=5.17x
Val:   Loss=0.8218 (C:0.8218, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/356: Loss=0.5743 (C:0.5743, R:0.0105)
Batch  25/356: Loss=0.6125 (C:0.6125, R:0.0105)
Batch  50/356: Loss=0.5903 (C:0.5903, R:0.0105)
Batch  75/356: Loss=0.6084 (C:0.6084, R:0.0105)
Batch 100/356: Loss=0.5850 (C:0.5850, R:0.0105)
Batch 125/356: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 150/356: Loss=0.6129 (C:0.6129, R:0.0105)
Batch 175/356: Loss=0.6205 (C:0.6205, R:0.0105)
Batch 200/356: Loss=0.6008 (C:0.6008, R:0.0105)
Batch 225/356: Loss=0.6333 (C:0.6333, R:0.0105)
Batch 250/356: Loss=0.6172 (C:0.6172, R:0.0105)
Batch 275/356: Loss=0.6095 (C:0.6095, R:0.0105)
Batch 300/356: Loss=0.6460 (C:0.6460, R:0.0105)
Batch 325/356: Loss=0.6682 (C:0.6682, R:0.0105)
Batch 350/356: Loss=0.6269 (C:0.6269, R:0.0105)

============================================================
Epoch 66/300 completed in 21.0s
Train: Loss=0.6076 (C:0.6076, R:0.0105) Ratio=5.16x
Val:   Loss=0.8120 (C:0.8120, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.312 ± 0.571
    Neg distances: 2.615 ± 1.087
    Separation ratio: 8.37x
    Gap: -4.408
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/356: Loss=0.5589 (C:0.5589, R:0.0105)
Batch  25/356: Loss=0.5793 (C:0.5793, R:0.0105)
Batch  50/356: Loss=0.5500 (C:0.5500, R:0.0105)
Batch  75/356: Loss=0.5512 (C:0.5512, R:0.0106)
Batch 100/356: Loss=0.5734 (C:0.5734, R:0.0105)
Batch 125/356: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 150/356: Loss=0.5809 (C:0.5809, R:0.0105)
Batch 175/356: Loss=0.5897 (C:0.5897, R:0.0105)
Batch 200/356: Loss=0.5757 (C:0.5757, R:0.0105)
Batch 225/356: Loss=0.5983 (C:0.5983, R:0.0105)
Batch 250/356: Loss=0.5624 (C:0.5624, R:0.0106)
Batch 275/356: Loss=0.5910 (C:0.5910, R:0.0105)
Batch 300/356: Loss=0.5646 (C:0.5646, R:0.0105)
Batch 325/356: Loss=0.5694 (C:0.5694, R:0.0105)
Batch 350/356: Loss=0.5798 (C:0.5798, R:0.0105)

============================================================
Epoch 67/300 completed in 27.1s
Train: Loss=0.5839 (C:0.5839, R:0.0105) Ratio=5.30x
Val:   Loss=0.7926 (C:0.7926, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7926)
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/356: Loss=0.5828 (C:0.5828, R:0.0105)
Batch  25/356: Loss=0.5466 (C:0.5466, R:0.0105)
Batch  50/356: Loss=0.6053 (C:0.6053, R:0.0105)
Batch  75/356: Loss=0.5652 (C:0.5652, R:0.0105)
Batch 100/356: Loss=0.5831 (C:0.5831, R:0.0105)
Batch 125/356: Loss=0.5878 (C:0.5878, R:0.0105)
Batch 150/356: Loss=0.6086 (C:0.6086, R:0.0105)
Batch 175/356: Loss=0.5851 (C:0.5851, R:0.0106)
Batch 200/356: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 225/356: Loss=0.5779 (C:0.5779, R:0.0105)
Batch 250/356: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 275/356: Loss=0.5885 (C:0.5885, R:0.0105)
Batch 300/356: Loss=0.6142 (C:0.6142, R:0.0105)
Batch 325/356: Loss=0.6180 (C:0.6180, R:0.0105)
Batch 350/356: Loss=0.5826 (C:0.5826, R:0.0105)

============================================================
Epoch 68/300 completed in 20.7s
Train: Loss=0.5827 (C:0.5827, R:0.0105) Ratio=5.13x
Val:   Loss=0.7813 (C:0.7813, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7813)
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/356: Loss=0.5960 (C:0.5960, R:0.0105)
Batch  25/356: Loss=0.5681 (C:0.5681, R:0.0105)
Batch  50/356: Loss=0.5302 (C:0.5302, R:0.0105)
Batch  75/356: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 100/356: Loss=0.5469 (C:0.5469, R:0.0105)
Batch 125/356: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 150/356: Loss=0.6041 (C:0.6041, R:0.0105)
Batch 175/356: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 200/356: Loss=0.5973 (C:0.5973, R:0.0105)
Batch 225/356: Loss=0.5677 (C:0.5677, R:0.0105)
Batch 250/356: Loss=0.5789 (C:0.5789, R:0.0105)
Batch 275/356: Loss=0.5697 (C:0.5697, R:0.0105)
Batch 300/356: Loss=0.5914 (C:0.5914, R:0.0105)
Batch 325/356: Loss=0.6021 (C:0.6021, R:0.0105)
Batch 350/356: Loss=0.5738 (C:0.5738, R:0.0105)

============================================================
Epoch 69/300 completed in 20.8s
Train: Loss=0.5822 (C:0.5822, R:0.0105) Ratio=5.27x
Val:   Loss=0.7953 (C:0.7953, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.309 ± 0.586
    Neg distances: 2.625 ± 1.097
    Separation ratio: 8.49x
    Gap: -4.350
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/356: Loss=0.5506 (C:0.5506, R:0.0105)
Batch  25/356: Loss=0.5665 (C:0.5665, R:0.0105)
Batch  50/356: Loss=0.5956 (C:0.5956, R:0.0105)
Batch  75/356: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 100/356: Loss=0.6040 (C:0.6040, R:0.0105)
Batch 125/356: Loss=0.5836 (C:0.5836, R:0.0105)
Batch 150/356: Loss=0.5761 (C:0.5761, R:0.0105)
Batch 175/356: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 200/356: Loss=0.5832 (C:0.5832, R:0.0105)
Batch 225/356: Loss=0.5774 (C:0.5774, R:0.0105)
Batch 250/356: Loss=0.5820 (C:0.5820, R:0.0105)
Batch 275/356: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 300/356: Loss=0.5719 (C:0.5719, R:0.0106)
Batch 325/356: Loss=0.5805 (C:0.5805, R:0.0105)
Batch 350/356: Loss=0.5851 (C:0.5851, R:0.0105)

============================================================
Epoch 70/300 completed in 27.4s
Train: Loss=0.5790 (C:0.5790, R:0.0105) Ratio=5.31x
Val:   Loss=0.7958 (C:0.7958, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/356: Loss=0.5710 (C:0.5710, R:0.0105)
Batch  25/356: Loss=0.5943 (C:0.5943, R:0.0105)
Batch  50/356: Loss=0.5877 (C:0.5877, R:0.0105)
Batch  75/356: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 100/356: Loss=0.6074 (C:0.6074, R:0.0105)
Batch 125/356: Loss=0.5755 (C:0.5755, R:0.0105)
Batch 150/356: Loss=0.5961 (C:0.5961, R:0.0105)
Batch 175/356: Loss=0.6002 (C:0.6002, R:0.0105)
Batch 200/356: Loss=0.6090 (C:0.6090, R:0.0105)
Batch 225/356: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 250/356: Loss=0.5765 (C:0.5765, R:0.0105)
Batch 275/356: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 300/356: Loss=0.5866 (C:0.5866, R:0.0105)
Batch 325/356: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 350/356: Loss=0.6136 (C:0.6136, R:0.0105)

============================================================
Epoch 71/300 completed in 21.2s
Train: Loss=0.5782 (C:0.5782, R:0.0105) Ratio=5.06x
Val:   Loss=0.7849 (C:0.7849, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/356: Loss=0.5908 (C:0.5908, R:0.0105)
Batch  25/356: Loss=0.5672 (C:0.5672, R:0.0105)
Batch  50/356: Loss=0.5887 (C:0.5887, R:0.0105)
Batch  75/356: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 100/356: Loss=0.5932 (C:0.5932, R:0.0105)
Batch 125/356: Loss=0.5907 (C:0.5907, R:0.0105)
Batch 150/356: Loss=0.5581 (C:0.5581, R:0.0105)
Batch 175/356: Loss=0.5964 (C:0.5964, R:0.0105)
Batch 200/356: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 225/356: Loss=0.5697 (C:0.5697, R:0.0105)
Batch 250/356: Loss=0.6003 (C:0.6003, R:0.0105)
Batch 275/356: Loss=0.6027 (C:0.6027, R:0.0105)
Batch 300/356: Loss=0.6081 (C:0.6081, R:0.0105)
Batch 325/356: Loss=0.5617 (C:0.5617, R:0.0105)
Batch 350/356: Loss=0.6095 (C:0.6095, R:0.0105)

============================================================
Epoch 72/300 completed in 20.8s
Train: Loss=0.5753 (C:0.5753, R:0.0105) Ratio=5.17x
Val:   Loss=0.7817 (C:0.7817, R:0.0104) Ratio=3.19x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.322 ± 0.596
    Neg distances: 2.631 ± 1.104
    Separation ratio: 8.18x
    Gap: -4.373
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/356: Loss=0.5691 (C:0.5691, R:0.0105)
Batch  25/356: Loss=0.5814 (C:0.5814, R:0.0105)
Batch  50/356: Loss=0.5567 (C:0.5567, R:0.0105)
Batch  75/356: Loss=0.5716 (C:0.5716, R:0.0105)
Batch 100/356: Loss=0.5600 (C:0.5600, R:0.0105)
Batch 125/356: Loss=0.6259 (C:0.6259, R:0.0105)
Batch 150/356: Loss=0.5579 (C:0.5579, R:0.0105)
Batch 175/356: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 200/356: Loss=0.5778 (C:0.5778, R:0.0105)
Batch 225/356: Loss=0.5697 (C:0.5697, R:0.0105)
Batch 250/356: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 275/356: Loss=0.6050 (C:0.6050, R:0.0105)
Batch 300/356: Loss=0.5648 (C:0.5648, R:0.0105)
Batch 325/356: Loss=0.6104 (C:0.6104, R:0.0105)
Batch 350/356: Loss=0.5775 (C:0.5775, R:0.0105)

============================================================
Epoch 73/300 completed in 26.5s
Train: Loss=0.5827 (C:0.5827, R:0.0105) Ratio=5.35x
Val:   Loss=0.7928 (C:0.7928, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/356: Loss=0.5543 (C:0.5543, R:0.0105)
Batch  25/356: Loss=0.6063 (C:0.6063, R:0.0105)
Batch  50/356: Loss=0.5798 (C:0.5798, R:0.0105)
Batch  75/356: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 100/356: Loss=0.5937 (C:0.5937, R:0.0105)
Batch 125/356: Loss=0.5824 (C:0.5824, R:0.0105)
Batch 150/356: Loss=0.5760 (C:0.5760, R:0.0105)
Batch 175/356: Loss=0.6266 (C:0.6266, R:0.0105)
Batch 200/356: Loss=0.5897 (C:0.5897, R:0.0105)
Batch 225/356: Loss=0.5808 (C:0.5808, R:0.0105)
Batch 250/356: Loss=0.5833 (C:0.5833, R:0.0105)
Batch 275/356: Loss=0.5775 (C:0.5775, R:0.0105)
Batch 300/356: Loss=0.5980 (C:0.5980, R:0.0105)
Batch 325/356: Loss=0.5984 (C:0.5984, R:0.0105)
Batch 350/356: Loss=0.5886 (C:0.5886, R:0.0105)

============================================================
Epoch 74/300 completed in 20.4s
Train: Loss=0.5815 (C:0.5815, R:0.0105) Ratio=5.27x
Val:   Loss=0.7991 (C:0.7991, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/356: Loss=0.5743 (C:0.5743, R:0.0105)
Batch  25/356: Loss=0.5545 (C:0.5545, R:0.0105)
Batch  50/356: Loss=0.5738 (C:0.5738, R:0.0105)
Batch  75/356: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 100/356: Loss=0.5682 (C:0.5682, R:0.0105)
Batch 125/356: Loss=0.6115 (C:0.6115, R:0.0105)
Batch 150/356: Loss=0.5571 (C:0.5571, R:0.0105)
Batch 175/356: Loss=0.5869 (C:0.5869, R:0.0105)
Batch 200/356: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 225/356: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 250/356: Loss=0.5707 (C:0.5707, R:0.0105)
Batch 275/356: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 300/356: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 325/356: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 350/356: Loss=0.5967 (C:0.5967, R:0.0105)

============================================================
Epoch 75/300 completed in 20.7s
Train: Loss=0.5802 (C:0.5802, R:0.0105) Ratio=5.38x
Val:   Loss=0.8040 (C:0.8040, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.299 ± 0.539
    Neg distances: 2.654 ± 1.094
    Separation ratio: 8.87x
    Gap: -4.377
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/356: Loss=0.5361 (C:0.5361, R:0.0105)
Batch  25/356: Loss=0.5383 (C:0.5383, R:0.0105)
Batch  50/356: Loss=0.5513 (C:0.5513, R:0.0105)
Batch  75/356: Loss=0.5482 (C:0.5482, R:0.0105)
Batch 100/356: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 125/356: Loss=0.5575 (C:0.5575, R:0.0105)
Batch 150/356: Loss=0.5402 (C:0.5402, R:0.0105)
Batch 175/356: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 200/356: Loss=0.5706 (C:0.5706, R:0.0105)
Batch 225/356: Loss=0.5843 (C:0.5843, R:0.0105)
Batch 250/356: Loss=0.5534 (C:0.5534, R:0.0105)
Batch 275/356: Loss=0.5399 (C:0.5399, R:0.0105)
Batch 300/356: Loss=0.5432 (C:0.5432, R:0.0105)
Batch 325/356: Loss=0.5545 (C:0.5545, R:0.0105)
Batch 350/356: Loss=0.5536 (C:0.5536, R:0.0105)

============================================================
Epoch 76/300 completed in 27.6s
Train: Loss=0.5613 (C:0.5613, R:0.0105) Ratio=5.55x
Val:   Loss=0.7835 (C:0.7835, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 76 epochs
Best model was at epoch 68 with Val Loss: 0.7813

Global Dataset Training Completed!
Best epoch: 68
Best validation loss: 0.7813
Final separation ratios: Train=5.55x, Val=3.08x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/7 batches
Extracted representations: torch.Size([9824, 100])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4591
  Adjusted Rand Score: 0.5294
  Clustering Accuracy: 0.8127
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
Extracted representations: torch.Size([546816, 100])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/6 batches
Extracted representations: torch.Size([9216, 100])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9216 samples
Classification Results:
  Accuracy: 0.8132
  Per-class F1: [0.8321338912133892, 0.746162546453385, 0.8611997447351627]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.761 ± 0.931
  Negative distances: 2.333 ± 1.240
  Separation ratio: 3.07x
  Gap: -4.341
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4591
  Clustering Accuracy: 0.8127
  Adjusted Rand Score: 0.5294

Classification Performance:
  Accuracy: 0.8132

Separation Quality:
  Separation Ratio: 3.07x
  Gap: -4.341
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216/results/evaluation_results_20250714_193121.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216/results/evaluation_results_20250714_193121.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr1e-04_lat100_bs1536_20250714_190216/final_results.json

Key Results:
  Separation ratio: 3.07x
  Perfect separation: False
  Classification accuracy: 0.8132
  Result: 0.8132% (improvement: +-80.86%)
  Cleaning up: coarse_lr1e-04_lat100_bs1536_20250714_190216

[7/12] Testing: coarse_lr2e-04_lat50_bs1020
  Learning rate: 0.0002
  Latent dim: 50
  Batch size: 1020
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 19:31:21.439492
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,863,730
Model created with 1,863,730 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0002)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,863,730
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.071 ± 0.009
    Neg distances: 0.071 ± 0.009
    Separation ratio: 1.00x
    Gap: -0.096
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9999 (C:1.9999, R:0.0118)
Batch  25/537: Loss=1.9827 (C:1.9827, R:0.0114)
Batch  50/537: Loss=1.9715 (C:1.9715, R:0.0111)
Batch  75/537: Loss=1.9549 (C:1.9549, R:0.0108)
Batch 100/537: Loss=1.9474 (C:1.9474, R:0.0107)
Batch 125/537: Loss=1.9298 (C:1.9298, R:0.0107)
Batch 150/537: Loss=1.9386 (C:1.9386, R:0.0106)
Batch 175/537: Loss=1.9044 (C:1.9044, R:0.0106)
Batch 200/537: Loss=1.9069 (C:1.9069, R:0.0105)
Batch 225/537: Loss=1.9083 (C:1.9083, R:0.0105)
Batch 250/537: Loss=1.9044 (C:1.9044, R:0.0106)
Batch 275/537: Loss=1.9006 (C:1.9006, R:0.0105)
Batch 300/537: Loss=1.8946 (C:1.8946, R:0.0105)
Batch 325/537: Loss=1.9055 (C:1.9055, R:0.0105)
Batch 350/537: Loss=1.8994 (C:1.8994, R:0.0105)
Batch 375/537: Loss=1.9074 (C:1.9074, R:0.0105)
Batch 400/537: Loss=1.9080 (C:1.9080, R:0.0105)
Batch 425/537: Loss=1.8894 (C:1.8894, R:0.0105)
Batch 450/537: Loss=1.8899 (C:1.8899, R:0.0105)
Batch 475/537: Loss=1.8912 (C:1.8912, R:0.0105)
Batch 500/537: Loss=1.9048 (C:1.9048, R:0.0105)
Batch 525/537: Loss=1.8851 (C:1.8851, R:0.0105)

============================================================
Epoch 1/300 completed in 27.3s
Train: Loss=1.9181 (C:1.9181, R:0.0106) Ratio=1.81x
Val:   Loss=1.8850 (C:1.8850, R:0.0104) Ratio=2.34x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8850)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.9002 (C:1.9002, R:0.0105)
Batch  25/537: Loss=1.8907 (C:1.8907, R:0.0105)
Batch  50/537: Loss=1.8763 (C:1.8763, R:0.0105)
Batch  75/537: Loss=1.8690 (C:1.8690, R:0.0106)
Batch 100/537: Loss=1.8975 (C:1.8975, R:0.0105)
Batch 125/537: Loss=1.8728 (C:1.8728, R:0.0105)
Batch 150/537: Loss=1.8831 (C:1.8831, R:0.0105)
Batch 175/537: Loss=1.8830 (C:1.8830, R:0.0105)
Batch 200/537: Loss=1.8922 (C:1.8922, R:0.0105)
Batch 225/537: Loss=1.8851 (C:1.8851, R:0.0105)
Batch 250/537: Loss=1.8751 (C:1.8751, R:0.0105)
Batch 275/537: Loss=1.8801 (C:1.8801, R:0.0105)
Batch 300/537: Loss=1.8835 (C:1.8835, R:0.0105)
Batch 325/537: Loss=1.8593 (C:1.8593, R:0.0105)
Batch 350/537: Loss=1.8736 (C:1.8736, R:0.0105)
Batch 375/537: Loss=1.8592 (C:1.8592, R:0.0105)
Batch 400/537: Loss=1.9021 (C:1.9021, R:0.0105)
Batch 425/537: Loss=1.8746 (C:1.8746, R:0.0105)
Batch 450/537: Loss=1.8736 (C:1.8736, R:0.0105)
Batch 475/537: Loss=1.8622 (C:1.8622, R:0.0105)
Batch 500/537: Loss=1.8845 (C:1.8845, R:0.0105)
Batch 525/537: Loss=1.8626 (C:1.8626, R:0.0105)

============================================================
Epoch 2/300 completed in 21.1s
Train: Loss=1.8817 (C:1.8817, R:0.0105) Ratio=2.39x
Val:   Loss=1.8758 (C:1.8758, R:0.0104) Ratio=2.56x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8758)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8701 (C:1.8701, R:0.0105)
Batch  25/537: Loss=1.8615 (C:1.8615, R:0.0105)
Batch  50/537: Loss=1.8754 (C:1.8754, R:0.0105)
Batch  75/537: Loss=1.8555 (C:1.8555, R:0.0105)
Batch 100/537: Loss=1.8567 (C:1.8567, R:0.0105)
Batch 125/537: Loss=1.8694 (C:1.8694, R:0.0105)
Batch 150/537: Loss=1.8594 (C:1.8594, R:0.0105)
Batch 175/537: Loss=1.8772 (C:1.8772, R:0.0105)
Batch 200/537: Loss=1.8545 (C:1.8545, R:0.0105)
Batch 225/537: Loss=1.8575 (C:1.8575, R:0.0105)
Batch 250/537: Loss=1.8718 (C:1.8718, R:0.0105)
Batch 275/537: Loss=1.8669 (C:1.8669, R:0.0105)
Batch 300/537: Loss=1.8713 (C:1.8713, R:0.0105)
Batch 325/537: Loss=1.8737 (C:1.8737, R:0.0105)
Batch 350/537: Loss=1.8769 (C:1.8769, R:0.0105)
Batch 375/537: Loss=1.8509 (C:1.8509, R:0.0105)
Batch 400/537: Loss=1.8815 (C:1.8815, R:0.0105)
Batch 425/537: Loss=1.8727 (C:1.8727, R:0.0105)
Batch 450/537: Loss=1.8675 (C:1.8675, R:0.0105)
Batch 475/537: Loss=1.8454 (C:1.8454, R:0.0105)
Batch 500/537: Loss=1.8742 (C:1.8742, R:0.0105)
Batch 525/537: Loss=1.8790 (C:1.8790, R:0.0105)

============================================================
Epoch 3/300 completed in 21.4s
Train: Loss=1.8695 (C:1.8695, R:0.0105) Ratio=2.67x
Val:   Loss=1.8639 (C:1.8639, R:0.0104) Ratio=2.78x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8639)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.405 ± 0.568
    Neg distances: 1.312 ± 0.842
    Separation ratio: 3.24x
    Gap: -2.380
    ✅ Excellent global separation!

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.1788 (C:1.1788, R:0.0105)
Batch  25/537: Loss=1.1568 (C:1.1568, R:0.0105)
Batch  50/537: Loss=1.1597 (C:1.1597, R:0.0106)
Batch  75/537: Loss=1.2005 (C:1.2005, R:0.0105)
Batch 100/537: Loss=1.1712 (C:1.1712, R:0.0105)
Batch 125/537: Loss=1.1993 (C:1.1993, R:0.0105)
Batch 150/537: Loss=1.2146 (C:1.2146, R:0.0105)
Batch 175/537: Loss=1.1885 (C:1.1885, R:0.0105)
Batch 200/537: Loss=1.1664 (C:1.1664, R:0.0105)
Batch 225/537: Loss=1.1558 (C:1.1558, R:0.0105)
Batch 250/537: Loss=1.1426 (C:1.1426, R:0.0105)
Batch 275/537: Loss=1.2155 (C:1.2155, R:0.0105)
Batch 300/537: Loss=1.1587 (C:1.1587, R:0.0105)
Batch 325/537: Loss=1.1943 (C:1.1943, R:0.0105)
Batch 350/537: Loss=1.1703 (C:1.1703, R:0.0105)
Batch 375/537: Loss=1.1796 (C:1.1796, R:0.0105)
Batch 400/537: Loss=1.2085 (C:1.2085, R:0.0105)
Batch 425/537: Loss=1.1838 (C:1.1838, R:0.0105)
Batch 450/537: Loss=1.1583 (C:1.1583, R:0.0105)
Batch 475/537: Loss=1.1682 (C:1.1682, R:0.0105)
Batch 500/537: Loss=1.1860 (C:1.1860, R:0.0105)
Batch 525/537: Loss=1.1940 (C:1.1940, R:0.0105)

============================================================
Epoch 4/300 completed in 27.6s
Train: Loss=1.1797 (C:1.1797, R:0.0105) Ratio=2.79x
Val:   Loss=1.1774 (C:1.1774, R:0.0104) Ratio=2.80x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1774)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.1575 (C:1.1575, R:0.0105)
Batch  25/537: Loss=1.1191 (C:1.1191, R:0.0105)
Batch  50/537: Loss=1.2023 (C:1.2023, R:0.0105)
Batch  75/537: Loss=1.1846 (C:1.1846, R:0.0105)
Batch 100/537: Loss=1.1588 (C:1.1588, R:0.0105)
Batch 125/537: Loss=1.1608 (C:1.1608, R:0.0105)
Batch 150/537: Loss=1.1557 (C:1.1557, R:0.0105)
Batch 175/537: Loss=1.1648 (C:1.1648, R:0.0105)
Batch 200/537: Loss=1.1709 (C:1.1709, R:0.0105)
Batch 225/537: Loss=1.1728 (C:1.1728, R:0.0105)
Batch 250/537: Loss=1.1504 (C:1.1504, R:0.0105)
Batch 275/537: Loss=1.1785 (C:1.1785, R:0.0105)
Batch 300/537: Loss=1.1427 (C:1.1427, R:0.0106)
Batch 325/537: Loss=1.1335 (C:1.1335, R:0.0105)
Batch 350/537: Loss=1.1308 (C:1.1308, R:0.0105)
Batch 375/537: Loss=1.1602 (C:1.1602, R:0.0106)
Batch 400/537: Loss=1.1514 (C:1.1514, R:0.0105)
Batch 425/537: Loss=1.1603 (C:1.1603, R:0.0105)
Batch 450/537: Loss=1.1471 (C:1.1471, R:0.0105)
Batch 475/537: Loss=1.1862 (C:1.1862, R:0.0105)
Batch 500/537: Loss=1.1652 (C:1.1652, R:0.0105)
Batch 525/537: Loss=1.1754 (C:1.1754, R:0.0105)

============================================================
Epoch 5/300 completed in 21.4s
Train: Loss=1.1592 (C:1.1592, R:0.0105) Ratio=3.02x
Val:   Loss=1.1688 (C:1.1688, R:0.0104) Ratio=2.91x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1688)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.1261 (C:1.1261, R:0.0105)
Batch  25/537: Loss=1.1491 (C:1.1491, R:0.0105)
Batch  50/537: Loss=1.1309 (C:1.1309, R:0.0105)
Batch  75/537: Loss=1.1270 (C:1.1270, R:0.0105)
Batch 100/537: Loss=1.1341 (C:1.1341, R:0.0105)
Batch 125/537: Loss=1.1094 (C:1.1094, R:0.0104)
Batch 150/537: Loss=1.1505 (C:1.1505, R:0.0105)
Batch 175/537: Loss=1.1442 (C:1.1442, R:0.0105)
Batch 200/537: Loss=1.1178 (C:1.1178, R:0.0105)
Batch 225/537: Loss=1.1760 (C:1.1760, R:0.0105)
Batch 250/537: Loss=1.1418 (C:1.1418, R:0.0105)
Batch 275/537: Loss=1.1703 (C:1.1703, R:0.0106)
Batch 300/537: Loss=1.1566 (C:1.1566, R:0.0105)
Batch 325/537: Loss=1.1430 (C:1.1430, R:0.0105)
Batch 350/537: Loss=1.1605 (C:1.1605, R:0.0105)
Batch 375/537: Loss=1.1824 (C:1.1824, R:0.0105)
Batch 400/537: Loss=1.1573 (C:1.1573, R:0.0105)
Batch 425/537: Loss=1.1524 (C:1.1524, R:0.0105)
Batch 450/537: Loss=1.1658 (C:1.1658, R:0.0105)
Batch 475/537: Loss=1.1379 (C:1.1379, R:0.0105)
Batch 500/537: Loss=1.1443 (C:1.1443, R:0.0105)
Batch 525/537: Loss=1.1287 (C:1.1287, R:0.0105)

============================================================
Epoch 6/300 completed in 21.7s
Train: Loss=1.1463 (C:1.1463, R:0.0105) Ratio=3.19x
Val:   Loss=1.1608 (C:1.1608, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1608)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.363 ± 0.563
    Neg distances: 1.398 ± 0.860
    Separation ratio: 3.85x
    Gap: -2.549
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.0318 (C:1.0318, R:0.0105)
Batch  25/537: Loss=1.1021 (C:1.1021, R:0.0105)
Batch  50/537: Loss=1.0664 (C:1.0664, R:0.0105)
Batch  75/537: Loss=1.0665 (C:1.0665, R:0.0105)
Batch 100/537: Loss=1.0891 (C:1.0891, R:0.0105)
Batch 125/537: Loss=1.0815 (C:1.0815, R:0.0105)
Batch 150/537: Loss=1.0647 (C:1.0647, R:0.0105)
Batch 175/537: Loss=1.0435 (C:1.0435, R:0.0105)
Batch 200/537: Loss=1.0810 (C:1.0810, R:0.0105)
Batch 225/537: Loss=1.0765 (C:1.0765, R:0.0105)
Batch 250/537: Loss=1.0817 (C:1.0817, R:0.0105)
Batch 275/537: Loss=1.0906 (C:1.0906, R:0.0105)
Batch 300/537: Loss=1.1075 (C:1.1075, R:0.0105)
Batch 325/537: Loss=1.1068 (C:1.1068, R:0.0105)
Batch 350/537: Loss=1.1123 (C:1.1123, R:0.0105)
Batch 375/537: Loss=1.0438 (C:1.0438, R:0.0105)
Batch 400/537: Loss=1.0592 (C:1.0592, R:0.0105)
Batch 425/537: Loss=1.0992 (C:1.0992, R:0.0105)
Batch 450/537: Loss=1.0435 (C:1.0435, R:0.0105)
Batch 475/537: Loss=1.1244 (C:1.1244, R:0.0105)
Batch 500/537: Loss=1.0796 (C:1.0796, R:0.0105)
Batch 525/537: Loss=1.1062 (C:1.1062, R:0.0105)

============================================================
Epoch 7/300 completed in 27.5s
Train: Loss=1.0769 (C:1.0769, R:0.0105) Ratio=3.30x
Val:   Loss=1.0972 (C:1.0972, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0972)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.0612 (C:1.0612, R:0.0105)
Batch  25/537: Loss=1.0807 (C:1.0807, R:0.0105)
Batch  50/537: Loss=1.0583 (C:1.0583, R:0.0105)
Batch  75/537: Loss=1.0716 (C:1.0716, R:0.0105)
Batch 100/537: Loss=1.0823 (C:1.0823, R:0.0105)
Batch 125/537: Loss=1.0852 (C:1.0852, R:0.0105)
Batch 150/537: Loss=1.0862 (C:1.0862, R:0.0105)
Batch 175/537: Loss=1.0938 (C:1.0938, R:0.0105)
Batch 200/537: Loss=1.1129 (C:1.1129, R:0.0105)
Batch 225/537: Loss=1.0763 (C:1.0763, R:0.0105)
Batch 250/537: Loss=1.0497 (C:1.0497, R:0.0105)
Batch 275/537: Loss=1.0877 (C:1.0877, R:0.0105)
Batch 300/537: Loss=1.0438 (C:1.0438, R:0.0105)
Batch 325/537: Loss=1.0951 (C:1.0951, R:0.0105)
Batch 350/537: Loss=1.0413 (C:1.0413, R:0.0105)
Batch 375/537: Loss=1.0693 (C:1.0693, R:0.0105)
Batch 400/537: Loss=1.0728 (C:1.0728, R:0.0105)
Batch 425/537: Loss=1.0753 (C:1.0753, R:0.0105)
Batch 450/537: Loss=1.0780 (C:1.0780, R:0.0105)
Batch 475/537: Loss=1.0621 (C:1.0621, R:0.0105)
Batch 500/537: Loss=1.0887 (C:1.0887, R:0.0105)
Batch 525/537: Loss=1.0546 (C:1.0546, R:0.0106)

============================================================
Epoch 8/300 completed in 21.4s
Train: Loss=1.0694 (C:1.0694, R:0.0105) Ratio=3.41x
Val:   Loss=1.1001 (C:1.1001, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0391 (C:1.0391, R:0.0105)
Batch  25/537: Loss=1.0532 (C:1.0532, R:0.0105)
Batch  50/537: Loss=1.0901 (C:1.0901, R:0.0105)
Batch  75/537: Loss=1.0550 (C:1.0550, R:0.0105)
Batch 100/537: Loss=1.0804 (C:1.0804, R:0.0105)
Batch 125/537: Loss=1.0624 (C:1.0624, R:0.0105)
Batch 150/537: Loss=1.0473 (C:1.0473, R:0.0105)
Batch 175/537: Loss=1.0233 (C:1.0233, R:0.0105)
Batch 200/537: Loss=1.0415 (C:1.0415, R:0.0105)
Batch 225/537: Loss=1.0438 (C:1.0438, R:0.0105)
Batch 250/537: Loss=1.0541 (C:1.0541, R:0.0105)
Batch 275/537: Loss=1.0275 (C:1.0275, R:0.0105)
Batch 300/537: Loss=1.0667 (C:1.0667, R:0.0105)
Batch 325/537: Loss=1.0727 (C:1.0727, R:0.0105)
Batch 350/537: Loss=1.0602 (C:1.0602, R:0.0105)
Batch 375/537: Loss=1.0801 (C:1.0801, R:0.0106)
Batch 400/537: Loss=1.0837 (C:1.0837, R:0.0105)
Batch 425/537: Loss=1.0666 (C:1.0666, R:0.0105)
Batch 450/537: Loss=1.0734 (C:1.0734, R:0.0105)
Batch 475/537: Loss=1.0613 (C:1.0613, R:0.0106)
Batch 500/537: Loss=1.0896 (C:1.0896, R:0.0105)
Batch 525/537: Loss=1.0389 (C:1.0389, R:0.0105)

============================================================
Epoch 9/300 completed in 21.8s
Train: Loss=1.0623 (C:1.0623, R:0.0105) Ratio=3.52x
Val:   Loss=1.1089 (C:1.1089, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.338 ± 0.554
    Neg distances: 1.462 ± 0.873
    Separation ratio: 4.32x
    Gap: -2.569
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.0153 (C:1.0153, R:0.0105)
Batch  25/537: Loss=1.0110 (C:1.0110, R:0.0105)
Batch  50/537: Loss=1.0042 (C:1.0042, R:0.0105)
Batch  75/537: Loss=0.9947 (C:0.9947, R:0.0105)
Batch 100/537: Loss=0.9838 (C:0.9838, R:0.0105)
Batch 125/537: Loss=1.0154 (C:1.0154, R:0.0105)
Batch 150/537: Loss=1.0125 (C:1.0125, R:0.0105)
Batch 175/537: Loss=0.9903 (C:0.9903, R:0.0105)
Batch 200/537: Loss=0.9928 (C:0.9928, R:0.0105)
Batch 225/537: Loss=1.0424 (C:1.0424, R:0.0106)
Batch 250/537: Loss=1.0182 (C:1.0182, R:0.0105)
Batch 275/537: Loss=1.0249 (C:1.0249, R:0.0105)
Batch 300/537: Loss=1.0394 (C:1.0394, R:0.0105)
Batch 325/537: Loss=1.0160 (C:1.0160, R:0.0105)
Batch 350/537: Loss=1.0371 (C:1.0371, R:0.0105)
Batch 375/537: Loss=1.0094 (C:1.0094, R:0.0105)
Batch 400/537: Loss=1.0159 (C:1.0159, R:0.0106)
Batch 425/537: Loss=1.0163 (C:1.0163, R:0.0105)
Batch 450/537: Loss=1.0027 (C:1.0027, R:0.0105)
Batch 475/537: Loss=1.0199 (C:1.0199, R:0.0105)
Batch 500/537: Loss=1.0311 (C:1.0311, R:0.0105)
Batch 525/537: Loss=1.0429 (C:1.0429, R:0.0105)

============================================================
Epoch 10/300 completed in 27.2s
Train: Loss=1.0177 (C:1.0177, R:0.0105) Ratio=3.63x
Val:   Loss=1.0614 (C:1.0614, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0614)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=0.9844 (C:0.9844, R:0.0105)
Batch  25/537: Loss=1.0153 (C:1.0153, R:0.0105)
Batch  50/537: Loss=0.9696 (C:0.9696, R:0.0105)
Batch  75/537: Loss=1.0347 (C:1.0347, R:0.0105)
Batch 100/537: Loss=0.9796 (C:0.9796, R:0.0105)
Batch 125/537: Loss=1.0251 (C:1.0251, R:0.0105)
Batch 150/537: Loss=1.0345 (C:1.0345, R:0.0105)
Batch 175/537: Loss=1.0228 (C:1.0228, R:0.0105)
Batch 200/537: Loss=1.0187 (C:1.0187, R:0.0105)
Batch 225/537: Loss=0.9954 (C:0.9954, R:0.0105)
Batch 250/537: Loss=0.9961 (C:0.9961, R:0.0105)
Batch 275/537: Loss=1.0591 (C:1.0591, R:0.0106)
Batch 300/537: Loss=1.0127 (C:1.0127, R:0.0105)
Batch 325/537: Loss=0.9815 (C:0.9815, R:0.0105)
Batch 350/537: Loss=1.0082 (C:1.0082, R:0.0105)
Batch 375/537: Loss=1.0089 (C:1.0089, R:0.0105)
Batch 400/537: Loss=1.0286 (C:1.0286, R:0.0105)
Batch 425/537: Loss=1.0264 (C:1.0264, R:0.0105)
Batch 450/537: Loss=0.9762 (C:0.9762, R:0.0105)
Batch 475/537: Loss=1.0436 (C:1.0436, R:0.0105)
Batch 500/537: Loss=0.9816 (C:0.9816, R:0.0105)
Batch 525/537: Loss=1.0305 (C:1.0305, R:0.0105)

============================================================
Epoch 11/300 completed in 21.2s
Train: Loss=1.0129 (C:1.0129, R:0.0105) Ratio=3.67x
Val:   Loss=1.0641 (C:1.0641, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=0.9958 (C:0.9958, R:0.0105)
Batch  25/537: Loss=0.9813 (C:0.9813, R:0.0105)
Batch  50/537: Loss=1.0208 (C:1.0208, R:0.0105)
Batch  75/537: Loss=1.0143 (C:1.0143, R:0.0105)
Batch 100/537: Loss=1.0278 (C:1.0278, R:0.0105)
Batch 125/537: Loss=1.0013 (C:1.0013, R:0.0105)
Batch 150/537: Loss=0.9874 (C:0.9874, R:0.0105)
Batch 175/537: Loss=1.0190 (C:1.0190, R:0.0105)
Batch 200/537: Loss=0.9951 (C:0.9951, R:0.0105)
Batch 225/537: Loss=1.0237 (C:1.0237, R:0.0105)
Batch 250/537: Loss=1.0457 (C:1.0457, R:0.0105)
Batch 275/537: Loss=1.0375 (C:1.0375, R:0.0105)
Batch 300/537: Loss=0.9935 (C:0.9935, R:0.0105)
Batch 325/537: Loss=1.0107 (C:1.0107, R:0.0105)
Batch 350/537: Loss=1.0288 (C:1.0288, R:0.0106)
Batch 375/537: Loss=1.0322 (C:1.0322, R:0.0105)
Batch 400/537: Loss=1.0310 (C:1.0310, R:0.0105)
Batch 425/537: Loss=1.0125 (C:1.0125, R:0.0105)
Batch 450/537: Loss=1.0473 (C:1.0473, R:0.0106)
Batch 475/537: Loss=1.0287 (C:1.0287, R:0.0106)
Batch 500/537: Loss=1.0127 (C:1.0127, R:0.0105)
Batch 525/537: Loss=1.0463 (C:1.0463, R:0.0105)

============================================================
Epoch 12/300 completed in 21.7s
Train: Loss=1.0101 (C:1.0101, R:0.0105) Ratio=3.73x
Val:   Loss=1.0542 (C:1.0542, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0542)
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.307 ± 0.515
    Neg distances: 1.507 ± 0.872
    Separation ratio: 4.91x
    Gap: -2.744
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9779 (C:0.9779, R:0.0105)
Batch  25/537: Loss=0.9619 (C:0.9619, R:0.0105)
Batch  50/537: Loss=0.9905 (C:0.9905, R:0.0105)
Batch  75/537: Loss=0.9496 (C:0.9496, R:0.0105)
Batch 100/537: Loss=0.9514 (C:0.9514, R:0.0105)
Batch 125/537: Loss=0.9420 (C:0.9420, R:0.0105)
Batch 150/537: Loss=0.9751 (C:0.9751, R:0.0105)
Batch 175/537: Loss=0.9801 (C:0.9801, R:0.0105)
Batch 200/537: Loss=0.9345 (C:0.9345, R:0.0105)
Batch 225/537: Loss=0.9496 (C:0.9496, R:0.0105)
Batch 250/537: Loss=0.9895 (C:0.9895, R:0.0105)
Batch 275/537: Loss=0.9540 (C:0.9540, R:0.0105)
Batch 300/537: Loss=0.9946 (C:0.9946, R:0.0105)
Batch 325/537: Loss=0.9864 (C:0.9864, R:0.0105)
Batch 350/537: Loss=0.9812 (C:0.9812, R:0.0105)
Batch 375/537: Loss=0.9671 (C:0.9671, R:0.0105)
Batch 400/537: Loss=0.9989 (C:0.9989, R:0.0105)
Batch 425/537: Loss=0.9666 (C:0.9666, R:0.0105)
Batch 450/537: Loss=1.0168 (C:1.0168, R:0.0105)
Batch 475/537: Loss=0.9896 (C:0.9896, R:0.0105)
Batch 500/537: Loss=0.9931 (C:0.9931, R:0.0105)
Batch 525/537: Loss=0.9564 (C:0.9564, R:0.0105)

============================================================
Epoch 13/300 completed in 27.3s
Train: Loss=0.9685 (C:0.9685, R:0.0105) Ratio=3.79x
Val:   Loss=1.0170 (C:1.0170, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0170)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9317 (C:0.9317, R:0.0105)
Batch  25/537: Loss=0.9388 (C:0.9388, R:0.0105)
Batch  50/537: Loss=0.9680 (C:0.9680, R:0.0105)
Batch  75/537: Loss=0.9841 (C:0.9841, R:0.0105)
Batch 100/537: Loss=0.9445 (C:0.9445, R:0.0105)
Batch 125/537: Loss=0.9819 (C:0.9819, R:0.0105)
Batch 150/537: Loss=0.9729 (C:0.9729, R:0.0105)
Batch 175/537: Loss=1.0036 (C:1.0036, R:0.0105)
Batch 200/537: Loss=0.9637 (C:0.9637, R:0.0105)
Batch 225/537: Loss=0.9618 (C:0.9618, R:0.0105)
Batch 250/537: Loss=0.9646 (C:0.9646, R:0.0105)
Batch 275/537: Loss=0.9432 (C:0.9432, R:0.0105)
Batch 300/537: Loss=0.9745 (C:0.9745, R:0.0105)
Batch 325/537: Loss=0.9504 (C:0.9504, R:0.0104)
Batch 350/537: Loss=0.9542 (C:0.9542, R:0.0105)
Batch 375/537: Loss=0.9850 (C:0.9850, R:0.0105)
Batch 400/537: Loss=0.9445 (C:0.9445, R:0.0105)
Batch 425/537: Loss=0.9742 (C:0.9742, R:0.0105)
Batch 450/537: Loss=0.9636 (C:0.9636, R:0.0105)
Batch 475/537: Loss=0.9593 (C:0.9593, R:0.0106)
Batch 500/537: Loss=0.9386 (C:0.9386, R:0.0105)
Batch 525/537: Loss=0.9515 (C:0.9515, R:0.0105)

============================================================
Epoch 14/300 completed in 21.3s
Train: Loss=0.9639 (C:0.9639, R:0.0105) Ratio=3.97x
Val:   Loss=1.0212 (C:1.0212, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.9504 (C:0.9504, R:0.0105)
Batch  25/537: Loss=0.9545 (C:0.9545, R:0.0105)
Batch  50/537: Loss=0.9469 (C:0.9469, R:0.0105)
Batch  75/537: Loss=0.9893 (C:0.9893, R:0.0105)
Batch 100/537: Loss=0.9987 (C:0.9987, R:0.0105)
Batch 125/537: Loss=0.9601 (C:0.9601, R:0.0105)
Batch 150/537: Loss=0.9727 (C:0.9727, R:0.0105)
Batch 175/537: Loss=0.9634 (C:0.9634, R:0.0105)
Batch 200/537: Loss=0.9542 (C:0.9542, R:0.0105)
Batch 225/537: Loss=1.0039 (C:1.0039, R:0.0105)
Batch 250/537: Loss=0.9695 (C:0.9695, R:0.0105)
Batch 275/537: Loss=0.9349 (C:0.9349, R:0.0105)
Batch 300/537: Loss=0.9562 (C:0.9562, R:0.0105)
Batch 325/537: Loss=0.9758 (C:0.9758, R:0.0105)
Batch 350/537: Loss=0.9784 (C:0.9784, R:0.0105)
Batch 375/537: Loss=0.9923 (C:0.9923, R:0.0105)
Batch 400/537: Loss=0.9279 (C:0.9279, R:0.0105)
Batch 425/537: Loss=0.9971 (C:0.9971, R:0.0105)
Batch 450/537: Loss=1.0160 (C:1.0160, R:0.0105)
Batch 475/537: Loss=0.9412 (C:0.9412, R:0.0105)
Batch 500/537: Loss=0.9555 (C:0.9555, R:0.0105)
Batch 525/537: Loss=0.9628 (C:0.9628, R:0.0105)

============================================================
Epoch 15/300 completed in 21.5s
Train: Loss=0.9612 (C:0.9612, R:0.0105) Ratio=4.00x
Val:   Loss=1.0165 (C:1.0165, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0165)
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.287 ± 0.492
    Neg distances: 1.571 ± 0.894
    Separation ratio: 5.46x
    Gap: -2.806
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.9474 (C:0.9474, R:0.0105)
Batch  25/537: Loss=0.9439 (C:0.9439, R:0.0105)
Batch  50/537: Loss=0.9328 (C:0.9328, R:0.0105)
Batch  75/537: Loss=0.9137 (C:0.9137, R:0.0106)
Batch 100/537: Loss=0.8854 (C:0.8854, R:0.0105)
Batch 125/537: Loss=0.9157 (C:0.9157, R:0.0105)
Batch 150/537: Loss=0.9117 (C:0.9117, R:0.0105)
Batch 175/537: Loss=0.9451 (C:0.9451, R:0.0106)
Batch 200/537: Loss=0.9258 (C:0.9258, R:0.0105)
Batch 225/537: Loss=0.9173 (C:0.9173, R:0.0105)
Batch 250/537: Loss=0.9462 (C:0.9462, R:0.0105)
Batch 275/537: Loss=0.9524 (C:0.9524, R:0.0105)
Batch 300/537: Loss=0.9534 (C:0.9534, R:0.0105)
Batch 325/537: Loss=0.9521 (C:0.9521, R:0.0105)
Batch 350/537: Loss=0.9412 (C:0.9412, R:0.0105)
Batch 375/537: Loss=0.9095 (C:0.9095, R:0.0105)
Batch 400/537: Loss=0.9278 (C:0.9278, R:0.0105)
Batch 425/537: Loss=0.9096 (C:0.9096, R:0.0105)
Batch 450/537: Loss=0.9144 (C:0.9144, R:0.0105)
Batch 475/537: Loss=0.9115 (C:0.9115, R:0.0105)
Batch 500/537: Loss=0.9318 (C:0.9318, R:0.0105)
Batch 525/537: Loss=0.9166 (C:0.9166, R:0.0105)

============================================================
Epoch 16/300 completed in 27.5s
Train: Loss=0.9282 (C:0.9282, R:0.0105) Ratio=4.05x
Val:   Loss=0.9901 (C:0.9901, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9901)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.9289 (C:0.9289, R:0.0105)
Batch  25/537: Loss=0.9183 (C:0.9183, R:0.0105)
Batch  50/537: Loss=0.9061 (C:0.9061, R:0.0105)
Batch  75/537: Loss=0.8980 (C:0.8980, R:0.0105)
Batch 100/537: Loss=0.9189 (C:0.9189, R:0.0106)
Batch 125/537: Loss=0.9197 (C:0.9197, R:0.0105)
Batch 150/537: Loss=0.9070 (C:0.9070, R:0.0105)
Batch 175/537: Loss=0.8931 (C:0.8931, R:0.0105)
Batch 200/537: Loss=0.8980 (C:0.8980, R:0.0105)
Batch 225/537: Loss=0.9344 (C:0.9344, R:0.0105)
Batch 250/537: Loss=0.9070 (C:0.9070, R:0.0105)
Batch 275/537: Loss=0.8905 (C:0.8905, R:0.0105)
Batch 300/537: Loss=0.9283 (C:0.9283, R:0.0105)
Batch 325/537: Loss=0.9501 (C:0.9501, R:0.0105)
Batch 350/537: Loss=0.9171 (C:0.9171, R:0.0105)
Batch 375/537: Loss=0.9005 (C:0.9005, R:0.0105)
Batch 400/537: Loss=0.9242 (C:0.9242, R:0.0105)
Batch 425/537: Loss=0.9367 (C:0.9367, R:0.0105)
Batch 450/537: Loss=0.9362 (C:0.9362, R:0.0105)
Batch 475/537: Loss=0.9335 (C:0.9335, R:0.0105)
Batch 500/537: Loss=0.9671 (C:0.9671, R:0.0105)
Batch 525/537: Loss=0.9272 (C:0.9272, R:0.0105)

============================================================
Epoch 17/300 completed in 21.4s
Train: Loss=0.9244 (C:0.9244, R:0.0105) Ratio=4.18x
Val:   Loss=0.9894 (C:0.9894, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9894)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.9059 (C:0.9059, R:0.0105)
Batch  25/537: Loss=0.8976 (C:0.8976, R:0.0106)
Batch  50/537: Loss=0.9122 (C:0.9122, R:0.0105)
Batch  75/537: Loss=0.9323 (C:0.9323, R:0.0105)
Batch 100/537: Loss=0.9340 (C:0.9340, R:0.0105)
Batch 125/537: Loss=0.9111 (C:0.9111, R:0.0105)
Batch 150/537: Loss=0.8805 (C:0.8805, R:0.0105)
Batch 175/537: Loss=0.9012 (C:0.9012, R:0.0105)
Batch 200/537: Loss=0.9515 (C:0.9515, R:0.0105)
Batch 225/537: Loss=0.9384 (C:0.9384, R:0.0105)
Batch 250/537: Loss=0.9137 (C:0.9137, R:0.0105)
Batch 275/537: Loss=0.9093 (C:0.9093, R:0.0105)
Batch 300/537: Loss=0.9019 (C:0.9019, R:0.0105)
Batch 325/537: Loss=0.8994 (C:0.8994, R:0.0105)
Batch 350/537: Loss=0.9162 (C:0.9162, R:0.0105)
Batch 375/537: Loss=0.9413 (C:0.9413, R:0.0105)
Batch 400/537: Loss=0.9447 (C:0.9447, R:0.0105)
Batch 425/537: Loss=0.9535 (C:0.9535, R:0.0105)
Batch 450/537: Loss=0.9431 (C:0.9431, R:0.0105)
Batch 475/537: Loss=0.9131 (C:0.9131, R:0.0105)
Batch 500/537: Loss=0.9572 (C:0.9572, R:0.0105)
Batch 525/537: Loss=0.8939 (C:0.8939, R:0.0105)

============================================================
Epoch 18/300 completed in 21.3s
Train: Loss=0.9213 (C:0.9213, R:0.0105) Ratio=4.23x
Val:   Loss=0.9939 (C:0.9939, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.281 ± 0.504
    Neg distances: 1.617 ± 0.897
    Separation ratio: 5.76x
    Gap: -2.770
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.8820 (C:0.8820, R:0.0105)
Batch  25/537: Loss=0.8699 (C:0.8699, R:0.0105)
Batch  50/537: Loss=0.9063 (C:0.9063, R:0.0105)
Batch  75/537: Loss=0.8948 (C:0.8948, R:0.0105)
Batch 100/537: Loss=0.9006 (C:0.9006, R:0.0105)
Batch 125/537: Loss=0.8832 (C:0.8832, R:0.0106)
Batch 150/537: Loss=0.9322 (C:0.9322, R:0.0105)
Batch 175/537: Loss=0.9146 (C:0.9146, R:0.0105)
Batch 200/537: Loss=0.8966 (C:0.8966, R:0.0105)
Batch 225/537: Loss=0.8960 (C:0.8960, R:0.0105)
Batch 250/537: Loss=0.8967 (C:0.8967, R:0.0105)
Batch 275/537: Loss=0.8843 (C:0.8843, R:0.0105)
Batch 300/537: Loss=0.9084 (C:0.9084, R:0.0105)
Batch 325/537: Loss=0.9477 (C:0.9477, R:0.0105)
Batch 350/537: Loss=0.9260 (C:0.9260, R:0.0105)
Batch 375/537: Loss=0.9214 (C:0.9214, R:0.0105)
Batch 400/537: Loss=0.9048 (C:0.9048, R:0.0105)
Batch 425/537: Loss=0.9589 (C:0.9589, R:0.0106)
Batch 450/537: Loss=0.9192 (C:0.9192, R:0.0105)
Batch 475/537: Loss=0.9061 (C:0.9061, R:0.0105)
Batch 500/537: Loss=0.9046 (C:0.9046, R:0.0105)
Batch 525/537: Loss=0.9220 (C:0.9220, R:0.0105)

============================================================
Epoch 19/300 completed in 27.3s
Train: Loss=0.8994 (C:0.8994, R:0.0105) Ratio=4.16x
Val:   Loss=0.9670 (C:0.9670, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9670)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.9184 (C:0.9184, R:0.0105)
Batch  25/537: Loss=0.8831 (C:0.8831, R:0.0105)
Batch  50/537: Loss=0.8949 (C:0.8949, R:0.0105)
Batch  75/537: Loss=0.9272 (C:0.9272, R:0.0105)
Batch 100/537: Loss=0.8664 (C:0.8664, R:0.0105)
Batch 125/537: Loss=0.8922 (C:0.8922, R:0.0105)
Batch 150/537: Loss=0.9005 (C:0.9005, R:0.0105)
Batch 175/537: Loss=0.8855 (C:0.8855, R:0.0105)
Batch 200/537: Loss=0.9047 (C:0.9047, R:0.0106)
Batch 225/537: Loss=0.8732 (C:0.8732, R:0.0105)
Batch 250/537: Loss=0.8895 (C:0.8895, R:0.0105)
Batch 275/537: Loss=0.9114 (C:0.9114, R:0.0105)
Batch 300/537: Loss=0.9146 (C:0.9146, R:0.0105)
Batch 325/537: Loss=0.8814 (C:0.8814, R:0.0105)
Batch 350/537: Loss=0.9258 (C:0.9258, R:0.0105)
Batch 375/537: Loss=0.8969 (C:0.8969, R:0.0105)
Batch 400/537: Loss=0.8930 (C:0.8930, R:0.0106)
Batch 425/537: Loss=0.9157 (C:0.9157, R:0.0105)
Batch 450/537: Loss=0.8889 (C:0.8889, R:0.0105)
Batch 475/537: Loss=0.9003 (C:0.9003, R:0.0105)
Batch 500/537: Loss=0.8838 (C:0.8838, R:0.0105)
Batch 525/537: Loss=0.9035 (C:0.9035, R:0.0105)

============================================================
Epoch 20/300 completed in 21.7s
Train: Loss=0.8967 (C:0.8967, R:0.0105) Ratio=4.32x
Val:   Loss=0.9768 (C:0.9768, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8967 (C:0.8967, R:0.0105)
Batch  25/537: Loss=0.8737 (C:0.8737, R:0.0105)
Batch  50/537: Loss=0.9152 (C:0.9152, R:0.0105)
Batch  75/537: Loss=0.9073 (C:0.9073, R:0.0105)
Batch 100/537: Loss=0.8677 (C:0.8677, R:0.0105)
Batch 125/537: Loss=0.8835 (C:0.8835, R:0.0105)
Batch 150/537: Loss=0.8921 (C:0.8921, R:0.0105)
Batch 175/537: Loss=0.9087 (C:0.9087, R:0.0105)
Batch 200/537: Loss=0.8732 (C:0.8732, R:0.0105)
Batch 225/537: Loss=0.9027 (C:0.9027, R:0.0105)
Batch 250/537: Loss=0.8886 (C:0.8886, R:0.0105)
Batch 275/537: Loss=0.9299 (C:0.9299, R:0.0106)
Batch 300/537: Loss=0.8848 (C:0.8848, R:0.0105)
Batch 325/537: Loss=0.9106 (C:0.9106, R:0.0105)
Batch 350/537: Loss=0.8932 (C:0.8932, R:0.0105)
Batch 375/537: Loss=0.8825 (C:0.8825, R:0.0105)
Batch 400/537: Loss=0.9088 (C:0.9088, R:0.0106)
Batch 425/537: Loss=0.9039 (C:0.9039, R:0.0105)
Batch 450/537: Loss=0.9472 (C:0.9472, R:0.0105)
Batch 475/537: Loss=0.8692 (C:0.8692, R:0.0105)
Batch 500/537: Loss=0.9228 (C:0.9228, R:0.0105)
Batch 525/537: Loss=0.9199 (C:0.9199, R:0.0105)

============================================================
Epoch 21/300 completed in 21.4s
Train: Loss=0.8924 (C:0.8924, R:0.0105) Ratio=4.23x
Val:   Loss=0.9731 (C:0.9731, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.265 ± 0.464
    Neg distances: 1.676 ± 0.893
    Separation ratio: 6.32x
    Gap: -2.866
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8577 (C:0.8577, R:0.0105)
Batch  25/537: Loss=0.8649 (C:0.8649, R:0.0105)
Batch  50/537: Loss=0.8415 (C:0.8415, R:0.0105)
Batch  75/537: Loss=0.8864 (C:0.8864, R:0.0105)
Batch 100/537: Loss=0.8553 (C:0.8553, R:0.0105)
Batch 125/537: Loss=0.8580 (C:0.8580, R:0.0105)
Batch 150/537: Loss=0.8326 (C:0.8326, R:0.0105)
Batch 175/537: Loss=0.8434 (C:0.8434, R:0.0105)
Batch 200/537: Loss=0.8609 (C:0.8609, R:0.0105)
Batch 225/537: Loss=0.8743 (C:0.8743, R:0.0105)
Batch 250/537: Loss=0.8788 (C:0.8788, R:0.0105)
Batch 275/537: Loss=0.8663 (C:0.8663, R:0.0105)
Batch 300/537: Loss=0.8551 (C:0.8551, R:0.0105)
Batch 325/537: Loss=0.8591 (C:0.8591, R:0.0105)
Batch 350/537: Loss=0.8528 (C:0.8528, R:0.0105)
Batch 375/537: Loss=0.8800 (C:0.8800, R:0.0105)
Batch 400/537: Loss=0.8912 (C:0.8912, R:0.0105)
Batch 425/537: Loss=0.8639 (C:0.8639, R:0.0105)
Batch 450/537: Loss=0.8481 (C:0.8481, R:0.0105)
Batch 475/537: Loss=0.8595 (C:0.8595, R:0.0105)
Batch 500/537: Loss=0.8631 (C:0.8631, R:0.0105)
Batch 525/537: Loss=0.8890 (C:0.8890, R:0.0105)

============================================================
Epoch 22/300 completed in 27.4s
Train: Loss=0.8627 (C:0.8627, R:0.0105) Ratio=4.33x
Val:   Loss=0.9355 (C:0.9355, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9355)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.8675 (C:0.8675, R:0.0105)
Batch  25/537: Loss=0.8349 (C:0.8349, R:0.0105)
Batch  50/537: Loss=0.8265 (C:0.8265, R:0.0106)
Batch  75/537: Loss=0.8692 (C:0.8692, R:0.0105)
Batch 100/537: Loss=0.8375 (C:0.8375, R:0.0105)
Batch 125/537: Loss=0.8768 (C:0.8768, R:0.0105)
Batch 150/537: Loss=0.8591 (C:0.8591, R:0.0105)
Batch 175/537: Loss=0.8687 (C:0.8687, R:0.0105)
Batch 200/537: Loss=0.8301 (C:0.8301, R:0.0105)
Batch 225/537: Loss=0.8530 (C:0.8530, R:0.0105)
Batch 250/537: Loss=0.8633 (C:0.8633, R:0.0105)
Batch 275/537: Loss=0.8658 (C:0.8658, R:0.0105)
Batch 300/537: Loss=0.8280 (C:0.8280, R:0.0105)
Batch 325/537: Loss=0.8516 (C:0.8516, R:0.0105)
Batch 350/537: Loss=0.8634 (C:0.8634, R:0.0105)
Batch 375/537: Loss=0.8437 (C:0.8437, R:0.0105)
Batch 400/537: Loss=0.8703 (C:0.8703, R:0.0105)
Batch 425/537: Loss=0.8498 (C:0.8498, R:0.0105)
Batch 450/537: Loss=0.8716 (C:0.8716, R:0.0105)
Batch 475/537: Loss=0.8976 (C:0.8976, R:0.0105)
Batch 500/537: Loss=0.8732 (C:0.8732, R:0.0105)
Batch 525/537: Loss=0.8622 (C:0.8622, R:0.0106)

============================================================
Epoch 23/300 completed in 21.5s
Train: Loss=0.8585 (C:0.8585, R:0.0105) Ratio=4.42x
Val:   Loss=0.9393 (C:0.9393, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8543 (C:0.8543, R:0.0105)
Batch  25/537: Loss=0.8567 (C:0.8567, R:0.0105)
Batch  50/537: Loss=0.8174 (C:0.8174, R:0.0105)
Batch  75/537: Loss=0.8602 (C:0.8602, R:0.0105)
Batch 100/537: Loss=0.8524 (C:0.8524, R:0.0105)
Batch 125/537: Loss=0.8201 (C:0.8201, R:0.0105)
Batch 150/537: Loss=0.8323 (C:0.8323, R:0.0105)
Batch 175/537: Loss=0.8781 (C:0.8781, R:0.0105)
Batch 200/537: Loss=0.8788 (C:0.8788, R:0.0105)
Batch 225/537: Loss=0.8611 (C:0.8611, R:0.0105)
Batch 250/537: Loss=0.8738 (C:0.8738, R:0.0105)
Batch 275/537: Loss=0.8592 (C:0.8592, R:0.0105)
Batch 300/537: Loss=0.8361 (C:0.8361, R:0.0105)
Batch 325/537: Loss=0.8422 (C:0.8422, R:0.0105)
Batch 350/537: Loss=0.8420 (C:0.8420, R:0.0105)
Batch 375/537: Loss=0.8646 (C:0.8646, R:0.0105)
Batch 400/537: Loss=0.8370 (C:0.8370, R:0.0105)
Batch 425/537: Loss=0.8883 (C:0.8883, R:0.0105)
Batch 450/537: Loss=0.8526 (C:0.8526, R:0.0105)
Batch 475/537: Loss=0.8762 (C:0.8762, R:0.0105)
Batch 500/537: Loss=0.8127 (C:0.8127, R:0.0105)
Batch 525/537: Loss=0.8783 (C:0.8783, R:0.0105)

============================================================
Epoch 24/300 completed in 21.2s
Train: Loss=0.8565 (C:0.8565, R:0.0105) Ratio=4.45x
Val:   Loss=0.9449 (C:0.9449, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.274 ± 0.476
    Neg distances: 1.705 ± 0.909
    Separation ratio: 6.23x
    Gap: -2.987
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.8560 (C:0.8560, R:0.0105)
Batch  25/537: Loss=0.8572 (C:0.8572, R:0.0105)
Batch  50/537: Loss=0.8551 (C:0.8551, R:0.0105)
Batch  75/537: Loss=0.8602 (C:0.8602, R:0.0105)
Batch 100/537: Loss=0.8251 (C:0.8251, R:0.0105)
Batch 125/537: Loss=0.8323 (C:0.8323, R:0.0105)
Batch 150/537: Loss=0.8184 (C:0.8184, R:0.0105)
Batch 175/537: Loss=0.8550 (C:0.8550, R:0.0105)
Batch 200/537: Loss=0.8669 (C:0.8669, R:0.0105)
Batch 225/537: Loss=0.8305 (C:0.8305, R:0.0105)
Batch 250/537: Loss=0.8626 (C:0.8626, R:0.0105)
Batch 275/537: Loss=0.8369 (C:0.8369, R:0.0106)
Batch 300/537: Loss=0.8476 (C:0.8476, R:0.0105)
Batch 325/537: Loss=0.8718 (C:0.8718, R:0.0105)
Batch 350/537: Loss=0.8994 (C:0.8994, R:0.0105)
Batch 375/537: Loss=0.8431 (C:0.8431, R:0.0105)
Batch 400/537: Loss=0.8586 (C:0.8586, R:0.0105)
Batch 425/537: Loss=0.8886 (C:0.8886, R:0.0105)
Batch 450/537: Loss=0.8624 (C:0.8624, R:0.0105)
Batch 475/537: Loss=0.8429 (C:0.8429, R:0.0105)
Batch 500/537: Loss=0.8181 (C:0.8181, R:0.0105)
Batch 525/537: Loss=0.8371 (C:0.8371, R:0.0105)

============================================================
Epoch 25/300 completed in 27.6s
Train: Loss=0.8545 (C:0.8545, R:0.0105) Ratio=4.50x
Val:   Loss=0.9434 (C:0.9434, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
No improvement for 3 epochs
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.8548 (C:0.8548, R:0.0105)
Batch  25/537: Loss=0.8584 (C:0.8584, R:0.0105)
Batch  50/537: Loss=0.8934 (C:0.8934, R:0.0105)
Batch  75/537: Loss=0.8376 (C:0.8376, R:0.0105)
Batch 100/537: Loss=0.8506 (C:0.8506, R:0.0105)
Batch 125/537: Loss=0.8557 (C:0.8557, R:0.0105)
Batch 150/537: Loss=0.8464 (C:0.8464, R:0.0105)
Batch 175/537: Loss=0.8347 (C:0.8347, R:0.0106)
Batch 200/537: Loss=0.8678 (C:0.8678, R:0.0105)
Batch 225/537: Loss=0.8641 (C:0.8641, R:0.0105)
Batch 250/537: Loss=0.8568 (C:0.8568, R:0.0105)
Batch 275/537: Loss=0.8422 (C:0.8422, R:0.0105)
Batch 300/537: Loss=0.8524 (C:0.8524, R:0.0105)
Batch 325/537: Loss=0.8304 (C:0.8304, R:0.0105)
Batch 350/537: Loss=0.8464 (C:0.8464, R:0.0105)
Batch 375/537: Loss=0.8344 (C:0.8344, R:0.0104)
Batch 400/537: Loss=0.8510 (C:0.8510, R:0.0105)
Batch 425/537: Loss=0.8429 (C:0.8429, R:0.0105)
Batch 450/537: Loss=0.8621 (C:0.8621, R:0.0105)
Batch 475/537: Loss=0.8679 (C:0.8679, R:0.0105)
Batch 500/537: Loss=0.8747 (C:0.8747, R:0.0105)
Batch 525/537: Loss=0.8519 (C:0.8519, R:0.0105)

============================================================
Epoch 26/300 completed in 21.8s
Train: Loss=0.8536 (C:0.8536, R:0.0105) Ratio=4.56x
Val:   Loss=0.9558 (C:0.9558, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 4 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.8286 (C:0.8286, R:0.0105)
Batch  25/537: Loss=0.8122 (C:0.8122, R:0.0105)
Batch  50/537: Loss=0.8798 (C:0.8798, R:0.0106)
Batch  75/537: Loss=0.8385 (C:0.8385, R:0.0105)
Batch 100/537: Loss=0.8729 (C:0.8729, R:0.0105)
Batch 125/537: Loss=0.8111 (C:0.8111, R:0.0105)
Batch 150/537: Loss=0.8472 (C:0.8472, R:0.0105)
Batch 175/537: Loss=0.8358 (C:0.8358, R:0.0105)
Batch 200/537: Loss=0.8359 (C:0.8359, R:0.0105)
Batch 225/537: Loss=0.8647 (C:0.8647, R:0.0105)
Batch 250/537: Loss=0.8545 (C:0.8545, R:0.0105)
Batch 275/537: Loss=0.8663 (C:0.8663, R:0.0105)
Batch 300/537: Loss=0.8410 (C:0.8410, R:0.0105)
Batch 325/537: Loss=0.8507 (C:0.8507, R:0.0105)
Batch 350/537: Loss=0.8664 (C:0.8664, R:0.0105)
Batch 375/537: Loss=0.8110 (C:0.8110, R:0.0105)
Batch 400/537: Loss=0.8425 (C:0.8425, R:0.0105)
Batch 425/537: Loss=0.8511 (C:0.8511, R:0.0105)
Batch 450/537: Loss=0.8266 (C:0.8266, R:0.0105)
Batch 475/537: Loss=0.8759 (C:0.8759, R:0.0105)
Batch 500/537: Loss=0.8812 (C:0.8812, R:0.0105)
Batch 525/537: Loss=0.8572 (C:0.8572, R:0.0105)

============================================================
Epoch 27/300 completed in 21.3s
Train: Loss=0.8507 (C:0.8507, R:0.0105) Ratio=4.55x
Val:   Loss=0.9472 (C:0.9472, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.000
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.273 ± 0.470
    Neg distances: 1.766 ± 0.916
    Separation ratio: 6.46x
    Gap: -3.038
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.8140 (C:0.8140, R:0.0105)
Batch  25/537: Loss=0.8050 (C:0.8050, R:0.0105)
Batch  50/537: Loss=0.8166 (C:0.8166, R:0.0105)
Batch  75/537: Loss=0.8124 (C:0.8124, R:0.0105)
Batch 100/537: Loss=0.8255 (C:0.8255, R:0.0105)
Batch 125/537: Loss=0.8094 (C:0.8094, R:0.0105)
Batch 150/537: Loss=0.8097 (C:0.8097, R:0.0105)
Batch 175/537: Loss=0.8327 (C:0.8327, R:0.0105)
Batch 200/537: Loss=0.8192 (C:0.8192, R:0.0105)
Batch 225/537: Loss=0.8187 (C:0.8187, R:0.0105)
Batch 250/537: Loss=0.8101 (C:0.8101, R:0.0105)
Batch 275/537: Loss=0.8197 (C:0.8197, R:0.0105)
Batch 300/537: Loss=0.8641 (C:0.8641, R:0.0105)
Batch 325/537: Loss=0.8523 (C:0.8523, R:0.0105)
Batch 350/537: Loss=0.8436 (C:0.8436, R:0.0105)
Batch 375/537: Loss=0.8156 (C:0.8156, R:0.0105)
Batch 400/537: Loss=0.8555 (C:0.8555, R:0.0105)
Batch 425/537: Loss=0.8456 (C:0.8456, R:0.0105)
Batch 450/537: Loss=0.8388 (C:0.8388, R:0.0105)
Batch 475/537: Loss=0.8707 (C:0.8707, R:0.0105)
Batch 500/537: Loss=0.8366 (C:0.8366, R:0.0105)
Batch 525/537: Loss=0.8278 (C:0.8278, R:0.0105)

============================================================
Epoch 28/300 completed in 26.8s
Train: Loss=0.8287 (C:0.8287, R:0.0105) Ratio=4.49x
Val:   Loss=0.9287 (C:0.9287, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9287)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.8487 (C:0.8487, R:0.0105)
Batch  25/537: Loss=0.8175 (C:0.8175, R:0.0105)
Batch  50/537: Loss=0.8300 (C:0.8300, R:0.0105)
Batch  75/537: Loss=0.8494 (C:0.8494, R:0.0105)
Batch 100/537: Loss=0.8098 (C:0.8098, R:0.0105)
Batch 125/537: Loss=0.8261 (C:0.8261, R:0.0105)
Batch 150/537: Loss=0.8116 (C:0.8116, R:0.0105)
Batch 175/537: Loss=0.8084 (C:0.8084, R:0.0106)
Batch 200/537: Loss=0.8219 (C:0.8219, R:0.0105)
Batch 225/537: Loss=0.8057 (C:0.8057, R:0.0105)
Batch 250/537: Loss=0.8049 (C:0.8049, R:0.0105)
Batch 275/537: Loss=0.8173 (C:0.8173, R:0.0105)
Batch 300/537: Loss=0.8447 (C:0.8447, R:0.0105)
Batch 325/537: Loss=0.8446 (C:0.8446, R:0.0105)
Batch 350/537: Loss=0.8570 (C:0.8570, R:0.0105)
Batch 375/537: Loss=0.8024 (C:0.8024, R:0.0104)
Batch 400/537: Loss=0.8371 (C:0.8371, R:0.0105)
Batch 425/537: Loss=0.8267 (C:0.8267, R:0.0105)
Batch 450/537: Loss=0.8578 (C:0.8578, R:0.0105)
Batch 475/537: Loss=0.7949 (C:0.7949, R:0.0105)
Batch 500/537: Loss=0.8421 (C:0.8421, R:0.0105)
Batch 525/537: Loss=0.8486 (C:0.8486, R:0.0105)

============================================================
Epoch 29/300 completed in 21.2s
Train: Loss=0.8275 (C:0.8275, R:0.0105) Ratio=4.56x
Val:   Loss=0.9361 (C:0.9361, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.8189 (C:0.8189, R:0.0105)
Batch  25/537: Loss=0.8343 (C:0.8343, R:0.0105)
Batch  50/537: Loss=0.8383 (C:0.8383, R:0.0105)
Batch  75/537: Loss=0.8391 (C:0.8391, R:0.0105)
Batch 100/537: Loss=0.8065 (C:0.8065, R:0.0105)
Batch 125/537: Loss=0.7994 (C:0.7994, R:0.0105)
Batch 150/537: Loss=0.8638 (C:0.8638, R:0.0105)
Batch 175/537: Loss=0.8169 (C:0.8169, R:0.0105)
Batch 200/537: Loss=0.8369 (C:0.8369, R:0.0105)
Batch 225/537: Loss=0.8147 (C:0.8147, R:0.0106)
Batch 250/537: Loss=0.8425 (C:0.8425, R:0.0105)
Batch 275/537: Loss=0.8259 (C:0.8259, R:0.0105)
Batch 300/537: Loss=0.8147 (C:0.8147, R:0.0105)
Batch 325/537: Loss=0.8149 (C:0.8149, R:0.0105)
Batch 350/537: Loss=0.8044 (C:0.8044, R:0.0105)
Batch 375/537: Loss=0.8034 (C:0.8034, R:0.0106)
Batch 400/537: Loss=0.8168 (C:0.8168, R:0.0105)
Batch 425/537: Loss=0.8281 (C:0.8281, R:0.0105)
Batch 450/537: Loss=0.8370 (C:0.8370, R:0.0105)
Batch 475/537: Loss=0.8491 (C:0.8491, R:0.0105)
Batch 500/537: Loss=0.8390 (C:0.8390, R:0.0105)
Batch 525/537: Loss=0.8372 (C:0.8372, R:0.0105)

============================================================
Epoch 30/300 completed in 21.5s
Train: Loss=0.8253 (C:0.8253, R:0.0105) Ratio=4.58x
Val:   Loss=0.9314 (C:0.9314, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.276 ± 0.486
    Neg distances: 1.822 ± 0.917
    Separation ratio: 6.61x
    Gap: -3.161
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.8055 (C:0.8055, R:0.0105)
Batch  25/537: Loss=0.8053 (C:0.8053, R:0.0105)
Batch  50/537: Loss=0.7739 (C:0.7739, R:0.0105)
Batch  75/537: Loss=0.7929 (C:0.7929, R:0.0105)
Batch 100/537: Loss=0.8008 (C:0.8008, R:0.0105)
Batch 125/537: Loss=0.7962 (C:0.7962, R:0.0105)
Batch 150/537: Loss=0.8085 (C:0.8085, R:0.0105)
Batch 175/537: Loss=0.7840 (C:0.7840, R:0.0105)
Batch 200/537: Loss=0.8044 (C:0.8044, R:0.0105)
Batch 225/537: Loss=0.8280 (C:0.8280, R:0.0105)
Batch 250/537: Loss=0.8156 (C:0.8156, R:0.0105)
Batch 275/537: Loss=0.8098 (C:0.8098, R:0.0106)
Batch 300/537: Loss=0.8045 (C:0.8045, R:0.0105)
Batch 325/537: Loss=0.7730 (C:0.7730, R:0.0105)
Batch 350/537: Loss=0.7655 (C:0.7655, R:0.0105)
Batch 375/537: Loss=0.8088 (C:0.8088, R:0.0105)
Batch 400/537: Loss=0.8376 (C:0.8376, R:0.0105)
Batch 425/537: Loss=0.8284 (C:0.8284, R:0.0105)
Batch 450/537: Loss=0.8330 (C:0.8330, R:0.0105)
Batch 475/537: Loss=0.8402 (C:0.8402, R:0.0105)
Batch 500/537: Loss=0.8054 (C:0.8054, R:0.0105)
Batch 525/537: Loss=0.8019 (C:0.8019, R:0.0106)

============================================================
Epoch 31/300 completed in 27.6s
Train: Loss=0.8065 (C:0.8065, R:0.0105) Ratio=4.61x
Val:   Loss=0.9128 (C:0.9128, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.9128)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.8199 (C:0.8199, R:0.0105)
Batch  25/537: Loss=0.8160 (C:0.8160, R:0.0105)
Batch  50/537: Loss=0.8017 (C:0.8017, R:0.0105)
Batch  75/537: Loss=0.7866 (C:0.7866, R:0.0106)
Batch 100/537: Loss=0.8183 (C:0.8183, R:0.0105)
Batch 125/537: Loss=0.8119 (C:0.8119, R:0.0105)
Batch 150/537: Loss=0.8211 (C:0.8211, R:0.0105)
Batch 175/537: Loss=0.8223 (C:0.8223, R:0.0105)
Batch 200/537: Loss=0.8016 (C:0.8016, R:0.0105)
Batch 225/537: Loss=0.7780 (C:0.7780, R:0.0105)
Batch 250/537: Loss=0.7958 (C:0.7958, R:0.0105)
Batch 275/537: Loss=0.7986 (C:0.7986, R:0.0105)
Batch 300/537: Loss=0.7997 (C:0.7997, R:0.0105)
Batch 325/537: Loss=0.8152 (C:0.8152, R:0.0105)
Batch 350/537: Loss=0.8327 (C:0.8327, R:0.0105)
Batch 375/537: Loss=0.8198 (C:0.8198, R:0.0105)
Batch 400/537: Loss=0.7918 (C:0.7918, R:0.0105)
Batch 425/537: Loss=0.7914 (C:0.7914, R:0.0105)
Batch 450/537: Loss=0.8454 (C:0.8454, R:0.0105)
Batch 475/537: Loss=0.8178 (C:0.8178, R:0.0105)
Batch 500/537: Loss=0.8254 (C:0.8254, R:0.0105)
Batch 525/537: Loss=0.8169 (C:0.8169, R:0.0105)

============================================================
Epoch 32/300 completed in 21.7s
Train: Loss=0.8048 (C:0.8048, R:0.0105) Ratio=4.57x
Val:   Loss=0.9107 (C:0.9107, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.030
✅ New best model saved (Val Loss: 0.9107)
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.8199 (C:0.8199, R:0.0105)
Batch  25/537: Loss=0.8046 (C:0.8046, R:0.0105)
Batch  50/537: Loss=0.8277 (C:0.8277, R:0.0105)
Batch  75/537: Loss=0.8318 (C:0.8318, R:0.0105)
Batch 100/537: Loss=0.7813 (C:0.7813, R:0.0105)
Batch 125/537: Loss=0.7955 (C:0.7955, R:0.0105)
Batch 150/537: Loss=0.8004 (C:0.8004, R:0.0105)
Batch 175/537: Loss=0.7873 (C:0.7873, R:0.0105)
Batch 200/537: Loss=0.8044 (C:0.8044, R:0.0105)
Batch 225/537: Loss=0.8138 (C:0.8138, R:0.0105)
Batch 250/537: Loss=0.8300 (C:0.8300, R:0.0106)
Batch 275/537: Loss=0.7988 (C:0.7988, R:0.0105)
Batch 300/537: Loss=0.8086 (C:0.8086, R:0.0105)
Batch 325/537: Loss=0.8143 (C:0.8143, R:0.0105)
Batch 350/537: Loss=0.7983 (C:0.7983, R:0.0105)
Batch 375/537: Loss=0.8082 (C:0.8082, R:0.0105)
Batch 400/537: Loss=0.7759 (C:0.7759, R:0.0105)
Batch 425/537: Loss=0.8425 (C:0.8425, R:0.0105)
Batch 450/537: Loss=0.8514 (C:0.8514, R:0.0105)
Batch 475/537: Loss=0.8047 (C:0.8047, R:0.0105)
Batch 500/537: Loss=0.7940 (C:0.7940, R:0.0105)
Batch 525/537: Loss=0.7736 (C:0.7736, R:0.0106)

============================================================
Epoch 33/300 completed in 22.1s
Train: Loss=0.8028 (C:0.8028, R:0.0105) Ratio=4.58x
Val:   Loss=0.9096 (C:0.9096, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.9096)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.286 ± 0.489
    Neg distances: 1.879 ± 0.919
    Separation ratio: 6.57x
    Gap: -3.222
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.7355 (C:0.7355, R:0.0106)
Batch  25/537: Loss=0.7753 (C:0.7753, R:0.0105)
Batch  50/537: Loss=0.7890 (C:0.7890, R:0.0105)
Batch  75/537: Loss=0.7755 (C:0.7755, R:0.0105)
Batch 100/537: Loss=0.8055 (C:0.8055, R:0.0105)
Batch 125/537: Loss=0.7930 (C:0.7930, R:0.0105)
Batch 150/537: Loss=0.7857 (C:0.7857, R:0.0105)
Batch 175/537: Loss=0.8040 (C:0.8040, R:0.0105)
Batch 200/537: Loss=0.7961 (C:0.7961, R:0.0105)
Batch 225/537: Loss=0.7871 (C:0.7871, R:0.0105)
Batch 250/537: Loss=0.8054 (C:0.8054, R:0.0105)
Batch 275/537: Loss=0.8041 (C:0.8041, R:0.0105)
Batch 300/537: Loss=0.8112 (C:0.8112, R:0.0106)
Batch 325/537: Loss=0.8030 (C:0.8030, R:0.0105)
Batch 350/537: Loss=0.7907 (C:0.7907, R:0.0105)
Batch 375/537: Loss=0.7640 (C:0.7640, R:0.0105)
Batch 400/537: Loss=0.7743 (C:0.7743, R:0.0105)
Batch 425/537: Loss=0.8255 (C:0.8255, R:0.0105)
Batch 450/537: Loss=0.7687 (C:0.7687, R:0.0105)
Batch 475/537: Loss=0.8035 (C:0.8035, R:0.0105)
Batch 500/537: Loss=0.7935 (C:0.7935, R:0.0105)
Batch 525/537: Loss=0.7769 (C:0.7769, R:0.0105)

============================================================
Epoch 34/300 completed in 27.9s
Train: Loss=0.7905 (C:0.7905, R:0.0105) Ratio=4.65x
Val:   Loss=0.8991 (C:0.8991, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8991)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.7720 (C:0.7720, R:0.0105)
Batch  25/537: Loss=0.7753 (C:0.7753, R:0.0105)
Batch  50/537: Loss=0.7733 (C:0.7733, R:0.0105)
Batch  75/537: Loss=0.7769 (C:0.7769, R:0.0105)
Batch 100/537: Loss=0.7695 (C:0.7695, R:0.0105)
Batch 125/537: Loss=0.7845 (C:0.7845, R:0.0105)
Batch 150/537: Loss=0.7704 (C:0.7704, R:0.0105)
Batch 175/537: Loss=0.7741 (C:0.7741, R:0.0105)
Batch 200/537: Loss=0.7960 (C:0.7960, R:0.0105)
Batch 225/537: Loss=0.7884 (C:0.7884, R:0.0105)
Batch 250/537: Loss=0.8121 (C:0.8121, R:0.0105)
Batch 275/537: Loss=0.7980 (C:0.7980, R:0.0105)
Batch 300/537: Loss=0.7662 (C:0.7662, R:0.0105)
Batch 325/537: Loss=0.7928 (C:0.7928, R:0.0105)
Batch 350/537: Loss=0.8024 (C:0.8024, R:0.0105)
Batch 375/537: Loss=0.7698 (C:0.7698, R:0.0105)
Batch 400/537: Loss=0.8112 (C:0.8112, R:0.0105)
Batch 425/537: Loss=0.8022 (C:0.8022, R:0.0106)
Batch 450/537: Loss=0.8001 (C:0.8001, R:0.0105)
Batch 475/537: Loss=0.8193 (C:0.8193, R:0.0105)
Batch 500/537: Loss=0.7668 (C:0.7668, R:0.0105)
Batch 525/537: Loss=0.8222 (C:0.8222, R:0.0105)

============================================================
Epoch 35/300 completed in 21.8s
Train: Loss=0.7861 (C:0.7861, R:0.0105) Ratio=4.72x
Val:   Loss=0.8882 (C:0.8882, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 0.8882)
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.7832 (C:0.7832, R:0.0105)
Batch  25/537: Loss=0.7961 (C:0.7961, R:0.0105)
Batch  50/537: Loss=0.7433 (C:0.7433, R:0.0105)
Batch  75/537: Loss=0.7724 (C:0.7724, R:0.0105)
Batch 100/537: Loss=0.7852 (C:0.7852, R:0.0105)
Batch 125/537: Loss=0.7651 (C:0.7651, R:0.0105)
Batch 150/537: Loss=0.7638 (C:0.7638, R:0.0105)
Batch 175/537: Loss=0.7788 (C:0.7788, R:0.0105)
Batch 200/537: Loss=0.8072 (C:0.8072, R:0.0105)
Batch 225/537: Loss=0.7962 (C:0.7962, R:0.0106)
Batch 250/537: Loss=0.7836 (C:0.7836, R:0.0105)
Batch 275/537: Loss=0.7524 (C:0.7524, R:0.0105)
Batch 300/537: Loss=0.7933 (C:0.7933, R:0.0105)
Batch 325/537: Loss=0.8166 (C:0.8166, R:0.0105)
Batch 350/537: Loss=0.7985 (C:0.7985, R:0.0105)
Batch 375/537: Loss=0.7942 (C:0.7942, R:0.0105)
Batch 400/537: Loss=0.7916 (C:0.7916, R:0.0105)
Batch 425/537: Loss=0.8241 (C:0.8241, R:0.0105)
Batch 450/537: Loss=0.7721 (C:0.7721, R:0.0105)
Batch 475/537: Loss=0.7920 (C:0.7920, R:0.0105)
Batch 500/537: Loss=0.7802 (C:0.7802, R:0.0105)
Batch 525/537: Loss=0.8161 (C:0.8161, R:0.0105)

============================================================
Epoch 36/300 completed in 21.7s
Train: Loss=0.7856 (C:0.7856, R:0.0105) Ratio=4.74x
Val:   Loss=0.8908 (C:0.8908, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.090
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.280 ± 0.476
    Neg distances: 1.934 ± 0.922
    Separation ratio: 6.91x
    Gap: -3.238
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.7544 (C:0.7544, R:0.0105)
Batch  25/537: Loss=0.7406 (C:0.7406, R:0.0105)
Batch  50/537: Loss=0.7674 (C:0.7674, R:0.0105)
Batch  75/537: Loss=0.7629 (C:0.7629, R:0.0106)
Batch 100/537: Loss=0.7526 (C:0.7526, R:0.0105)
Batch 125/537: Loss=0.7500 (C:0.7500, R:0.0105)
Batch 150/537: Loss=0.7257 (C:0.7257, R:0.0105)
Batch 175/537: Loss=0.7822 (C:0.7822, R:0.0105)
Batch 200/537: Loss=0.7538 (C:0.7538, R:0.0105)
Batch 225/537: Loss=0.7845 (C:0.7845, R:0.0105)
Batch 250/537: Loss=0.7825 (C:0.7825, R:0.0105)
Batch 275/537: Loss=0.7704 (C:0.7704, R:0.0105)
Batch 300/537: Loss=0.7948 (C:0.7948, R:0.0105)
Batch 325/537: Loss=0.7585 (C:0.7585, R:0.0105)
Batch 350/537: Loss=0.7677 (C:0.7677, R:0.0105)
Batch 375/537: Loss=0.7512 (C:0.7512, R:0.0105)
Batch 400/537: Loss=0.7567 (C:0.7567, R:0.0105)
Batch 425/537: Loss=0.7918 (C:0.7918, R:0.0105)
Batch 450/537: Loss=0.7795 (C:0.7795, R:0.0105)
Batch 475/537: Loss=0.8061 (C:0.8061, R:0.0105)
Batch 500/537: Loss=0.8046 (C:0.8046, R:0.0105)
Batch 525/537: Loss=0.7854 (C:0.7854, R:0.0105)

============================================================
Epoch 37/300 completed in 27.8s
Train: Loss=0.7644 (C:0.7644, R:0.0105) Ratio=4.59x
Val:   Loss=0.8810 (C:0.8810, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8810)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.7772 (C:0.7772, R:0.0105)
Batch  25/537: Loss=0.7722 (C:0.7722, R:0.0105)
Batch  50/537: Loss=0.7594 (C:0.7594, R:0.0105)
Batch  75/537: Loss=0.7557 (C:0.7557, R:0.0105)
Batch 100/537: Loss=0.7673 (C:0.7673, R:0.0106)
Batch 125/537: Loss=0.7582 (C:0.7582, R:0.0105)
Batch 150/537: Loss=0.7475 (C:0.7475, R:0.0105)
Batch 175/537: Loss=0.7431 (C:0.7431, R:0.0105)
Batch 200/537: Loss=0.7569 (C:0.7569, R:0.0105)
Batch 225/537: Loss=0.7616 (C:0.7616, R:0.0105)
Batch 250/537: Loss=0.7524 (C:0.7524, R:0.0105)
Batch 275/537: Loss=0.7734 (C:0.7734, R:0.0105)
Batch 300/537: Loss=0.7653 (C:0.7653, R:0.0105)
Batch 325/537: Loss=0.7479 (C:0.7479, R:0.0105)
Batch 350/537: Loss=0.7422 (C:0.7422, R:0.0105)
Batch 375/537: Loss=0.7995 (C:0.7995, R:0.0105)
Batch 400/537: Loss=0.7677 (C:0.7677, R:0.0105)
Batch 425/537: Loss=0.7546 (C:0.7546, R:0.0105)
Batch 450/537: Loss=0.7789 (C:0.7789, R:0.0105)
Batch 475/537: Loss=0.7561 (C:0.7561, R:0.0105)
Batch 500/537: Loss=0.7638 (C:0.7638, R:0.0105)
Batch 525/537: Loss=0.7890 (C:0.7890, R:0.0105)

============================================================
Epoch 38/300 completed in 21.7s
Train: Loss=0.7614 (C:0.7614, R:0.0105) Ratio=4.69x
Val:   Loss=0.8751 (C:0.8751, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.120
✅ New best model saved (Val Loss: 0.8751)
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch  25/537: Loss=0.7686 (C:0.7686, R:0.0105)
Batch  50/537: Loss=0.7562 (C:0.7562, R:0.0105)
Batch  75/537: Loss=0.7474 (C:0.7474, R:0.0105)
Batch 100/537: Loss=0.7359 (C:0.7359, R:0.0105)
Batch 125/537: Loss=0.7264 (C:0.7264, R:0.0105)
Batch 150/537: Loss=0.7735 (C:0.7735, R:0.0105)
Batch 175/537: Loss=0.7724 (C:0.7724, R:0.0105)
Batch 200/537: Loss=0.7481 (C:0.7481, R:0.0105)
Batch 225/537: Loss=0.7793 (C:0.7793, R:0.0105)
Batch 250/537: Loss=0.7987 (C:0.7987, R:0.0105)
Batch 275/537: Loss=0.7465 (C:0.7465, R:0.0105)
Batch 300/537: Loss=0.7573 (C:0.7573, R:0.0105)
Batch 325/537: Loss=0.7681 (C:0.7681, R:0.0105)
Batch 350/537: Loss=0.7609 (C:0.7609, R:0.0105)
Batch 375/537: Loss=0.7596 (C:0.7596, R:0.0105)
Batch 400/537: Loss=0.7770 (C:0.7770, R:0.0105)
Batch 425/537: Loss=0.7661 (C:0.7661, R:0.0105)
Batch 450/537: Loss=0.7604 (C:0.7604, R:0.0105)
Batch 475/537: Loss=0.7800 (C:0.7800, R:0.0106)
Batch 500/537: Loss=0.7656 (C:0.7656, R:0.0105)
Batch 525/537: Loss=0.7536 (C:0.7536, R:0.0106)

============================================================
Epoch 39/300 completed in 21.4s
Train: Loss=0.7593 (C:0.7593, R:0.0105) Ratio=4.82x
Val:   Loss=0.8771 (C:0.8771, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.135
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.290 ± 0.507
    Neg distances: 2.011 ± 0.938
    Separation ratio: 6.93x
    Gap: -3.510
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.7329 (C:0.7329, R:0.0105)
Batch  25/537: Loss=0.7697 (C:0.7697, R:0.0105)
Batch  50/537: Loss=0.7506 (C:0.7506, R:0.0105)
Batch  75/537: Loss=0.7521 (C:0.7521, R:0.0105)
Batch 100/537: Loss=0.7482 (C:0.7482, R:0.0105)
Batch 125/537: Loss=0.7236 (C:0.7236, R:0.0105)
Batch 150/537: Loss=0.7496 (C:0.7496, R:0.0105)
Batch 175/537: Loss=0.7546 (C:0.7546, R:0.0105)
Batch 200/537: Loss=0.7695 (C:0.7695, R:0.0105)
Batch 225/537: Loss=0.7395 (C:0.7395, R:0.0105)
Batch 250/537: Loss=0.7638 (C:0.7638, R:0.0105)
Batch 275/537: Loss=0.7244 (C:0.7244, R:0.0105)
Batch 300/537: Loss=0.7123 (C:0.7123, R:0.0105)
Batch 325/537: Loss=0.7250 (C:0.7250, R:0.0105)
Batch 350/537: Loss=0.7649 (C:0.7649, R:0.0105)
Batch 375/537: Loss=0.7198 (C:0.7198, R:0.0105)
Batch 400/537: Loss=0.7646 (C:0.7646, R:0.0105)
Batch 425/537: Loss=0.7293 (C:0.7293, R:0.0105)
Batch 450/537: Loss=0.7601 (C:0.7601, R:0.0105)
Batch 475/537: Loss=0.7404 (C:0.7404, R:0.0105)
Batch 500/537: Loss=0.7506 (C:0.7506, R:0.0105)
Batch 525/537: Loss=0.7654 (C:0.7654, R:0.0105)

============================================================
Epoch 40/300 completed in 27.4s
Train: Loss=0.7405 (C:0.7405, R:0.0105) Ratio=4.75x
Val:   Loss=0.8640 (C:0.8640, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.8640)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.7722 (C:0.7722, R:0.0105)
Batch  25/537: Loss=0.7558 (C:0.7558, R:0.0105)
Batch  50/537: Loss=0.7124 (C:0.7124, R:0.0105)
Batch  75/537: Loss=0.7067 (C:0.7067, R:0.0105)
Batch 100/537: Loss=0.7266 (C:0.7266, R:0.0105)
Batch 125/537: Loss=0.7532 (C:0.7532, R:0.0105)
Batch 150/537: Loss=0.7131 (C:0.7131, R:0.0105)
Batch 175/537: Loss=0.7248 (C:0.7248, R:0.0105)
Batch 200/537: Loss=0.7379 (C:0.7379, R:0.0105)
Batch 225/537: Loss=0.7400 (C:0.7400, R:0.0105)
Batch 250/537: Loss=0.7468 (C:0.7468, R:0.0105)
Batch 275/537: Loss=0.7410 (C:0.7410, R:0.0105)
Batch 300/537: Loss=0.7165 (C:0.7165, R:0.0105)
Batch 325/537: Loss=0.7255 (C:0.7255, R:0.0105)
Batch 350/537: Loss=0.7440 (C:0.7440, R:0.0106)
Batch 375/537: Loss=0.7181 (C:0.7181, R:0.0105)
Batch 400/537: Loss=0.7260 (C:0.7260, R:0.0105)
Batch 425/537: Loss=0.7475 (C:0.7475, R:0.0105)
Batch 450/537: Loss=0.7410 (C:0.7410, R:0.0105)
Batch 475/537: Loss=0.7160 (C:0.7160, R:0.0105)
Batch 500/537: Loss=0.7906 (C:0.7906, R:0.0105)
Batch 525/537: Loss=0.7366 (C:0.7366, R:0.0105)

============================================================
Epoch 41/300 completed in 21.0s
Train: Loss=0.7373 (C:0.7373, R:0.0105) Ratio=4.79x
Val:   Loss=0.8597 (C:0.8597, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.8597)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.7386 (C:0.7386, R:0.0105)
Batch  25/537: Loss=0.7173 (C:0.7173, R:0.0105)
Batch  50/537: Loss=0.7147 (C:0.7147, R:0.0105)
Batch  75/537: Loss=0.7500 (C:0.7500, R:0.0105)
Batch 100/537: Loss=0.7384 (C:0.7384, R:0.0105)
Batch 125/537: Loss=0.7298 (C:0.7298, R:0.0105)
Batch 150/537: Loss=0.7254 (C:0.7254, R:0.0105)
Batch 175/537: Loss=0.7236 (C:0.7236, R:0.0105)
Batch 200/537: Loss=0.7175 (C:0.7175, R:0.0105)
Batch 225/537: Loss=0.7377 (C:0.7377, R:0.0105)
Batch 250/537: Loss=0.7512 (C:0.7512, R:0.0105)
Batch 275/537: Loss=0.7383 (C:0.7383, R:0.0105)
Batch 300/537: Loss=0.7356 (C:0.7356, R:0.0106)
Batch 325/537: Loss=0.7305 (C:0.7305, R:0.0105)
Batch 350/537: Loss=0.7326 (C:0.7326, R:0.0105)
Batch 375/537: Loss=0.7122 (C:0.7122, R:0.0106)
Batch 400/537: Loss=0.7735 (C:0.7735, R:0.0105)
Batch 425/537: Loss=0.7596 (C:0.7596, R:0.0105)
Batch 450/537: Loss=0.7643 (C:0.7643, R:0.0105)
Batch 475/537: Loss=0.7354 (C:0.7354, R:0.0105)
Batch 500/537: Loss=0.7207 (C:0.7207, R:0.0105)
Batch 525/537: Loss=0.7669 (C:0.7669, R:0.0105)

============================================================
Epoch 42/300 completed in 21.1s
Train: Loss=0.7371 (C:0.7371, R:0.0105) Ratio=4.80x
Val:   Loss=0.8612 (C:0.8612, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.180
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.293 ± 0.505
    Neg distances: 2.091 ± 0.959
    Separation ratio: 7.13x
    Gap: -3.575
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.7343 (C:0.7343, R:0.0105)
Batch  25/537: Loss=0.7441 (C:0.7441, R:0.0105)
Batch  50/537: Loss=0.7316 (C:0.7316, R:0.0105)
Batch  75/537: Loss=0.7215 (C:0.7215, R:0.0105)
Batch 100/537: Loss=0.7212 (C:0.7212, R:0.0105)
Batch 125/537: Loss=0.7470 (C:0.7470, R:0.0105)
Batch 150/537: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 175/537: Loss=0.7161 (C:0.7161, R:0.0105)
Batch 200/537: Loss=0.6704 (C:0.6704, R:0.0105)
Batch 225/537: Loss=0.7089 (C:0.7089, R:0.0105)
Batch 250/537: Loss=0.6714 (C:0.6714, R:0.0105)
Batch 275/537: Loss=0.7377 (C:0.7377, R:0.0105)
Batch 300/537: Loss=0.7110 (C:0.7110, R:0.0105)
Batch 325/537: Loss=0.7402 (C:0.7402, R:0.0105)
Batch 350/537: Loss=0.7060 (C:0.7060, R:0.0105)
Batch 375/537: Loss=0.7068 (C:0.7068, R:0.0106)
Batch 400/537: Loss=0.6950 (C:0.6950, R:0.0105)
Batch 425/537: Loss=0.7370 (C:0.7370, R:0.0105)
Batch 450/537: Loss=0.7157 (C:0.7157, R:0.0105)
Batch 475/537: Loss=0.7400 (C:0.7400, R:0.0105)
Batch 500/537: Loss=0.7334 (C:0.7334, R:0.0105)
Batch 525/537: Loss=0.7508 (C:0.7508, R:0.0105)

============================================================
Epoch 43/300 completed in 26.9s
Train: Loss=0.7172 (C:0.7172, R:0.0105) Ratio=4.78x
Val:   Loss=0.8402 (C:0.8402, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.8402)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.7127 (C:0.7127, R:0.0105)
Batch  25/537: Loss=0.7005 (C:0.7005, R:0.0105)
Batch  50/537: Loss=0.7300 (C:0.7300, R:0.0105)
Batch  75/537: Loss=0.7094 (C:0.7094, R:0.0105)
Batch 100/537: Loss=0.7015 (C:0.7015, R:0.0105)
Batch 125/537: Loss=0.7092 (C:0.7092, R:0.0105)
Batch 150/537: Loss=0.7429 (C:0.7429, R:0.0105)
Batch 175/537: Loss=0.6784 (C:0.6784, R:0.0105)
Batch 200/537: Loss=0.7230 (C:0.7230, R:0.0105)
Batch 225/537: Loss=0.7035 (C:0.7035, R:0.0105)
Batch 250/537: Loss=0.7272 (C:0.7272, R:0.0105)
Batch 275/537: Loss=0.7217 (C:0.7217, R:0.0105)
Batch 300/537: Loss=0.7304 (C:0.7304, R:0.0105)
Batch 325/537: Loss=0.7410 (C:0.7410, R:0.0105)
Batch 350/537: Loss=0.7272 (C:0.7272, R:0.0105)
Batch 375/537: Loss=0.6858 (C:0.6858, R:0.0105)
Batch 400/537: Loss=0.6940 (C:0.6940, R:0.0105)
Batch 425/537: Loss=0.7024 (C:0.7024, R:0.0105)
Batch 450/537: Loss=0.7285 (C:0.7285, R:0.0106)
Batch 475/537: Loss=0.7402 (C:0.7402, R:0.0105)
Batch 500/537: Loss=0.6912 (C:0.6912, R:0.0105)
Batch 525/537: Loss=0.7036 (C:0.7036, R:0.0105)

============================================================
Epoch 44/300 completed in 21.1s
Train: Loss=0.7149 (C:0.7149, R:0.0105) Ratio=4.79x
Val:   Loss=0.8413 (C:0.8413, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.7239 (C:0.7239, R:0.0105)
Batch  25/537: Loss=0.6988 (C:0.6988, R:0.0105)
Batch  50/537: Loss=0.7033 (C:0.7033, R:0.0105)
Batch  75/537: Loss=0.7143 (C:0.7143, R:0.0105)
Batch 100/537: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 125/537: Loss=0.7108 (C:0.7108, R:0.0105)
Batch 150/537: Loss=0.6905 (C:0.6905, R:0.0105)
Batch 175/537: Loss=0.7198 (C:0.7198, R:0.0105)
Batch 200/537: Loss=0.7051 (C:0.7051, R:0.0105)
Batch 225/537: Loss=0.7052 (C:0.7052, R:0.0105)
Batch 250/537: Loss=0.6928 (C:0.6928, R:0.0105)
Batch 275/537: Loss=0.6934 (C:0.6934, R:0.0106)
Batch 300/537: Loss=0.7322 (C:0.7322, R:0.0105)
Batch 325/537: Loss=0.7180 (C:0.7180, R:0.0105)
Batch 350/537: Loss=0.6891 (C:0.6891, R:0.0105)
Batch 375/537: Loss=0.7419 (C:0.7419, R:0.0106)
Batch 400/537: Loss=0.6876 (C:0.6876, R:0.0105)
Batch 425/537: Loss=0.7107 (C:0.7107, R:0.0105)
Batch 450/537: Loss=0.7286 (C:0.7286, R:0.0105)
Batch 475/537: Loss=0.7105 (C:0.7105, R:0.0105)
Batch 500/537: Loss=0.7182 (C:0.7182, R:0.0105)
Batch 525/537: Loss=0.7520 (C:0.7520, R:0.0105)

============================================================
Epoch 45/300 completed in 21.0s
Train: Loss=0.7128 (C:0.7128, R:0.0105) Ratio=4.85x
Val:   Loss=0.8441 (C:0.8441, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.225
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.301 ± 0.517
    Neg distances: 2.128 ± 0.961
    Separation ratio: 7.08x
    Gap: -3.655
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.7176 (C:0.7176, R:0.0105)
Batch  25/537: Loss=0.7139 (C:0.7139, R:0.0105)
Batch  50/537: Loss=0.7058 (C:0.7058, R:0.0105)
Batch  75/537: Loss=0.6911 (C:0.6911, R:0.0105)
Batch 100/537: Loss=0.7187 (C:0.7187, R:0.0105)
Batch 125/537: Loss=0.6984 (C:0.6984, R:0.0106)
Batch 150/537: Loss=0.7274 (C:0.7274, R:0.0105)
Batch 175/537: Loss=0.7033 (C:0.7033, R:0.0105)
Batch 200/537: Loss=0.7153 (C:0.7153, R:0.0105)
Batch 225/537: Loss=0.7127 (C:0.7127, R:0.0105)
Batch 250/537: Loss=0.7123 (C:0.7123, R:0.0106)
Batch 275/537: Loss=0.7104 (C:0.7104, R:0.0105)
Batch 300/537: Loss=0.7404 (C:0.7404, R:0.0105)
Batch 325/537: Loss=0.7072 (C:0.7072, R:0.0105)
Batch 350/537: Loss=0.6993 (C:0.6993, R:0.0105)
Batch 375/537: Loss=0.7168 (C:0.7168, R:0.0105)
Batch 400/537: Loss=0.7531 (C:0.7531, R:0.0105)
Batch 425/537: Loss=0.7002 (C:0.7002, R:0.0105)
Batch 450/537: Loss=0.7173 (C:0.7173, R:0.0106)
Batch 475/537: Loss=0.7059 (C:0.7059, R:0.0105)
Batch 500/537: Loss=0.6916 (C:0.6916, R:0.0105)
Batch 525/537: Loss=0.7059 (C:0.7059, R:0.0105)

============================================================
Epoch 46/300 completed in 26.8s
Train: Loss=0.7015 (C:0.7015, R:0.0105) Ratio=4.70x
Val:   Loss=0.8311 (C:0.8311, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.240
✅ New best model saved (Val Loss: 0.8311)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.6952 (C:0.6952, R:0.0105)
Batch  25/537: Loss=0.6890 (C:0.6890, R:0.0105)
Batch  50/537: Loss=0.7091 (C:0.7091, R:0.0105)
Batch  75/537: Loss=0.7039 (C:0.7039, R:0.0105)
Batch 100/537: Loss=0.6989 (C:0.6989, R:0.0105)
Batch 125/537: Loss=0.6862 (C:0.6862, R:0.0105)
Batch 150/537: Loss=0.7064 (C:0.7064, R:0.0105)
Batch 175/537: Loss=0.6776 (C:0.6776, R:0.0105)
Batch 200/537: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 225/537: Loss=0.6696 (C:0.6696, R:0.0105)
Batch 250/537: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 275/537: Loss=0.6914 (C:0.6914, R:0.0105)
Batch 300/537: Loss=0.7285 (C:0.7285, R:0.0105)
Batch 325/537: Loss=0.6841 (C:0.6841, R:0.0105)
Batch 350/537: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 375/537: Loss=0.7141 (C:0.7141, R:0.0105)
Batch 400/537: Loss=0.7276 (C:0.7276, R:0.0105)
Batch 425/537: Loss=0.6914 (C:0.6914, R:0.0105)
Batch 450/537: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 475/537: Loss=0.7023 (C:0.7023, R:0.0105)
Batch 500/537: Loss=0.7216 (C:0.7216, R:0.0105)
Batch 525/537: Loss=0.7225 (C:0.7225, R:0.0105)

============================================================
Epoch 47/300 completed in 21.1s
Train: Loss=0.6994 (C:0.6994, R:0.0105) Ratio=4.77x
Val:   Loss=0.8368 (C:0.8368, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.255
No improvement for 1 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.7049 (C:0.7049, R:0.0105)
Batch  25/537: Loss=0.6990 (C:0.6990, R:0.0105)
Batch  50/537: Loss=0.6664 (C:0.6664, R:0.0105)
Batch  75/537: Loss=0.7090 (C:0.7090, R:0.0105)
Batch 100/537: Loss=0.6970 (C:0.6970, R:0.0105)
Batch 125/537: Loss=0.6723 (C:0.6723, R:0.0105)
Batch 150/537: Loss=0.7222 (C:0.7222, R:0.0105)
Batch 175/537: Loss=0.7005 (C:0.7005, R:0.0105)
Batch 200/537: Loss=0.7164 (C:0.7164, R:0.0105)
Batch 225/537: Loss=0.7037 (C:0.7037, R:0.0105)
Batch 250/537: Loss=0.6644 (C:0.6644, R:0.0105)
Batch 275/537: Loss=0.6865 (C:0.6865, R:0.0105)
Batch 300/537: Loss=0.7157 (C:0.7157, R:0.0105)
Batch 325/537: Loss=0.7283 (C:0.7283, R:0.0105)
Batch 350/537: Loss=0.7041 (C:0.7041, R:0.0105)
Batch 375/537: Loss=0.6770 (C:0.6770, R:0.0105)
Batch 400/537: Loss=0.6898 (C:0.6898, R:0.0106)
Batch 425/537: Loss=0.7178 (C:0.7178, R:0.0105)
Batch 450/537: Loss=0.6784 (C:0.6784, R:0.0105)
Batch 475/537: Loss=0.7066 (C:0.7066, R:0.0105)
Batch 500/537: Loss=0.7034 (C:0.7034, R:0.0105)
Batch 525/537: Loss=0.6996 (C:0.6996, R:0.0105)

============================================================
Epoch 48/300 completed in 21.1s
Train: Loss=0.6983 (C:0.6983, R:0.0105) Ratio=4.87x
Val:   Loss=0.8364 (C:0.8364, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.270
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.315 ± 0.542
    Neg distances: 2.174 ± 0.972
    Separation ratio: 6.91x
    Gap: -3.771
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.6929 (C:0.6929, R:0.0105)
Batch  25/537: Loss=0.6824 (C:0.6824, R:0.0105)
Batch  50/537: Loss=0.6635 (C:0.6635, R:0.0105)
Batch  75/537: Loss=0.6662 (C:0.6662, R:0.0105)
Batch 100/537: Loss=0.6692 (C:0.6692, R:0.0105)
Batch 125/537: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 150/537: Loss=0.6651 (C:0.6651, R:0.0105)
Batch 175/537: Loss=0.6441 (C:0.6441, R:0.0105)
Batch 200/537: Loss=0.6774 (C:0.6774, R:0.0105)
Batch 225/537: Loss=0.6760 (C:0.6760, R:0.0105)
Batch 250/537: Loss=0.7167 (C:0.7167, R:0.0105)
Batch 275/537: Loss=0.7121 (C:0.7121, R:0.0105)
Batch 300/537: Loss=0.6762 (C:0.6762, R:0.0105)
Batch 325/537: Loss=0.7102 (C:0.7102, R:0.0105)
Batch 350/537: Loss=0.6899 (C:0.6899, R:0.0105)
Batch 375/537: Loss=0.6931 (C:0.6931, R:0.0105)
Batch 400/537: Loss=0.6582 (C:0.6582, R:0.0105)
Batch 425/537: Loss=0.6780 (C:0.6780, R:0.0105)
Batch 450/537: Loss=0.6976 (C:0.6976, R:0.0105)
Batch 475/537: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 500/537: Loss=0.7475 (C:0.7475, R:0.0105)
Batch 525/537: Loss=0.7091 (C:0.7091, R:0.0105)

============================================================
Epoch 49/300 completed in 27.1s
Train: Loss=0.6867 (C:0.6867, R:0.0105) Ratio=4.90x
Val:   Loss=0.8281 (C:0.8281, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.8281)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.6744 (C:0.6744, R:0.0105)
Batch  25/537: Loss=0.6476 (C:0.6476, R:0.0105)
Batch  50/537: Loss=0.6493 (C:0.6493, R:0.0105)
Batch  75/537: Loss=0.7025 (C:0.7025, R:0.0105)
Batch 100/537: Loss=0.6999 (C:0.6999, R:0.0105)
Batch 125/537: Loss=0.6838 (C:0.6838, R:0.0105)
Batch 150/537: Loss=0.6791 (C:0.6791, R:0.0105)
Batch 175/537: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 200/537: Loss=0.6707 (C:0.6707, R:0.0105)
Batch 225/537: Loss=0.6793 (C:0.6793, R:0.0105)
Batch 250/537: Loss=0.6805 (C:0.6805, R:0.0105)
Batch 275/537: Loss=0.6747 (C:0.6747, R:0.0105)
Batch 300/537: Loss=0.7067 (C:0.7067, R:0.0105)
Batch 325/537: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 350/537: Loss=0.6769 (C:0.6769, R:0.0105)
Batch 375/537: Loss=0.7044 (C:0.7044, R:0.0105)
Batch 400/537: Loss=0.7028 (C:0.7028, R:0.0105)
Batch 425/537: Loss=0.7221 (C:0.7221, R:0.0105)
Batch 450/537: Loss=0.6755 (C:0.6755, R:0.0105)
Batch 475/537: Loss=0.7195 (C:0.7195, R:0.0105)
Batch 500/537: Loss=0.6962 (C:0.6962, R:0.0105)
Batch 525/537: Loss=0.7083 (C:0.7083, R:0.0105)

============================================================
Epoch 50/300 completed in 21.1s
Train: Loss=0.6842 (C:0.6842, R:0.0105) Ratio=4.91x
Val:   Loss=0.8234 (C:0.8234, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8234)
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.7408 (C:0.7408, R:0.0105)
Batch  25/537: Loss=0.6463 (C:0.6463, R:0.0105)
Batch  50/537: Loss=0.6790 (C:0.6790, R:0.0105)
Batch  75/537: Loss=0.6712 (C:0.6712, R:0.0105)
Batch 100/537: Loss=0.6909 (C:0.6909, R:0.0105)
Batch 125/537: Loss=0.6748 (C:0.6748, R:0.0105)
Batch 150/537: Loss=0.6835 (C:0.6835, R:0.0105)
Batch 175/537: Loss=0.6883 (C:0.6883, R:0.0105)
Batch 200/537: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 225/537: Loss=0.6815 (C:0.6815, R:0.0105)
Batch 250/537: Loss=0.6953 (C:0.6953, R:0.0105)
Batch 275/537: Loss=0.6722 (C:0.6722, R:0.0105)
Batch 300/537: Loss=0.6511 (C:0.6511, R:0.0106)
Batch 325/537: Loss=0.6553 (C:0.6553, R:0.0105)
Batch 350/537: Loss=0.6680 (C:0.6680, R:0.0105)
Batch 375/537: Loss=0.7051 (C:0.7051, R:0.0105)
Batch 400/537: Loss=0.6774 (C:0.6774, R:0.0105)
Batch 425/537: Loss=0.7229 (C:0.7229, R:0.0105)
Batch 450/537: Loss=0.6777 (C:0.6777, R:0.0105)
Batch 475/537: Loss=0.6618 (C:0.6618, R:0.0105)
Batch 500/537: Loss=0.6760 (C:0.6760, R:0.0105)
Batch 525/537: Loss=0.7001 (C:0.7001, R:0.0105)

============================================================
Epoch 51/300 completed in 21.0s
Train: Loss=0.6837 (C:0.6837, R:0.0105) Ratio=4.94x
Val:   Loss=0.8184 (C:0.8184, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8184)
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.319 ± 0.552
    Neg distances: 2.263 ± 0.997
    Separation ratio: 7.10x
    Gap: -3.863
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.6512 (C:0.6512, R:0.0105)
Batch  25/537: Loss=0.6510 (C:0.6510, R:0.0105)
Batch  50/537: Loss=0.6556 (C:0.6556, R:0.0105)
Batch  75/537: Loss=0.6435 (C:0.6435, R:0.0105)
Batch 100/537: Loss=0.6690 (C:0.6690, R:0.0105)
Batch 125/537: Loss=0.6532 (C:0.6532, R:0.0105)
Batch 150/537: Loss=0.6347 (C:0.6347, R:0.0105)
Batch 175/537: Loss=0.6674 (C:0.6674, R:0.0105)
Batch 200/537: Loss=0.6769 (C:0.6769, R:0.0105)
Batch 225/537: Loss=0.6655 (C:0.6655, R:0.0105)
Batch 250/537: Loss=0.6775 (C:0.6775, R:0.0105)
Batch 275/537: Loss=0.6745 (C:0.6745, R:0.0105)
Batch 300/537: Loss=0.6616 (C:0.6616, R:0.0105)
Batch 325/537: Loss=0.6549 (C:0.6549, R:0.0105)
Batch 350/537: Loss=0.6627 (C:0.6627, R:0.0105)
Batch 375/537: Loss=0.6851 (C:0.6851, R:0.0105)
Batch 400/537: Loss=0.6777 (C:0.6777, R:0.0105)
Batch 425/537: Loss=0.6675 (C:0.6675, R:0.0105)
Batch 450/537: Loss=0.7094 (C:0.7094, R:0.0105)
Batch 475/537: Loss=0.6977 (C:0.6977, R:0.0105)
Batch 500/537: Loss=0.6338 (C:0.6338, R:0.0105)
Batch 525/537: Loss=0.6767 (C:0.6767, R:0.0105)

============================================================
Epoch 52/300 completed in 26.7s
Train: Loss=0.6611 (C:0.6611, R:0.0105) Ratio=4.85x
Val:   Loss=0.8001 (C:0.8001, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8001)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.6832 (C:0.6832, R:0.0105)
Batch  25/537: Loss=0.6566 (C:0.6566, R:0.0106)
Batch  50/537: Loss=0.6238 (C:0.6238, R:0.0105)
Batch  75/537: Loss=0.6497 (C:0.6497, R:0.0105)
Batch 100/537: Loss=0.6743 (C:0.6743, R:0.0105)
Batch 125/537: Loss=0.6514 (C:0.6514, R:0.0105)
Batch 150/537: Loss=0.6512 (C:0.6512, R:0.0105)
Batch 175/537: Loss=0.6444 (C:0.6444, R:0.0105)
Batch 200/537: Loss=0.7150 (C:0.7150, R:0.0105)
Batch 225/537: Loss=0.6614 (C:0.6614, R:0.0105)
Batch 250/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch 275/537: Loss=0.7020 (C:0.7020, R:0.0105)
Batch 300/537: Loss=0.6691 (C:0.6691, R:0.0105)
Batch 325/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 350/537: Loss=0.6384 (C:0.6384, R:0.0105)
Batch 375/537: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 400/537: Loss=0.6548 (C:0.6548, R:0.0105)
Batch 425/537: Loss=0.6606 (C:0.6606, R:0.0105)
Batch 450/537: Loss=0.6692 (C:0.6692, R:0.0105)
Batch 475/537: Loss=0.6807 (C:0.6807, R:0.0105)
Batch 500/537: Loss=0.6670 (C:0.6670, R:0.0105)
Batch 525/537: Loss=0.6283 (C:0.6283, R:0.0106)

============================================================
Epoch 53/300 completed in 21.1s
Train: Loss=0.6607 (C:0.6607, R:0.0105) Ratio=4.89x
Val:   Loss=0.7984 (C:0.7984, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7984)
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.6488 (C:0.6488, R:0.0105)
Batch  25/537: Loss=0.6549 (C:0.6549, R:0.0105)
Batch  50/537: Loss=0.6659 (C:0.6659, R:0.0105)
Batch  75/537: Loss=0.6301 (C:0.6301, R:0.0105)
Batch 100/537: Loss=0.6809 (C:0.6809, R:0.0105)
Batch 125/537: Loss=0.6546 (C:0.6546, R:0.0105)
Batch 150/537: Loss=0.6676 (C:0.6676, R:0.0105)
Batch 175/537: Loss=0.6443 (C:0.6443, R:0.0105)
Batch 200/537: Loss=0.6915 (C:0.6915, R:0.0105)
Batch 225/537: Loss=0.6575 (C:0.6575, R:0.0105)
Batch 250/537: Loss=0.6377 (C:0.6377, R:0.0105)
Batch 275/537: Loss=0.6774 (C:0.6774, R:0.0105)
Batch 300/537: Loss=0.6634 (C:0.6634, R:0.0105)
Batch 325/537: Loss=0.6491 (C:0.6491, R:0.0105)
Batch 350/537: Loss=0.6646 (C:0.6646, R:0.0105)
Batch 375/537: Loss=0.6590 (C:0.6590, R:0.0105)
Batch 400/537: Loss=0.6435 (C:0.6435, R:0.0105)
Batch 425/537: Loss=0.6657 (C:0.6657, R:0.0105)
Batch 450/537: Loss=0.6439 (C:0.6439, R:0.0105)
Batch 475/537: Loss=0.6548 (C:0.6548, R:0.0105)
Batch 500/537: Loss=0.6674 (C:0.6674, R:0.0105)
Batch 525/537: Loss=0.6938 (C:0.6938, R:0.0105)

============================================================
Epoch 54/300 completed in 21.3s
Train: Loss=0.6578 (C:0.6578, R:0.0105) Ratio=4.97x
Val:   Loss=0.7973 (C:0.7973, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7973)
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.320 ± 0.546
    Neg distances: 2.327 ± 1.015
    Separation ratio: 7.28x
    Gap: -4.229
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch  25/537: Loss=0.6286 (C:0.6286, R:0.0105)
Batch  50/537: Loss=0.6438 (C:0.6438, R:0.0105)
Batch  75/537: Loss=0.6333 (C:0.6333, R:0.0105)
Batch 100/537: Loss=0.6746 (C:0.6746, R:0.0105)
Batch 125/537: Loss=0.6142 (C:0.6142, R:0.0105)
Batch 150/537: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 175/537: Loss=0.5939 (C:0.5939, R:0.0105)
Batch 200/537: Loss=0.6457 (C:0.6457, R:0.0105)
Batch 225/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 250/537: Loss=0.6566 (C:0.6566, R:0.0105)
Batch 275/537: Loss=0.6506 (C:0.6506, R:0.0105)
Batch 300/537: Loss=0.6360 (C:0.6360, R:0.0105)
Batch 325/537: Loss=0.6521 (C:0.6521, R:0.0105)
Batch 350/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 375/537: Loss=0.6474 (C:0.6474, R:0.0105)
Batch 400/537: Loss=0.6440 (C:0.6440, R:0.0105)
Batch 425/537: Loss=0.6553 (C:0.6553, R:0.0105)
Batch 450/537: Loss=0.6793 (C:0.6793, R:0.0105)
Batch 475/537: Loss=0.6044 (C:0.6044, R:0.0105)
Batch 500/537: Loss=0.6615 (C:0.6615, R:0.0105)
Batch 525/537: Loss=0.5938 (C:0.5938, R:0.0105)

============================================================
Epoch 55/300 completed in 26.7s
Train: Loss=0.6406 (C:0.6406, R:0.0105) Ratio=4.93x
Val:   Loss=0.7823 (C:0.7823, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7823)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.6270 (C:0.6270, R:0.0105)
Batch  25/537: Loss=0.5960 (C:0.5960, R:0.0105)
Batch  50/537: Loss=0.6340 (C:0.6340, R:0.0105)
Batch  75/537: Loss=0.6207 (C:0.6207, R:0.0105)
Batch 100/537: Loss=0.6596 (C:0.6596, R:0.0106)
Batch 125/537: Loss=0.6092 (C:0.6092, R:0.0105)
Batch 150/537: Loss=0.6527 (C:0.6527, R:0.0105)
Batch 175/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 200/537: Loss=0.6187 (C:0.6187, R:0.0105)
Batch 225/537: Loss=0.6258 (C:0.6258, R:0.0105)
Batch 250/537: Loss=0.6202 (C:0.6202, R:0.0105)
Batch 275/537: Loss=0.6380 (C:0.6380, R:0.0105)
Batch 300/537: Loss=0.6221 (C:0.6221, R:0.0105)
Batch 325/537: Loss=0.6150 (C:0.6150, R:0.0105)
Batch 350/537: Loss=0.6600 (C:0.6600, R:0.0105)
Batch 375/537: Loss=0.6477 (C:0.6477, R:0.0105)
Batch 400/537: Loss=0.6649 (C:0.6649, R:0.0105)
Batch 425/537: Loss=0.6512 (C:0.6512, R:0.0105)
Batch 450/537: Loss=0.6355 (C:0.6355, R:0.0106)
Batch 475/537: Loss=0.6816 (C:0.6816, R:0.0105)
Batch 500/537: Loss=0.6420 (C:0.6420, R:0.0105)
Batch 525/537: Loss=0.5871 (C:0.5871, R:0.0106)

============================================================
Epoch 56/300 completed in 20.9s
Train: Loss=0.6395 (C:0.6395, R:0.0105) Ratio=4.99x
Val:   Loss=0.7871 (C:0.7871, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch  25/537: Loss=0.6549 (C:0.6549, R:0.0105)
Batch  50/537: Loss=0.6115 (C:0.6115, R:0.0105)
Batch  75/537: Loss=0.6246 (C:0.6246, R:0.0105)
Batch 100/537: Loss=0.6304 (C:0.6304, R:0.0105)
Batch 125/537: Loss=0.6376 (C:0.6376, R:0.0105)
Batch 150/537: Loss=0.6261 (C:0.6261, R:0.0105)
Batch 175/537: Loss=0.6578 (C:0.6578, R:0.0105)
Batch 200/537: Loss=0.6290 (C:0.6290, R:0.0105)
Batch 225/537: Loss=0.6267 (C:0.6267, R:0.0105)
Batch 250/537: Loss=0.6809 (C:0.6809, R:0.0105)
Batch 275/537: Loss=0.6633 (C:0.6633, R:0.0105)
Batch 300/537: Loss=0.6215 (C:0.6215, R:0.0105)
Batch 325/537: Loss=0.6183 (C:0.6183, R:0.0105)
Batch 350/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 375/537: Loss=0.6062 (C:0.6062, R:0.0105)
Batch 400/537: Loss=0.6544 (C:0.6544, R:0.0105)
Batch 425/537: Loss=0.6404 (C:0.6404, R:0.0105)
Batch 450/537: Loss=0.6244 (C:0.6244, R:0.0105)
Batch 475/537: Loss=0.5981 (C:0.5981, R:0.0106)
Batch 500/537: Loss=0.6326 (C:0.6326, R:0.0105)
Batch 525/537: Loss=0.6357 (C:0.6357, R:0.0105)

============================================================
Epoch 57/300 completed in 20.9s
Train: Loss=0.6369 (C:0.6369, R:0.0105) Ratio=5.03x
Val:   Loss=0.7946 (C:0.7946, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.347 ± 0.605
    Neg distances: 2.356 ± 1.041
    Separation ratio: 6.78x
    Gap: -3.997
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.6460 (C:0.6460, R:0.0105)
Batch  25/537: Loss=0.6558 (C:0.6558, R:0.0105)
Batch  50/537: Loss=0.6693 (C:0.6693, R:0.0105)
Batch  75/537: Loss=0.6375 (C:0.6375, R:0.0105)
Batch 100/537: Loss=0.6685 (C:0.6685, R:0.0105)
Batch 125/537: Loss=0.6436 (C:0.6436, R:0.0105)
Batch 150/537: Loss=0.6316 (C:0.6316, R:0.0105)
Batch 175/537: Loss=0.6350 (C:0.6350, R:0.0105)
Batch 200/537: Loss=0.6440 (C:0.6440, R:0.0105)
Batch 225/537: Loss=0.6606 (C:0.6606, R:0.0105)
Batch 250/537: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 275/537: Loss=0.6356 (C:0.6356, R:0.0105)
Batch 300/537: Loss=0.6420 (C:0.6420, R:0.0105)
Batch 325/537: Loss=0.6429 (C:0.6429, R:0.0105)
Batch 350/537: Loss=0.6322 (C:0.6322, R:0.0105)
Batch 375/537: Loss=0.6219 (C:0.6219, R:0.0105)
Batch 400/537: Loss=0.6619 (C:0.6619, R:0.0105)
Batch 425/537: Loss=0.6562 (C:0.6562, R:0.0105)
Batch 450/537: Loss=0.6197 (C:0.6197, R:0.0105)
Batch 475/537: Loss=0.6343 (C:0.6343, R:0.0105)
Batch 500/537: Loss=0.6416 (C:0.6416, R:0.0105)
Batch 525/537: Loss=0.6524 (C:0.6524, R:0.0105)

============================================================
Epoch 58/300 completed in 26.8s
Train: Loss=0.6459 (C:0.6459, R:0.0105) Ratio=5.06x
Val:   Loss=0.7957 (C:0.7957, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.6548 (C:0.6548, R:0.0105)
Batch  25/537: Loss=0.6136 (C:0.6136, R:0.0105)
Batch  50/537: Loss=0.6058 (C:0.6058, R:0.0105)
Batch  75/537: Loss=0.6258 (C:0.6258, R:0.0105)
Batch 100/537: Loss=0.6317 (C:0.6317, R:0.0105)
Batch 125/537: Loss=0.6194 (C:0.6194, R:0.0105)
Batch 150/537: Loss=0.6676 (C:0.6676, R:0.0105)
Batch 175/537: Loss=0.6284 (C:0.6284, R:0.0105)
Batch 200/537: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 225/537: Loss=0.6244 (C:0.6244, R:0.0105)
Batch 250/537: Loss=0.7128 (C:0.7128, R:0.0105)
Batch 275/537: Loss=0.6214 (C:0.6214, R:0.0105)
Batch 300/537: Loss=0.6530 (C:0.6530, R:0.0105)
Batch 325/537: Loss=0.6866 (C:0.6866, R:0.0105)
Batch 350/537: Loss=0.6810 (C:0.6810, R:0.0105)
Batch 375/537: Loss=0.6406 (C:0.6406, R:0.0105)
Batch 400/537: Loss=0.6401 (C:0.6401, R:0.0105)
Batch 425/537: Loss=0.6518 (C:0.6518, R:0.0105)
Batch 450/537: Loss=0.6216 (C:0.6216, R:0.0105)
Batch 475/537: Loss=0.6759 (C:0.6759, R:0.0105)
Batch 500/537: Loss=0.6204 (C:0.6204, R:0.0105)
Batch 525/537: Loss=0.6605 (C:0.6605, R:0.0105)

============================================================
Epoch 59/300 completed in 20.9s
Train: Loss=0.6446 (C:0.6446, R:0.0105) Ratio=5.05x
Val:   Loss=0.7999 (C:0.7999, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.6003 (C:0.6003, R:0.0105)
Batch  25/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch  50/537: Loss=0.6218 (C:0.6218, R:0.0105)
Batch  75/537: Loss=0.6755 (C:0.6755, R:0.0105)
Batch 100/537: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 125/537: Loss=0.6246 (C:0.6246, R:0.0105)
Batch 150/537: Loss=0.6657 (C:0.6657, R:0.0105)
Batch 175/537: Loss=0.6446 (C:0.6446, R:0.0105)
Batch 200/537: Loss=0.6689 (C:0.6689, R:0.0105)
Batch 225/537: Loss=0.6581 (C:0.6581, R:0.0105)
Batch 250/537: Loss=0.6677 (C:0.6677, R:0.0105)
Batch 275/537: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 300/537: Loss=0.6780 (C:0.6780, R:0.0105)
Batch 325/537: Loss=0.6369 (C:0.6369, R:0.0105)
Batch 350/537: Loss=0.6439 (C:0.6439, R:0.0105)
Batch 375/537: Loss=0.6690 (C:0.6690, R:0.0105)
Batch 400/537: Loss=0.6774 (C:0.6774, R:0.0105)
Batch 425/537: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 450/537: Loss=0.6381 (C:0.6381, R:0.0105)
Batch 475/537: Loss=0.6502 (C:0.6502, R:0.0105)
Batch 500/537: Loss=0.6679 (C:0.6679, R:0.0105)
Batch 525/537: Loss=0.6197 (C:0.6197, R:0.0105)

============================================================
Epoch 60/300 completed in 20.9s
Train: Loss=0.6442 (C:0.6442, R:0.0105) Ratio=5.04x
Val:   Loss=0.7938 (C:0.7938, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 5 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.312 ± 0.560
    Neg distances: 2.417 ± 1.036
    Separation ratio: 7.75x
    Gap: -4.037
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.5985 (C:0.5985, R:0.0105)
Batch  25/537: Loss=0.5938 (C:0.5938, R:0.0105)
Batch  50/537: Loss=0.6088 (C:0.6088, R:0.0105)
Batch  75/537: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 100/537: Loss=0.6048 (C:0.6048, R:0.0105)
Batch 125/537: Loss=0.6034 (C:0.6034, R:0.0105)
Batch 150/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 175/537: Loss=0.6269 (C:0.6269, R:0.0105)
Batch 200/537: Loss=0.5823 (C:0.5823, R:0.0105)
Batch 225/537: Loss=0.6136 (C:0.6136, R:0.0105)
Batch 250/537: Loss=0.6197 (C:0.6197, R:0.0105)
Batch 275/537: Loss=0.6039 (C:0.6039, R:0.0105)
Batch 300/537: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 325/537: Loss=0.5892 (C:0.5892, R:0.0105)
Batch 350/537: Loss=0.5909 (C:0.5909, R:0.0105)
Batch 375/537: Loss=0.6129 (C:0.6129, R:0.0105)
Batch 400/537: Loss=0.6102 (C:0.6102, R:0.0105)
Batch 425/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 450/537: Loss=0.6119 (C:0.6119, R:0.0105)
Batch 475/537: Loss=0.6176 (C:0.6176, R:0.0105)
Batch 500/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 525/537: Loss=0.5914 (C:0.5914, R:0.0105)

============================================================
Epoch 61/300 completed in 27.0s
Train: Loss=0.6033 (C:0.6033, R:0.0105) Ratio=5.11x
Val:   Loss=0.7676 (C:0.7676, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7676)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch  25/537: Loss=0.5701 (C:0.5701, R:0.0105)
Batch  50/537: Loss=0.5630 (C:0.5630, R:0.0105)
Batch  75/537: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 100/537: Loss=0.5983 (C:0.5983, R:0.0105)
Batch 125/537: Loss=0.5739 (C:0.5739, R:0.0105)
Batch 150/537: Loss=0.6059 (C:0.6059, R:0.0105)
Batch 175/537: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 200/537: Loss=0.6025 (C:0.6025, R:0.0105)
Batch 225/537: Loss=0.5979 (C:0.5979, R:0.0105)
Batch 250/537: Loss=0.6229 (C:0.6229, R:0.0105)
Batch 275/537: Loss=0.6104 (C:0.6104, R:0.0105)
Batch 300/537: Loss=0.6268 (C:0.6268, R:0.0105)
Batch 325/537: Loss=0.5850 (C:0.5850, R:0.0105)
Batch 350/537: Loss=0.5961 (C:0.5961, R:0.0105)
Batch 375/537: Loss=0.5730 (C:0.5730, R:0.0105)
Batch 400/537: Loss=0.6423 (C:0.6423, R:0.0105)
Batch 425/537: Loss=0.6021 (C:0.6021, R:0.0105)
Batch 450/537: Loss=0.6379 (C:0.6379, R:0.0105)
Batch 475/537: Loss=0.6303 (C:0.6303, R:0.0105)
Batch 500/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 525/537: Loss=0.6003 (C:0.6003, R:0.0105)

============================================================
Epoch 62/300 completed in 21.0s
Train: Loss=0.6026 (C:0.6026, R:0.0105) Ratio=5.08x
Val:   Loss=0.7629 (C:0.7629, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7629)
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.6122 (C:0.6122, R:0.0105)
Batch  25/537: Loss=0.5958 (C:0.5958, R:0.0105)
Batch  50/537: Loss=0.5970 (C:0.5970, R:0.0105)
Batch  75/537: Loss=0.6225 (C:0.6225, R:0.0106)
Batch 100/537: Loss=0.5996 (C:0.5996, R:0.0105)
Batch 125/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 150/537: Loss=0.5919 (C:0.5919, R:0.0105)
Batch 175/537: Loss=0.5590 (C:0.5590, R:0.0105)
Batch 200/537: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 225/537: Loss=0.5903 (C:0.5903, R:0.0106)
Batch 250/537: Loss=0.5784 (C:0.5784, R:0.0105)
Batch 275/537: Loss=0.6512 (C:0.6512, R:0.0105)
Batch 300/537: Loss=0.6369 (C:0.6369, R:0.0105)
Batch 325/537: Loss=0.5947 (C:0.5947, R:0.0105)
Batch 350/537: Loss=0.6226 (C:0.6226, R:0.0105)
Batch 375/537: Loss=0.5928 (C:0.5928, R:0.0105)
Batch 400/537: Loss=0.5960 (C:0.5960, R:0.0105)
Batch 425/537: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 450/537: Loss=0.6004 (C:0.6004, R:0.0105)
Batch 475/537: Loss=0.6183 (C:0.6183, R:0.0105)
Batch 500/537: Loss=0.6254 (C:0.6254, R:0.0105)
Batch 525/537: Loss=0.6346 (C:0.6346, R:0.0105)

============================================================
Epoch 63/300 completed in 21.3s
Train: Loss=0.6008 (C:0.6008, R:0.0105) Ratio=5.08x
Val:   Loss=0.7646 (C:0.7646, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.300 ± 0.559
    Neg distances: 2.471 ± 1.047
    Separation ratio: 8.23x
    Gap: -4.081
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.5682 (C:0.5682, R:0.0105)
Batch  25/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch  50/537: Loss=0.5647 (C:0.5647, R:0.0105)
Batch  75/537: Loss=0.5912 (C:0.5912, R:0.0105)
Batch 100/537: Loss=0.5786 (C:0.5786, R:0.0105)
Batch 125/537: Loss=0.5804 (C:0.5804, R:0.0105)
Batch 150/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch 175/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 200/537: Loss=0.6209 (C:0.6209, R:0.0105)
Batch 225/537: Loss=0.5449 (C:0.5449, R:0.0105)
Batch 250/537: Loss=0.5985 (C:0.5985, R:0.0105)
Batch 275/537: Loss=0.5792 (C:0.5792, R:0.0105)
Batch 300/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 325/537: Loss=0.6189 (C:0.6189, R:0.0105)
Batch 350/537: Loss=0.6026 (C:0.6026, R:0.0105)
Batch 375/537: Loss=0.5664 (C:0.5664, R:0.0105)
Batch 400/537: Loss=0.5718 (C:0.5718, R:0.0105)
Batch 425/537: Loss=0.5737 (C:0.5737, R:0.0105)
Batch 450/537: Loss=0.6064 (C:0.6064, R:0.0105)
Batch 475/537: Loss=0.5741 (C:0.5741, R:0.0105)
Batch 500/537: Loss=0.5930 (C:0.5930, R:0.0105)
Batch 525/537: Loss=0.5879 (C:0.5879, R:0.0105)

============================================================
Epoch 64/300 completed in 27.3s
Train: Loss=0.5829 (C:0.5829, R:0.0105) Ratio=5.16x
Val:   Loss=0.7556 (C:0.7556, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7556)
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.6063 (C:0.6063, R:0.0105)
Batch  25/537: Loss=0.6056 (C:0.6056, R:0.0105)
Batch  50/537: Loss=0.5647 (C:0.5647, R:0.0105)
Batch  75/537: Loss=0.5946 (C:0.5946, R:0.0105)
Batch 100/537: Loss=0.5715 (C:0.5715, R:0.0105)
Batch 125/537: Loss=0.5966 (C:0.5966, R:0.0105)
Batch 150/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 175/537: Loss=0.5922 (C:0.5922, R:0.0105)
Batch 200/537: Loss=0.5827 (C:0.5827, R:0.0105)
Batch 225/537: Loss=0.5342 (C:0.5342, R:0.0105)
Batch 250/537: Loss=0.6257 (C:0.6257, R:0.0105)
Batch 275/537: Loss=0.5853 (C:0.5853, R:0.0106)
Batch 300/537: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 325/537: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 350/537: Loss=0.5894 (C:0.5894, R:0.0105)
Batch 375/537: Loss=0.6482 (C:0.6482, R:0.0105)
Batch 400/537: Loss=0.6293 (C:0.6293, R:0.0105)
Batch 425/537: Loss=0.5761 (C:0.5761, R:0.0105)
Batch 450/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 475/537: Loss=0.5988 (C:0.5988, R:0.0105)
Batch 500/537: Loss=0.6011 (C:0.6011, R:0.0105)
Batch 525/537: Loss=0.6055 (C:0.6055, R:0.0105)

============================================================
Epoch 65/300 completed in 21.7s
Train: Loss=0.5817 (C:0.5817, R:0.0105) Ratio=5.09x
Val:   Loss=0.7505 (C:0.7505, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7505)
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.5703 (C:0.5703, R:0.0105)
Batch  25/537: Loss=0.5530 (C:0.5530, R:0.0105)
Batch  50/537: Loss=0.5538 (C:0.5538, R:0.0105)
Batch  75/537: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 100/537: Loss=0.5730 (C:0.5730, R:0.0105)
Batch 125/537: Loss=0.6007 (C:0.6007, R:0.0105)
Batch 150/537: Loss=0.5818 (C:0.5818, R:0.0105)
Batch 175/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 200/537: Loss=0.5873 (C:0.5873, R:0.0105)
Batch 225/537: Loss=0.6091 (C:0.6091, R:0.0105)
Batch 250/537: Loss=0.5943 (C:0.5943, R:0.0105)
Batch 275/537: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 300/537: Loss=0.5763 (C:0.5763, R:0.0105)
Batch 325/537: Loss=0.5324 (C:0.5324, R:0.0105)
Batch 350/537: Loss=0.5788 (C:0.5788, R:0.0105)
Batch 375/537: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 400/537: Loss=0.6086 (C:0.6086, R:0.0105)
Batch 425/537: Loss=0.5452 (C:0.5452, R:0.0106)
Batch 450/537: Loss=0.5784 (C:0.5784, R:0.0105)
Batch 475/537: Loss=0.5797 (C:0.5797, R:0.0105)
Batch 500/537: Loss=0.6218 (C:0.6218, R:0.0105)
Batch 525/537: Loss=0.5798 (C:0.5798, R:0.0105)

============================================================
Epoch 66/300 completed in 21.5s
Train: Loss=0.5814 (C:0.5814, R:0.0105) Ratio=5.25x
Val:   Loss=0.7414 (C:0.7414, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7414)
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.331 ± 0.589
    Neg distances: 2.452 ± 1.060
    Separation ratio: 7.41x
    Gap: -4.204
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.6293 (C:0.6293, R:0.0105)
Batch  25/537: Loss=0.5788 (C:0.5788, R:0.0105)
Batch  50/537: Loss=0.5952 (C:0.5952, R:0.0105)
Batch  75/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 100/537: Loss=0.5646 (C:0.5646, R:0.0105)
Batch 125/537: Loss=0.6141 (C:0.6141, R:0.0105)
Batch 150/537: Loss=0.5841 (C:0.5841, R:0.0105)
Batch 175/537: Loss=0.5969 (C:0.5969, R:0.0105)
Batch 200/537: Loss=0.5722 (C:0.5722, R:0.0105)
Batch 225/537: Loss=0.5768 (C:0.5768, R:0.0105)
Batch 250/537: Loss=0.6148 (C:0.6148, R:0.0105)
Batch 275/537: Loss=0.6638 (C:0.6638, R:0.0105)
Batch 300/537: Loss=0.5970 (C:0.5970, R:0.0105)
Batch 325/537: Loss=0.5999 (C:0.5999, R:0.0105)
Batch 350/537: Loss=0.6165 (C:0.6165, R:0.0105)
Batch 375/537: Loss=0.5753 (C:0.5753, R:0.0105)
Batch 400/537: Loss=0.5964 (C:0.5964, R:0.0105)
Batch 425/537: Loss=0.6077 (C:0.6077, R:0.0105)
Batch 450/537: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 475/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch 500/537: Loss=0.6046 (C:0.6046, R:0.0106)
Batch 525/537: Loss=0.5912 (C:0.5912, R:0.0105)

============================================================
Epoch 67/300 completed in 26.9s
Train: Loss=0.6032 (C:0.6032, R:0.0105) Ratio=5.26x
Val:   Loss=0.7703 (C:0.7703, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.6080 (C:0.6080, R:0.0105)
Batch  25/537: Loss=0.5698 (C:0.5698, R:0.0106)
Batch  50/537: Loss=0.5949 (C:0.5949, R:0.0105)
Batch  75/537: Loss=0.6029 (C:0.6029, R:0.0105)
Batch 100/537: Loss=0.5946 (C:0.5946, R:0.0105)
Batch 125/537: Loss=0.6387 (C:0.6387, R:0.0105)
Batch 150/537: Loss=0.5613 (C:0.5613, R:0.0105)
Batch 175/537: Loss=0.5922 (C:0.5922, R:0.0105)
Batch 200/537: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 225/537: Loss=0.6020 (C:0.6020, R:0.0105)
Batch 250/537: Loss=0.6417 (C:0.6417, R:0.0105)
Batch 275/537: Loss=0.6076 (C:0.6076, R:0.0105)
Batch 300/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 325/537: Loss=0.5776 (C:0.5776, R:0.0106)
Batch 350/537: Loss=0.5704 (C:0.5704, R:0.0105)
Batch 375/537: Loss=0.5871 (C:0.5871, R:0.0105)
Batch 400/537: Loss=0.5939 (C:0.5939, R:0.0105)
Batch 425/537: Loss=0.6049 (C:0.6049, R:0.0105)
Batch 450/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 475/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 500/537: Loss=0.6085 (C:0.6085, R:0.0105)
Batch 525/537: Loss=0.5466 (C:0.5466, R:0.0105)

============================================================
Epoch 68/300 completed in 21.0s
Train: Loss=0.6028 (C:0.6028, R:0.0105) Ratio=5.19x
Val:   Loss=0.7605 (C:0.7605, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.6017 (C:0.6017, R:0.0105)
Batch  25/537: Loss=0.6131 (C:0.6131, R:0.0105)
Batch  50/537: Loss=0.6349 (C:0.6349, R:0.0105)
Batch  75/537: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 100/537: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 125/537: Loss=0.6002 (C:0.6002, R:0.0106)
Batch 150/537: Loss=0.5992 (C:0.5992, R:0.0105)
Batch 175/537: Loss=0.5833 (C:0.5833, R:0.0105)
Batch 200/537: Loss=0.5827 (C:0.5827, R:0.0105)
Batch 225/537: Loss=0.5659 (C:0.5659, R:0.0105)
Batch 250/537: Loss=0.6258 (C:0.6258, R:0.0105)
Batch 275/537: Loss=0.5894 (C:0.5894, R:0.0105)
Batch 300/537: Loss=0.5966 (C:0.5966, R:0.0105)
Batch 325/537: Loss=0.6145 (C:0.6145, R:0.0105)
Batch 350/537: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 375/537: Loss=0.5693 (C:0.5693, R:0.0105)
Batch 400/537: Loss=0.5969 (C:0.5969, R:0.0105)
Batch 425/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 450/537: Loss=0.5994 (C:0.5994, R:0.0105)
Batch 475/537: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 500/537: Loss=0.6230 (C:0.6230, R:0.0105)
Batch 525/537: Loss=0.5905 (C:0.5905, R:0.0105)

============================================================
Epoch 69/300 completed in 21.5s
Train: Loss=0.6004 (C:0.6004, R:0.0105) Ratio=5.28x
Val:   Loss=0.7633 (C:0.7633, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.296 ± 0.544
    Neg distances: 2.520 ± 1.061
    Separation ratio: 8.50x
    Gap: -4.190
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.5611 (C:0.5611, R:0.0105)
Batch  25/537: Loss=0.5876 (C:0.5876, R:0.0105)
Batch  50/537: Loss=0.6038 (C:0.6038, R:0.0105)
Batch  75/537: Loss=0.5792 (C:0.5792, R:0.0105)
Batch 100/537: Loss=0.5668 (C:0.5668, R:0.0105)
Batch 125/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 150/537: Loss=0.5606 (C:0.5606, R:0.0106)
Batch 175/537: Loss=0.5693 (C:0.5693, R:0.0105)
Batch 200/537: Loss=0.6064 (C:0.6064, R:0.0105)
Batch 225/537: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 250/537: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 275/537: Loss=0.6170 (C:0.6170, R:0.0105)
Batch 300/537: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 325/537: Loss=0.5433 (C:0.5433, R:0.0105)
Batch 350/537: Loss=0.5507 (C:0.5507, R:0.0105)
Batch 375/537: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 400/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 425/537: Loss=0.6093 (C:0.6093, R:0.0105)
Batch 450/537: Loss=0.5843 (C:0.5843, R:0.0105)
Batch 475/537: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 500/537: Loss=0.5680 (C:0.5680, R:0.0106)
Batch 525/537: Loss=0.5489 (C:0.5489, R:0.0105)

============================================================
Epoch 70/300 completed in 26.9s
Train: Loss=0.5650 (C:0.5650, R:0.0105) Ratio=5.15x
Val:   Loss=0.7342 (C:0.7342, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7342)
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.5421 (C:0.5421, R:0.0105)
Batch  25/537: Loss=0.5378 (C:0.5378, R:0.0105)
Batch  50/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch  75/537: Loss=0.5467 (C:0.5467, R:0.0105)
Batch 100/537: Loss=0.5733 (C:0.5733, R:0.0105)
Batch 125/537: Loss=0.5830 (C:0.5830, R:0.0105)
Batch 150/537: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 175/537: Loss=0.5294 (C:0.5294, R:0.0105)
Batch 200/537: Loss=0.5593 (C:0.5593, R:0.0105)
Batch 225/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 250/537: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 275/537: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 300/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 325/537: Loss=0.5900 (C:0.5900, R:0.0105)
Batch 350/537: Loss=0.5409 (C:0.5409, R:0.0105)
Batch 375/537: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 400/537: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 425/537: Loss=0.5884 (C:0.5884, R:0.0105)
Batch 450/537: Loss=0.5469 (C:0.5469, R:0.0105)
Batch 475/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 500/537: Loss=0.5622 (C:0.5622, R:0.0105)
Batch 525/537: Loss=0.5810 (C:0.5810, R:0.0105)

============================================================
Epoch 71/300 completed in 21.4s
Train: Loss=0.5624 (C:0.5624, R:0.0105) Ratio=5.28x
Val:   Loss=0.7391 (C:0.7391, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.5645 (C:0.5645, R:0.0105)
Batch  25/537: Loss=0.5437 (C:0.5437, R:0.0105)
Batch  50/537: Loss=0.5329 (C:0.5329, R:0.0105)
Batch  75/537: Loss=0.5718 (C:0.5718, R:0.0105)
Batch 100/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 125/537: Loss=0.5496 (C:0.5496, R:0.0105)
Batch 150/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 175/537: Loss=0.5519 (C:0.5519, R:0.0105)
Batch 200/537: Loss=0.6015 (C:0.6015, R:0.0105)
Batch 225/537: Loss=0.5763 (C:0.5763, R:0.0106)
Batch 250/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 275/537: Loss=0.5731 (C:0.5731, R:0.0105)
Batch 300/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch 325/537: Loss=0.5943 (C:0.5943, R:0.0105)
Batch 350/537: Loss=0.5397 (C:0.5397, R:0.0105)
Batch 375/537: Loss=0.5421 (C:0.5421, R:0.0105)
Batch 400/537: Loss=0.5576 (C:0.5576, R:0.0105)
Batch 425/537: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 450/537: Loss=0.5619 (C:0.5619, R:0.0105)
Batch 475/537: Loss=0.5399 (C:0.5399, R:0.0106)
Batch 500/537: Loss=0.5750 (C:0.5750, R:0.0105)
Batch 525/537: Loss=0.5756 (C:0.5756, R:0.0105)

============================================================
Epoch 72/300 completed in 21.5s
Train: Loss=0.5616 (C:0.5616, R:0.0105) Ratio=5.18x
Val:   Loss=0.7344 (C:0.7344, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.322 ± 0.609
    Neg distances: 2.517 ± 1.075
    Separation ratio: 7.81x
    Gap: -4.474
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=0.5749 (C:0.5749, R:0.0106)
Batch  25/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch  50/537: Loss=0.5383 (C:0.5383, R:0.0106)
Batch  75/537: Loss=0.5669 (C:0.5669, R:0.0105)
Batch 100/537: Loss=0.5818 (C:0.5818, R:0.0105)
Batch 125/537: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 150/537: Loss=0.5855 (C:0.5855, R:0.0105)
Batch 175/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 200/537: Loss=0.6120 (C:0.6120, R:0.0105)
Batch 225/537: Loss=0.6234 (C:0.6234, R:0.0105)
Batch 250/537: Loss=0.5523 (C:0.5523, R:0.0105)
Batch 275/537: Loss=0.5549 (C:0.5549, R:0.0105)
Batch 300/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 325/537: Loss=0.6038 (C:0.6038, R:0.0105)
Batch 350/537: Loss=0.5857 (C:0.5857, R:0.0106)
Batch 375/537: Loss=0.5788 (C:0.5788, R:0.0105)
Batch 400/537: Loss=0.5752 (C:0.5752, R:0.0105)
Batch 425/537: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 450/537: Loss=0.5879 (C:0.5879, R:0.0106)
Batch 475/537: Loss=0.6045 (C:0.6045, R:0.0105)
Batch 500/537: Loss=0.5719 (C:0.5719, R:0.0105)
Batch 525/537: Loss=0.5349 (C:0.5349, R:0.0105)

============================================================
Epoch 73/300 completed in 27.3s
Train: Loss=0.5761 (C:0.5761, R:0.0105) Ratio=5.29x
Val:   Loss=0.7505 (C:0.7505, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=0.5811 (C:0.5811, R:0.0106)
Batch  25/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch  50/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch  75/537: Loss=0.5223 (C:0.5223, R:0.0105)
Batch 100/537: Loss=0.6148 (C:0.6148, R:0.0105)
Batch 125/537: Loss=0.5499 (C:0.5499, R:0.0105)
Batch 150/537: Loss=0.5924 (C:0.5924, R:0.0105)
Batch 175/537: Loss=0.5888 (C:0.5888, R:0.0105)
Batch 200/537: Loss=0.5812 (C:0.5812, R:0.0105)
Batch 225/537: Loss=0.5785 (C:0.5785, R:0.0105)
Batch 250/537: Loss=0.5685 (C:0.5685, R:0.0105)
Batch 275/537: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 300/537: Loss=0.5569 (C:0.5569, R:0.0105)
Batch 325/537: Loss=0.5446 (C:0.5446, R:0.0105)
Batch 350/537: Loss=0.5913 (C:0.5913, R:0.0106)
Batch 375/537: Loss=0.5718 (C:0.5718, R:0.0105)
Batch 400/537: Loss=0.5776 (C:0.5776, R:0.0105)
Batch 425/537: Loss=0.5842 (C:0.5842, R:0.0105)
Batch 450/537: Loss=0.5512 (C:0.5512, R:0.0105)
Batch 475/537: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 500/537: Loss=0.5556 (C:0.5556, R:0.0105)
Batch 525/537: Loss=0.6004 (C:0.6004, R:0.0105)

============================================================
Epoch 74/300 completed in 21.3s
Train: Loss=0.5765 (C:0.5765, R:0.0105) Ratio=5.35x
Val:   Loss=0.7549 (C:0.7549, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=0.5716 (C:0.5716, R:0.0105)
Batch  25/537: Loss=0.5754 (C:0.5754, R:0.0105)
Batch  50/537: Loss=0.5765 (C:0.5765, R:0.0105)
Batch  75/537: Loss=0.5550 (C:0.5550, R:0.0105)
Batch 100/537: Loss=0.5940 (C:0.5940, R:0.0105)
Batch 125/537: Loss=0.5305 (C:0.5305, R:0.0105)
Batch 150/537: Loss=0.5826 (C:0.5826, R:0.0105)
Batch 175/537: Loss=0.5674 (C:0.5674, R:0.0106)
Batch 200/537: Loss=0.5778 (C:0.5778, R:0.0105)
Batch 225/537: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 250/537: Loss=0.5949 (C:0.5949, R:0.0105)
Batch 275/537: Loss=0.5879 (C:0.5879, R:0.0105)
Batch 300/537: Loss=0.5396 (C:0.5396, R:0.0105)
Batch 325/537: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 350/537: Loss=0.5594 (C:0.5594, R:0.0105)
Batch 375/537: Loss=0.5695 (C:0.5695, R:0.0105)
Batch 400/537: Loss=0.5618 (C:0.5618, R:0.0105)
Batch 425/537: Loss=0.5612 (C:0.5612, R:0.0105)
Batch 450/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 475/537: Loss=0.5649 (C:0.5649, R:0.0105)
Batch 500/537: Loss=0.5598 (C:0.5598, R:0.0105)
Batch 525/537: Loss=0.5547 (C:0.5547, R:0.0105)

============================================================
Epoch 75/300 completed in 21.0s
Train: Loss=0.5736 (C:0.5736, R:0.0105) Ratio=5.31x
Val:   Loss=0.7473 (C:0.7473, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.304 ± 0.568
    Neg distances: 2.558 ± 1.082
    Separation ratio: 8.43x
    Gap: -4.293
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch  25/537: Loss=0.5085 (C:0.5085, R:0.0105)
Batch  50/537: Loss=0.5306 (C:0.5306, R:0.0105)
Batch  75/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 100/537: Loss=0.5498 (C:0.5498, R:0.0105)
Batch 125/537: Loss=0.5722 (C:0.5722, R:0.0105)
Batch 150/537: Loss=0.5697 (C:0.5697, R:0.0105)
Batch 175/537: Loss=0.4978 (C:0.4978, R:0.0105)
Batch 200/537: Loss=0.5446 (C:0.5446, R:0.0105)
Batch 225/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 250/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 275/537: Loss=0.5460 (C:0.5460, R:0.0105)
Batch 300/537: Loss=0.6029 (C:0.6029, R:0.0105)
Batch 325/537: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 350/537: Loss=0.5911 (C:0.5911, R:0.0105)
Batch 375/537: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 400/537: Loss=0.5295 (C:0.5295, R:0.0105)
Batch 425/537: Loss=0.5862 (C:0.5862, R:0.0105)
Batch 450/537: Loss=0.5394 (C:0.5394, R:0.0105)
Batch 475/537: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 500/537: Loss=0.5820 (C:0.5820, R:0.0105)
Batch 525/537: Loss=0.5295 (C:0.5295, R:0.0105)

============================================================
Epoch 76/300 completed in 26.8s
Train: Loss=0.5574 (C:0.5574, R:0.0105) Ratio=5.34x
Val:   Loss=0.7324 (C:0.7324, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7324)
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=0.5428 (C:0.5428, R:0.0105)
Batch  25/537: Loss=0.5243 (C:0.5243, R:0.0105)
Batch  50/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch  75/537: Loss=0.5497 (C:0.5497, R:0.0105)
Batch 100/537: Loss=0.5269 (C:0.5269, R:0.0105)
Batch 125/537: Loss=0.5392 (C:0.5392, R:0.0105)
Batch 150/537: Loss=0.5575 (C:0.5575, R:0.0105)
Batch 175/537: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 200/537: Loss=0.5293 (C:0.5293, R:0.0105)
Batch 225/537: Loss=0.5570 (C:0.5570, R:0.0105)
Batch 250/537: Loss=0.5435 (C:0.5435, R:0.0105)
Batch 275/537: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 300/537: Loss=0.5482 (C:0.5482, R:0.0105)
Batch 325/537: Loss=0.5457 (C:0.5457, R:0.0105)
Batch 350/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 375/537: Loss=0.5792 (C:0.5792, R:0.0105)
Batch 400/537: Loss=0.5341 (C:0.5341, R:0.0105)
Batch 425/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch 450/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 475/537: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 500/537: Loss=0.5729 (C:0.5729, R:0.0105)
Batch 525/537: Loss=0.5832 (C:0.5832, R:0.0105)

============================================================
Epoch 77/300 completed in 21.2s
Train: Loss=0.5567 (C:0.5567, R:0.0105) Ratio=5.53x
Val:   Loss=0.7350 (C:0.7350, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=0.5620 (C:0.5620, R:0.0105)
Batch  25/537: Loss=0.5707 (C:0.5707, R:0.0105)
Batch  50/537: Loss=0.5408 (C:0.5408, R:0.0105)
Batch  75/537: Loss=0.5734 (C:0.5734, R:0.0105)
Batch 100/537: Loss=0.5745 (C:0.5745, R:0.0105)
Batch 125/537: Loss=0.5716 (C:0.5716, R:0.0105)
Batch 150/537: Loss=0.5920 (C:0.5920, R:0.0105)
Batch 175/537: Loss=0.5425 (C:0.5425, R:0.0105)
Batch 200/537: Loss=0.5376 (C:0.5376, R:0.0105)
Batch 225/537: Loss=0.5483 (C:0.5483, R:0.0105)
Batch 250/537: Loss=0.5912 (C:0.5912, R:0.0105)
Batch 275/537: Loss=0.4818 (C:0.4818, R:0.0105)
Batch 300/537: Loss=0.5769 (C:0.5769, R:0.0105)
Batch 325/537: Loss=0.5549 (C:0.5549, R:0.0105)
Batch 350/537: Loss=0.5780 (C:0.5780, R:0.0105)
Batch 375/537: Loss=0.5670 (C:0.5670, R:0.0105)
Batch 400/537: Loss=0.5337 (C:0.5337, R:0.0105)
Batch 425/537: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 450/537: Loss=0.6013 (C:0.6013, R:0.0105)
Batch 475/537: Loss=0.5669 (C:0.5669, R:0.0105)
Batch 500/537: Loss=0.5655 (C:0.5655, R:0.0105)
Batch 525/537: Loss=0.5237 (C:0.5237, R:0.0105)

============================================================
Epoch 78/300 completed in 21.8s
Train: Loss=0.5545 (C:0.5545, R:0.0105) Ratio=5.39x
Val:   Loss=0.7213 (C:0.7213, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7213)
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.316 ± 0.588
    Neg distances: 2.547 ± 1.080
    Separation ratio: 8.07x
    Gap: -4.420
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=0.5592 (C:0.5592, R:0.0105)
Batch  25/537: Loss=0.5770 (C:0.5770, R:0.0105)
Batch  50/537: Loss=0.5411 (C:0.5411, R:0.0105)
Batch  75/537: Loss=0.5636 (C:0.5636, R:0.0105)
Batch 100/537: Loss=0.5522 (C:0.5522, R:0.0105)
Batch 125/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 150/537: Loss=0.5603 (C:0.5603, R:0.0105)
Batch 175/537: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 200/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 225/537: Loss=0.5902 (C:0.5902, R:0.0105)
Batch 250/537: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 275/537: Loss=0.5889 (C:0.5889, R:0.0105)
Batch 300/537: Loss=0.6037 (C:0.6037, R:0.0105)
Batch 325/537: Loss=0.5642 (C:0.5642, R:0.0106)
Batch 350/537: Loss=0.5694 (C:0.5694, R:0.0105)
Batch 375/537: Loss=0.5615 (C:0.5615, R:0.0105)
Batch 400/537: Loss=0.5266 (C:0.5266, R:0.0105)
Batch 425/537: Loss=0.5370 (C:0.5370, R:0.0105)
Batch 450/537: Loss=0.5851 (C:0.5851, R:0.0105)
Batch 475/537: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 500/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 525/537: Loss=0.5790 (C:0.5790, R:0.0105)

============================================================
Epoch 79/300 completed in 27.2s
Train: Loss=0.5600 (C:0.5600, R:0.0105) Ratio=5.31x
Val:   Loss=0.7389 (C:0.7389, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch  25/537: Loss=0.5476 (C:0.5476, R:0.0105)
Batch  50/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch  75/537: Loss=0.5428 (C:0.5428, R:0.0105)
Batch 100/537: Loss=0.5248 (C:0.5248, R:0.0105)
Batch 125/537: Loss=0.5836 (C:0.5836, R:0.0105)
Batch 150/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 175/537: Loss=0.5235 (C:0.5235, R:0.0106)
Batch 200/537: Loss=0.5536 (C:0.5536, R:0.0105)
Batch 225/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 250/537: Loss=0.5844 (C:0.5844, R:0.0105)
Batch 275/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 300/537: Loss=0.5390 (C:0.5390, R:0.0105)
Batch 325/537: Loss=0.5632 (C:0.5632, R:0.0105)
Batch 350/537: Loss=0.5915 (C:0.5915, R:0.0105)
Batch 375/537: Loss=0.5212 (C:0.5212, R:0.0105)
Batch 400/537: Loss=0.5909 (C:0.5909, R:0.0105)
Batch 425/537: Loss=0.5576 (C:0.5576, R:0.0105)
Batch 450/537: Loss=0.5568 (C:0.5568, R:0.0105)
Batch 475/537: Loss=0.5976 (C:0.5976, R:0.0105)
Batch 500/537: Loss=0.5648 (C:0.5648, R:0.0105)
Batch 525/537: Loss=0.5690 (C:0.5690, R:0.0105)

============================================================
Epoch 80/300 completed in 21.2s
Train: Loss=0.5585 (C:0.5585, R:0.0105) Ratio=5.47x
Val:   Loss=0.7451 (C:0.7451, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch  25/537: Loss=0.5488 (C:0.5488, R:0.0105)
Batch  50/537: Loss=0.5646 (C:0.5646, R:0.0105)
Batch  75/537: Loss=0.5693 (C:0.5693, R:0.0105)
Batch 100/537: Loss=0.5488 (C:0.5488, R:0.0105)
Batch 125/537: Loss=0.5630 (C:0.5630, R:0.0105)
Batch 150/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 175/537: Loss=0.5545 (C:0.5545, R:0.0105)
Batch 200/537: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 225/537: Loss=0.5382 (C:0.5382, R:0.0105)
Batch 250/537: Loss=0.5318 (C:0.5318, R:0.0105)
Batch 275/537: Loss=0.5314 (C:0.5314, R:0.0105)
Batch 300/537: Loss=0.5555 (C:0.5555, R:0.0105)
Batch 325/537: Loss=0.5592 (C:0.5592, R:0.0105)
Batch 350/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 375/537: Loss=0.5471 (C:0.5471, R:0.0105)
Batch 400/537: Loss=0.5754 (C:0.5754, R:0.0105)
Batch 425/537: Loss=0.5707 (C:0.5707, R:0.0106)
Batch 450/537: Loss=0.5333 (C:0.5333, R:0.0106)
Batch 475/537: Loss=0.5477 (C:0.5477, R:0.0105)
Batch 500/537: Loss=0.5813 (C:0.5813, R:0.0105)
Batch 525/537: Loss=0.5713 (C:0.5713, R:0.0105)

============================================================
Epoch 81/300 completed in 21.0s
Train: Loss=0.5588 (C:0.5588, R:0.0105) Ratio=5.43x
Val:   Loss=0.7253 (C:0.7253, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.296 ± 0.569
    Neg distances: 2.603 ± 1.090
    Separation ratio: 8.79x
    Gap: -4.441
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=0.5188 (C:0.5188, R:0.0105)
Batch  25/537: Loss=0.5242 (C:0.5242, R:0.0105)
Batch  50/537: Loss=0.4990 (C:0.4990, R:0.0105)
Batch  75/537: Loss=0.5151 (C:0.5151, R:0.0106)
Batch 100/537: Loss=0.5217 (C:0.5217, R:0.0105)
Batch 125/537: Loss=0.5472 (C:0.5472, R:0.0105)
Batch 150/537: Loss=0.5187 (C:0.5187, R:0.0105)
Batch 175/537: Loss=0.5738 (C:0.5738, R:0.0105)
Batch 200/537: Loss=0.5242 (C:0.5242, R:0.0105)
Batch 225/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 250/537: Loss=0.5432 (C:0.5432, R:0.0105)
Batch 275/537: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 300/537: Loss=0.5524 (C:0.5524, R:0.0105)
Batch 325/537: Loss=0.5250 (C:0.5250, R:0.0105)
Batch 350/537: Loss=0.5189 (C:0.5189, R:0.0105)
Batch 375/537: Loss=0.5266 (C:0.5266, R:0.0105)
Batch 400/537: Loss=0.5190 (C:0.5190, R:0.0105)
Batch 425/537: Loss=0.5433 (C:0.5433, R:0.0105)
Batch 450/537: Loss=0.5137 (C:0.5137, R:0.0105)
Batch 475/537: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 500/537: Loss=0.5290 (C:0.5290, R:0.0105)
Batch 525/537: Loss=0.5623 (C:0.5623, R:0.0105)

============================================================
Epoch 82/300 completed in 27.0s
Train: Loss=0.5360 (C:0.5360, R:0.0105) Ratio=5.40x
Val:   Loss=0.7093 (C:0.7093, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7093)
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch  25/537: Loss=0.5051 (C:0.5051, R:0.0105)
Batch  50/537: Loss=0.5345 (C:0.5345, R:0.0105)
Batch  75/537: Loss=0.5295 (C:0.5295, R:0.0105)
Batch 100/537: Loss=0.5399 (C:0.5399, R:0.0105)
Batch 125/537: Loss=0.5268 (C:0.5268, R:0.0105)
Batch 150/537: Loss=0.5499 (C:0.5499, R:0.0105)
Batch 175/537: Loss=0.5256 (C:0.5256, R:0.0105)
Batch 200/537: Loss=0.5586 (C:0.5586, R:0.0105)
Batch 225/537: Loss=0.5210 (C:0.5210, R:0.0105)
Batch 250/537: Loss=0.5194 (C:0.5194, R:0.0105)
Batch 275/537: Loss=0.5182 (C:0.5182, R:0.0105)
Batch 300/537: Loss=0.5391 (C:0.5391, R:0.0105)
Batch 325/537: Loss=0.5274 (C:0.5274, R:0.0105)
Batch 350/537: Loss=0.5268 (C:0.5268, R:0.0105)
Batch 375/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 400/537: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 425/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 450/537: Loss=0.5473 (C:0.5473, R:0.0105)
Batch 475/537: Loss=0.4968 (C:0.4968, R:0.0105)
Batch 500/537: Loss=0.5577 (C:0.5577, R:0.0105)
Batch 525/537: Loss=0.5381 (C:0.5381, R:0.0105)

============================================================
Epoch 83/300 completed in 21.0s
Train: Loss=0.5353 (C:0.5353, R:0.0105) Ratio=5.48x
Val:   Loss=0.7231 (C:0.7231, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=0.5253 (C:0.5253, R:0.0105)
Batch  25/537: Loss=0.5118 (C:0.5118, R:0.0105)
Batch  50/537: Loss=0.5602 (C:0.5602, R:0.0105)
Batch  75/537: Loss=0.5349 (C:0.5349, R:0.0105)
Batch 100/537: Loss=0.5421 (C:0.5421, R:0.0105)
Batch 125/537: Loss=0.5909 (C:0.5909, R:0.0106)
Batch 150/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 175/537: Loss=0.5546 (C:0.5546, R:0.0105)
Batch 200/537: Loss=0.5162 (C:0.5162, R:0.0105)
Batch 225/537: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 250/537: Loss=0.5777 (C:0.5777, R:0.0105)
Batch 275/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 300/537: Loss=0.5265 (C:0.5265, R:0.0105)
Batch 325/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 350/537: Loss=0.4887 (C:0.4887, R:0.0105)
Batch 375/537: Loss=0.5273 (C:0.5273, R:0.0105)
Batch 400/537: Loss=0.5057 (C:0.5057, R:0.0105)
Batch 425/537: Loss=0.5547 (C:0.5547, R:0.0105)
Batch 450/537: Loss=0.5651 (C:0.5651, R:0.0105)
Batch 475/537: Loss=0.5577 (C:0.5577, R:0.0105)
Batch 500/537: Loss=0.5375 (C:0.5375, R:0.0105)
Batch 525/537: Loss=0.5421 (C:0.5421, R:0.0105)

============================================================
Epoch 84/300 completed in 21.3s
Train: Loss=0.5349 (C:0.5349, R:0.0105) Ratio=5.44x
Val:   Loss=0.7203 (C:0.7203, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 85
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.306 ± 0.594
    Neg distances: 2.616 ± 1.097
    Separation ratio: 8.55x
    Gap: -4.476
    ✅ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=0.5476 (C:0.5476, R:0.0105)
Batch  25/537: Loss=0.5167 (C:0.5167, R:0.0105)
Batch  50/537: Loss=0.5477 (C:0.5477, R:0.0105)
Batch  75/537: Loss=0.5157 (C:0.5157, R:0.0105)
Batch 100/537: Loss=0.5567 (C:0.5567, R:0.0106)
Batch 125/537: Loss=0.5617 (C:0.5617, R:0.0105)
Batch 150/537: Loss=0.4863 (C:0.4863, R:0.0105)
Batch 175/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 200/537: Loss=0.5293 (C:0.5293, R:0.0105)
Batch 225/537: Loss=0.5476 (C:0.5476, R:0.0105)
Batch 250/537: Loss=0.5132 (C:0.5132, R:0.0105)
Batch 275/537: Loss=0.5039 (C:0.5039, R:0.0105)
Batch 300/537: Loss=0.5791 (C:0.5791, R:0.0105)
Batch 325/537: Loss=0.6045 (C:0.6045, R:0.0105)
Batch 350/537: Loss=0.5662 (C:0.5662, R:0.0105)
Batch 375/537: Loss=0.5470 (C:0.5470, R:0.0105)
Batch 400/537: Loss=0.5206 (C:0.5206, R:0.0105)
Batch 425/537: Loss=0.5830 (C:0.5830, R:0.0105)
Batch 450/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 475/537: Loss=0.5422 (C:0.5422, R:0.0105)
Batch 500/537: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 525/537: Loss=0.5296 (C:0.5296, R:0.0106)

============================================================
Epoch 85/300 completed in 26.8s
Train: Loss=0.5396 (C:0.5396, R:0.0105) Ratio=5.48x
Val:   Loss=0.7257 (C:0.7257, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=0.5077 (C:0.5077, R:0.0105)
Batch  25/537: Loss=0.5473 (C:0.5473, R:0.0105)
Batch  50/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch  75/537: Loss=0.4912 (C:0.4912, R:0.0105)
Batch 100/537: Loss=0.5495 (C:0.5495, R:0.0105)
Batch 125/537: Loss=0.5577 (C:0.5577, R:0.0105)
Batch 150/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 175/537: Loss=0.5261 (C:0.5261, R:0.0105)
Batch 200/537: Loss=0.5246 (C:0.5246, R:0.0105)
Batch 225/537: Loss=0.5411 (C:0.5411, R:0.0105)
Batch 250/537: Loss=0.5406 (C:0.5406, R:0.0105)
Batch 275/537: Loss=0.5214 (C:0.5214, R:0.0106)
Batch 300/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch 325/537: Loss=0.5367 (C:0.5367, R:0.0105)
Batch 350/537: Loss=0.5292 (C:0.5292, R:0.0105)
Batch 375/537: Loss=0.5378 (C:0.5378, R:0.0105)
Batch 400/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 425/537: Loss=0.5327 (C:0.5327, R:0.0105)
Batch 450/537: Loss=0.5403 (C:0.5403, R:0.0105)
Batch 475/537: Loss=0.5146 (C:0.5146, R:0.0105)
Batch 500/537: Loss=0.5647 (C:0.5647, R:0.0105)
Batch 525/537: Loss=0.5265 (C:0.5265, R:0.0105)

============================================================
Epoch 86/300 completed in 20.9s
Train: Loss=0.5391 (C:0.5391, R:0.0105) Ratio=5.52x
Val:   Loss=0.7313 (C:0.7313, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=0.5467 (C:0.5467, R:0.0105)
Batch  25/537: Loss=0.5231 (C:0.5231, R:0.0105)
Batch  50/537: Loss=0.5248 (C:0.5248, R:0.0105)
Batch  75/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 100/537: Loss=0.5195 (C:0.5195, R:0.0105)
Batch 125/537: Loss=0.5395 (C:0.5395, R:0.0105)
Batch 150/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 175/537: Loss=0.5720 (C:0.5720, R:0.0105)
Batch 200/537: Loss=0.5190 (C:0.5190, R:0.0105)
Batch 225/537: Loss=0.5145 (C:0.5145, R:0.0105)
Batch 250/537: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 275/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 300/537: Loss=0.5311 (C:0.5311, R:0.0105)
Batch 325/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 350/537: Loss=0.5756 (C:0.5756, R:0.0105)
Batch 375/537: Loss=0.5168 (C:0.5168, R:0.0105)
Batch 400/537: Loss=0.5548 (C:0.5548, R:0.0105)
Batch 425/537: Loss=0.5284 (C:0.5284, R:0.0105)
Batch 450/537: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 475/537: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 500/537: Loss=0.5328 (C:0.5328, R:0.0105)
Batch 525/537: Loss=0.5389 (C:0.5389, R:0.0105)

============================================================
Epoch 87/300 completed in 20.7s
Train: Loss=0.5384 (C:0.5384, R:0.0105) Ratio=5.53x
Val:   Loss=0.7320 (C:0.7320, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 88
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.292 ± 0.556
    Neg distances: 2.622 ± 1.088
    Separation ratio: 8.98x
    Gap: -4.472
    ✅ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=0.4862 (C:0.4862, R:0.0105)
Batch  25/537: Loss=0.5159 (C:0.5159, R:0.0105)
Batch  50/537: Loss=0.5465 (C:0.5465, R:0.0105)
Batch  75/537: Loss=0.5264 (C:0.5264, R:0.0105)
Batch 100/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 125/537: Loss=0.4998 (C:0.4998, R:0.0105)
Batch 150/537: Loss=0.5535 (C:0.5535, R:0.0105)
Batch 175/537: Loss=0.5099 (C:0.5099, R:0.0105)
Batch 200/537: Loss=0.5293 (C:0.5293, R:0.0105)
Batch 225/537: Loss=0.5216 (C:0.5216, R:0.0105)
Batch 250/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 275/537: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 300/537: Loss=0.5027 (C:0.5027, R:0.0105)
Batch 325/537: Loss=0.5303 (C:0.5303, R:0.0105)
Batch 350/537: Loss=0.5351 (C:0.5351, R:0.0105)
Batch 375/537: Loss=0.5295 (C:0.5295, R:0.0105)
Batch 400/537: Loss=0.4986 (C:0.4986, R:0.0105)
Batch 425/537: Loss=0.5222 (C:0.5222, R:0.0105)
Batch 450/537: Loss=0.5154 (C:0.5154, R:0.0105)
Batch 475/537: Loss=0.5175 (C:0.5175, R:0.0105)
Batch 500/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 525/537: Loss=0.4967 (C:0.4967, R:0.0105)

============================================================
Epoch 88/300 completed in 26.2s
Train: Loss=0.5247 (C:0.5247, R:0.0105) Ratio=5.57x
Val:   Loss=0.7061 (C:0.7061, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7061)
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=0.5242 (C:0.5242, R:0.0105)
Batch  25/537: Loss=0.4776 (C:0.4776, R:0.0105)
Batch  50/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch  75/537: Loss=0.4834 (C:0.4834, R:0.0105)
Batch 100/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 125/537: Loss=0.5284 (C:0.5284, R:0.0105)
Batch 150/537: Loss=0.5294 (C:0.5294, R:0.0105)
Batch 175/537: Loss=0.5311 (C:0.5311, R:0.0105)
Batch 200/537: Loss=0.5040 (C:0.5040, R:0.0105)
Batch 225/537: Loss=0.5377 (C:0.5377, R:0.0105)
Batch 250/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 275/537: Loss=0.4857 (C:0.4857, R:0.0106)
Batch 300/537: Loss=0.5352 (C:0.5352, R:0.0106)
Batch 325/537: Loss=0.5110 (C:0.5110, R:0.0105)
Batch 350/537: Loss=0.5111 (C:0.5111, R:0.0105)
Batch 375/537: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 400/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch 425/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 450/537: Loss=0.5446 (C:0.5446, R:0.0105)
Batch 475/537: Loss=0.5191 (C:0.5191, R:0.0105)
Batch 500/537: Loss=0.5345 (C:0.5345, R:0.0105)
Batch 525/537: Loss=0.5602 (C:0.5602, R:0.0105)

============================================================
Epoch 89/300 completed in 20.8s
Train: Loss=0.5253 (C:0.5253, R:0.0105) Ratio=5.67x
Val:   Loss=0.7156 (C:0.7156, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 90 Training
----------------------------------------
Batch   0/537: Loss=0.5086 (C:0.5086, R:0.0105)
Batch  25/537: Loss=0.5091 (C:0.5091, R:0.0105)
Batch  50/537: Loss=0.5180 (C:0.5180, R:0.0105)
Batch  75/537: Loss=0.5083 (C:0.5083, R:0.0105)
Batch 100/537: Loss=0.4973 (C:0.4973, R:0.0105)
Batch 125/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 150/537: Loss=0.5010 (C:0.5010, R:0.0105)
Batch 175/537: Loss=0.5360 (C:0.5360, R:0.0105)
Batch 200/537: Loss=0.5108 (C:0.5108, R:0.0105)
Batch 225/537: Loss=0.5344 (C:0.5344, R:0.0105)
Batch 250/537: Loss=0.5061 (C:0.5061, R:0.0105)
Batch 275/537: Loss=0.5471 (C:0.5471, R:0.0106)
Batch 300/537: Loss=0.5318 (C:0.5318, R:0.0105)
Batch 325/537: Loss=0.5407 (C:0.5407, R:0.0105)
Batch 350/537: Loss=0.4983 (C:0.4983, R:0.0105)
Batch 375/537: Loss=0.5010 (C:0.5010, R:0.0105)
Batch 400/537: Loss=0.5330 (C:0.5330, R:0.0105)
Batch 425/537: Loss=0.5267 (C:0.5267, R:0.0105)
Batch 450/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 475/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 500/537: Loss=0.5236 (C:0.5236, R:0.0106)
Batch 525/537: Loss=0.5183 (C:0.5183, R:0.0105)

============================================================
Epoch 90/300 completed in 21.3s
Train: Loss=0.5254 (C:0.5254, R:0.0105) Ratio=5.64x
Val:   Loss=0.7157 (C:0.7157, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 91
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.296 ± 0.566
    Neg distances: 2.583 ± 1.083
    Separation ratio: 8.74x
    Gap: -4.248
    ✅ Excellent global separation!

Epoch 91 Training
----------------------------------------
Batch   0/537: Loss=0.5153 (C:0.5153, R:0.0105)
Batch  25/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch  50/537: Loss=0.5354 (C:0.5354, R:0.0105)
Batch  75/537: Loss=0.5482 (C:0.5482, R:0.0105)
Batch 100/537: Loss=0.5287 (C:0.5287, R:0.0105)
Batch 125/537: Loss=0.5053 (C:0.5053, R:0.0105)
Batch 150/537: Loss=0.5133 (C:0.5133, R:0.0105)
Batch 175/537: Loss=0.5091 (C:0.5091, R:0.0105)
Batch 200/537: Loss=0.5631 (C:0.5631, R:0.0105)
Batch 225/537: Loss=0.5055 (C:0.5055, R:0.0106)
Batch 250/537: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 275/537: Loss=0.5773 (C:0.5773, R:0.0106)
Batch 300/537: Loss=0.5785 (C:0.5785, R:0.0105)
Batch 325/537: Loss=0.5550 (C:0.5550, R:0.0105)
Batch 350/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 375/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 400/537: Loss=0.5091 (C:0.5091, R:0.0105)
Batch 425/537: Loss=0.5381 (C:0.5381, R:0.0105)
Batch 450/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch 475/537: Loss=0.5368 (C:0.5368, R:0.0105)
Batch 500/537: Loss=0.5169 (C:0.5169, R:0.0105)
Batch 525/537: Loss=0.5427 (C:0.5427, R:0.0106)

============================================================
Epoch 91/300 completed in 27.0s
Train: Loss=0.5311 (C:0.5311, R:0.0105) Ratio=5.58x
Val:   Loss=0.7230 (C:0.7230, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 92 Training
----------------------------------------
Batch   0/537: Loss=0.4806 (C:0.4806, R:0.0105)
Batch  25/537: Loss=0.5501 (C:0.5501, R:0.0105)
Batch  50/537: Loss=0.4869 (C:0.4869, R:0.0105)
Batch  75/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 100/537: Loss=0.5451 (C:0.5451, R:0.0105)
Batch 125/537: Loss=0.5488 (C:0.5488, R:0.0105)
Batch 150/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 175/537: Loss=0.5076 (C:0.5076, R:0.0105)
Batch 200/537: Loss=0.5163 (C:0.5163, R:0.0105)
Batch 225/537: Loss=0.5248 (C:0.5248, R:0.0105)
Batch 250/537: Loss=0.5276 (C:0.5276, R:0.0105)
Batch 275/537: Loss=0.4942 (C:0.4942, R:0.0105)
Batch 300/537: Loss=0.5415 (C:0.5415, R:0.0105)
Batch 325/537: Loss=0.5026 (C:0.5026, R:0.0105)
Batch 350/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 375/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 400/537: Loss=0.5169 (C:0.5169, R:0.0105)
Batch 425/537: Loss=0.5306 (C:0.5306, R:0.0105)
Batch 450/537: Loss=0.5305 (C:0.5305, R:0.0105)
Batch 475/537: Loss=0.5419 (C:0.5419, R:0.0105)
Batch 500/537: Loss=0.5466 (C:0.5466, R:0.0106)
Batch 525/537: Loss=0.4957 (C:0.4957, R:0.0105)

============================================================
Epoch 92/300 completed in 21.5s
Train: Loss=0.5310 (C:0.5310, R:0.0105) Ratio=5.77x
Val:   Loss=0.7208 (C:0.7208, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 93 Training
----------------------------------------
Batch   0/537: Loss=0.5124 (C:0.5124, R:0.0105)
Batch  25/537: Loss=0.5185 (C:0.5185, R:0.0105)
Batch  50/537: Loss=0.5577 (C:0.5577, R:0.0105)
Batch  75/537: Loss=0.5278 (C:0.5278, R:0.0105)
Batch 100/537: Loss=0.5084 (C:0.5084, R:0.0105)
Batch 125/537: Loss=0.5247 (C:0.5247, R:0.0105)
Batch 150/537: Loss=0.4865 (C:0.4865, R:0.0105)
Batch 175/537: Loss=0.5217 (C:0.5217, R:0.0105)
Batch 200/537: Loss=0.5122 (C:0.5122, R:0.0105)
Batch 225/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 250/537: Loss=0.5617 (C:0.5617, R:0.0105)
Batch 275/537: Loss=0.5714 (C:0.5714, R:0.0105)
Batch 300/537: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 325/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 350/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 375/537: Loss=0.4985 (C:0.4985, R:0.0105)
Batch 400/537: Loss=0.5141 (C:0.5141, R:0.0105)
Batch 425/537: Loss=0.5415 (C:0.5415, R:0.0105)
Batch 450/537: Loss=0.5072 (C:0.5072, R:0.0105)
Batch 475/537: Loss=0.5170 (C:0.5170, R:0.0105)
Batch 500/537: Loss=0.5297 (C:0.5297, R:0.0105)
Batch 525/537: Loss=0.5351 (C:0.5351, R:0.0105)

============================================================
Epoch 93/300 completed in 21.8s
Train: Loss=0.5289 (C:0.5289, R:0.0105) Ratio=5.63x
Val:   Loss=0.7315 (C:0.7315, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 94
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.273 ± 0.539
    Neg distances: 2.625 ± 1.081
    Separation ratio: 9.62x
    Gap: -4.334
    ✅ Excellent global separation!

Epoch 94 Training
----------------------------------------
Batch   0/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch  25/537: Loss=0.4866 (C:0.4866, R:0.0105)
Batch  50/537: Loss=0.5076 (C:0.5076, R:0.0105)
Batch  75/537: Loss=0.4924 (C:0.4924, R:0.0105)
Batch 100/537: Loss=0.4776 (C:0.4776, R:0.0105)
Batch 125/537: Loss=0.5249 (C:0.5249, R:0.0105)
Batch 150/537: Loss=0.5121 (C:0.5121, R:0.0105)
Batch 175/537: Loss=0.4911 (C:0.4911, R:0.0105)
Batch 200/537: Loss=0.5628 (C:0.5628, R:0.0105)
Batch 225/537: Loss=0.5258 (C:0.5258, R:0.0106)
Batch 250/537: Loss=0.4910 (C:0.4910, R:0.0105)
Batch 275/537: Loss=0.5260 (C:0.5260, R:0.0105)
Batch 300/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 325/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 350/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 375/537: Loss=0.4719 (C:0.4719, R:0.0105)
Batch 400/537: Loss=0.5487 (C:0.5487, R:0.0105)
Batch 425/537: Loss=0.5481 (C:0.5481, R:0.0105)
Batch 450/537: Loss=0.5006 (C:0.5006, R:0.0105)
Batch 475/537: Loss=0.5093 (C:0.5093, R:0.0105)
Batch 500/537: Loss=0.4889 (C:0.4889, R:0.0106)
Batch 525/537: Loss=0.5190 (C:0.5190, R:0.0105)

============================================================
Epoch 94/300 completed in 27.6s
Train: Loss=0.5068 (C:0.5068, R:0.0105) Ratio=5.56x
Val:   Loss=0.7078 (C:0.7078, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 95 Training
----------------------------------------
Batch   0/537: Loss=0.5051 (C:0.5051, R:0.0105)
Batch  25/537: Loss=0.5111 (C:0.5111, R:0.0105)
Batch  50/537: Loss=0.4806 (C:0.4806, R:0.0105)
Batch  75/537: Loss=0.4720 (C:0.4720, R:0.0105)
Batch 100/537: Loss=0.4648 (C:0.4648, R:0.0105)
Batch 125/537: Loss=0.5375 (C:0.5375, R:0.0106)
Batch 150/537: Loss=0.4995 (C:0.4995, R:0.0105)
Batch 175/537: Loss=0.5050 (C:0.5050, R:0.0105)
Batch 200/537: Loss=0.4858 (C:0.4858, R:0.0105)
Batch 225/537: Loss=0.4919 (C:0.4919, R:0.0105)
Batch 250/537: Loss=0.4851 (C:0.4851, R:0.0105)
Batch 275/537: Loss=0.5074 (C:0.5074, R:0.0105)
Batch 300/537: Loss=0.5351 (C:0.5351, R:0.0105)
Batch 325/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 350/537: Loss=0.4638 (C:0.4638, R:0.0105)
Batch 375/537: Loss=0.5060 (C:0.5060, R:0.0105)
Batch 400/537: Loss=0.5125 (C:0.5125, R:0.0106)
Batch 425/537: Loss=0.5135 (C:0.5135, R:0.0105)
Batch 450/537: Loss=0.4855 (C:0.4855, R:0.0105)
Batch 475/537: Loss=0.5080 (C:0.5080, R:0.0105)
Batch 500/537: Loss=0.4864 (C:0.4864, R:0.0105)
Batch 525/537: Loss=0.5152 (C:0.5152, R:0.0105)

============================================================
Epoch 95/300 completed in 21.5s
Train: Loss=0.5054 (C:0.5054, R:0.0105) Ratio=5.75x
Val:   Loss=0.7081 (C:0.7081, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 96 Training
----------------------------------------
Batch   0/537: Loss=0.5288 (C:0.5288, R:0.0105)
Batch  25/537: Loss=0.4746 (C:0.4746, R:0.0105)
Batch  50/537: Loss=0.4659 (C:0.4659, R:0.0105)
Batch  75/537: Loss=0.5007 (C:0.5007, R:0.0105)
Batch 100/537: Loss=0.5196 (C:0.5196, R:0.0106)
Batch 125/537: Loss=0.5035 (C:0.5035, R:0.0105)
Batch 150/537: Loss=0.5219 (C:0.5219, R:0.0105)
Batch 175/537: Loss=0.4887 (C:0.4887, R:0.0106)
Batch 200/537: Loss=0.5062 (C:0.5062, R:0.0105)
Batch 225/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch 250/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch 275/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 300/537: Loss=0.4832 (C:0.4832, R:0.0105)
Batch 325/537: Loss=0.4993 (C:0.4993, R:0.0105)
Batch 350/537: Loss=0.4955 (C:0.4955, R:0.0105)
Batch 375/537: Loss=0.5174 (C:0.5174, R:0.0106)
Batch 400/537: Loss=0.4678 (C:0.4678, R:0.0105)
Batch 425/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch 450/537: Loss=0.4861 (C:0.4861, R:0.0105)
Batch 475/537: Loss=0.5073 (C:0.5073, R:0.0105)
Batch 500/537: Loss=0.4804 (C:0.4804, R:0.0105)
Batch 525/537: Loss=0.5281 (C:0.5281, R:0.0105)

============================================================
Epoch 96/300 completed in 21.6s
Train: Loss=0.5043 (C:0.5043, R:0.0105) Ratio=5.69x
Val:   Loss=0.7021 (C:0.7021, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7021)
============================================================

🌍 Updating global dataset at epoch 97
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.257 ± 0.534
    Neg distances: 2.626 ± 1.067
    Separation ratio: 10.22x
    Gap: -4.695
    ✅ Excellent global separation!

Epoch 97 Training
----------------------------------------
Batch   0/537: Loss=0.4930 (C:0.4930, R:0.0105)
Batch  25/537: Loss=0.5040 (C:0.5040, R:0.0105)
Batch  50/537: Loss=0.4801 (C:0.4801, R:0.0105)
Batch  75/537: Loss=0.4994 (C:0.4994, R:0.0106)
Batch 100/537: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 125/537: Loss=0.4709 (C:0.4709, R:0.0105)
Batch 150/537: Loss=0.4645 (C:0.4645, R:0.0105)
Batch 175/537: Loss=0.4774 (C:0.4774, R:0.0105)
Batch 200/537: Loss=0.5213 (C:0.5213, R:0.0105)
Batch 225/537: Loss=0.4785 (C:0.4785, R:0.0105)
Batch 250/537: Loss=0.5291 (C:0.5291, R:0.0105)
Batch 275/537: Loss=0.5050 (C:0.5050, R:0.0105)
Batch 300/537: Loss=0.4825 (C:0.4825, R:0.0105)
Batch 325/537: Loss=0.4921 (C:0.4921, R:0.0105)
Batch 350/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 375/537: Loss=0.4895 (C:0.4895, R:0.0105)
Batch 400/537: Loss=0.4878 (C:0.4878, R:0.0105)
Batch 425/537: Loss=0.4627 (C:0.4627, R:0.0106)
Batch 450/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 475/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch 500/537: Loss=0.5016 (C:0.5016, R:0.0105)
Batch 525/537: Loss=0.4870 (C:0.4870, R:0.0105)

============================================================
Epoch 97/300 completed in 27.5s
Train: Loss=0.4901 (C:0.4901, R:0.0105) Ratio=5.57x
Val:   Loss=0.6975 (C:0.6975, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6975)
============================================================

Epoch 98 Training
----------------------------------------
Batch   0/537: Loss=0.4836 (C:0.4836, R:0.0105)
Batch  25/537: Loss=0.4802 (C:0.4802, R:0.0105)
Batch  50/537: Loss=0.5031 (C:0.5031, R:0.0105)
Batch  75/537: Loss=0.4662 (C:0.4662, R:0.0105)
Batch 100/537: Loss=0.4581 (C:0.4581, R:0.0105)
Batch 125/537: Loss=0.4320 (C:0.4320, R:0.0105)
Batch 150/537: Loss=0.4729 (C:0.4729, R:0.0105)
Batch 175/537: Loss=0.4914 (C:0.4914, R:0.0105)
Batch 200/537: Loss=0.4966 (C:0.4966, R:0.0105)
Batch 225/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 250/537: Loss=0.5230 (C:0.5230, R:0.0105)
Batch 275/537: Loss=0.4983 (C:0.4983, R:0.0105)
Batch 300/537: Loss=0.4670 (C:0.4670, R:0.0105)
Batch 325/537: Loss=0.4707 (C:0.4707, R:0.0105)
Batch 350/537: Loss=0.5065 (C:0.5065, R:0.0105)
Batch 375/537: Loss=0.4662 (C:0.4662, R:0.0105)
Batch 400/537: Loss=0.5176 (C:0.5176, R:0.0105)
Batch 425/537: Loss=0.4914 (C:0.4914, R:0.0105)
Batch 450/537: Loss=0.4630 (C:0.4630, R:0.0105)
Batch 475/537: Loss=0.4618 (C:0.4618, R:0.0105)
Batch 500/537: Loss=0.5276 (C:0.5276, R:0.0105)
Batch 525/537: Loss=0.5223 (C:0.5223, R:0.0105)

============================================================
Epoch 98/300 completed in 21.6s
Train: Loss=0.4894 (C:0.4894, R:0.0105) Ratio=5.70x
Val:   Loss=0.6899 (C:0.6899, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6899)
============================================================

Epoch 99 Training
----------------------------------------
Batch   0/537: Loss=0.4956 (C:0.4956, R:0.0105)
Batch  25/537: Loss=0.4794 (C:0.4794, R:0.0105)
Batch  50/537: Loss=0.4567 (C:0.4567, R:0.0105)
Batch  75/537: Loss=0.4818 (C:0.4818, R:0.0105)
Batch 100/537: Loss=0.4804 (C:0.4804, R:0.0105)
Batch 125/537: Loss=0.5136 (C:0.5136, R:0.0106)
Batch 150/537: Loss=0.4855 (C:0.4855, R:0.0105)
Batch 175/537: Loss=0.4887 (C:0.4887, R:0.0105)
Batch 200/537: Loss=0.5032 (C:0.5032, R:0.0105)
Batch 225/537: Loss=0.4913 (C:0.4913, R:0.0105)
Batch 250/537: Loss=0.4922 (C:0.4922, R:0.0105)
Batch 275/537: Loss=0.5014 (C:0.5014, R:0.0105)
Batch 300/537: Loss=0.4820 (C:0.4820, R:0.0105)
Batch 325/537: Loss=0.5168 (C:0.5168, R:0.0105)
Batch 350/537: Loss=0.4926 (C:0.4926, R:0.0105)
Batch 375/537: Loss=0.4676 (C:0.4676, R:0.0105)
Batch 400/537: Loss=0.4990 (C:0.4990, R:0.0105)
Batch 425/537: Loss=0.4765 (C:0.4765, R:0.0105)
Batch 450/537: Loss=0.4895 (C:0.4895, R:0.0105)
Batch 475/537: Loss=0.4834 (C:0.4834, R:0.0105)
Batch 500/537: Loss=0.4934 (C:0.4934, R:0.0105)
Batch 525/537: Loss=0.5033 (C:0.5033, R:0.0105)

============================================================
Epoch 99/300 completed in 21.5s
Train: Loss=0.4889 (C:0.4889, R:0.0105) Ratio=5.66x
Val:   Loss=0.6905 (C:0.6905, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 100
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.285 ± 0.567
    Neg distances: 2.648 ± 1.094
    Separation ratio: 9.29x
    Gap: -4.556
    ✅ Excellent global separation!

Epoch 100 Training
----------------------------------------
Batch   0/537: Loss=0.5392 (C:0.5392, R:0.0106)
Batch  25/537: Loss=0.5126 (C:0.5126, R:0.0105)
Batch  50/537: Loss=0.5096 (C:0.5096, R:0.0105)
Batch  75/537: Loss=0.5117 (C:0.5117, R:0.0105)
Batch 100/537: Loss=0.4735 (C:0.4735, R:0.0105)
Batch 125/537: Loss=0.4600 (C:0.4600, R:0.0105)
Batch 150/537: Loss=0.4670 (C:0.4670, R:0.0105)
Batch 175/537: Loss=0.4767 (C:0.4767, R:0.0105)
Batch 200/537: Loss=0.5214 (C:0.5214, R:0.0105)
Batch 225/537: Loss=0.4798 (C:0.4798, R:0.0105)
Batch 250/537: Loss=0.5315 (C:0.5315, R:0.0105)
Batch 275/537: Loss=0.4943 (C:0.4943, R:0.0105)
Batch 300/537: Loss=0.4951 (C:0.4951, R:0.0105)
Batch 325/537: Loss=0.5078 (C:0.5078, R:0.0105)
Batch 350/537: Loss=0.4874 (C:0.4874, R:0.0105)
Batch 375/537: Loss=0.5302 (C:0.5302, R:0.0105)
Batch 400/537: Loss=0.5271 (C:0.5271, R:0.0105)
Batch 425/537: Loss=0.4870 (C:0.4870, R:0.0105)
Batch 450/537: Loss=0.5029 (C:0.5029, R:0.0105)
Batch 475/537: Loss=0.5287 (C:0.5287, R:0.0105)
Batch 500/537: Loss=0.4865 (C:0.4865, R:0.0105)
Batch 525/537: Loss=0.4876 (C:0.4876, R:0.0105)

============================================================
Epoch 100/300 completed in 26.7s
Train: Loss=0.5050 (C:0.5050, R:0.0105) Ratio=5.77x
Val:   Loss=0.7145 (C:0.7145, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 100
============================================================

Epoch 101 Training
----------------------------------------
Batch   0/537: Loss=0.4750 (C:0.4750, R:0.0105)
Batch  25/537: Loss=0.4981 (C:0.4981, R:0.0105)
Batch  50/537: Loss=0.5122 (C:0.5122, R:0.0105)
Batch  75/537: Loss=0.5681 (C:0.5681, R:0.0105)
Batch 100/537: Loss=0.4948 (C:0.4948, R:0.0105)
Batch 125/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 150/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch 175/537: Loss=0.5218 (C:0.5218, R:0.0105)
Batch 200/537: Loss=0.5387 (C:0.5387, R:0.0105)
Batch 225/537: Loss=0.5573 (C:0.5573, R:0.0105)
Batch 250/537: Loss=0.4783 (C:0.4783, R:0.0105)
Batch 275/537: Loss=0.5065 (C:0.5065, R:0.0105)
Batch 300/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 325/537: Loss=0.4883 (C:0.4883, R:0.0105)
Batch 350/537: Loss=0.5413 (C:0.5413, R:0.0105)
Batch 375/537: Loss=0.5257 (C:0.5257, R:0.0105)
Batch 400/537: Loss=0.5219 (C:0.5219, R:0.0105)
Batch 425/537: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 450/537: Loss=0.4962 (C:0.4962, R:0.0105)
Batch 475/537: Loss=0.5155 (C:0.5155, R:0.0105)
Batch 500/537: Loss=0.4996 (C:0.4996, R:0.0105)
Batch 525/537: Loss=0.5135 (C:0.5135, R:0.0105)

============================================================
Epoch 101/300 completed in 21.0s
Train: Loss=0.5051 (C:0.5051, R:0.0105) Ratio=5.65x
Val:   Loss=0.7098 (C:0.7098, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 102 Training
----------------------------------------
Batch   0/537: Loss=0.5018 (C:0.5018, R:0.0105)
Batch  25/537: Loss=0.4846 (C:0.4846, R:0.0105)
Batch  50/537: Loss=0.5189 (C:0.5189, R:0.0105)
Batch  75/537: Loss=0.5103 (C:0.5103, R:0.0105)
Batch 100/537: Loss=0.4949 (C:0.4949, R:0.0105)
Batch 125/537: Loss=0.5206 (C:0.5206, R:0.0105)
Batch 150/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 175/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 200/537: Loss=0.5445 (C:0.5445, R:0.0105)
Batch 225/537: Loss=0.5247 (C:0.5247, R:0.0105)
Batch 250/537: Loss=0.5088 (C:0.5088, R:0.0105)
Batch 275/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch 300/537: Loss=0.4891 (C:0.4891, R:0.0105)
Batch 325/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 350/537: Loss=0.5068 (C:0.5068, R:0.0105)
Batch 375/537: Loss=0.4944 (C:0.4944, R:0.0105)
Batch 400/537: Loss=0.5193 (C:0.5193, R:0.0105)
Batch 425/537: Loss=0.5437 (C:0.5437, R:0.0105)
Batch 450/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 475/537: Loss=0.4709 (C:0.4709, R:0.0106)
Batch 500/537: Loss=0.4896 (C:0.4896, R:0.0105)
Batch 525/537: Loss=0.5297 (C:0.5297, R:0.0105)

============================================================
Epoch 102/300 completed in 21.0s
Train: Loss=0.5052 (C:0.5052, R:0.0105) Ratio=5.74x
Val:   Loss=0.7053 (C:0.7053, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 103
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.268 ± 0.544
    Neg distances: 2.667 ± 1.094
    Separation ratio: 9.94x
    Gap: -4.370
    ✅ Excellent global separation!

Epoch 103 Training
----------------------------------------
Batch   0/537: Loss=0.4632 (C:0.4632, R:0.0105)
Batch  25/537: Loss=0.4774 (C:0.4774, R:0.0106)
Batch  50/537: Loss=0.4837 (C:0.4837, R:0.0105)
Batch  75/537: Loss=0.4983 (C:0.4983, R:0.0105)
Batch 100/537: Loss=0.4796 (C:0.4796, R:0.0105)
Batch 125/537: Loss=0.4788 (C:0.4788, R:0.0106)
Batch 150/537: Loss=0.4647 (C:0.4647, R:0.0105)
Batch 175/537: Loss=0.4461 (C:0.4461, R:0.0105)
Batch 200/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 225/537: Loss=0.5090 (C:0.5090, R:0.0105)
Batch 250/537: Loss=0.4987 (C:0.4987, R:0.0105)
Batch 275/537: Loss=0.5094 (C:0.5094, R:0.0105)
Batch 300/537: Loss=0.4901 (C:0.4901, R:0.0105)
Batch 325/537: Loss=0.5021 (C:0.5021, R:0.0105)
Batch 350/537: Loss=0.4823 (C:0.4823, R:0.0105)
Batch 375/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 400/537: Loss=0.5184 (C:0.5184, R:0.0106)
Batch 425/537: Loss=0.5077 (C:0.5077, R:0.0105)
Batch 450/537: Loss=0.5212 (C:0.5212, R:0.0105)
Batch 475/537: Loss=0.4679 (C:0.4679, R:0.0105)
Batch 500/537: Loss=0.4911 (C:0.4911, R:0.0105)
Batch 525/537: Loss=0.4933 (C:0.4933, R:0.0105)

============================================================
Epoch 103/300 completed in 27.3s
Train: Loss=0.4909 (C:0.4909, R:0.0105) Ratio=5.87x
Val:   Loss=0.6983 (C:0.6983, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 104 Training
----------------------------------------
Batch   0/537: Loss=0.4774 (C:0.4774, R:0.0105)
Batch  25/537: Loss=0.5049 (C:0.5049, R:0.0105)
Batch  50/537: Loss=0.5036 (C:0.5036, R:0.0105)
Batch  75/537: Loss=0.5049 (C:0.5049, R:0.0105)
Batch 100/537: Loss=0.4906 (C:0.4906, R:0.0105)
Batch 125/537: Loss=0.4917 (C:0.4917, R:0.0105)
Batch 150/537: Loss=0.4782 (C:0.4782, R:0.0105)
Batch 175/537: Loss=0.4981 (C:0.4981, R:0.0105)
Batch 200/537: Loss=0.4877 (C:0.4877, R:0.0105)
Batch 225/537: Loss=0.5140 (C:0.5140, R:0.0105)
Batch 250/537: Loss=0.5278 (C:0.5278, R:0.0105)
Batch 275/537: Loss=0.4855 (C:0.4855, R:0.0105)
Batch 300/537: Loss=0.4801 (C:0.4801, R:0.0105)
Batch 325/537: Loss=0.5304 (C:0.5304, R:0.0105)
Batch 350/537: Loss=0.4807 (C:0.4807, R:0.0106)
Batch 375/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 400/537: Loss=0.4611 (C:0.4611, R:0.0105)
Batch 425/537: Loss=0.4905 (C:0.4905, R:0.0105)
Batch 450/537: Loss=0.5145 (C:0.5145, R:0.0105)
Batch 475/537: Loss=0.4924 (C:0.4924, R:0.0105)
Batch 500/537: Loss=0.4866 (C:0.4866, R:0.0105)
Batch 525/537: Loss=0.4786 (C:0.4786, R:0.0105)

============================================================
Epoch 104/300 completed in 21.0s
Train: Loss=0.4905 (C:0.4905, R:0.0105) Ratio=5.80x
Val:   Loss=0.6961 (C:0.6961, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 105 Training
----------------------------------------
Batch   0/537: Loss=0.4999 (C:0.4999, R:0.0105)
Batch  25/537: Loss=0.4720 (C:0.4720, R:0.0105)
Batch  50/537: Loss=0.5160 (C:0.5160, R:0.0105)
Batch  75/537: Loss=0.4785 (C:0.4785, R:0.0105)
Batch 100/537: Loss=0.4870 (C:0.4870, R:0.0105)
Batch 125/537: Loss=0.4656 (C:0.4656, R:0.0105)
Batch 150/537: Loss=0.4951 (C:0.4951, R:0.0105)
Batch 175/537: Loss=0.4856 (C:0.4856, R:0.0105)
Batch 200/537: Loss=0.4819 (C:0.4819, R:0.0105)
Batch 225/537: Loss=0.5101 (C:0.5101, R:0.0105)
Batch 250/537: Loss=0.4859 (C:0.4859, R:0.0105)
Batch 275/537: Loss=0.4880 (C:0.4880, R:0.0105)
Batch 300/537: Loss=0.4926 (C:0.4926, R:0.0105)
Batch 325/537: Loss=0.4849 (C:0.4849, R:0.0105)
Batch 350/537: Loss=0.4772 (C:0.4772, R:0.0105)
Batch 375/537: Loss=0.4686 (C:0.4686, R:0.0105)
Batch 400/537: Loss=0.4734 (C:0.4734, R:0.0105)
Batch 425/537: Loss=0.5066 (C:0.5066, R:0.0105)
Batch 450/537: Loss=0.4830 (C:0.4830, R:0.0105)
Batch 475/537: Loss=0.4966 (C:0.4966, R:0.0106)
Batch 500/537: Loss=0.4975 (C:0.4975, R:0.0105)
Batch 525/537: Loss=0.5019 (C:0.5019, R:0.0105)

============================================================
Epoch 105/300 completed in 21.0s
Train: Loss=0.4899 (C:0.4899, R:0.0105) Ratio=5.84x
Val:   Loss=0.6968 (C:0.6968, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

🌍 Updating global dataset at epoch 106
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.271 ± 0.555
    Neg distances: 2.666 ± 1.091
    Separation ratio: 9.84x
    Gap: -4.528
    ✅ Excellent global separation!

Epoch 106 Training
----------------------------------------
Batch   0/537: Loss=0.4929 (C:0.4929, R:0.0105)
Batch  25/537: Loss=0.4903 (C:0.4903, R:0.0105)
Batch  50/537: Loss=0.5208 (C:0.5208, R:0.0105)
Batch  75/537: Loss=0.5006 (C:0.5006, R:0.0105)
Batch 100/537: Loss=0.4734 (C:0.4734, R:0.0105)
Batch 125/537: Loss=0.4721 (C:0.4721, R:0.0105)
Batch 150/537: Loss=0.5103 (C:0.5103, R:0.0105)
Batch 175/537: Loss=0.4867 (C:0.4867, R:0.0105)
Batch 200/537: Loss=0.5182 (C:0.5182, R:0.0105)
Batch 225/537: Loss=0.4896 (C:0.4896, R:0.0105)
Batch 250/537: Loss=0.4820 (C:0.4820, R:0.0105)
Batch 275/537: Loss=0.4880 (C:0.4880, R:0.0105)
Batch 300/537: Loss=0.5026 (C:0.5026, R:0.0105)
Batch 325/537: Loss=0.4987 (C:0.4987, R:0.0106)
Batch 350/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch 375/537: Loss=0.5150 (C:0.5150, R:0.0105)
Batch 400/537: Loss=0.5145 (C:0.5145, R:0.0105)
Batch 425/537: Loss=0.4699 (C:0.4699, R:0.0105)
Batch 450/537: Loss=0.4670 (C:0.4670, R:0.0105)
Batch 475/537: Loss=0.4925 (C:0.4925, R:0.0105)
Batch 500/537: Loss=0.5010 (C:0.5010, R:0.0105)
Batch 525/537: Loss=0.4987 (C:0.4987, R:0.0105)

============================================================
Epoch 106/300 completed in 26.7s
Train: Loss=0.4891 (C:0.4891, R:0.0105) Ratio=5.84x
Val:   Loss=0.6935 (C:0.6935, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 106 epochs
Best model was at epoch 98 with Val Loss: 0.6899

Global Dataset Training Completed!
Best epoch: 98
Best validation loss: 0.6899
Final separation ratios: Train=5.84x, Val=3.07x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 50])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4556
  Adjusted Rand Score: 0.5210
  Clustering Accuracy: 0.8090
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 50])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 50])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8089
  Per-class F1: [0.8233082706766917, 0.746763498579097, 0.859086491739553]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.772 ± 0.952
  Negative distances: 2.329 ± 1.251
  Separation ratio: 3.02x
  Gap: -4.434
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4556
  Clustering Accuracy: 0.8090
  Adjusted Rand Score: 0.5210

Classification Performance:
  Accuracy: 0.8089

Separation Quality:
  Separation Ratio: 3.02x
  Gap: -4.434
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121/results/evaluation_results_20250714_201249.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121/results/evaluation_results_20250714_201249.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1020_20250714_193121/final_results.json

Key Results:
  Separation ratio: 3.02x
  Perfect separation: False
  Classification accuracy: 0.8089
  Result: 0.8089% (improvement: +-80.86%)
  Cleaning up: coarse_lr2e-04_lat50_bs1020_20250714_193121

[8/12] Testing: coarse_lr2e-04_lat50_bs1536
  Learning rate: 0.0002
  Latent dim: 50
  Batch size: 1536
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 20:12:49.578013
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1536
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 356
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 6
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1536
  Balanced sampling: True
  Train batches: 356
  Val batches: 6
  Test batches: 7
Data loading completed!
  Train: 549367 samples, 356 batches
  Val: 9842 samples, 6 batches
  Test: 9824 samples, 7 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,863,730
Model created with 1,863,730 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0002)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,863,730
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.075 ± 0.009
    Neg distances: 0.075 ± 0.009
    Separation ratio: 1.00x
    Gap: -0.110
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/356: Loss=1.9999 (C:1.9999, R:0.0117)
Batch  25/356: Loss=1.9702 (C:1.9702, R:0.0113)
Batch  50/356: Loss=1.9431 (C:1.9431, R:0.0110)
Batch  75/356: Loss=1.9295 (C:1.9295, R:0.0108)
Batch 100/356: Loss=1.9069 (C:1.9069, R:0.0108)
Batch 125/356: Loss=1.8927 (C:1.8927, R:0.0107)
Batch 150/356: Loss=1.8827 (C:1.8827, R:0.0106)
Batch 175/356: Loss=1.8741 (C:1.8741, R:0.0106)
Batch 200/356: Loss=1.8730 (C:1.8730, R:0.0106)
Batch 225/356: Loss=1.8528 (C:1.8528, R:0.0105)
Batch 250/356: Loss=1.8543 (C:1.8543, R:0.0106)
Batch 275/356: Loss=1.8570 (C:1.8570, R:0.0105)
Batch 300/356: Loss=1.8397 (C:1.8397, R:0.0105)
Batch 325/356: Loss=1.8654 (C:1.8654, R:0.0105)
Batch 350/356: Loss=1.8281 (C:1.8281, R:0.0105)

============================================================
Epoch 1/300 completed in 26.3s
Train: Loss=1.8896 (C:1.8896, R:0.0107) Ratio=1.72x
Val:   Loss=1.8296 (C:1.8296, R:0.0104) Ratio=2.22x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8296)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/356: Loss=1.8329 (C:1.8329, R:0.0105)
Batch  25/356: Loss=1.8263 (C:1.8263, R:0.0105)
Batch  50/356: Loss=1.8203 (C:1.8203, R:0.0105)
Batch  75/356: Loss=1.8232 (C:1.8232, R:0.0105)
Batch 100/356: Loss=1.8338 (C:1.8338, R:0.0105)
Batch 125/356: Loss=1.8400 (C:1.8400, R:0.0105)
Batch 150/356: Loss=1.8324 (C:1.8324, R:0.0105)
Batch 175/356: Loss=1.8341 (C:1.8341, R:0.0105)
Batch 200/356: Loss=1.8290 (C:1.8290, R:0.0105)
Batch 225/356: Loss=1.8372 (C:1.8372, R:0.0105)
Batch 250/356: Loss=1.8193 (C:1.8193, R:0.0105)
Batch 275/356: Loss=1.8209 (C:1.8209, R:0.0106)
Batch 300/356: Loss=1.8153 (C:1.8153, R:0.0106)
Batch 325/356: Loss=1.8343 (C:1.8343, R:0.0105)
Batch 350/356: Loss=1.8228 (C:1.8228, R:0.0105)

============================================================
Epoch 2/300 completed in 20.5s
Train: Loss=1.8271 (C:1.8271, R:0.0105) Ratio=2.24x
Val:   Loss=1.8106 (C:1.8106, R:0.0104) Ratio=2.46x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8106)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/356: Loss=1.8220 (C:1.8220, R:0.0105)
Batch  25/356: Loss=1.8225 (C:1.8225, R:0.0105)
Batch  50/356: Loss=1.8137 (C:1.8137, R:0.0105)
Batch  75/356: Loss=1.8149 (C:1.8149, R:0.0105)
Batch 100/356: Loss=1.8103 (C:1.8103, R:0.0105)
Batch 125/356: Loss=1.7966 (C:1.7966, R:0.0105)
Batch 150/356: Loss=1.8073 (C:1.8073, R:0.0105)
Batch 175/356: Loss=1.8210 (C:1.8210, R:0.0105)
Batch 200/356: Loss=1.8079 (C:1.8079, R:0.0105)
Batch 225/356: Loss=1.8157 (C:1.8157, R:0.0105)
Batch 250/356: Loss=1.7986 (C:1.7986, R:0.0105)
Batch 275/356: Loss=1.8053 (C:1.8053, R:0.0105)
Batch 300/356: Loss=1.8053 (C:1.8053, R:0.0105)
Batch 325/356: Loss=1.8204 (C:1.8204, R:0.0105)
Batch 350/356: Loss=1.8102 (C:1.8102, R:0.0105)

============================================================
Epoch 3/300 completed in 20.4s
Train: Loss=1.8093 (C:1.8093, R:0.0105) Ratio=2.47x
Val:   Loss=1.8048 (C:1.8048, R:0.0104) Ratio=2.55x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8048)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.547 ± 0.560
    Neg distances: 1.549 ± 0.841
    Separation ratio: 2.83x
    Gap: -3.422
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/356: Loss=1.1966 (C:1.1966, R:0.0105)
Batch  25/356: Loss=1.2438 (C:1.2438, R:0.0105)
Batch  50/356: Loss=1.2015 (C:1.2015, R:0.0105)
Batch  75/356: Loss=1.1876 (C:1.1876, R:0.0105)
Batch 100/356: Loss=1.2152 (C:1.2152, R:0.0106)
Batch 125/356: Loss=1.1959 (C:1.1959, R:0.0105)
Batch 150/356: Loss=1.2153 (C:1.2153, R:0.0105)
Batch 175/356: Loss=1.2003 (C:1.2003, R:0.0105)
Batch 200/356: Loss=1.1902 (C:1.1902, R:0.0105)
Batch 225/356: Loss=1.2032 (C:1.2032, R:0.0105)
Batch 250/356: Loss=1.1986 (C:1.1986, R:0.0105)
Batch 275/356: Loss=1.1948 (C:1.1948, R:0.0105)
Batch 300/356: Loss=1.2164 (C:1.2164, R:0.0105)
Batch 325/356: Loss=1.2032 (C:1.2032, R:0.0105)
Batch 350/356: Loss=1.1519 (C:1.1519, R:0.0105)

============================================================
Epoch 4/300 completed in 26.4s
Train: Loss=1.1990 (C:1.1990, R:0.0105) Ratio=2.53x
Val:   Loss=1.1880 (C:1.1880, R:0.0104) Ratio=2.66x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1880)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/356: Loss=1.1723 (C:1.1723, R:0.0105)
Batch  25/356: Loss=1.1739 (C:1.1739, R:0.0105)
Batch  50/356: Loss=1.1530 (C:1.1530, R:0.0105)
Batch  75/356: Loss=1.1602 (C:1.1602, R:0.0105)
Batch 100/356: Loss=1.1741 (C:1.1741, R:0.0105)
Batch 125/356: Loss=1.1521 (C:1.1521, R:0.0105)
Batch 150/356: Loss=1.1943 (C:1.1943, R:0.0105)
Batch 175/356: Loss=1.2008 (C:1.2008, R:0.0105)
Batch 200/356: Loss=1.1818 (C:1.1818, R:0.0105)
Batch 225/356: Loss=1.1525 (C:1.1525, R:0.0105)
Batch 250/356: Loss=1.1899 (C:1.1899, R:0.0105)
Batch 275/356: Loss=1.1389 (C:1.1389, R:0.0105)
Batch 300/356: Loss=1.1562 (C:1.1562, R:0.0105)
Batch 325/356: Loss=1.1538 (C:1.1538, R:0.0105)
Batch 350/356: Loss=1.1978 (C:1.1978, R:0.0106)

============================================================
Epoch 5/300 completed in 20.9s
Train: Loss=1.1738 (C:1.1738, R:0.0105) Ratio=2.80x
Val:   Loss=1.1782 (C:1.1782, R:0.0104) Ratio=2.76x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1782)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/356: Loss=1.1305 (C:1.1305, R:0.0105)
Batch  25/356: Loss=1.1218 (C:1.1218, R:0.0105)
Batch  50/356: Loss=1.1535 (C:1.1535, R:0.0105)
Batch  75/356: Loss=1.1486 (C:1.1486, R:0.0105)
Batch 100/356: Loss=1.1375 (C:1.1375, R:0.0105)
Batch 125/356: Loss=1.1586 (C:1.1586, R:0.0105)
Batch 150/356: Loss=1.1622 (C:1.1622, R:0.0105)
Batch 175/356: Loss=1.1566 (C:1.1566, R:0.0105)
Batch 200/356: Loss=1.1805 (C:1.1805, R:0.0105)
Batch 225/356: Loss=1.1626 (C:1.1626, R:0.0105)
Batch 250/356: Loss=1.1432 (C:1.1432, R:0.0106)
Batch 275/356: Loss=1.1619 (C:1.1619, R:0.0105)
Batch 300/356: Loss=1.1715 (C:1.1715, R:0.0105)
Batch 325/356: Loss=1.1572 (C:1.1572, R:0.0105)
Batch 350/356: Loss=1.1717 (C:1.1717, R:0.0105)

============================================================
Epoch 6/300 completed in 20.8s
Train: Loss=1.1565 (C:1.1565, R:0.0105) Ratio=2.94x
Val:   Loss=1.1783 (C:1.1783, R:0.0104) Ratio=2.77x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.489 ± 0.562
    Neg distances: 1.623 ± 0.835
    Separation ratio: 3.32x
    Gap: -3.120
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/356: Loss=1.0798 (C:1.0798, R:0.0105)
Batch  25/356: Loss=1.0743 (C:1.0743, R:0.0105)
Batch  50/356: Loss=1.1000 (C:1.1000, R:0.0105)
Batch  75/356: Loss=1.0840 (C:1.0840, R:0.0105)
Batch 100/356: Loss=1.1103 (C:1.1103, R:0.0105)
Batch 125/356: Loss=1.1034 (C:1.1034, R:0.0105)
Batch 150/356: Loss=1.1046 (C:1.1046, R:0.0105)
Batch 175/356: Loss=1.0816 (C:1.0816, R:0.0105)
Batch 200/356: Loss=1.0611 (C:1.0611, R:0.0105)
Batch 225/356: Loss=1.0840 (C:1.0840, R:0.0105)
Batch 250/356: Loss=1.0633 (C:1.0633, R:0.0105)
Batch 275/356: Loss=1.1067 (C:1.1067, R:0.0105)
Batch 300/356: Loss=1.0848 (C:1.0848, R:0.0105)
Batch 325/356: Loss=1.0617 (C:1.0617, R:0.0105)
Batch 350/356: Loss=1.0627 (C:1.0627, R:0.0105)

============================================================
Epoch 7/300 completed in 26.7s
Train: Loss=1.0922 (C:1.0922, R:0.0105) Ratio=3.07x
Val:   Loss=1.1263 (C:1.1263, R:0.0104) Ratio=2.84x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1263)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/356: Loss=1.0891 (C:1.0891, R:0.0105)
Batch  25/356: Loss=1.0724 (C:1.0724, R:0.0105)
Batch  50/356: Loss=1.0911 (C:1.0911, R:0.0105)
Batch  75/356: Loss=1.0583 (C:1.0583, R:0.0105)
Batch 100/356: Loss=1.0701 (C:1.0701, R:0.0106)
Batch 125/356: Loss=1.0878 (C:1.0878, R:0.0105)
Batch 150/356: Loss=1.0675 (C:1.0675, R:0.0105)
Batch 175/356: Loss=1.0564 (C:1.0564, R:0.0105)
Batch 200/356: Loss=1.0872 (C:1.0872, R:0.0105)
Batch 225/356: Loss=1.0801 (C:1.0801, R:0.0105)
Batch 250/356: Loss=1.0700 (C:1.0700, R:0.0105)
Batch 275/356: Loss=1.0867 (C:1.0867, R:0.0105)
Batch 300/356: Loss=1.0800 (C:1.0800, R:0.0105)
Batch 325/356: Loss=1.0561 (C:1.0561, R:0.0105)
Batch 350/356: Loss=1.0682 (C:1.0682, R:0.0105)

============================================================
Epoch 8/300 completed in 21.0s
Train: Loss=1.0810 (C:1.0810, R:0.0105) Ratio=3.18x
Val:   Loss=1.1219 (C:1.1219, R:0.0104) Ratio=2.85x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1219)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/356: Loss=1.0670 (C:1.0670, R:0.0105)
Batch  25/356: Loss=1.0741 (C:1.0741, R:0.0105)
Batch  50/356: Loss=1.0490 (C:1.0490, R:0.0105)
Batch  75/356: Loss=1.0851 (C:1.0851, R:0.0105)
Batch 100/356: Loss=1.0584 (C:1.0584, R:0.0105)
Batch 125/356: Loss=1.0762 (C:1.0762, R:0.0105)
Batch 150/356: Loss=1.0788 (C:1.0788, R:0.0105)
Batch 175/356: Loss=1.0878 (C:1.0878, R:0.0105)
Batch 200/356: Loss=1.0619 (C:1.0619, R:0.0105)
Batch 225/356: Loss=1.1052 (C:1.1052, R:0.0105)
Batch 250/356: Loss=1.0914 (C:1.0914, R:0.0105)
Batch 275/356: Loss=1.0807 (C:1.0807, R:0.0105)
Batch 300/356: Loss=1.0788 (C:1.0788, R:0.0105)
Batch 325/356: Loss=1.0746 (C:1.0746, R:0.0105)
Batch 350/356: Loss=1.0957 (C:1.0957, R:0.0105)

============================================================
Epoch 9/300 completed in 21.5s
Train: Loss=1.0732 (C:1.0732, R:0.0105) Ratio=3.18x
Val:   Loss=1.1014 (C:1.1014, R:0.0104) Ratio=2.91x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1014)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.436 ± 0.532
    Neg distances: 1.733 ± 0.847
    Separation ratio: 3.98x
    Gap: -3.165
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/356: Loss=0.9862 (C:0.9862, R:0.0105)
Batch  25/356: Loss=1.0306 (C:1.0306, R:0.0106)
Batch  50/356: Loss=1.0148 (C:1.0148, R:0.0105)
Batch  75/356: Loss=1.0032 (C:1.0032, R:0.0105)
Batch 100/356: Loss=0.9979 (C:0.9979, R:0.0105)
Batch 125/356: Loss=0.9848 (C:0.9848, R:0.0105)
Batch 150/356: Loss=1.0240 (C:1.0240, R:0.0105)
Batch 175/356: Loss=1.0124 (C:1.0124, R:0.0105)
Batch 200/356: Loss=1.0179 (C:1.0179, R:0.0105)
Batch 225/356: Loss=1.0396 (C:1.0396, R:0.0105)
Batch 250/356: Loss=1.0222 (C:1.0222, R:0.0105)
Batch 275/356: Loss=1.0202 (C:1.0202, R:0.0105)
Batch 300/356: Loss=1.0042 (C:1.0042, R:0.0105)
Batch 325/356: Loss=0.9982 (C:0.9982, R:0.0105)
Batch 350/356: Loss=1.0088 (C:1.0088, R:0.0105)

============================================================
Epoch 10/300 completed in 27.0s
Train: Loss=1.0071 (C:1.0071, R:0.0105) Ratio=3.25x
Val:   Loss=1.0589 (C:1.0589, R:0.0104) Ratio=2.91x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0589)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/356: Loss=0.9536 (C:0.9536, R:0.0105)
Batch  25/356: Loss=1.0089 (C:1.0089, R:0.0105)
Batch  50/356: Loss=1.0222 (C:1.0222, R:0.0105)
Batch  75/356: Loss=0.9865 (C:0.9865, R:0.0105)
Batch 100/356: Loss=0.9965 (C:0.9965, R:0.0105)
Batch 125/356: Loss=1.0194 (C:1.0194, R:0.0105)
Batch 150/356: Loss=0.9874 (C:0.9874, R:0.0105)
Batch 175/356: Loss=0.9966 (C:0.9966, R:0.0105)
Batch 200/356: Loss=1.0434 (C:1.0434, R:0.0105)
Batch 225/356: Loss=1.0269 (C:1.0269, R:0.0105)
Batch 250/356: Loss=1.0096 (C:1.0096, R:0.0105)
Batch 275/356: Loss=1.0125 (C:1.0125, R:0.0105)
Batch 300/356: Loss=1.0358 (C:1.0358, R:0.0105)
Batch 325/356: Loss=1.0257 (C:1.0257, R:0.0105)
Batch 350/356: Loss=1.0114 (C:1.0114, R:0.0105)

============================================================
Epoch 11/300 completed in 20.6s
Train: Loss=0.9993 (C:0.9993, R:0.0105) Ratio=3.29x
Val:   Loss=1.0638 (C:1.0638, R:0.0104) Ratio=2.88x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/356: Loss=0.9737 (C:0.9737, R:0.0105)
Batch  25/356: Loss=0.9645 (C:0.9645, R:0.0105)
Batch  50/356: Loss=0.9572 (C:0.9572, R:0.0105)
Batch  75/356: Loss=1.0050 (C:1.0050, R:0.0105)
Batch 100/356: Loss=0.9799 (C:0.9799, R:0.0105)
Batch 125/356: Loss=0.9794 (C:0.9794, R:0.0105)
Batch 150/356: Loss=0.9848 (C:0.9848, R:0.0105)
Batch 175/356: Loss=0.9959 (C:0.9959, R:0.0105)
Batch 200/356: Loss=0.9933 (C:0.9933, R:0.0105)
Batch 225/356: Loss=0.9618 (C:0.9618, R:0.0105)
Batch 250/356: Loss=0.9931 (C:0.9931, R:0.0105)
Batch 275/356: Loss=0.9837 (C:0.9837, R:0.0105)
Batch 300/356: Loss=0.9845 (C:0.9845, R:0.0105)
Batch 325/356: Loss=0.9919 (C:0.9919, R:0.0105)
Batch 350/356: Loss=1.0051 (C:1.0051, R:0.0105)

============================================================
Epoch 12/300 completed in 20.9s
Train: Loss=0.9927 (C:0.9927, R:0.0105) Ratio=3.49x
Val:   Loss=1.0451 (C:1.0451, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0451)
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.438 ± 0.553
    Neg distances: 1.834 ± 0.880
    Separation ratio: 4.19x
    Gap: -3.290
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/356: Loss=0.9289 (C:0.9289, R:0.0105)
Batch  25/356: Loss=0.9469 (C:0.9469, R:0.0105)
Batch  50/356: Loss=0.9517 (C:0.9517, R:0.0105)
Batch  75/356: Loss=0.9378 (C:0.9378, R:0.0105)
Batch 100/356: Loss=0.9584 (C:0.9584, R:0.0105)
Batch 125/356: Loss=0.9638 (C:0.9638, R:0.0105)
Batch 150/356: Loss=0.9505 (C:0.9505, R:0.0105)
Batch 175/356: Loss=0.9418 (C:0.9418, R:0.0105)
Batch 200/356: Loss=0.9464 (C:0.9464, R:0.0105)
Batch 225/356: Loss=1.0030 (C:1.0030, R:0.0105)
Batch 250/356: Loss=0.9961 (C:0.9961, R:0.0105)
Batch 275/356: Loss=0.9499 (C:0.9499, R:0.0105)
Batch 300/356: Loss=0.9655 (C:0.9655, R:0.0105)
Batch 325/356: Loss=0.9790 (C:0.9790, R:0.0105)
Batch 350/356: Loss=0.9829 (C:0.9829, R:0.0105)

============================================================
Epoch 13/300 completed in 26.4s
Train: Loss=0.9615 (C:0.9615, R:0.0105) Ratio=3.50x
Val:   Loss=1.0238 (C:1.0238, R:0.0104) Ratio=2.94x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0238)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/356: Loss=0.9125 (C:0.9125, R:0.0106)
Batch  25/356: Loss=0.9451 (C:0.9451, R:0.0105)
Batch  50/356: Loss=0.9423 (C:0.9423, R:0.0105)
Batch  75/356: Loss=0.9349 (C:0.9349, R:0.0105)
Batch 100/356: Loss=0.9592 (C:0.9592, R:0.0105)
Batch 125/356: Loss=0.9756 (C:0.9756, R:0.0105)
Batch 150/356: Loss=0.9425 (C:0.9425, R:0.0105)
Batch 175/356: Loss=0.9671 (C:0.9671, R:0.0105)
Batch 200/356: Loss=0.9295 (C:0.9295, R:0.0105)
Batch 225/356: Loss=0.9637 (C:0.9637, R:0.0105)
Batch 250/356: Loss=0.9557 (C:0.9557, R:0.0105)
Batch 275/356: Loss=0.9883 (C:0.9883, R:0.0105)
Batch 300/356: Loss=0.9535 (C:0.9535, R:0.0105)
Batch 325/356: Loss=0.9603 (C:0.9603, R:0.0105)
Batch 350/356: Loss=0.9694 (C:0.9694, R:0.0105)

============================================================
Epoch 14/300 completed in 20.8s
Train: Loss=0.9558 (C:0.9558, R:0.0105) Ratio=3.56x
Val:   Loss=1.0293 (C:1.0293, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/356: Loss=0.9540 (C:0.9540, R:0.0105)
Batch  25/356: Loss=0.9267 (C:0.9267, R:0.0105)
Batch  50/356: Loss=0.9413 (C:0.9413, R:0.0105)
Batch  75/356: Loss=0.9372 (C:0.9372, R:0.0105)
Batch 100/356: Loss=0.9438 (C:0.9438, R:0.0105)
Batch 125/356: Loss=0.9310 (C:0.9310, R:0.0105)
Batch 150/356: Loss=0.9370 (C:0.9370, R:0.0105)
Batch 175/356: Loss=0.9425 (C:0.9425, R:0.0105)
Batch 200/356: Loss=0.9590 (C:0.9590, R:0.0105)
Batch 225/356: Loss=0.9352 (C:0.9352, R:0.0105)
Batch 250/356: Loss=0.9345 (C:0.9345, R:0.0105)
Batch 275/356: Loss=0.9428 (C:0.9428, R:0.0105)
Batch 300/356: Loss=0.9694 (C:0.9694, R:0.0105)
Batch 325/356: Loss=0.9747 (C:0.9747, R:0.0105)
Batch 350/356: Loss=0.9580 (C:0.9580, R:0.0105)

============================================================
Epoch 15/300 completed in 20.8s
Train: Loss=0.9483 (C:0.9483, R:0.0105) Ratio=3.59x
Val:   Loss=1.0233 (C:1.0233, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0233)
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.414 ± 0.549
    Neg distances: 1.953 ± 0.907
    Separation ratio: 4.72x
    Gap: -3.445
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/356: Loss=0.9214 (C:0.9214, R:0.0105)
Batch  25/356: Loss=0.8929 (C:0.8929, R:0.0105)
Batch  50/356: Loss=0.8751 (C:0.8751, R:0.0105)
Batch  75/356: Loss=0.9077 (C:0.9077, R:0.0105)
Batch 100/356: Loss=0.8864 (C:0.8864, R:0.0105)
Batch 125/356: Loss=0.9098 (C:0.9098, R:0.0105)
Batch 150/356: Loss=0.9139 (C:0.9139, R:0.0105)
Batch 175/356: Loss=0.8829 (C:0.8829, R:0.0105)
Batch 200/356: Loss=0.9013 (C:0.9013, R:0.0105)
Batch 225/356: Loss=0.9062 (C:0.9062, R:0.0105)
Batch 250/356: Loss=0.9103 (C:0.9103, R:0.0105)
Batch 275/356: Loss=0.9006 (C:0.9006, R:0.0105)
Batch 300/356: Loss=0.8900 (C:0.8900, R:0.0105)
Batch 325/356: Loss=0.9217 (C:0.9217, R:0.0105)
Batch 350/356: Loss=0.8995 (C:0.8995, R:0.0105)

============================================================
Epoch 16/300 completed in 26.3s
Train: Loss=0.9032 (C:0.9032, R:0.0105) Ratio=3.67x
Val:   Loss=0.9708 (C:0.9708, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9708)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/356: Loss=0.8837 (C:0.8837, R:0.0105)
Batch  25/356: Loss=0.8715 (C:0.8715, R:0.0105)
Batch  50/356: Loss=0.9073 (C:0.9073, R:0.0105)
Batch  75/356: Loss=0.8916 (C:0.8916, R:0.0105)
Batch 100/356: Loss=0.9025 (C:0.9025, R:0.0105)
Batch 125/356: Loss=0.8909 (C:0.8909, R:0.0105)
Batch 150/356: Loss=0.9049 (C:0.9049, R:0.0105)
Batch 175/356: Loss=0.8877 (C:0.8877, R:0.0105)
Batch 200/356: Loss=0.8861 (C:0.8861, R:0.0105)
Batch 225/356: Loss=0.9014 (C:0.9014, R:0.0105)
Batch 250/356: Loss=0.8947 (C:0.8947, R:0.0105)
Batch 275/356: Loss=0.9082 (C:0.9082, R:0.0105)
Batch 300/356: Loss=0.9004 (C:0.9004, R:0.0105)
Batch 325/356: Loss=0.9056 (C:0.9056, R:0.0105)
Batch 350/356: Loss=0.9132 (C:0.9132, R:0.0105)

============================================================
Epoch 17/300 completed in 20.9s
Train: Loss=0.8970 (C:0.8970, R:0.0105) Ratio=3.67x
Val:   Loss=0.9800 (C:0.9800, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/356: Loss=0.8823 (C:0.8823, R:0.0105)
Batch  25/356: Loss=0.8611 (C:0.8611, R:0.0105)
Batch  50/356: Loss=0.8896 (C:0.8896, R:0.0105)
Batch  75/356: Loss=0.8657 (C:0.8657, R:0.0105)
Batch 100/356: Loss=0.9056 (C:0.9056, R:0.0105)
Batch 125/356: Loss=0.8991 (C:0.8991, R:0.0105)
Batch 150/356: Loss=0.8898 (C:0.8898, R:0.0105)
Batch 175/356: Loss=0.8975 (C:0.8975, R:0.0105)
Batch 200/356: Loss=0.8912 (C:0.8912, R:0.0105)
Batch 225/356: Loss=0.9156 (C:0.9156, R:0.0105)
Batch 250/356: Loss=0.9174 (C:0.9174, R:0.0105)
Batch 275/356: Loss=0.8962 (C:0.8962, R:0.0105)
Batch 300/356: Loss=0.9011 (C:0.9011, R:0.0105)
Batch 325/356: Loss=0.8868 (C:0.8868, R:0.0105)
Batch 350/356: Loss=0.8911 (C:0.8911, R:0.0105)

============================================================
Epoch 18/300 completed in 21.0s
Train: Loss=0.8920 (C:0.8920, R:0.0105) Ratio=3.72x
Val:   Loss=0.9764 (C:0.9764, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.421 ± 0.572
    Neg distances: 2.053 ± 0.945
    Separation ratio: 4.87x
    Gap: -3.595
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/356: Loss=0.8724 (C:0.8724, R:0.0105)
Batch  25/356: Loss=0.8428 (C:0.8428, R:0.0105)
Batch  50/356: Loss=0.8589 (C:0.8589, R:0.0105)
Batch  75/356: Loss=0.8811 (C:0.8811, R:0.0105)
Batch 100/356: Loss=0.8658 (C:0.8658, R:0.0105)
Batch 125/356: Loss=0.8700 (C:0.8700, R:0.0105)
Batch 150/356: Loss=0.8875 (C:0.8875, R:0.0105)
Batch 175/356: Loss=0.8435 (C:0.8435, R:0.0105)
Batch 200/356: Loss=0.8692 (C:0.8692, R:0.0105)
Batch 225/356: Loss=0.8602 (C:0.8602, R:0.0105)
Batch 250/356: Loss=0.9154 (C:0.9154, R:0.0105)
Batch 275/356: Loss=0.8578 (C:0.8578, R:0.0105)
Batch 300/356: Loss=0.8647 (C:0.8647, R:0.0105)
Batch 325/356: Loss=0.8540 (C:0.8540, R:0.0105)
Batch 350/356: Loss=0.8922 (C:0.8922, R:0.0105)

============================================================
Epoch 19/300 completed in 26.4s
Train: Loss=0.8672 (C:0.8672, R:0.0105) Ratio=3.75x
Val:   Loss=0.9656 (C:0.9656, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9656)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/356: Loss=0.8246 (C:0.8246, R:0.0105)
Batch  25/356: Loss=0.8444 (C:0.8444, R:0.0105)
Batch  50/356: Loss=0.8585 (C:0.8585, R:0.0105)
Batch  75/356: Loss=0.8470 (C:0.8470, R:0.0105)
Batch 100/356: Loss=0.8899 (C:0.8899, R:0.0105)
Batch 125/356: Loss=0.8397 (C:0.8397, R:0.0105)
Batch 150/356: Loss=0.8686 (C:0.8686, R:0.0105)
Batch 175/356: Loss=0.8396 (C:0.8396, R:0.0105)
Batch 200/356: Loss=0.8288 (C:0.8288, R:0.0105)
Batch 225/356: Loss=0.8843 (C:0.8843, R:0.0105)
Batch 250/356: Loss=0.8781 (C:0.8781, R:0.0105)
Batch 275/356: Loss=0.8494 (C:0.8494, R:0.0105)
Batch 300/356: Loss=0.8724 (C:0.8724, R:0.0105)
Batch 325/356: Loss=0.8667 (C:0.8667, R:0.0105)
Batch 350/356: Loss=0.8727 (C:0.8727, R:0.0105)

============================================================
Epoch 20/300 completed in 20.3s
Train: Loss=0.8637 (C:0.8637, R:0.0105) Ratio=3.89x
Val:   Loss=0.9569 (C:0.9569, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9569)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/356: Loss=0.8408 (C:0.8408, R:0.0105)
Batch  25/356: Loss=0.8666 (C:0.8666, R:0.0105)
Batch  50/356: Loss=0.8116 (C:0.8116, R:0.0105)
Batch  75/356: Loss=0.8519 (C:0.8519, R:0.0105)
Batch 100/356: Loss=0.8303 (C:0.8303, R:0.0105)
Batch 125/356: Loss=0.8256 (C:0.8256, R:0.0105)
Batch 150/356: Loss=0.8790 (C:0.8790, R:0.0105)
Batch 175/356: Loss=0.8510 (C:0.8510, R:0.0105)
Batch 200/356: Loss=0.8570 (C:0.8570, R:0.0105)
Batch 225/356: Loss=0.8608 (C:0.8608, R:0.0105)
Batch 250/356: Loss=0.8653 (C:0.8653, R:0.0105)
Batch 275/356: Loss=0.8693 (C:0.8693, R:0.0105)
Batch 300/356: Loss=0.8325 (C:0.8325, R:0.0105)
Batch 325/356: Loss=0.8745 (C:0.8745, R:0.0105)
Batch 350/356: Loss=0.8948 (C:0.8948, R:0.0105)

============================================================
Epoch 21/300 completed in 20.2s
Train: Loss=0.8576 (C:0.8576, R:0.0105) Ratio=3.90x
Val:   Loss=0.9529 (C:0.9529, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9529)
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.400 ± 0.529
    Neg distances: 2.127 ± 0.951
    Separation ratio: 5.31x
    Gap: -3.759
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/356: Loss=0.8254 (C:0.8254, R:0.0105)
Batch  25/356: Loss=0.8085 (C:0.8085, R:0.0105)
Batch  50/356: Loss=0.8093 (C:0.8093, R:0.0105)
Batch  75/356: Loss=0.8256 (C:0.8256, R:0.0105)
Batch 100/356: Loss=0.7918 (C:0.7918, R:0.0105)
Batch 125/356: Loss=0.8089 (C:0.8089, R:0.0105)
Batch 150/356: Loss=0.8305 (C:0.8305, R:0.0105)
Batch 175/356: Loss=0.7778 (C:0.7778, R:0.0105)
Batch 200/356: Loss=0.8200 (C:0.8200, R:0.0105)
Batch 225/356: Loss=0.8204 (C:0.8204, R:0.0105)
Batch 250/356: Loss=0.8239 (C:0.8239, R:0.0105)
Batch 275/356: Loss=0.8466 (C:0.8466, R:0.0105)
Batch 300/356: Loss=0.8161 (C:0.8161, R:0.0105)
Batch 325/356: Loss=0.8580 (C:0.8580, R:0.0105)
Batch 350/356: Loss=0.8577 (C:0.8577, R:0.0105)

============================================================
Epoch 22/300 completed in 26.0s
Train: Loss=0.8215 (C:0.8215, R:0.0105) Ratio=3.88x
Val:   Loss=0.9201 (C:0.9201, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9201)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/356: Loss=0.7928 (C:0.7928, R:0.0105)
Batch  25/356: Loss=0.8014 (C:0.8014, R:0.0105)
Batch  50/356: Loss=0.8171 (C:0.8171, R:0.0105)
Batch  75/356: Loss=0.7884 (C:0.7884, R:0.0105)
Batch 100/356: Loss=0.7880 (C:0.7880, R:0.0105)
Batch 125/356: Loss=0.8441 (C:0.8441, R:0.0105)
Batch 150/356: Loss=0.8259 (C:0.8259, R:0.0105)
Batch 175/356: Loss=0.8457 (C:0.8457, R:0.0105)
Batch 200/356: Loss=0.7920 (C:0.7920, R:0.0105)
Batch 225/356: Loss=0.8275 (C:0.8275, R:0.0105)
Batch 250/356: Loss=0.8210 (C:0.8210, R:0.0105)
Batch 275/356: Loss=0.8481 (C:0.8481, R:0.0105)
Batch 300/356: Loss=0.8098 (C:0.8098, R:0.0105)
Batch 325/356: Loss=0.8439 (C:0.8439, R:0.0105)
Batch 350/356: Loss=0.8534 (C:0.8534, R:0.0105)

============================================================
Epoch 23/300 completed in 20.4s
Train: Loss=0.8158 (C:0.8158, R:0.0105) Ratio=3.93x
Val:   Loss=0.9271 (C:0.9271, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/356: Loss=0.8224 (C:0.8224, R:0.0105)
Batch  25/356: Loss=0.8040 (C:0.8040, R:0.0105)
Batch  50/356: Loss=0.8203 (C:0.8203, R:0.0105)
Batch  75/356: Loss=0.8141 (C:0.8141, R:0.0105)
Batch 100/356: Loss=0.8095 (C:0.8095, R:0.0105)
Batch 125/356: Loss=0.8016 (C:0.8016, R:0.0105)
Batch 150/356: Loss=0.7961 (C:0.7961, R:0.0105)
Batch 175/356: Loss=0.8155 (C:0.8155, R:0.0105)
Batch 200/356: Loss=0.8016 (C:0.8016, R:0.0105)
Batch 225/356: Loss=0.8286 (C:0.8286, R:0.0105)
Batch 250/356: Loss=0.8328 (C:0.8328, R:0.0105)
Batch 275/356: Loss=0.8090 (C:0.8090, R:0.0105)
Batch 300/356: Loss=0.8047 (C:0.8047, R:0.0105)
Batch 325/356: Loss=0.8235 (C:0.8235, R:0.0105)
Batch 350/356: Loss=0.8023 (C:0.8023, R:0.0105)

============================================================
Epoch 24/300 completed in 20.5s
Train: Loss=0.8115 (C:0.8115, R:0.0105) Ratio=3.96x
Val:   Loss=0.9268 (C:0.9268, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.411 ± 0.583
    Neg distances: 2.201 ± 0.983
    Separation ratio: 5.36x
    Gap: -3.860
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/356: Loss=0.7574 (C:0.7574, R:0.0105)
Batch  25/356: Loss=0.7962 (C:0.7962, R:0.0105)
Batch  50/356: Loss=0.8048 (C:0.8048, R:0.0105)
Batch  75/356: Loss=0.8015 (C:0.8015, R:0.0105)
Batch 100/356: Loss=0.8002 (C:0.8002, R:0.0105)
Batch 125/356: Loss=0.7786 (C:0.7786, R:0.0105)
Batch 150/356: Loss=0.7871 (C:0.7871, R:0.0105)
Batch 175/356: Loss=0.7798 (C:0.7798, R:0.0105)
Batch 200/356: Loss=0.7855 (C:0.7855, R:0.0105)
Batch 225/356: Loss=0.7831 (C:0.7831, R:0.0105)
Batch 250/356: Loss=0.8140 (C:0.8140, R:0.0105)
Batch 275/356: Loss=0.8002 (C:0.8002, R:0.0105)
Batch 300/356: Loss=0.7965 (C:0.7965, R:0.0105)
Batch 325/356: Loss=0.7870 (C:0.7870, R:0.0105)
Batch 350/356: Loss=0.8015 (C:0.8015, R:0.0105)

============================================================
Epoch 25/300 completed in 26.1s
Train: Loss=0.7964 (C:0.7964, R:0.0105) Ratio=4.07x
Val:   Loss=0.9108 (C:0.9108, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9108)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/356: Loss=0.7865 (C:0.7865, R:0.0105)
Batch  25/356: Loss=0.7711 (C:0.7711, R:0.0105)
Batch  50/356: Loss=0.7865 (C:0.7865, R:0.0105)
Batch  75/356: Loss=0.7690 (C:0.7690, R:0.0105)
Batch 100/356: Loss=0.7936 (C:0.7936, R:0.0105)
Batch 125/356: Loss=0.7921 (C:0.7921, R:0.0105)
Batch 150/356: Loss=0.7560 (C:0.7560, R:0.0105)
Batch 175/356: Loss=0.7945 (C:0.7945, R:0.0105)
Batch 200/356: Loss=0.7563 (C:0.7563, R:0.0105)
Batch 225/356: Loss=0.8092 (C:0.8092, R:0.0105)
Batch 250/356: Loss=0.7988 (C:0.7988, R:0.0105)
Batch 275/356: Loss=0.7953 (C:0.7953, R:0.0105)
Batch 300/356: Loss=0.7941 (C:0.7941, R:0.0105)
Batch 325/356: Loss=0.7829 (C:0.7829, R:0.0105)
Batch 350/356: Loss=0.8059 (C:0.8059, R:0.0105)

============================================================
Epoch 26/300 completed in 20.4s
Train: Loss=0.7929 (C:0.7929, R:0.0105) Ratio=4.10x
Val:   Loss=0.9193 (C:0.9193, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/356: Loss=0.7702 (C:0.7702, R:0.0105)
Batch  25/356: Loss=0.7655 (C:0.7655, R:0.0105)
Batch  50/356: Loss=0.7851 (C:0.7851, R:0.0105)
Batch  75/356: Loss=0.7615 (C:0.7615, R:0.0105)
Batch 100/356: Loss=0.7407 (C:0.7407, R:0.0105)
Batch 125/356: Loss=0.7921 (C:0.7921, R:0.0105)
Batch 150/356: Loss=0.7772 (C:0.7772, R:0.0105)
Batch 175/356: Loss=0.8130 (C:0.8130, R:0.0105)
Batch 200/356: Loss=0.8245 (C:0.8245, R:0.0105)
Batch 225/356: Loss=0.7945 (C:0.7945, R:0.0105)
Batch 250/356: Loss=0.7707 (C:0.7707, R:0.0105)
Batch 275/356: Loss=0.7585 (C:0.7585, R:0.0105)
Batch 300/356: Loss=0.7755 (C:0.7755, R:0.0105)
Batch 325/356: Loss=0.7911 (C:0.7911, R:0.0105)
Batch 350/356: Loss=0.7982 (C:0.7982, R:0.0105)

============================================================
Epoch 27/300 completed in 20.4s
Train: Loss=0.7877 (C:0.7877, R:0.0105) Ratio=4.19x
Val:   Loss=0.9175 (C:0.9175, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.407 ± 0.579
    Neg distances: 2.260 ± 1.003
    Separation ratio: 5.56x
    Gap: -3.962
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/356: Loss=0.7636 (C:0.7636, R:0.0105)
Batch  25/356: Loss=0.7529 (C:0.7529, R:0.0105)
Batch  50/356: Loss=0.7749 (C:0.7749, R:0.0105)
Batch  75/356: Loss=0.7679 (C:0.7679, R:0.0105)
Batch 100/356: Loss=0.7597 (C:0.7597, R:0.0105)
Batch 125/356: Loss=0.7683 (C:0.7683, R:0.0105)
Batch 150/356: Loss=0.7852 (C:0.7852, R:0.0105)
Batch 175/356: Loss=0.7880 (C:0.7880, R:0.0105)
Batch 200/356: Loss=0.7067 (C:0.7067, R:0.0105)
Batch 225/356: Loss=0.7948 (C:0.7948, R:0.0105)
Batch 250/356: Loss=0.7614 (C:0.7614, R:0.0105)
Batch 275/356: Loss=0.7779 (C:0.7779, R:0.0105)
Batch 300/356: Loss=0.7943 (C:0.7943, R:0.0105)
Batch 325/356: Loss=0.7678 (C:0.7678, R:0.0105)
Batch 350/356: Loss=0.8022 (C:0.8022, R:0.0105)

============================================================
Epoch 28/300 completed in 26.1s
Train: Loss=0.7691 (C:0.7691, R:0.0105) Ratio=4.14x
Val:   Loss=0.8871 (C:0.8871, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8871)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/356: Loss=0.7634 (C:0.7634, R:0.0105)
Batch  25/356: Loss=0.7280 (C:0.7280, R:0.0105)
Batch  50/356: Loss=0.7601 (C:0.7601, R:0.0105)
Batch  75/356: Loss=0.7359 (C:0.7359, R:0.0105)
Batch 100/356: Loss=0.7799 (C:0.7799, R:0.0105)
Batch 125/356: Loss=0.7709 (C:0.7709, R:0.0105)
Batch 150/356: Loss=0.7567 (C:0.7567, R:0.0105)
Batch 175/356: Loss=0.7720 (C:0.7720, R:0.0105)
Batch 200/356: Loss=0.7789 (C:0.7789, R:0.0105)
Batch 225/356: Loss=0.7864 (C:0.7864, R:0.0105)
Batch 250/356: Loss=0.7833 (C:0.7833, R:0.0105)
Batch 275/356: Loss=0.7484 (C:0.7484, R:0.0105)
Batch 300/356: Loss=0.7920 (C:0.7920, R:0.0105)
Batch 325/356: Loss=0.7568 (C:0.7568, R:0.0105)
Batch 350/356: Loss=0.7564 (C:0.7564, R:0.0105)

============================================================
Epoch 29/300 completed in 20.0s
Train: Loss=0.7652 (C:0.7652, R:0.0105) Ratio=4.26x
Val:   Loss=0.8941 (C:0.8941, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/356: Loss=0.7307 (C:0.7307, R:0.0105)
Batch  25/356: Loss=0.7662 (C:0.7662, R:0.0105)
Batch  50/356: Loss=0.7491 (C:0.7491, R:0.0105)
Batch  75/356: Loss=0.7812 (C:0.7812, R:0.0105)
Batch 100/356: Loss=0.7418 (C:0.7418, R:0.0105)
Batch 125/356: Loss=0.7710 (C:0.7710, R:0.0105)
Batch 150/356: Loss=0.7345 (C:0.7345, R:0.0105)
Batch 175/356: Loss=0.7667 (C:0.7667, R:0.0105)
Batch 200/356: Loss=0.7688 (C:0.7688, R:0.0105)
Batch 225/356: Loss=0.7683 (C:0.7683, R:0.0105)
Batch 250/356: Loss=0.7498 (C:0.7498, R:0.0105)
Batch 275/356: Loss=0.7844 (C:0.7844, R:0.0106)
Batch 300/356: Loss=0.7417 (C:0.7417, R:0.0105)
Batch 325/356: Loss=0.7662 (C:0.7662, R:0.0105)
Batch 350/356: Loss=0.7875 (C:0.7875, R:0.0105)

============================================================
Epoch 30/300 completed in 20.2s
Train: Loss=0.7613 (C:0.7613, R:0.0105) Ratio=4.27x
Val:   Loss=0.9063 (C:0.9063, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.378 ± 0.566
    Neg distances: 2.367 ± 1.023
    Separation ratio: 6.26x
    Gap: -4.199
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/356: Loss=0.7028 (C:0.7028, R:0.0105)
Batch  25/356: Loss=0.7280 (C:0.7280, R:0.0105)
Batch  50/356: Loss=0.7093 (C:0.7093, R:0.0105)
Batch  75/356: Loss=0.7221 (C:0.7221, R:0.0105)
Batch 100/356: Loss=0.7492 (C:0.7492, R:0.0105)
Batch 125/356: Loss=0.6941 (C:0.6941, R:0.0105)
Batch 150/356: Loss=0.7059 (C:0.7059, R:0.0105)
Batch 175/356: Loss=0.7112 (C:0.7112, R:0.0105)
Batch 200/356: Loss=0.7304 (C:0.7304, R:0.0105)
Batch 225/356: Loss=0.7792 (C:0.7792, R:0.0105)
Batch 250/356: Loss=0.7384 (C:0.7384, R:0.0105)
Batch 275/356: Loss=0.7320 (C:0.7320, R:0.0105)
Batch 300/356: Loss=0.7440 (C:0.7440, R:0.0105)
Batch 325/356: Loss=0.7659 (C:0.7659, R:0.0105)
Batch 350/356: Loss=0.7258 (C:0.7258, R:0.0105)

============================================================
Epoch 31/300 completed in 25.8s
Train: Loss=0.7233 (C:0.7233, R:0.0105) Ratio=4.26x
Val:   Loss=0.8609 (C:0.8609, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8609)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/356: Loss=0.6858 (C:0.6858, R:0.0105)
Batch  25/356: Loss=0.7057 (C:0.7057, R:0.0105)
Batch  50/356: Loss=0.7295 (C:0.7295, R:0.0105)
Batch  75/356: Loss=0.7416 (C:0.7416, R:0.0105)
Batch 100/356: Loss=0.7019 (C:0.7019, R:0.0105)
Batch 125/356: Loss=0.7042 (C:0.7042, R:0.0105)
Batch 150/356: Loss=0.7628 (C:0.7628, R:0.0105)
Batch 175/356: Loss=0.7434 (C:0.7434, R:0.0105)
Batch 200/356: Loss=0.7349 (C:0.7349, R:0.0105)
Batch 225/356: Loss=0.7423 (C:0.7423, R:0.0105)
Batch 250/356: Loss=0.6815 (C:0.6815, R:0.0105)
Batch 275/356: Loss=0.7170 (C:0.7170, R:0.0105)
Batch 300/356: Loss=0.7027 (C:0.7027, R:0.0105)
Batch 325/356: Loss=0.7279 (C:0.7279, R:0.0105)
Batch 350/356: Loss=0.7170 (C:0.7170, R:0.0105)

============================================================
Epoch 32/300 completed in 20.1s
Train: Loss=0.7199 (C:0.7199, R:0.0105) Ratio=4.33x
Val:   Loss=0.8675 (C:0.8675, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/356: Loss=0.7013 (C:0.7013, R:0.0105)
Batch  25/356: Loss=0.6872 (C:0.6872, R:0.0105)
Batch  50/356: Loss=0.7404 (C:0.7404, R:0.0105)
Batch  75/356: Loss=0.7126 (C:0.7126, R:0.0105)
Batch 100/356: Loss=0.6960 (C:0.6960, R:0.0105)
Batch 125/356: Loss=0.7519 (C:0.7519, R:0.0105)
Batch 150/356: Loss=0.6826 (C:0.6826, R:0.0105)
Batch 175/356: Loss=0.7239 (C:0.7239, R:0.0105)
Batch 200/356: Loss=0.6952 (C:0.6952, R:0.0105)
Batch 225/356: Loss=0.7402 (C:0.7402, R:0.0105)
Batch 250/356: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 275/356: Loss=0.7070 (C:0.7070, R:0.0105)
Batch 300/356: Loss=0.7395 (C:0.7395, R:0.0105)
Batch 325/356: Loss=0.6957 (C:0.6957, R:0.0105)
Batch 350/356: Loss=0.7293 (C:0.7293, R:0.0105)

============================================================
Epoch 33/300 completed in 20.2s
Train: Loss=0.7146 (C:0.7146, R:0.0105) Ratio=4.39x
Val:   Loss=0.8605 (C:0.8605, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.8605)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.372 ± 0.564
    Neg distances: 2.409 ± 1.040
    Separation ratio: 6.47x
    Gap: -4.352
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/356: Loss=0.6926 (C:0.6926, R:0.0105)
Batch  25/356: Loss=0.6982 (C:0.6982, R:0.0105)
Batch  50/356: Loss=0.6951 (C:0.6951, R:0.0105)
Batch  75/356: Loss=0.7013 (C:0.7013, R:0.0105)
Batch 100/356: Loss=0.7102 (C:0.7102, R:0.0105)
Batch 125/356: Loss=0.6719 (C:0.6719, R:0.0105)
Batch 150/356: Loss=0.7395 (C:0.7395, R:0.0105)
Batch 175/356: Loss=0.7160 (C:0.7160, R:0.0105)
Batch 200/356: Loss=0.6960 (C:0.6960, R:0.0105)
Batch 225/356: Loss=0.7101 (C:0.7101, R:0.0105)
Batch 250/356: Loss=0.7126 (C:0.7126, R:0.0105)
Batch 275/356: Loss=0.7024 (C:0.7024, R:0.0106)
Batch 300/356: Loss=0.6825 (C:0.6825, R:0.0105)
Batch 325/356: Loss=0.6858 (C:0.6858, R:0.0105)
Batch 350/356: Loss=0.7508 (C:0.7508, R:0.0105)

============================================================
Epoch 34/300 completed in 26.1s
Train: Loss=0.7004 (C:0.7004, R:0.0105) Ratio=4.35x
Val:   Loss=0.8462 (C:0.8462, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8462)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/356: Loss=0.6658 (C:0.6658, R:0.0105)
Batch  25/356: Loss=0.7184 (C:0.7184, R:0.0105)
Batch  50/356: Loss=0.7067 (C:0.7067, R:0.0105)
Batch  75/356: Loss=0.7240 (C:0.7240, R:0.0105)
Batch 100/356: Loss=0.6778 (C:0.6778, R:0.0105)
Batch 125/356: Loss=0.7052 (C:0.7052, R:0.0105)
Batch 150/356: Loss=0.6920 (C:0.6920, R:0.0105)
Batch 175/356: Loss=0.6808 (C:0.6808, R:0.0105)
Batch 200/356: Loss=0.7118 (C:0.7118, R:0.0105)
Batch 225/356: Loss=0.7039 (C:0.7039, R:0.0105)
Batch 250/356: Loss=0.6929 (C:0.6929, R:0.0105)
Batch 275/356: Loss=0.6833 (C:0.6833, R:0.0105)
Batch 300/356: Loss=0.7095 (C:0.7095, R:0.0105)
Batch 325/356: Loss=0.7066 (C:0.7066, R:0.0105)
Batch 350/356: Loss=0.7103 (C:0.7103, R:0.0105)

============================================================
Epoch 35/300 completed in 20.6s
Train: Loss=0.6960 (C:0.6960, R:0.0105) Ratio=4.47x
Val:   Loss=0.8485 (C:0.8485, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.075
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/356: Loss=0.6579 (C:0.6579, R:0.0105)
Batch  25/356: Loss=0.6518 (C:0.6518, R:0.0105)
Batch  50/356: Loss=0.6749 (C:0.6749, R:0.0106)
Batch  75/356: Loss=0.6729 (C:0.6729, R:0.0106)
Batch 100/356: Loss=0.6948 (C:0.6948, R:0.0105)
Batch 125/356: Loss=0.6792 (C:0.6792, R:0.0105)
Batch 150/356: Loss=0.6692 (C:0.6692, R:0.0105)
Batch 175/356: Loss=0.6755 (C:0.6755, R:0.0105)
Batch 200/356: Loss=0.6776 (C:0.6776, R:0.0105)
Batch 225/356: Loss=0.7123 (C:0.7123, R:0.0105)
Batch 250/356: Loss=0.6767 (C:0.6767, R:0.0106)
Batch 275/356: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 300/356: Loss=0.7130 (C:0.7130, R:0.0105)
Batch 325/356: Loss=0.7068 (C:0.7068, R:0.0105)
Batch 350/356: Loss=0.7161 (C:0.7161, R:0.0105)

============================================================
Epoch 36/300 completed in 20.5s
Train: Loss=0.6921 (C:0.6921, R:0.0105) Ratio=4.48x
Val:   Loss=0.8601 (C:0.8601, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.090
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.372 ± 0.576
    Neg distances: 2.470 ± 1.052
    Separation ratio: 6.64x
    Gap: -4.273
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/356: Loss=0.6683 (C:0.6683, R:0.0105)
Batch  25/356: Loss=0.6460 (C:0.6460, R:0.0105)
Batch  50/356: Loss=0.6255 (C:0.6255, R:0.0105)
Batch  75/356: Loss=0.6930 (C:0.6930, R:0.0105)
Batch 100/356: Loss=0.6484 (C:0.6484, R:0.0105)
Batch 125/356: Loss=0.6981 (C:0.6981, R:0.0105)
Batch 150/356: Loss=0.6496 (C:0.6496, R:0.0105)
Batch 175/356: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 200/356: Loss=0.6677 (C:0.6677, R:0.0105)
Batch 225/356: Loss=0.6953 (C:0.6953, R:0.0105)
Batch 250/356: Loss=0.7035 (C:0.7035, R:0.0105)
Batch 275/356: Loss=0.7197 (C:0.7197, R:0.0105)
Batch 300/356: Loss=0.6891 (C:0.6891, R:0.0106)
Batch 325/356: Loss=0.7072 (C:0.7072, R:0.0105)
Batch 350/356: Loss=0.7050 (C:0.7050, R:0.0105)

============================================================
Epoch 37/300 completed in 26.9s
Train: Loss=0.6784 (C:0.6784, R:0.0105) Ratio=4.53x
Val:   Loss=0.8408 (C:0.8408, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8408)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/356: Loss=0.6706 (C:0.6706, R:0.0105)
Batch  25/356: Loss=0.6855 (C:0.6855, R:0.0106)
Batch  50/356: Loss=0.6522 (C:0.6522, R:0.0105)
Batch  75/356: Loss=0.6830 (C:0.6830, R:0.0105)
Batch 100/356: Loss=0.6553 (C:0.6553, R:0.0105)
Batch 125/356: Loss=0.6810 (C:0.6810, R:0.0105)
Batch 150/356: Loss=0.6784 (C:0.6784, R:0.0105)
Batch 175/356: Loss=0.7113 (C:0.7113, R:0.0105)
Batch 200/356: Loss=0.6847 (C:0.6847, R:0.0105)
Batch 225/356: Loss=0.6714 (C:0.6714, R:0.0105)
Batch 250/356: Loss=0.6980 (C:0.6980, R:0.0105)
Batch 275/356: Loss=0.6611 (C:0.6611, R:0.0105)
Batch 300/356: Loss=0.6715 (C:0.6715, R:0.0105)
Batch 325/356: Loss=0.6808 (C:0.6808, R:0.0105)
Batch 350/356: Loss=0.6677 (C:0.6677, R:0.0105)

============================================================
Epoch 38/300 completed in 21.0s
Train: Loss=0.6756 (C:0.6756, R:0.0105) Ratio=4.57x
Val:   Loss=0.8441 (C:0.8441, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/356: Loss=0.6402 (C:0.6402, R:0.0105)
Batch  25/356: Loss=0.6270 (C:0.6270, R:0.0105)
Batch  50/356: Loss=0.6717 (C:0.6717, R:0.0105)
Batch  75/356: Loss=0.6438 (C:0.6438, R:0.0105)
Batch 100/356: Loss=0.6496 (C:0.6496, R:0.0105)
Batch 125/356: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 150/356: Loss=0.6624 (C:0.6624, R:0.0105)
Batch 175/356: Loss=0.6654 (C:0.6654, R:0.0105)
Batch 200/356: Loss=0.6770 (C:0.6770, R:0.0105)
Batch 225/356: Loss=0.6668 (C:0.6668, R:0.0105)
Batch 250/356: Loss=0.6925 (C:0.6925, R:0.0105)
Batch 275/356: Loss=0.6770 (C:0.6770, R:0.0105)
Batch 300/356: Loss=0.6282 (C:0.6282, R:0.0105)
Batch 325/356: Loss=0.6644 (C:0.6644, R:0.0105)
Batch 350/356: Loss=0.6891 (C:0.6891, R:0.0105)

============================================================
Epoch 39/300 completed in 20.4s
Train: Loss=0.6743 (C:0.6743, R:0.0105) Ratio=4.65x
Val:   Loss=0.8487 (C:0.8487, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.357 ± 0.567
    Neg distances: 2.540 ± 1.075
    Separation ratio: 7.12x
    Gap: -4.316
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/356: Loss=0.6456 (C:0.6456, R:0.0105)
Batch  25/356: Loss=0.6324 (C:0.6324, R:0.0105)
Batch  50/356: Loss=0.6483 (C:0.6483, R:0.0105)
Batch  75/356: Loss=0.6554 (C:0.6554, R:0.0105)
Batch 100/356: Loss=0.6199 (C:0.6199, R:0.0105)
Batch 125/356: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 150/356: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 175/356: Loss=0.6670 (C:0.6670, R:0.0105)
Batch 200/356: Loss=0.6527 (C:0.6527, R:0.0105)
Batch 225/356: Loss=0.6488 (C:0.6488, R:0.0105)
Batch 250/356: Loss=0.6536 (C:0.6536, R:0.0105)
Batch 275/356: Loss=0.6326 (C:0.6326, R:0.0105)
Batch 300/356: Loss=0.6957 (C:0.6957, R:0.0105)
Batch 325/356: Loss=0.6714 (C:0.6714, R:0.0105)
Batch 350/356: Loss=0.6814 (C:0.6814, R:0.0105)

============================================================
Epoch 40/300 completed in 27.0s
Train: Loss=0.6558 (C:0.6558, R:0.0105) Ratio=4.62x
Val:   Loss=0.8341 (C:0.8341, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.8341)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/356: Loss=0.6715 (C:0.6715, R:0.0106)
Batch  25/356: Loss=0.6336 (C:0.6336, R:0.0105)
Batch  50/356: Loss=0.6608 (C:0.6608, R:0.0105)
Batch  75/356: Loss=0.6299 (C:0.6299, R:0.0105)
Batch 100/356: Loss=0.6583 (C:0.6583, R:0.0106)
Batch 125/356: Loss=0.6403 (C:0.6403, R:0.0105)
Batch 150/356: Loss=0.6699 (C:0.6699, R:0.0105)
Batch 175/356: Loss=0.6798 (C:0.6798, R:0.0105)
Batch 200/356: Loss=0.6575 (C:0.6575, R:0.0106)
Batch 225/356: Loss=0.6350 (C:0.6350, R:0.0105)
Batch 250/356: Loss=0.6859 (C:0.6859, R:0.0105)
Batch 275/356: Loss=0.6471 (C:0.6471, R:0.0105)
Batch 300/356: Loss=0.6647 (C:0.6647, R:0.0105)
Batch 325/356: Loss=0.6716 (C:0.6716, R:0.0105)
Batch 350/356: Loss=0.6732 (C:0.6732, R:0.0105)

============================================================
Epoch 41/300 completed in 20.6s
Train: Loss=0.6529 (C:0.6529, R:0.0105) Ratio=4.63x
Val:   Loss=0.8269 (C:0.8269, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.8269)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/356: Loss=0.6562 (C:0.6562, R:0.0105)
Batch  25/356: Loss=0.6531 (C:0.6531, R:0.0105)
Batch  50/356: Loss=0.6787 (C:0.6787, R:0.0105)
Batch  75/356: Loss=0.6418 (C:0.6418, R:0.0105)
Batch 100/356: Loss=0.6240 (C:0.6240, R:0.0105)
Batch 125/356: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 150/356: Loss=0.6726 (C:0.6726, R:0.0105)
Batch 175/356: Loss=0.6721 (C:0.6721, R:0.0105)
Batch 200/356: Loss=0.6351 (C:0.6351, R:0.0105)
Batch 225/356: Loss=0.6252 (C:0.6252, R:0.0105)
Batch 250/356: Loss=0.6323 (C:0.6323, R:0.0105)
Batch 275/356: Loss=0.6425 (C:0.6425, R:0.0105)
Batch 300/356: Loss=0.6437 (C:0.6437, R:0.0105)
Batch 325/356: Loss=0.6672 (C:0.6672, R:0.0105)
Batch 350/356: Loss=0.6520 (C:0.6520, R:0.0105)

============================================================
Epoch 42/300 completed in 20.9s
Train: Loss=0.6507 (C:0.6507, R:0.0105) Ratio=4.66x
Val:   Loss=0.8167 (C:0.8167, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.180
✅ New best model saved (Val Loss: 0.8167)
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.342 ± 0.553
    Neg distances: 2.550 ± 1.066
    Separation ratio: 7.46x
    Gap: -4.288
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/356: Loss=0.6231 (C:0.6231, R:0.0105)
Batch  25/356: Loss=0.6269 (C:0.6269, R:0.0105)
Batch  50/356: Loss=0.6097 (C:0.6097, R:0.0105)
Batch  75/356: Loss=0.6141 (C:0.6141, R:0.0105)
Batch 100/356: Loss=0.6392 (C:0.6392, R:0.0105)
Batch 125/356: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 150/356: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 175/356: Loss=0.6357 (C:0.6357, R:0.0105)
Batch 200/356: Loss=0.6394 (C:0.6394, R:0.0105)
Batch 225/356: Loss=0.6106 (C:0.6106, R:0.0105)
Batch 250/356: Loss=0.6120 (C:0.6120, R:0.0105)
Batch 275/356: Loss=0.6433 (C:0.6433, R:0.0105)
Batch 300/356: Loss=0.6219 (C:0.6219, R:0.0105)
Batch 325/356: Loss=0.6287 (C:0.6287, R:0.0105)
Batch 350/356: Loss=0.6330 (C:0.6330, R:0.0105)

============================================================
Epoch 43/300 completed in 26.7s
Train: Loss=0.6359 (C:0.6359, R:0.0105) Ratio=4.83x
Val:   Loss=0.8159 (C:0.8159, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.8159)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/356: Loss=0.6498 (C:0.6498, R:0.0105)
Batch  25/356: Loss=0.6400 (C:0.6400, R:0.0105)
Batch  50/356: Loss=0.6392 (C:0.6392, R:0.0105)
Batch  75/356: Loss=0.5975 (C:0.5975, R:0.0105)
Batch 100/356: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 125/356: Loss=0.6097 (C:0.6097, R:0.0105)
Batch 150/356: Loss=0.6178 (C:0.6178, R:0.0105)
Batch 175/356: Loss=0.6417 (C:0.6417, R:0.0105)
Batch 200/356: Loss=0.6288 (C:0.6288, R:0.0105)
Batch 225/356: Loss=0.6655 (C:0.6655, R:0.0105)
Batch 250/356: Loss=0.6214 (C:0.6214, R:0.0105)
Batch 275/356: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 300/356: Loss=0.6284 (C:0.6284, R:0.0105)
Batch 325/356: Loss=0.6553 (C:0.6553, R:0.0105)
Batch 350/356: Loss=0.6266 (C:0.6266, R:0.0105)

============================================================
Epoch 44/300 completed in 20.4s
Train: Loss=0.6320 (C:0.6320, R:0.0105) Ratio=4.71x
Val:   Loss=0.8129 (C:0.8129, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.210
✅ New best model saved (Val Loss: 0.8129)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/356: Loss=0.6005 (C:0.6005, R:0.0105)
Batch  25/356: Loss=0.6209 (C:0.6209, R:0.0105)
Batch  50/356: Loss=0.6304 (C:0.6304, R:0.0105)
Batch  75/356: Loss=0.5968 (C:0.5968, R:0.0105)
Batch 100/356: Loss=0.6269 (C:0.6269, R:0.0105)
Batch 125/356: Loss=0.6400 (C:0.6400, R:0.0105)
Batch 150/356: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 175/356: Loss=0.6356 (C:0.6356, R:0.0105)
Batch 200/356: Loss=0.6570 (C:0.6570, R:0.0105)
Batch 225/356: Loss=0.6331 (C:0.6331, R:0.0105)
Batch 250/356: Loss=0.6153 (C:0.6153, R:0.0105)
Batch 275/356: Loss=0.6121 (C:0.6121, R:0.0105)
Batch 300/356: Loss=0.6341 (C:0.6341, R:0.0105)
Batch 325/356: Loss=0.6336 (C:0.6336, R:0.0105)
Batch 350/356: Loss=0.6418 (C:0.6418, R:0.0105)

============================================================
Epoch 45/300 completed in 20.3s
Train: Loss=0.6289 (C:0.6289, R:0.0105) Ratio=4.80x
Val:   Loss=0.8158 (C:0.8158, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.225
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.356 ± 0.567
    Neg distances: 2.586 ± 1.095
    Separation ratio: 7.27x
    Gap: -4.534
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/356: Loss=0.6394 (C:0.6394, R:0.0105)
Batch  25/356: Loss=0.6143 (C:0.6143, R:0.0105)
Batch  50/356: Loss=0.6549 (C:0.6549, R:0.0105)
Batch  75/356: Loss=0.6266 (C:0.6266, R:0.0105)
Batch 100/356: Loss=0.6439 (C:0.6439, R:0.0105)
Batch 125/356: Loss=0.6084 (C:0.6084, R:0.0106)
Batch 150/356: Loss=0.6604 (C:0.6604, R:0.0105)
Batch 175/356: Loss=0.6596 (C:0.6596, R:0.0105)
Batch 200/356: Loss=0.6417 (C:0.6417, R:0.0105)
Batch 225/356: Loss=0.6513 (C:0.6513, R:0.0105)
Batch 250/356: Loss=0.6573 (C:0.6573, R:0.0105)
Batch 275/356: Loss=0.6412 (C:0.6412, R:0.0105)
Batch 300/356: Loss=0.6291 (C:0.6291, R:0.0105)
Batch 325/356: Loss=0.6325 (C:0.6325, R:0.0105)
Batch 350/356: Loss=0.6342 (C:0.6342, R:0.0105)

============================================================
Epoch 46/300 completed in 26.6s
Train: Loss=0.6335 (C:0.6335, R:0.0105) Ratio=4.70x
Val:   Loss=0.8183 (C:0.8183, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.240
No improvement for 2 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/356: Loss=0.6297 (C:0.6297, R:0.0105)
Batch  25/356: Loss=0.6183 (C:0.6183, R:0.0105)
Batch  50/356: Loss=0.6100 (C:0.6100, R:0.0105)
Batch  75/356: Loss=0.6105 (C:0.6105, R:0.0105)
Batch 100/356: Loss=0.6385 (C:0.6385, R:0.0105)
Batch 125/356: Loss=0.5992 (C:0.5992, R:0.0105)
Batch 150/356: Loss=0.6213 (C:0.6213, R:0.0106)
Batch 175/356: Loss=0.6266 (C:0.6266, R:0.0105)
Batch 200/356: Loss=0.6086 (C:0.6086, R:0.0105)
Batch 225/356: Loss=0.6237 (C:0.6237, R:0.0105)
Batch 250/356: Loss=0.6567 (C:0.6567, R:0.0105)
Batch 275/356: Loss=0.6361 (C:0.6361, R:0.0105)
Batch 300/356: Loss=0.6372 (C:0.6372, R:0.0105)
Batch 325/356: Loss=0.6669 (C:0.6669, R:0.0105)
Batch 350/356: Loss=0.6691 (C:0.6691, R:0.0105)

============================================================
Epoch 47/300 completed in 20.6s
Train: Loss=0.6305 (C:0.6305, R:0.0105) Ratio=4.80x
Val:   Loss=0.8277 (C:0.8277, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.255
No improvement for 3 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/356: Loss=0.6017 (C:0.6017, R:0.0105)
Batch  25/356: Loss=0.5851 (C:0.5851, R:0.0105)
Batch  50/356: Loss=0.6166 (C:0.6166, R:0.0105)
Batch  75/356: Loss=0.6227 (C:0.6227, R:0.0105)
Batch 100/356: Loss=0.6304 (C:0.6304, R:0.0105)
Batch 125/356: Loss=0.6186 (C:0.6186, R:0.0105)
Batch 150/356: Loss=0.6211 (C:0.6211, R:0.0105)
Batch 175/356: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 200/356: Loss=0.6163 (C:0.6163, R:0.0105)
Batch 225/356: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 250/356: Loss=0.6352 (C:0.6352, R:0.0105)
Batch 275/356: Loss=0.6542 (C:0.6542, R:0.0105)
Batch 300/356: Loss=0.6056 (C:0.6056, R:0.0105)
Batch 325/356: Loss=0.6349 (C:0.6349, R:0.0105)
Batch 350/356: Loss=0.6250 (C:0.6250, R:0.0105)

============================================================
Epoch 48/300 completed in 20.8s
Train: Loss=0.6264 (C:0.6264, R:0.0105) Ratio=4.86x
Val:   Loss=0.8155 (C:0.8155, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.270
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.348 ± 0.584
    Neg distances: 2.598 ± 1.094
    Separation ratio: 7.46x
    Gap: -4.479
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/356: Loss=0.6124 (C:0.6124, R:0.0105)
Batch  25/356: Loss=0.6457 (C:0.6457, R:0.0105)
Batch  50/356: Loss=0.6120 (C:0.6120, R:0.0105)
Batch  75/356: Loss=0.6070 (C:0.6070, R:0.0105)
Batch 100/356: Loss=0.6356 (C:0.6356, R:0.0105)
Batch 125/356: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 150/356: Loss=0.6593 (C:0.6593, R:0.0105)
Batch 175/356: Loss=0.6011 (C:0.6011, R:0.0105)
Batch 200/356: Loss=0.6311 (C:0.6311, R:0.0105)
Batch 225/356: Loss=0.6473 (C:0.6473, R:0.0105)
Batch 250/356: Loss=0.6172 (C:0.6172, R:0.0105)
Batch 275/356: Loss=0.6314 (C:0.6314, R:0.0105)
Batch 300/356: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 325/356: Loss=0.6634 (C:0.6634, R:0.0105)
Batch 350/356: Loss=0.6209 (C:0.6209, R:0.0105)

============================================================
Epoch 49/300 completed in 26.3s
Train: Loss=0.6196 (C:0.6196, R:0.0105) Ratio=4.80x
Val:   Loss=0.8187 (C:0.8187, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.285
No improvement for 5 epochs
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/356: Loss=0.5983 (C:0.5983, R:0.0105)
Batch  25/356: Loss=0.5904 (C:0.5904, R:0.0105)
Batch  50/356: Loss=0.5975 (C:0.5975, R:0.0105)
Batch  75/356: Loss=0.6626 (C:0.6626, R:0.0105)
Batch 100/356: Loss=0.6134 (C:0.6134, R:0.0105)
Batch 125/356: Loss=0.6078 (C:0.6078, R:0.0105)
Batch 150/356: Loss=0.6544 (C:0.6544, R:0.0105)
Batch 175/356: Loss=0.5935 (C:0.5935, R:0.0105)
Batch 200/356: Loss=0.6145 (C:0.6145, R:0.0105)
Batch 225/356: Loss=0.6390 (C:0.6390, R:0.0105)
Batch 250/356: Loss=0.6208 (C:0.6208, R:0.0105)
Batch 275/356: Loss=0.6002 (C:0.6002, R:0.0105)
Batch 300/356: Loss=0.6068 (C:0.6068, R:0.0105)
Batch 325/356: Loss=0.5968 (C:0.5968, R:0.0105)
Batch 350/356: Loss=0.6307 (C:0.6307, R:0.0105)

============================================================
Epoch 50/300 completed in 20.3s
Train: Loss=0.6177 (C:0.6177, R:0.0105) Ratio=4.83x
Val:   Loss=0.8168 (C:0.8168, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/356: Loss=0.5851 (C:0.5851, R:0.0105)
Batch  25/356: Loss=0.6432 (C:0.6432, R:0.0105)
Batch  50/356: Loss=0.6368 (C:0.6368, R:0.0105)
Batch  75/356: Loss=0.5931 (C:0.5931, R:0.0105)
Batch 100/356: Loss=0.6018 (C:0.6018, R:0.0105)
Batch 125/356: Loss=0.6153 (C:0.6153, R:0.0105)
Batch 150/356: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 175/356: Loss=0.6294 (C:0.6294, R:0.0105)
Batch 200/356: Loss=0.6229 (C:0.6229, R:0.0105)
Batch 225/356: Loss=0.6449 (C:0.6449, R:0.0105)
Batch 250/356: Loss=0.6087 (C:0.6087, R:0.0105)
Batch 275/356: Loss=0.6489 (C:0.6489, R:0.0105)
Batch 300/356: Loss=0.6055 (C:0.6055, R:0.0105)
Batch 325/356: Loss=0.5780 (C:0.5780, R:0.0105)
Batch 350/356: Loss=0.6178 (C:0.6178, R:0.0106)

============================================================
Epoch 51/300 completed in 20.5s
Train: Loss=0.6147 (C:0.6147, R:0.0105) Ratio=4.92x
Val:   Loss=0.8056 (C:0.8056, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8056)
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.342 ± 0.563
    Neg distances: 2.591 ± 1.085
    Separation ratio: 7.58x
    Gap: -4.537
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/356: Loss=0.5837 (C:0.5837, R:0.0105)
Batch  25/356: Loss=0.5910 (C:0.5910, R:0.0105)
Batch  50/356: Loss=0.5857 (C:0.5857, R:0.0105)
Batch  75/356: Loss=0.6156 (C:0.6156, R:0.0105)
Batch 100/356: Loss=0.6252 (C:0.6252, R:0.0105)
Batch 125/356: Loss=0.5996 (C:0.5996, R:0.0105)
Batch 150/356: Loss=0.6057 (C:0.6057, R:0.0105)
Batch 175/356: Loss=0.5941 (C:0.5941, R:0.0105)
Batch 200/356: Loss=0.6056 (C:0.6056, R:0.0105)
Batch 225/356: Loss=0.6045 (C:0.6045, R:0.0105)
Batch 250/356: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 275/356: Loss=0.5921 (C:0.5921, R:0.0105)
Batch 300/356: Loss=0.6309 (C:0.6309, R:0.0105)
Batch 325/356: Loss=0.6271 (C:0.6271, R:0.0105)
Batch 350/356: Loss=0.6234 (C:0.6234, R:0.0105)

============================================================
Epoch 52/300 completed in 26.5s
Train: Loss=0.6063 (C:0.6063, R:0.0105) Ratio=4.98x
Val:   Loss=0.8136 (C:0.8136, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/356: Loss=0.6055 (C:0.6055, R:0.0105)
Batch  25/356: Loss=0.5718 (C:0.5718, R:0.0105)
Batch  50/356: Loss=0.6148 (C:0.6148, R:0.0105)
Batch  75/356: Loss=0.5584 (C:0.5584, R:0.0105)
Batch 100/356: Loss=0.5920 (C:0.5920, R:0.0105)
Batch 125/356: Loss=0.5980 (C:0.5980, R:0.0105)
Batch 150/356: Loss=0.6126 (C:0.6126, R:0.0105)
Batch 175/356: Loss=0.6063 (C:0.6063, R:0.0105)
Batch 200/356: Loss=0.6073 (C:0.6073, R:0.0105)
Batch 225/356: Loss=0.5935 (C:0.5935, R:0.0105)
Batch 250/356: Loss=0.6314 (C:0.6314, R:0.0105)
Batch 275/356: Loss=0.5966 (C:0.5966, R:0.0105)
Batch 300/356: Loss=0.5888 (C:0.5888, R:0.0105)
Batch 325/356: Loss=0.6304 (C:0.6304, R:0.0105)
Batch 350/356: Loss=0.6027 (C:0.6027, R:0.0105)

============================================================
Epoch 53/300 completed in 20.3s
Train: Loss=0.6049 (C:0.6049, R:0.0105) Ratio=4.99x
Val:   Loss=0.8072 (C:0.8072, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/356: Loss=0.5698 (C:0.5698, R:0.0105)
Batch  25/356: Loss=0.6098 (C:0.6098, R:0.0105)
Batch  50/356: Loss=0.5742 (C:0.5742, R:0.0105)
Batch  75/356: Loss=0.5687 (C:0.5687, R:0.0105)
Batch 100/356: Loss=0.5934 (C:0.5934, R:0.0105)
Batch 125/356: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 150/356: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 175/356: Loss=0.6112 (C:0.6112, R:0.0105)
Batch 200/356: Loss=0.6190 (C:0.6190, R:0.0105)
Batch 225/356: Loss=0.6139 (C:0.6139, R:0.0105)
Batch 250/356: Loss=0.5812 (C:0.5812, R:0.0105)
Batch 275/356: Loss=0.5999 (C:0.5999, R:0.0105)
Batch 300/356: Loss=0.6058 (C:0.6058, R:0.0105)
Batch 325/356: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 350/356: Loss=0.6326 (C:0.6326, R:0.0105)

============================================================
Epoch 54/300 completed in 20.4s
Train: Loss=0.6034 (C:0.6034, R:0.0105) Ratio=5.06x
Val:   Loss=0.8124 (C:0.8124, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.323 ± 0.558
    Neg distances: 2.636 ± 1.096
    Separation ratio: 8.17x
    Gap: -4.383
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/356: Loss=0.5684 (C:0.5684, R:0.0105)
Batch  25/356: Loss=0.5797 (C:0.5797, R:0.0105)
Batch  50/356: Loss=0.5614 (C:0.5614, R:0.0105)
Batch  75/356: Loss=0.5789 (C:0.5789, R:0.0105)
Batch 100/356: Loss=0.5919 (C:0.5919, R:0.0105)
Batch 125/356: Loss=0.5692 (C:0.5692, R:0.0105)
Batch 150/356: Loss=0.5706 (C:0.5706, R:0.0105)
Batch 175/356: Loss=0.6205 (C:0.6205, R:0.0105)
Batch 200/356: Loss=0.5804 (C:0.5804, R:0.0105)
Batch 225/356: Loss=0.5783 (C:0.5783, R:0.0105)
Batch 250/356: Loss=0.6037 (C:0.6037, R:0.0105)
Batch 275/356: Loss=0.6122 (C:0.6122, R:0.0105)
Batch 300/356: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 325/356: Loss=0.6089 (C:0.6089, R:0.0105)
Batch 350/356: Loss=0.5987 (C:0.5987, R:0.0105)

============================================================
Epoch 55/300 completed in 26.6s
Train: Loss=0.5877 (C:0.5877, R:0.0105) Ratio=5.02x
Val:   Loss=0.7979 (C:0.7979, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7979)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/356: Loss=0.5800 (C:0.5800, R:0.0105)
Batch  25/356: Loss=0.5922 (C:0.5922, R:0.0105)
Batch  50/356: Loss=0.5834 (C:0.5834, R:0.0105)
Batch  75/356: Loss=0.5783 (C:0.5783, R:0.0105)
Batch 100/356: Loss=0.6179 (C:0.6179, R:0.0105)
Batch 125/356: Loss=0.5667 (C:0.5667, R:0.0105)
Batch 150/356: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 175/356: Loss=0.5757 (C:0.5757, R:0.0105)
Batch 200/356: Loss=0.5956 (C:0.5956, R:0.0105)
Batch 225/356: Loss=0.5905 (C:0.5905, R:0.0105)
Batch 250/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 275/356: Loss=0.6114 (C:0.6114, R:0.0105)
Batch 300/356: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 325/356: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 350/356: Loss=0.6165 (C:0.6165, R:0.0105)

============================================================
Epoch 56/300 completed in 20.8s
Train: Loss=0.5861 (C:0.5861, R:0.0105) Ratio=5.02x
Val:   Loss=0.8061 (C:0.8061, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/356: Loss=0.5774 (C:0.5774, R:0.0105)
Batch  25/356: Loss=0.5597 (C:0.5597, R:0.0105)
Batch  50/356: Loss=0.6085 (C:0.6085, R:0.0105)
Batch  75/356: Loss=0.5826 (C:0.5826, R:0.0105)
Batch 100/356: Loss=0.5961 (C:0.5961, R:0.0105)
Batch 125/356: Loss=0.5986 (C:0.5986, R:0.0105)
Batch 150/356: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 175/356: Loss=0.5958 (C:0.5958, R:0.0105)
Batch 200/356: Loss=0.5892 (C:0.5892, R:0.0105)
Batch 225/356: Loss=0.5809 (C:0.5809, R:0.0105)
Batch 250/356: Loss=0.6153 (C:0.6153, R:0.0105)
Batch 275/356: Loss=0.5883 (C:0.5883, R:0.0105)
Batch 300/356: Loss=0.5750 (C:0.5750, R:0.0105)
Batch 325/356: Loss=0.6160 (C:0.6160, R:0.0105)
Batch 350/356: Loss=0.6136 (C:0.6136, R:0.0105)

============================================================
Epoch 57/300 completed in 20.9s
Train: Loss=0.5842 (C:0.5842, R:0.0105) Ratio=4.99x
Val:   Loss=0.8027 (C:0.8027, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.318 ± 0.557
    Neg distances: 2.635 ± 1.090
    Separation ratio: 8.30x
    Gap: -4.445
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/356: Loss=0.5708 (C:0.5708, R:0.0105)
Batch  25/356: Loss=0.5714 (C:0.5714, R:0.0105)
Batch  50/356: Loss=0.5458 (C:0.5458, R:0.0105)
Batch  75/356: Loss=0.5856 (C:0.5856, R:0.0105)
Batch 100/356: Loss=0.5619 (C:0.5619, R:0.0105)
Batch 125/356: Loss=0.6149 (C:0.6149, R:0.0105)
Batch 150/356: Loss=0.5741 (C:0.5741, R:0.0105)
Batch 175/356: Loss=0.5787 (C:0.5787, R:0.0105)
Batch 200/356: Loss=0.5741 (C:0.5741, R:0.0105)
Batch 225/356: Loss=0.5790 (C:0.5790, R:0.0105)
Batch 250/356: Loss=0.5488 (C:0.5488, R:0.0105)
Batch 275/356: Loss=0.5797 (C:0.5797, R:0.0105)
Batch 300/356: Loss=0.5870 (C:0.5870, R:0.0105)
Batch 325/356: Loss=0.5903 (C:0.5903, R:0.0105)
Batch 350/356: Loss=0.5767 (C:0.5767, R:0.0105)

============================================================
Epoch 58/300 completed in 27.0s
Train: Loss=0.5774 (C:0.5774, R:0.0105) Ratio=5.09x
Val:   Loss=0.7881 (C:0.7881, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7881)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/356: Loss=0.5504 (C:0.5504, R:0.0105)
Batch  25/356: Loss=0.5696 (C:0.5696, R:0.0105)
Batch  50/356: Loss=0.5630 (C:0.5630, R:0.0105)
Batch  75/356: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 100/356: Loss=0.5772 (C:0.5772, R:0.0105)
Batch 125/356: Loss=0.5556 (C:0.5556, R:0.0105)
Batch 150/356: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 175/356: Loss=0.6063 (C:0.6063, R:0.0105)
Batch 200/356: Loss=0.5826 (C:0.5826, R:0.0105)
Batch 225/356: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 250/356: Loss=0.5470 (C:0.5470, R:0.0105)
Batch 275/356: Loss=0.6105 (C:0.6105, R:0.0105)
Batch 300/356: Loss=0.6283 (C:0.6283, R:0.0105)
Batch 325/356: Loss=0.5958 (C:0.5958, R:0.0105)
Batch 350/356: Loss=0.5943 (C:0.5943, R:0.0106)

============================================================
Epoch 59/300 completed in 20.5s
Train: Loss=0.5768 (C:0.5768, R:0.0105) Ratio=5.01x
Val:   Loss=0.7937 (C:0.7937, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/356: Loss=0.5360 (C:0.5360, R:0.0105)
Batch  25/356: Loss=0.5893 (C:0.5893, R:0.0105)
Batch  50/356: Loss=0.5598 (C:0.5598, R:0.0105)
Batch  75/356: Loss=0.5612 (C:0.5612, R:0.0105)
Batch 100/356: Loss=0.5471 (C:0.5471, R:0.0105)
Batch 125/356: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 150/356: Loss=0.5985 (C:0.5985, R:0.0105)
Batch 175/356: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 200/356: Loss=0.5516 (C:0.5516, R:0.0105)
Batch 225/356: Loss=0.5594 (C:0.5594, R:0.0105)
Batch 250/356: Loss=0.5519 (C:0.5519, R:0.0105)
Batch 275/356: Loss=0.5598 (C:0.5598, R:0.0105)
Batch 300/356: Loss=0.5988 (C:0.5988, R:0.0105)
Batch 325/356: Loss=0.5963 (C:0.5963, R:0.0105)
Batch 350/356: Loss=0.6060 (C:0.6060, R:0.0105)

============================================================
Epoch 60/300 completed in 20.4s
Train: Loss=0.5757 (C:0.5757, R:0.0105) Ratio=5.22x
Val:   Loss=0.8010 (C:0.8010, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.330 ± 0.578
    Neg distances: 2.656 ± 1.105
    Separation ratio: 8.06x
    Gap: -4.603
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/356: Loss=0.5597 (C:0.5597, R:0.0105)
Batch  25/356: Loss=0.5399 (C:0.5399, R:0.0105)
Batch  50/356: Loss=0.5614 (C:0.5614, R:0.0105)
Batch  75/356: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 100/356: Loss=0.5966 (C:0.5966, R:0.0105)
Batch 125/356: Loss=0.5631 (C:0.5631, R:0.0105)
Batch 150/356: Loss=0.5745 (C:0.5745, R:0.0105)
Batch 175/356: Loss=0.5548 (C:0.5548, R:0.0105)
Batch 200/356: Loss=0.6048 (C:0.6048, R:0.0106)
Batch 225/356: Loss=0.5853 (C:0.5853, R:0.0105)
Batch 250/356: Loss=0.5941 (C:0.5941, R:0.0105)
Batch 275/356: Loss=0.5502 (C:0.5502, R:0.0105)
Batch 300/356: Loss=0.5946 (C:0.5946, R:0.0105)
Batch 325/356: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 350/356: Loss=0.5575 (C:0.5575, R:0.0105)

============================================================
Epoch 61/300 completed in 26.9s
Train: Loss=0.5795 (C:0.5795, R:0.0105) Ratio=5.23x
Val:   Loss=0.8027 (C:0.8027, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/356: Loss=0.5325 (C:0.5325, R:0.0105)
Batch  25/356: Loss=0.5954 (C:0.5954, R:0.0105)
Batch  50/356: Loss=0.5735 (C:0.5735, R:0.0105)
Batch  75/356: Loss=0.5958 (C:0.5958, R:0.0105)
Batch 100/356: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 125/356: Loss=0.5744 (C:0.5744, R:0.0105)
Batch 150/356: Loss=0.5952 (C:0.5952, R:0.0105)
Batch 175/356: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 200/356: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 225/356: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 250/356: Loss=0.5691 (C:0.5691, R:0.0105)
Batch 275/356: Loss=0.5812 (C:0.5812, R:0.0105)
Batch 300/356: Loss=0.5853 (C:0.5853, R:0.0105)
Batch 325/356: Loss=0.6243 (C:0.6243, R:0.0105)
Batch 350/356: Loss=0.6123 (C:0.6123, R:0.0105)

============================================================
Epoch 62/300 completed in 20.7s
Train: Loss=0.5782 (C:0.5782, R:0.0105) Ratio=5.14x
Val:   Loss=0.8030 (C:0.8030, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/356: Loss=0.5491 (C:0.5491, R:0.0106)
Batch  25/356: Loss=0.5816 (C:0.5816, R:0.0105)
Batch  50/356: Loss=0.6064 (C:0.6064, R:0.0105)
Batch  75/356: Loss=0.5769 (C:0.5769, R:0.0105)
Batch 100/356: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 125/356: Loss=0.5415 (C:0.5415, R:0.0105)
Batch 150/356: Loss=0.5634 (C:0.5634, R:0.0105)
Batch 175/356: Loss=0.5695 (C:0.5695, R:0.0105)
Batch 200/356: Loss=0.6171 (C:0.6171, R:0.0105)
Batch 225/356: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 250/356: Loss=0.5901 (C:0.5901, R:0.0105)
Batch 275/356: Loss=0.5882 (C:0.5882, R:0.0105)
Batch 300/356: Loss=0.6007 (C:0.6007, R:0.0105)
Batch 325/356: Loss=0.6065 (C:0.6065, R:0.0106)
Batch 350/356: Loss=0.5906 (C:0.5906, R:0.0105)

============================================================
Epoch 63/300 completed in 20.7s
Train: Loss=0.5770 (C:0.5770, R:0.0105) Ratio=5.18x
Val:   Loss=0.8138 (C:0.8138, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.309 ± 0.538
    Neg distances: 2.652 ± 1.090
    Separation ratio: 8.58x
    Gap: -4.582
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/356: Loss=0.5359 (C:0.5359, R:0.0105)
Batch  25/356: Loss=0.5650 (C:0.5650, R:0.0105)
Batch  50/356: Loss=0.5540 (C:0.5540, R:0.0105)
Batch  75/356: Loss=0.5513 (C:0.5513, R:0.0105)
Batch 100/356: Loss=0.5528 (C:0.5528, R:0.0105)
Batch 125/356: Loss=0.5750 (C:0.5750, R:0.0105)
Batch 150/356: Loss=0.5584 (C:0.5584, R:0.0105)
Batch 175/356: Loss=0.5477 (C:0.5477, R:0.0105)
Batch 200/356: Loss=0.5422 (C:0.5422, R:0.0105)
Batch 225/356: Loss=0.5799 (C:0.5799, R:0.0105)
Batch 250/356: Loss=0.6007 (C:0.6007, R:0.0105)
Batch 275/356: Loss=0.5658 (C:0.5658, R:0.0106)
Batch 300/356: Loss=0.5754 (C:0.5754, R:0.0105)
Batch 325/356: Loss=0.5842 (C:0.5842, R:0.0105)
Batch 350/356: Loss=0.5513 (C:0.5513, R:0.0105)

============================================================
Epoch 64/300 completed in 27.1s
Train: Loss=0.5629 (C:0.5629, R:0.0105) Ratio=5.15x
Val:   Loss=0.7940 (C:0.7940, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/356: Loss=0.5412 (C:0.5412, R:0.0105)
Batch  25/356: Loss=0.5572 (C:0.5572, R:0.0105)
Batch  50/356: Loss=0.5769 (C:0.5769, R:0.0105)
Batch  75/356: Loss=0.5579 (C:0.5579, R:0.0105)
Batch 100/356: Loss=0.5509 (C:0.5509, R:0.0105)
Batch 125/356: Loss=0.5374 (C:0.5374, R:0.0105)
Batch 150/356: Loss=0.5376 (C:0.5376, R:0.0105)
Batch 175/356: Loss=0.5455 (C:0.5455, R:0.0105)
Batch 200/356: Loss=0.5304 (C:0.5304, R:0.0105)
Batch 225/356: Loss=0.5743 (C:0.5743, R:0.0105)
Batch 250/356: Loss=0.5369 (C:0.5369, R:0.0105)
Batch 275/356: Loss=0.5989 (C:0.5989, R:0.0105)
Batch 300/356: Loss=0.5935 (C:0.5935, R:0.0105)
Batch 325/356: Loss=0.5554 (C:0.5554, R:0.0105)
Batch 350/356: Loss=0.5980 (C:0.5980, R:0.0105)

============================================================
Epoch 65/300 completed in 21.2s
Train: Loss=0.5612 (C:0.5612, R:0.0105) Ratio=5.23x
Val:   Loss=0.7975 (C:0.7975, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/356: Loss=0.5566 (C:0.5566, R:0.0105)
Batch  25/356: Loss=0.5394 (C:0.5394, R:0.0105)
Batch  50/356: Loss=0.5462 (C:0.5462, R:0.0105)
Batch  75/356: Loss=0.5553 (C:0.5553, R:0.0105)
Batch 100/356: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 125/356: Loss=0.5590 (C:0.5590, R:0.0105)
Batch 150/356: Loss=0.5576 (C:0.5576, R:0.0105)
Batch 175/356: Loss=0.5364 (C:0.5364, R:0.0105)
Batch 200/356: Loss=0.5699 (C:0.5699, R:0.0105)
Batch 225/356: Loss=0.5412 (C:0.5412, R:0.0105)
Batch 250/356: Loss=0.5712 (C:0.5712, R:0.0105)
Batch 275/356: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 300/356: Loss=0.5513 (C:0.5513, R:0.0105)
Batch 325/356: Loss=0.5996 (C:0.5996, R:0.0105)
Batch 350/356: Loss=0.6072 (C:0.6072, R:0.0105)

============================================================
Epoch 66/300 completed in 21.1s
Train: Loss=0.5596 (C:0.5596, R:0.0105) Ratio=5.18x
Val:   Loss=0.7981 (C:0.7981, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 66 epochs
Best model was at epoch 58 with Val Loss: 0.7881

Global Dataset Training Completed!
Best epoch: 58
Best validation loss: 0.7881
Final separation ratios: Train=5.18x, Val=3.01x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/7 batches
Extracted representations: torch.Size([9824, 50])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4524
  Adjusted Rand Score: 0.5269
  Clustering Accuracy: 0.8126
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
Extracted representations: torch.Size([546816, 50])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/6 batches
Extracted representations: torch.Size([9216, 50])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9216 samples
Classification Results:
  Accuracy: 0.8116
  Per-class F1: [0.8289205702647658, 0.7510835913312693, 0.8592105263157895]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.782 ± 0.910
  Negative distances: 2.330 ± 1.239
  Separation ratio: 2.98x
  Gap: -4.571
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4524
  Clustering Accuracy: 0.8126
  Adjusted Rand Score: 0.5269

Classification Performance:
  Accuracy: 0.8116

Separation Quality:
  Separation Ratio: 2.98x
  Gap: -4.571
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249/results/evaluation_results_20250714_203758.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249/results/evaluation_results_20250714_203758.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat50_bs1536_20250714_201249/final_results.json

Key Results:
  Separation ratio: 2.98x
  Perfect separation: False
  Classification accuracy: 0.8116
  Result: 0.8116% (improvement: +-80.86%)
  Cleaning up: coarse_lr2e-04_lat50_bs1536_20250714_201249

[9/12] Testing: coarse_lr2e-04_lat75_bs1020
  Learning rate: 0.0002
  Latent dim: 75
  Batch size: 1020
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 20:37:58.982749
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,876,555
Model created with 1,876,555 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0002)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,876,555
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.092 ± 0.011
    Neg distances: 0.093 ± 0.011
    Separation ratio: 1.00x
    Gap: -0.127
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9998 (C:1.9998, R:0.0119)
Batch  25/537: Loss=1.9813 (C:1.9813, R:0.0114)
Batch  50/537: Loss=1.9712 (C:1.9712, R:0.0111)
Batch  75/537: Loss=1.9620 (C:1.9620, R:0.0109)
Batch 100/537: Loss=1.9521 (C:1.9521, R:0.0107)
Batch 125/537: Loss=1.9354 (C:1.9354, R:0.0107)
Batch 150/537: Loss=1.9248 (C:1.9248, R:0.0106)
Batch 175/537: Loss=1.9279 (C:1.9279, R:0.0106)
Batch 200/537: Loss=1.9160 (C:1.9160, R:0.0105)
Batch 225/537: Loss=1.9140 (C:1.9140, R:0.0105)
Batch 250/537: Loss=1.9022 (C:1.9022, R:0.0105)
Batch 275/537: Loss=1.9021 (C:1.9021, R:0.0105)
Batch 300/537: Loss=1.9114 (C:1.9114, R:0.0105)
Batch 325/537: Loss=1.9100 (C:1.9100, R:0.0105)
Batch 350/537: Loss=1.9013 (C:1.9013, R:0.0105)
Batch 375/537: Loss=1.8954 (C:1.8954, R:0.0105)
Batch 400/537: Loss=1.9013 (C:1.9013, R:0.0105)
Batch 425/537: Loss=1.8938 (C:1.8938, R:0.0105)
Batch 450/537: Loss=1.8996 (C:1.8996, R:0.0105)
Batch 475/537: Loss=1.9019 (C:1.9019, R:0.0105)
Batch 500/537: Loss=1.9028 (C:1.9028, R:0.0106)
Batch 525/537: Loss=1.8809 (C:1.8809, R:0.0105)

============================================================
Epoch 1/300 completed in 27.2s
Train: Loss=1.9205 (C:1.9205, R:0.0107) Ratio=1.84x
Val:   Loss=1.8873 (C:1.8873, R:0.0104) Ratio=2.34x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8873)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.8769 (C:1.8769, R:0.0105)
Batch  25/537: Loss=1.8849 (C:1.8849, R:0.0105)
Batch  50/537: Loss=1.8848 (C:1.8848, R:0.0105)
Batch  75/537: Loss=1.8914 (C:1.8914, R:0.0106)
Batch 100/537: Loss=1.8960 (C:1.8960, R:0.0105)
Batch 125/537: Loss=1.8863 (C:1.8863, R:0.0106)
Batch 150/537: Loss=1.8878 (C:1.8878, R:0.0105)
Batch 175/537: Loss=1.8827 (C:1.8827, R:0.0105)
Batch 200/537: Loss=1.8837 (C:1.8837, R:0.0105)
Batch 225/537: Loss=1.8860 (C:1.8860, R:0.0105)
Batch 250/537: Loss=1.8892 (C:1.8892, R:0.0105)
Batch 275/537: Loss=1.8938 (C:1.8938, R:0.0105)
Batch 300/537: Loss=1.8919 (C:1.8919, R:0.0105)
Batch 325/537: Loss=1.8831 (C:1.8831, R:0.0105)
Batch 350/537: Loss=1.8776 (C:1.8776, R:0.0105)
Batch 375/537: Loss=1.8784 (C:1.8784, R:0.0105)
Batch 400/537: Loss=1.8937 (C:1.8937, R:0.0105)
Batch 425/537: Loss=1.8835 (C:1.8835, R:0.0105)
Batch 450/537: Loss=1.8862 (C:1.8862, R:0.0105)
Batch 475/537: Loss=1.8960 (C:1.8960, R:0.0105)
Batch 500/537: Loss=1.8922 (C:1.8922, R:0.0105)
Batch 525/537: Loss=1.8651 (C:1.8651, R:0.0105)

============================================================
Epoch 2/300 completed in 21.2s
Train: Loss=1.8846 (C:1.8846, R:0.0105) Ratio=2.37x
Val:   Loss=1.8758 (C:1.8758, R:0.0104) Ratio=2.56x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8758)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8713 (C:1.8713, R:0.0105)
Batch  25/537: Loss=1.8631 (C:1.8631, R:0.0105)
Batch  50/537: Loss=1.8677 (C:1.8677, R:0.0105)
Batch  75/537: Loss=1.8765 (C:1.8765, R:0.0105)
Batch 100/537: Loss=1.8730 (C:1.8730, R:0.0105)
Batch 125/537: Loss=1.8671 (C:1.8671, R:0.0105)
Batch 150/537: Loss=1.8782 (C:1.8782, R:0.0105)
Batch 175/537: Loss=1.8731 (C:1.8731, R:0.0105)
Batch 200/537: Loss=1.8781 (C:1.8781, R:0.0105)
Batch 225/537: Loss=1.8755 (C:1.8755, R:0.0105)
Batch 250/537: Loss=1.8672 (C:1.8672, R:0.0105)
Batch 275/537: Loss=1.8713 (C:1.8713, R:0.0105)
Batch 300/537: Loss=1.8818 (C:1.8818, R:0.0105)
Batch 325/537: Loss=1.8658 (C:1.8658, R:0.0105)
Batch 350/537: Loss=1.8695 (C:1.8695, R:0.0105)
Batch 375/537: Loss=1.8770 (C:1.8770, R:0.0105)
Batch 400/537: Loss=1.8581 (C:1.8581, R:0.0105)
Batch 425/537: Loss=1.8833 (C:1.8833, R:0.0105)
Batch 450/537: Loss=1.8712 (C:1.8712, R:0.0105)
Batch 475/537: Loss=1.8769 (C:1.8769, R:0.0105)
Batch 500/537: Loss=1.8669 (C:1.8669, R:0.0105)
Batch 525/537: Loss=1.8676 (C:1.8676, R:0.0105)

============================================================
Epoch 3/300 completed in 21.5s
Train: Loss=1.8732 (C:1.8732, R:0.0105) Ratio=2.64x
Val:   Loss=1.8718 (C:1.8718, R:0.0104) Ratio=2.71x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8718)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.451 ± 0.545
    Neg distances: 1.471 ± 0.836
    Separation ratio: 3.26x
    Gap: -2.738
    ✅ Excellent global separation!

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.1205 (C:1.1205, R:0.0105)
Batch  25/537: Loss=1.1553 (C:1.1553, R:0.0105)
Batch  50/537: Loss=1.1573 (C:1.1573, R:0.0105)
Batch  75/537: Loss=1.1354 (C:1.1354, R:0.0105)
Batch 100/537: Loss=1.1666 (C:1.1666, R:0.0105)
Batch 125/537: Loss=1.1934 (C:1.1934, R:0.0105)
Batch 150/537: Loss=1.1441 (C:1.1441, R:0.0105)
Batch 175/537: Loss=1.1535 (C:1.1535, R:0.0105)
Batch 200/537: Loss=1.1936 (C:1.1936, R:0.0105)
Batch 225/537: Loss=1.1495 (C:1.1495, R:0.0105)
Batch 250/537: Loss=1.1812 (C:1.1812, R:0.0105)
Batch 275/537: Loss=1.1556 (C:1.1556, R:0.0105)
Batch 300/537: Loss=1.1276 (C:1.1276, R:0.0106)
Batch 325/537: Loss=1.1686 (C:1.1686, R:0.0105)
Batch 350/537: Loss=1.1266 (C:1.1266, R:0.0105)
Batch 375/537: Loss=1.1368 (C:1.1368, R:0.0105)
Batch 400/537: Loss=1.1789 (C:1.1789, R:0.0105)
Batch 425/537: Loss=1.1950 (C:1.1950, R:0.0105)
Batch 450/537: Loss=1.2033 (C:1.2033, R:0.0105)
Batch 475/537: Loss=1.1450 (C:1.1450, R:0.0105)
Batch 500/537: Loss=1.1482 (C:1.1482, R:0.0105)
Batch 525/537: Loss=1.1585 (C:1.1585, R:0.0105)

============================================================
Epoch 4/300 completed in 28.4s
Train: Loss=1.1524 (C:1.1524, R:0.0105) Ratio=2.68x
Val:   Loss=1.1331 (C:1.1331, R:0.0104) Ratio=2.80x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1331)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.1398 (C:1.1398, R:0.0105)
Batch  25/537: Loss=1.1230 (C:1.1230, R:0.0105)
Batch  50/537: Loss=1.1519 (C:1.1519, R:0.0106)
Batch  75/537: Loss=1.1250 (C:1.1250, R:0.0105)
Batch 100/537: Loss=1.1660 (C:1.1660, R:0.0105)
Batch 125/537: Loss=1.1417 (C:1.1417, R:0.0105)
Batch 150/537: Loss=1.1846 (C:1.1846, R:0.0105)
Batch 175/537: Loss=1.1056 (C:1.1056, R:0.0105)
Batch 200/537: Loss=1.1513 (C:1.1513, R:0.0105)
Batch 225/537: Loss=1.1548 (C:1.1548, R:0.0105)
Batch 250/537: Loss=1.1096 (C:1.1096, R:0.0105)
Batch 275/537: Loss=1.1466 (C:1.1466, R:0.0106)
Batch 300/537: Loss=1.0881 (C:1.0881, R:0.0105)
Batch 325/537: Loss=1.1193 (C:1.1193, R:0.0105)
Batch 350/537: Loss=1.1174 (C:1.1174, R:0.0105)
Batch 375/537: Loss=1.1073 (C:1.1073, R:0.0105)
Batch 400/537: Loss=1.1288 (C:1.1288, R:0.0105)
Batch 425/537: Loss=1.1283 (C:1.1283, R:0.0105)
Batch 450/537: Loss=1.1491 (C:1.1491, R:0.0105)
Batch 475/537: Loss=1.1292 (C:1.1292, R:0.0105)
Batch 500/537: Loss=1.1220 (C:1.1220, R:0.0105)
Batch 525/537: Loss=1.1403 (C:1.1403, R:0.0105)

============================================================
Epoch 5/300 completed in 21.8s
Train: Loss=1.1315 (C:1.1315, R:0.0105) Ratio=2.91x
Val:   Loss=1.1437 (C:1.1437, R:0.0104) Ratio=2.85x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.0920 (C:1.0920, R:0.0105)
Batch  25/537: Loss=1.1300 (C:1.1300, R:0.0105)
Batch  50/537: Loss=1.1125 (C:1.1125, R:0.0105)
Batch  75/537: Loss=1.1206 (C:1.1206, R:0.0105)
Batch 100/537: Loss=1.0911 (C:1.0911, R:0.0105)
Batch 125/537: Loss=1.1508 (C:1.1508, R:0.0105)
Batch 150/537: Loss=1.1263 (C:1.1263, R:0.0105)
Batch 175/537: Loss=1.1405 (C:1.1405, R:0.0105)
Batch 200/537: Loss=1.1315 (C:1.1315, R:0.0105)
Batch 225/537: Loss=1.1267 (C:1.1267, R:0.0105)
Batch 250/537: Loss=1.0964 (C:1.0964, R:0.0105)
Batch 275/537: Loss=1.1147 (C:1.1147, R:0.0105)
Batch 300/537: Loss=1.1330 (C:1.1330, R:0.0105)
Batch 325/537: Loss=1.1245 (C:1.1245, R:0.0105)
Batch 350/537: Loss=1.1101 (C:1.1101, R:0.0105)
Batch 375/537: Loss=1.1265 (C:1.1265, R:0.0105)
Batch 400/537: Loss=1.0955 (C:1.0955, R:0.0105)
Batch 425/537: Loss=1.1469 (C:1.1469, R:0.0105)
Batch 450/537: Loss=1.1350 (C:1.1350, R:0.0105)
Batch 475/537: Loss=1.1086 (C:1.1086, R:0.0105)
Batch 500/537: Loss=1.1315 (C:1.1315, R:0.0104)
Batch 525/537: Loss=1.1533 (C:1.1533, R:0.0105)

============================================================
Epoch 6/300 completed in 21.6s
Train: Loss=1.1187 (C:1.1187, R:0.0105) Ratio=3.08x
Val:   Loss=1.1382 (C:1.1382, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.411 ± 0.556
    Neg distances: 1.538 ± 0.846
    Separation ratio: 3.74x
    Gap: -2.769
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.0675 (C:1.0675, R:0.0105)
Batch  25/537: Loss=1.0759 (C:1.0759, R:0.0105)
Batch  50/537: Loss=1.0176 (C:1.0176, R:0.0105)
Batch  75/537: Loss=1.0344 (C:1.0344, R:0.0105)
Batch 100/537: Loss=1.0644 (C:1.0644, R:0.0105)
Batch 125/537: Loss=1.0286 (C:1.0286, R:0.0105)
Batch 150/537: Loss=1.0991 (C:1.0991, R:0.0105)
Batch 175/537: Loss=1.0617 (C:1.0617, R:0.0105)
Batch 200/537: Loss=1.0486 (C:1.0486, R:0.0105)
Batch 225/537: Loss=1.0355 (C:1.0355, R:0.0105)
Batch 250/537: Loss=1.1104 (C:1.1104, R:0.0105)
Batch 275/537: Loss=1.0571 (C:1.0571, R:0.0105)
Batch 300/537: Loss=1.0664 (C:1.0664, R:0.0105)
Batch 325/537: Loss=1.0734 (C:1.0734, R:0.0105)
Batch 350/537: Loss=1.0652 (C:1.0652, R:0.0105)
Batch 375/537: Loss=1.0721 (C:1.0721, R:0.0105)
Batch 400/537: Loss=1.0699 (C:1.0699, R:0.0105)
Batch 425/537: Loss=1.0793 (C:1.0793, R:0.0105)
Batch 450/537: Loss=1.0426 (C:1.0426, R:0.0105)
Batch 475/537: Loss=1.0657 (C:1.0657, R:0.0106)
Batch 500/537: Loss=1.0588 (C:1.0588, R:0.0105)
Batch 525/537: Loss=1.0443 (C:1.0443, R:0.0105)

============================================================
Epoch 7/300 completed in 27.7s
Train: Loss=1.0609 (C:1.0609, R:0.0105) Ratio=3.23x
Val:   Loss=1.0917 (C:1.0917, R:0.0104) Ratio=2.94x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0917)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.0647 (C:1.0647, R:0.0105)
Batch  25/537: Loss=1.0644 (C:1.0644, R:0.0105)
Batch  50/537: Loss=1.0635 (C:1.0635, R:0.0105)
Batch  75/537: Loss=1.0222 (C:1.0222, R:0.0105)
Batch 100/537: Loss=1.0907 (C:1.0907, R:0.0106)
Batch 125/537: Loss=1.0331 (C:1.0331, R:0.0105)
Batch 150/537: Loss=1.0763 (C:1.0763, R:0.0105)
Batch 175/537: Loss=1.0211 (C:1.0211, R:0.0105)
Batch 200/537: Loss=1.0834 (C:1.0834, R:0.0105)
Batch 225/537: Loss=1.0707 (C:1.0707, R:0.0105)
Batch 250/537: Loss=1.0467 (C:1.0467, R:0.0105)
Batch 275/537: Loss=1.0403 (C:1.0403, R:0.0105)
Batch 300/537: Loss=1.0710 (C:1.0710, R:0.0105)
Batch 325/537: Loss=1.0596 (C:1.0596, R:0.0106)
Batch 350/537: Loss=1.0153 (C:1.0153, R:0.0105)
Batch 375/537: Loss=1.0880 (C:1.0880, R:0.0105)
Batch 400/537: Loss=1.0688 (C:1.0688, R:0.0105)
Batch 425/537: Loss=1.0456 (C:1.0456, R:0.0105)
Batch 450/537: Loss=1.1038 (C:1.1038, R:0.0105)
Batch 475/537: Loss=1.0563 (C:1.0563, R:0.0105)
Batch 500/537: Loss=1.0344 (C:1.0344, R:0.0105)
Batch 525/537: Loss=1.0593 (C:1.0593, R:0.0105)

============================================================
Epoch 8/300 completed in 22.0s
Train: Loss=1.0513 (C:1.0513, R:0.0105) Ratio=3.29x
Val:   Loss=1.0797 (C:1.0797, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0797)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0295 (C:1.0295, R:0.0105)
Batch  25/537: Loss=1.0232 (C:1.0232, R:0.0105)
Batch  50/537: Loss=1.0420 (C:1.0420, R:0.0105)
Batch  75/537: Loss=1.0120 (C:1.0120, R:0.0105)
Batch 100/537: Loss=1.0191 (C:1.0191, R:0.0105)
Batch 125/537: Loss=0.9981 (C:0.9981, R:0.0105)
Batch 150/537: Loss=1.0556 (C:1.0556, R:0.0105)
Batch 175/537: Loss=1.0573 (C:1.0573, R:0.0105)
Batch 200/537: Loss=1.0488 (C:1.0488, R:0.0105)
Batch 225/537: Loss=1.0512 (C:1.0512, R:0.0105)
Batch 250/537: Loss=1.0383 (C:1.0383, R:0.0105)
Batch 275/537: Loss=1.0410 (C:1.0410, R:0.0105)
Batch 300/537: Loss=1.0595 (C:1.0595, R:0.0105)
Batch 325/537: Loss=1.0484 (C:1.0484, R:0.0105)
Batch 350/537: Loss=1.0519 (C:1.0519, R:0.0106)
Batch 375/537: Loss=1.0320 (C:1.0320, R:0.0105)
Batch 400/537: Loss=1.0651 (C:1.0651, R:0.0106)
Batch 425/537: Loss=1.0619 (C:1.0619, R:0.0105)
Batch 450/537: Loss=1.0595 (C:1.0595, R:0.0105)
Batch 475/537: Loss=1.0634 (C:1.0634, R:0.0105)
Batch 500/537: Loss=1.0365 (C:1.0365, R:0.0105)
Batch 525/537: Loss=1.0524 (C:1.0524, R:0.0105)

============================================================
Epoch 9/300 completed in 22.2s
Train: Loss=1.0438 (C:1.0438, R:0.0105) Ratio=3.41x
Val:   Loss=1.0805 (C:1.0805, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.364 ± 0.528
    Neg distances: 1.626 ± 0.867
    Separation ratio: 4.47x
    Gap: -2.871
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=0.9534 (C:0.9534, R:0.0105)
Batch  25/537: Loss=1.0150 (C:1.0150, R:0.0105)
Batch  50/537: Loss=0.9785 (C:0.9785, R:0.0105)
Batch  75/537: Loss=0.9565 (C:0.9565, R:0.0105)
Batch 100/537: Loss=0.9691 (C:0.9691, R:0.0105)
Batch 125/537: Loss=0.9971 (C:0.9971, R:0.0105)
Batch 150/537: Loss=1.0250 (C:1.0250, R:0.0105)
Batch 175/537: Loss=1.0149 (C:1.0149, R:0.0105)
Batch 200/537: Loss=0.9704 (C:0.9704, R:0.0105)
Batch 225/537: Loss=0.9933 (C:0.9933, R:0.0105)
Batch 250/537: Loss=0.9461 (C:0.9461, R:0.0105)
Batch 275/537: Loss=0.9983 (C:0.9983, R:0.0105)
Batch 300/537: Loss=0.9756 (C:0.9756, R:0.0105)
Batch 325/537: Loss=0.9594 (C:0.9594, R:0.0106)
Batch 350/537: Loss=0.9633 (C:0.9633, R:0.0105)
Batch 375/537: Loss=0.9906 (C:0.9906, R:0.0105)
Batch 400/537: Loss=0.9803 (C:0.9803, R:0.0105)
Batch 425/537: Loss=1.0063 (C:1.0063, R:0.0105)
Batch 450/537: Loss=0.9941 (C:0.9941, R:0.0105)
Batch 475/537: Loss=0.9856 (C:0.9856, R:0.0105)
Batch 500/537: Loss=0.9815 (C:0.9815, R:0.0106)
Batch 525/537: Loss=1.0096 (C:1.0096, R:0.0105)

============================================================
Epoch 10/300 completed in 27.9s
Train: Loss=0.9817 (C:0.9817, R:0.0105) Ratio=3.51x
Val:   Loss=1.0265 (C:1.0265, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0265)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.0164 (C:1.0164, R:0.0105)
Batch  25/537: Loss=0.9552 (C:0.9552, R:0.0105)
Batch  50/537: Loss=0.9248 (C:0.9248, R:0.0105)
Batch  75/537: Loss=0.9673 (C:0.9673, R:0.0105)
Batch 100/537: Loss=0.9876 (C:0.9876, R:0.0105)
Batch 125/537: Loss=0.9696 (C:0.9696, R:0.0106)
Batch 150/537: Loss=0.9538 (C:0.9538, R:0.0105)
Batch 175/537: Loss=0.9715 (C:0.9715, R:0.0105)
Batch 200/537: Loss=1.0036 (C:1.0036, R:0.0105)
Batch 225/537: Loss=0.9882 (C:0.9882, R:0.0105)
Batch 250/537: Loss=0.9504 (C:0.9504, R:0.0105)
Batch 275/537: Loss=0.9971 (C:0.9971, R:0.0105)
Batch 300/537: Loss=0.9852 (C:0.9852, R:0.0105)
Batch 325/537: Loss=0.9954 (C:0.9954, R:0.0105)
Batch 350/537: Loss=1.0032 (C:1.0032, R:0.0106)
Batch 375/537: Loss=1.0056 (C:1.0056, R:0.0105)
Batch 400/537: Loss=0.9477 (C:0.9477, R:0.0105)
Batch 425/537: Loss=0.9758 (C:0.9758, R:0.0105)
Batch 450/537: Loss=0.9717 (C:0.9717, R:0.0105)
Batch 475/537: Loss=0.9828 (C:0.9828, R:0.0105)
Batch 500/537: Loss=1.0130 (C:1.0130, R:0.0105)
Batch 525/537: Loss=0.9732 (C:0.9732, R:0.0105)

============================================================
Epoch 11/300 completed in 21.7s
Train: Loss=0.9761 (C:0.9761, R:0.0105) Ratio=3.60x
Val:   Loss=1.0308 (C:1.0308, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=0.9617 (C:0.9617, R:0.0105)
Batch  25/537: Loss=0.9483 (C:0.9483, R:0.0105)
Batch  50/537: Loss=0.9593 (C:0.9593, R:0.0105)
Batch  75/537: Loss=0.9693 (C:0.9693, R:0.0105)
Batch 100/537: Loss=0.9731 (C:0.9731, R:0.0105)
Batch 125/537: Loss=0.9880 (C:0.9880, R:0.0105)
Batch 150/537: Loss=0.9602 (C:0.9602, R:0.0105)
Batch 175/537: Loss=0.9472 (C:0.9472, R:0.0105)
Batch 200/537: Loss=0.9674 (C:0.9674, R:0.0105)
Batch 225/537: Loss=0.9354 (C:0.9354, R:0.0105)
Batch 250/537: Loss=0.9559 (C:0.9559, R:0.0105)
Batch 275/537: Loss=0.9692 (C:0.9692, R:0.0105)
Batch 300/537: Loss=0.9430 (C:0.9430, R:0.0105)
Batch 325/537: Loss=0.9759 (C:0.9759, R:0.0105)
Batch 350/537: Loss=0.9442 (C:0.9442, R:0.0105)
Batch 375/537: Loss=0.9514 (C:0.9514, R:0.0105)
Batch 400/537: Loss=0.9707 (C:0.9707, R:0.0105)
Batch 425/537: Loss=0.9701 (C:0.9701, R:0.0105)
Batch 450/537: Loss=0.9639 (C:0.9639, R:0.0105)
Batch 475/537: Loss=0.9716 (C:0.9716, R:0.0105)
Batch 500/537: Loss=1.0167 (C:1.0167, R:0.0105)
Batch 525/537: Loss=0.9710 (C:0.9710, R:0.0105)

============================================================
Epoch 12/300 completed in 21.7s
Train: Loss=0.9716 (C:0.9716, R:0.0105) Ratio=3.80x
Val:   Loss=1.0137 (C:1.0137, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0137)
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.346 ± 0.520
    Neg distances: 1.664 ± 0.866
    Separation ratio: 4.81x
    Gap: -2.939
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9212 (C:0.9212, R:0.0105)
Batch  25/537: Loss=0.9302 (C:0.9302, R:0.0105)
Batch  50/537: Loss=0.9396 (C:0.9396, R:0.0105)
Batch  75/537: Loss=0.9237 (C:0.9237, R:0.0105)
Batch 100/537: Loss=0.9292 (C:0.9292, R:0.0105)
Batch 125/537: Loss=0.9409 (C:0.9409, R:0.0105)
Batch 150/537: Loss=0.9300 (C:0.9300, R:0.0105)
Batch 175/537: Loss=0.9516 (C:0.9516, R:0.0105)
Batch 200/537: Loss=0.9651 (C:0.9651, R:0.0105)
Batch 225/537: Loss=0.9456 (C:0.9456, R:0.0105)
Batch 250/537: Loss=0.9394 (C:0.9394, R:0.0105)
Batch 275/537: Loss=0.9467 (C:0.9467, R:0.0105)
Batch 300/537: Loss=0.9651 (C:0.9651, R:0.0105)
Batch 325/537: Loss=0.9413 (C:0.9413, R:0.0105)
Batch 350/537: Loss=0.9071 (C:0.9071, R:0.0105)
Batch 375/537: Loss=0.9463 (C:0.9463, R:0.0105)
Batch 400/537: Loss=0.9257 (C:0.9257, R:0.0105)
Batch 425/537: Loss=0.9458 (C:0.9458, R:0.0105)
Batch 450/537: Loss=0.9630 (C:0.9630, R:0.0105)
Batch 475/537: Loss=0.9459 (C:0.9459, R:0.0105)
Batch 500/537: Loss=0.9573 (C:0.9573, R:0.0105)
Batch 525/537: Loss=0.9713 (C:0.9713, R:0.0105)

============================================================
Epoch 13/300 completed in 27.1s
Train: Loss=0.9449 (C:0.9449, R:0.0105) Ratio=3.72x
Val:   Loss=0.9917 (C:0.9917, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9917)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9151 (C:0.9151, R:0.0105)
Batch  25/537: Loss=0.9070 (C:0.9070, R:0.0105)
Batch  50/537: Loss=0.9434 (C:0.9434, R:0.0105)
Batch  75/537: Loss=0.9242 (C:0.9242, R:0.0105)
Batch 100/537: Loss=0.9402 (C:0.9402, R:0.0105)
Batch 125/537: Loss=0.9331 (C:0.9331, R:0.0106)
Batch 150/537: Loss=0.9019 (C:0.9019, R:0.0105)
Batch 175/537: Loss=0.9678 (C:0.9678, R:0.0105)
Batch 200/537: Loss=0.9414 (C:0.9414, R:0.0105)
Batch 225/537: Loss=0.9511 (C:0.9511, R:0.0105)
Batch 250/537: Loss=0.9311 (C:0.9311, R:0.0105)
Batch 275/537: Loss=0.9331 (C:0.9331, R:0.0105)
Batch 300/537: Loss=0.9072 (C:0.9072, R:0.0105)
Batch 325/537: Loss=0.9292 (C:0.9292, R:0.0105)
Batch 350/537: Loss=0.9492 (C:0.9492, R:0.0105)
Batch 375/537: Loss=0.9372 (C:0.9372, R:0.0105)
Batch 400/537: Loss=0.9393 (C:0.9393, R:0.0105)
Batch 425/537: Loss=0.9361 (C:0.9361, R:0.0105)
Batch 450/537: Loss=0.9280 (C:0.9280, R:0.0105)
Batch 475/537: Loss=0.9434 (C:0.9434, R:0.0105)
Batch 500/537: Loss=0.9306 (C:0.9306, R:0.0105)
Batch 525/537: Loss=0.9604 (C:0.9604, R:0.0105)

============================================================
Epoch 14/300 completed in 21.6s
Train: Loss=0.9412 (C:0.9412, R:0.0105) Ratio=3.92x
Val:   Loss=1.0066 (C:1.0066, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.9140 (C:0.9140, R:0.0105)
Batch  25/537: Loss=0.9245 (C:0.9245, R:0.0105)
Batch  50/537: Loss=0.9311 (C:0.9311, R:0.0105)
Batch  75/537: Loss=0.9365 (C:0.9365, R:0.0105)
Batch 100/537: Loss=0.9526 (C:0.9526, R:0.0105)
Batch 125/537: Loss=0.9485 (C:0.9485, R:0.0105)
Batch 150/537: Loss=0.9378 (C:0.9378, R:0.0105)
Batch 175/537: Loss=0.9207 (C:0.9207, R:0.0105)
Batch 200/537: Loss=0.9421 (C:0.9421, R:0.0105)
Batch 225/537: Loss=0.9454 (C:0.9454, R:0.0106)
Batch 250/537: Loss=0.9437 (C:0.9437, R:0.0105)
Batch 275/537: Loss=0.9299 (C:0.9299, R:0.0105)
Batch 300/537: Loss=0.9350 (C:0.9350, R:0.0105)
Batch 325/537: Loss=0.9446 (C:0.9446, R:0.0105)
Batch 350/537: Loss=0.9445 (C:0.9445, R:0.0105)
Batch 375/537: Loss=0.9598 (C:0.9598, R:0.0105)
Batch 400/537: Loss=0.9503 (C:0.9503, R:0.0105)
Batch 425/537: Loss=0.9453 (C:0.9453, R:0.0105)
Batch 450/537: Loss=0.9368 (C:0.9368, R:0.0106)
Batch 475/537: Loss=0.9293 (C:0.9293, R:0.0105)
Batch 500/537: Loss=0.9094 (C:0.9094, R:0.0105)
Batch 525/537: Loss=0.9595 (C:0.9595, R:0.0105)

============================================================
Epoch 15/300 completed in 21.5s
Train: Loss=0.9358 (C:0.9358, R:0.0105) Ratio=3.84x
Val:   Loss=1.0001 (C:1.0001, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.343 ± 0.518
    Neg distances: 1.721 ± 0.871
    Separation ratio: 5.02x
    Gap: -3.000
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.9098 (C:0.9098, R:0.0105)
Batch  25/537: Loss=0.8861 (C:0.8861, R:0.0105)
Batch  50/537: Loss=0.8968 (C:0.8968, R:0.0105)
Batch  75/537: Loss=0.9215 (C:0.9215, R:0.0105)
Batch 100/537: Loss=0.9512 (C:0.9512, R:0.0105)
Batch 125/537: Loss=0.8929 (C:0.8929, R:0.0105)
Batch 150/537: Loss=0.8838 (C:0.8838, R:0.0105)
Batch 175/537: Loss=0.8915 (C:0.8915, R:0.0105)
Batch 200/537: Loss=0.9441 (C:0.9441, R:0.0105)
Batch 225/537: Loss=0.8886 (C:0.8886, R:0.0105)
Batch 250/537: Loss=0.9153 (C:0.9153, R:0.0105)
Batch 275/537: Loss=0.9162 (C:0.9162, R:0.0105)
Batch 300/537: Loss=0.8848 (C:0.8848, R:0.0105)
Batch 325/537: Loss=0.9059 (C:0.9059, R:0.0105)
Batch 350/537: Loss=0.9577 (C:0.9577, R:0.0105)
Batch 375/537: Loss=0.9310 (C:0.9310, R:0.0105)
Batch 400/537: Loss=0.9285 (C:0.9285, R:0.0105)
Batch 425/537: Loss=0.9320 (C:0.9320, R:0.0105)
Batch 450/537: Loss=0.8668 (C:0.8668, R:0.0105)
Batch 475/537: Loss=0.8877 (C:0.8877, R:0.0105)
Batch 500/537: Loss=0.8886 (C:0.8886, R:0.0105)
Batch 525/537: Loss=0.9172 (C:0.9172, R:0.0105)

============================================================
Epoch 16/300 completed in 27.1s
Train: Loss=0.9116 (C:0.9116, R:0.0105) Ratio=3.97x
Val:   Loss=0.9791 (C:0.9791, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9791)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.9137 (C:0.9137, R:0.0105)
Batch  25/537: Loss=0.8960 (C:0.8960, R:0.0105)
Batch  50/537: Loss=0.8853 (C:0.8853, R:0.0105)
Batch  75/537: Loss=0.8883 (C:0.8883, R:0.0105)
Batch 100/537: Loss=0.9175 (C:0.9175, R:0.0105)
Batch 125/537: Loss=0.8932 (C:0.8932, R:0.0105)
Batch 150/537: Loss=0.8932 (C:0.8932, R:0.0106)
Batch 175/537: Loss=0.9157 (C:0.9157, R:0.0105)
Batch 200/537: Loss=0.9220 (C:0.9220, R:0.0105)
Batch 225/537: Loss=0.9331 (C:0.9331, R:0.0105)
Batch 250/537: Loss=0.9165 (C:0.9165, R:0.0105)
Batch 275/537: Loss=0.9193 (C:0.9193, R:0.0105)
Batch 300/537: Loss=0.9050 (C:0.9050, R:0.0105)
Batch 325/537: Loss=0.9055 (C:0.9055, R:0.0105)
Batch 350/537: Loss=0.9144 (C:0.9144, R:0.0106)
Batch 375/537: Loss=0.9290 (C:0.9290, R:0.0105)
Batch 400/537: Loss=0.9333 (C:0.9333, R:0.0106)
Batch 425/537: Loss=0.9091 (C:0.9091, R:0.0105)
Batch 450/537: Loss=0.9002 (C:0.9002, R:0.0105)
Batch 475/537: Loss=0.8933 (C:0.8933, R:0.0105)
Batch 500/537: Loss=0.8919 (C:0.8919, R:0.0105)
Batch 525/537: Loss=0.9154 (C:0.9154, R:0.0105)

============================================================
Epoch 17/300 completed in 21.2s
Train: Loss=0.9078 (C:0.9078, R:0.0105) Ratio=4.00x
Val:   Loss=0.9859 (C:0.9859, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.8539 (C:0.8539, R:0.0105)
Batch  25/537: Loss=0.8778 (C:0.8778, R:0.0105)
Batch  50/537: Loss=0.9162 (C:0.9162, R:0.0105)
Batch  75/537: Loss=0.9080 (C:0.9080, R:0.0105)
Batch 100/537: Loss=0.9057 (C:0.9057, R:0.0105)
Batch 125/537: Loss=0.9463 (C:0.9463, R:0.0105)
Batch 150/537: Loss=0.9200 (C:0.9200, R:0.0105)
Batch 175/537: Loss=0.8832 (C:0.8832, R:0.0105)
Batch 200/537: Loss=0.9005 (C:0.9005, R:0.0105)
Batch 225/537: Loss=0.9007 (C:0.9007, R:0.0105)
Batch 250/537: Loss=0.8806 (C:0.8806, R:0.0105)
Batch 275/537: Loss=0.9045 (C:0.9045, R:0.0105)
Batch 300/537: Loss=0.9223 (C:0.9223, R:0.0105)
Batch 325/537: Loss=0.8697 (C:0.8697, R:0.0105)
Batch 350/537: Loss=0.8812 (C:0.8812, R:0.0105)
Batch 375/537: Loss=0.8888 (C:0.8888, R:0.0105)
Batch 400/537: Loss=0.8735 (C:0.8735, R:0.0105)
Batch 425/537: Loss=0.9009 (C:0.9009, R:0.0105)
Batch 450/537: Loss=0.8993 (C:0.8993, R:0.0105)
Batch 475/537: Loss=0.9076 (C:0.9076, R:0.0105)
Batch 500/537: Loss=0.8655 (C:0.8655, R:0.0105)
Batch 525/537: Loss=0.9171 (C:0.9171, R:0.0105)

============================================================
Epoch 18/300 completed in 21.8s
Train: Loss=0.9041 (C:0.9041, R:0.0105) Ratio=4.06x
Val:   Loss=0.9753 (C:0.9753, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9753)
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.332 ± 0.510
    Neg distances: 1.783 ± 0.888
    Separation ratio: 5.37x
    Gap: -3.076
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.8881 (C:0.8881, R:0.0105)
Batch  25/537: Loss=0.8253 (C:0.8253, R:0.0105)
Batch  50/537: Loss=0.8565 (C:0.8565, R:0.0105)
Batch  75/537: Loss=0.8627 (C:0.8627, R:0.0105)
Batch 100/537: Loss=0.8402 (C:0.8402, R:0.0105)
Batch 125/537: Loss=0.8600 (C:0.8600, R:0.0105)
Batch 150/537: Loss=0.8872 (C:0.8872, R:0.0105)
Batch 175/537: Loss=0.8778 (C:0.8778, R:0.0105)
Batch 200/537: Loss=0.8830 (C:0.8830, R:0.0105)
Batch 225/537: Loss=0.8885 (C:0.8885, R:0.0105)
Batch 250/537: Loss=0.8661 (C:0.8661, R:0.0105)
Batch 275/537: Loss=0.8781 (C:0.8781, R:0.0105)
Batch 300/537: Loss=0.8765 (C:0.8765, R:0.0105)
Batch 325/537: Loss=0.8780 (C:0.8780, R:0.0105)
Batch 350/537: Loss=0.8559 (C:0.8559, R:0.0105)
Batch 375/537: Loss=0.8628 (C:0.8628, R:0.0105)
Batch 400/537: Loss=0.8979 (C:0.8979, R:0.0105)
Batch 425/537: Loss=0.8591 (C:0.8591, R:0.0105)
Batch 450/537: Loss=0.8898 (C:0.8898, R:0.0105)
Batch 475/537: Loss=0.8640 (C:0.8640, R:0.0105)
Batch 500/537: Loss=0.8918 (C:0.8918, R:0.0105)
Batch 525/537: Loss=0.9207 (C:0.9207, R:0.0104)

============================================================
Epoch 19/300 completed in 27.9s
Train: Loss=0.8780 (C:0.8780, R:0.0105) Ratio=4.06x
Val:   Loss=0.9543 (C:0.9543, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9543)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.8770 (C:0.8770, R:0.0105)
Batch  25/537: Loss=0.8600 (C:0.8600, R:0.0105)
Batch  50/537: Loss=0.8747 (C:0.8747, R:0.0105)
Batch  75/537: Loss=0.8897 (C:0.8897, R:0.0105)
Batch 100/537: Loss=0.9007 (C:0.9007, R:0.0105)
Batch 125/537: Loss=0.8630 (C:0.8630, R:0.0105)
Batch 150/537: Loss=0.8511 (C:0.8511, R:0.0105)
Batch 175/537: Loss=0.8572 (C:0.8572, R:0.0105)
Batch 200/537: Loss=0.8722 (C:0.8722, R:0.0106)
Batch 225/537: Loss=0.8617 (C:0.8617, R:0.0105)
Batch 250/537: Loss=0.8675 (C:0.8675, R:0.0105)
Batch 275/537: Loss=0.8971 (C:0.8971, R:0.0105)
Batch 300/537: Loss=0.8332 (C:0.8332, R:0.0105)
Batch 325/537: Loss=0.8540 (C:0.8540, R:0.0105)
Batch 350/537: Loss=0.8555 (C:0.8555, R:0.0106)
Batch 375/537: Loss=0.9012 (C:0.9012, R:0.0105)
Batch 400/537: Loss=0.8807 (C:0.8807, R:0.0105)
Batch 425/537: Loss=0.9158 (C:0.9158, R:0.0105)
Batch 450/537: Loss=0.8743 (C:0.8743, R:0.0105)
Batch 475/537: Loss=0.8740 (C:0.8740, R:0.0105)
Batch 500/537: Loss=0.8777 (C:0.8777, R:0.0105)
Batch 525/537: Loss=0.8807 (C:0.8807, R:0.0105)

============================================================
Epoch 20/300 completed in 21.2s
Train: Loss=0.8752 (C:0.8752, R:0.0105) Ratio=4.15x
Val:   Loss=0.9485 (C:0.9485, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9485)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8465 (C:0.8465, R:0.0105)
Batch  25/537: Loss=0.8428 (C:0.8428, R:0.0105)
Batch  50/537: Loss=0.8717 (C:0.8717, R:0.0105)
Batch  75/537: Loss=0.8650 (C:0.8650, R:0.0105)
Batch 100/537: Loss=0.8604 (C:0.8604, R:0.0105)
Batch 125/537: Loss=0.8738 (C:0.8738, R:0.0105)
Batch 150/537: Loss=0.8712 (C:0.8712, R:0.0105)
Batch 175/537: Loss=0.8677 (C:0.8677, R:0.0105)
Batch 200/537: Loss=0.8890 (C:0.8890, R:0.0105)
Batch 225/537: Loss=0.8608 (C:0.8608, R:0.0105)
Batch 250/537: Loss=0.8595 (C:0.8595, R:0.0105)
Batch 275/537: Loss=0.8722 (C:0.8722, R:0.0105)
Batch 300/537: Loss=0.8574 (C:0.8574, R:0.0105)
Batch 325/537: Loss=0.8888 (C:0.8888, R:0.0105)
Batch 350/537: Loss=0.8539 (C:0.8539, R:0.0105)
Batch 375/537: Loss=0.8912 (C:0.8912, R:0.0105)
Batch 400/537: Loss=0.8502 (C:0.8502, R:0.0105)
Batch 425/537: Loss=0.8984 (C:0.8984, R:0.0105)
Batch 450/537: Loss=0.8392 (C:0.8392, R:0.0105)
Batch 475/537: Loss=0.8490 (C:0.8490, R:0.0105)
Batch 500/537: Loss=0.9111 (C:0.9111, R:0.0105)
Batch 525/537: Loss=0.8744 (C:0.8744, R:0.0105)

============================================================
Epoch 21/300 completed in 21.0s
Train: Loss=0.8721 (C:0.8721, R:0.0105) Ratio=4.20x
Val:   Loss=0.9449 (C:0.9449, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9449)
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.322 ± 0.506
    Neg distances: 1.859 ± 0.885
    Separation ratio: 5.78x
    Gap: -3.160
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8165 (C:0.8165, R:0.0105)
Batch  25/537: Loss=0.8184 (C:0.8184, R:0.0105)
Batch  50/537: Loss=0.7837 (C:0.7837, R:0.0105)
Batch  75/537: Loss=0.8449 (C:0.8449, R:0.0105)
Batch 100/537: Loss=0.8370 (C:0.8370, R:0.0105)
Batch 125/537: Loss=0.8307 (C:0.8307, R:0.0105)
Batch 150/537: Loss=0.8230 (C:0.8230, R:0.0105)
Batch 175/537: Loss=0.8630 (C:0.8630, R:0.0105)
Batch 200/537: Loss=0.8360 (C:0.8360, R:0.0105)
Batch 225/537: Loss=0.8181 (C:0.8181, R:0.0105)
Batch 250/537: Loss=0.8115 (C:0.8115, R:0.0105)
Batch 275/537: Loss=0.7986 (C:0.7986, R:0.0105)
Batch 300/537: Loss=0.8094 (C:0.8094, R:0.0105)
Batch 325/537: Loss=0.8590 (C:0.8590, R:0.0104)
Batch 350/537: Loss=0.8384 (C:0.8384, R:0.0105)
Batch 375/537: Loss=0.8369 (C:0.8369, R:0.0105)
Batch 400/537: Loss=0.8045 (C:0.8045, R:0.0105)
Batch 425/537: Loss=0.8106 (C:0.8106, R:0.0105)
Batch 450/537: Loss=0.8244 (C:0.8244, R:0.0105)
Batch 475/537: Loss=0.8523 (C:0.8523, R:0.0105)
Batch 500/537: Loss=0.8778 (C:0.8778, R:0.0105)
Batch 525/537: Loss=0.8412 (C:0.8412, R:0.0105)

============================================================
Epoch 22/300 completed in 26.7s
Train: Loss=0.8313 (C:0.8313, R:0.0105) Ratio=4.21x
Val:   Loss=0.9218 (C:0.9218, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9218)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.8199 (C:0.8199, R:0.0105)
Batch  25/537: Loss=0.8403 (C:0.8403, R:0.0105)
Batch  50/537: Loss=0.8224 (C:0.8224, R:0.0105)
Batch  75/537: Loss=0.8209 (C:0.8209, R:0.0105)
Batch 100/537: Loss=0.8476 (C:0.8476, R:0.0105)
Batch 125/537: Loss=0.8071 (C:0.8071, R:0.0105)
Batch 150/537: Loss=0.8094 (C:0.8094, R:0.0105)
Batch 175/537: Loss=0.8204 (C:0.8204, R:0.0105)
Batch 200/537: Loss=0.8700 (C:0.8700, R:0.0105)
Batch 225/537: Loss=0.8556 (C:0.8556, R:0.0105)
Batch 250/537: Loss=0.8246 (C:0.8246, R:0.0105)
Batch 275/537: Loss=0.8104 (C:0.8104, R:0.0105)
Batch 300/537: Loss=0.8191 (C:0.8191, R:0.0105)
Batch 325/537: Loss=0.7866 (C:0.7866, R:0.0105)
Batch 350/537: Loss=0.8665 (C:0.8665, R:0.0105)
Batch 375/537: Loss=0.8209 (C:0.8209, R:0.0105)
Batch 400/537: Loss=0.8315 (C:0.8315, R:0.0105)
Batch 425/537: Loss=0.8179 (C:0.8179, R:0.0106)
Batch 450/537: Loss=0.8684 (C:0.8684, R:0.0105)
Batch 475/537: Loss=0.8479 (C:0.8479, R:0.0105)
Batch 500/537: Loss=0.8472 (C:0.8472, R:0.0105)
Batch 525/537: Loss=0.8531 (C:0.8531, R:0.0105)

============================================================
Epoch 23/300 completed in 21.1s
Train: Loss=0.8301 (C:0.8301, R:0.0105) Ratio=4.28x
Val:   Loss=0.9202 (C:0.9202, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9202)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8436 (C:0.8436, R:0.0105)
Batch  25/537: Loss=0.8086 (C:0.8086, R:0.0105)
Batch  50/537: Loss=0.8082 (C:0.8082, R:0.0105)
Batch  75/537: Loss=0.7867 (C:0.7867, R:0.0105)
Batch 100/537: Loss=0.8433 (C:0.8433, R:0.0105)
Batch 125/537: Loss=0.8387 (C:0.8387, R:0.0105)
Batch 150/537: Loss=0.8520 (C:0.8520, R:0.0105)
Batch 175/537: Loss=0.8234 (C:0.8234, R:0.0105)
Batch 200/537: Loss=0.8302 (C:0.8302, R:0.0105)
Batch 225/537: Loss=0.8452 (C:0.8452, R:0.0105)
Batch 250/537: Loss=0.8542 (C:0.8542, R:0.0105)
Batch 275/537: Loss=0.8085 (C:0.8085, R:0.0105)
Batch 300/537: Loss=0.8303 (C:0.8303, R:0.0105)
Batch 325/537: Loss=0.8287 (C:0.8287, R:0.0105)
Batch 350/537: Loss=0.8829 (C:0.8829, R:0.0105)
Batch 375/537: Loss=0.8226 (C:0.8226, R:0.0105)
Batch 400/537: Loss=0.8423 (C:0.8423, R:0.0105)
Batch 425/537: Loss=0.8508 (C:0.8508, R:0.0105)
Batch 450/537: Loss=0.8169 (C:0.8169, R:0.0105)
Batch 475/537: Loss=0.8205 (C:0.8205, R:0.0105)
Batch 500/537: Loss=0.8197 (C:0.8197, R:0.0105)
Batch 525/537: Loss=0.8237 (C:0.8237, R:0.0105)

============================================================
Epoch 24/300 completed in 21.0s
Train: Loss=0.8264 (C:0.8264, R:0.0105) Ratio=4.23x
Val:   Loss=0.9222 (C:0.9222, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.309 ± 0.486
    Neg distances: 1.923 ± 0.902
    Separation ratio: 6.22x
    Gap: -3.337
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.7924 (C:0.7924, R:0.0105)
Batch  25/537: Loss=0.7964 (C:0.7964, R:0.0105)
Batch  50/537: Loss=0.7987 (C:0.7987, R:0.0105)
Batch  75/537: Loss=0.8182 (C:0.8182, R:0.0105)
Batch 100/537: Loss=0.8105 (C:0.8105, R:0.0105)
Batch 125/537: Loss=0.7838 (C:0.7838, R:0.0105)
Batch 150/537: Loss=0.8196 (C:0.8196, R:0.0105)
Batch 175/537: Loss=0.7758 (C:0.7758, R:0.0105)
Batch 200/537: Loss=0.7783 (C:0.7783, R:0.0105)
Batch 225/537: Loss=0.7901 (C:0.7901, R:0.0105)
Batch 250/537: Loss=0.7890 (C:0.7890, R:0.0106)
Batch 275/537: Loss=0.7978 (C:0.7978, R:0.0105)
Batch 300/537: Loss=0.8343 (C:0.8343, R:0.0105)
Batch 325/537: Loss=0.7764 (C:0.7764, R:0.0105)
Batch 350/537: Loss=0.8000 (C:0.8000, R:0.0105)
Batch 375/537: Loss=0.8202 (C:0.8202, R:0.0105)
Batch 400/537: Loss=0.8218 (C:0.8218, R:0.0105)
Batch 425/537: Loss=0.8322 (C:0.8322, R:0.0105)
Batch 450/537: Loss=0.7882 (C:0.7882, R:0.0105)
Batch 475/537: Loss=0.8266 (C:0.8266, R:0.0105)
Batch 500/537: Loss=0.8179 (C:0.8179, R:0.0106)
Batch 525/537: Loss=0.8236 (C:0.8236, R:0.0105)

============================================================
Epoch 25/300 completed in 26.5s
Train: Loss=0.7994 (C:0.7994, R:0.0105) Ratio=4.35x
Val:   Loss=0.8897 (C:0.8897, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8897)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.7558 (C:0.7558, R:0.0105)
Batch  25/537: Loss=0.7564 (C:0.7564, R:0.0105)
Batch  50/537: Loss=0.7798 (C:0.7798, R:0.0105)
Batch  75/537: Loss=0.7730 (C:0.7730, R:0.0105)
Batch 100/537: Loss=0.7712 (C:0.7712, R:0.0105)
Batch 125/537: Loss=0.7846 (C:0.7846, R:0.0105)
Batch 150/537: Loss=0.7674 (C:0.7674, R:0.0105)
Batch 175/537: Loss=0.8105 (C:0.8105, R:0.0105)
Batch 200/537: Loss=0.8338 (C:0.8338, R:0.0105)
Batch 225/537: Loss=0.7862 (C:0.7862, R:0.0105)
Batch 250/537: Loss=0.8190 (C:0.8190, R:0.0105)
Batch 275/537: Loss=0.7946 (C:0.7946, R:0.0105)
Batch 300/537: Loss=0.7995 (C:0.7995, R:0.0105)
Batch 325/537: Loss=0.8096 (C:0.8096, R:0.0105)
Batch 350/537: Loss=0.8019 (C:0.8019, R:0.0105)
Batch 375/537: Loss=0.7952 (C:0.7952, R:0.0105)
Batch 400/537: Loss=0.7995 (C:0.7995, R:0.0105)
Batch 425/537: Loss=0.7856 (C:0.7856, R:0.0105)
Batch 450/537: Loss=0.7457 (C:0.7457, R:0.0105)
Batch 475/537: Loss=0.8042 (C:0.8042, R:0.0105)
Batch 500/537: Loss=0.7805 (C:0.7805, R:0.0105)
Batch 525/537: Loss=0.7945 (C:0.7945, R:0.0105)

============================================================
Epoch 26/300 completed in 21.2s
Train: Loss=0.7970 (C:0.7970, R:0.0105) Ratio=4.46x
Val:   Loss=0.9021 (C:0.9021, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.7800 (C:0.7800, R:0.0105)
Batch  25/537: Loss=0.7729 (C:0.7729, R:0.0105)
Batch  50/537: Loss=0.7761 (C:0.7761, R:0.0105)
Batch  75/537: Loss=0.7684 (C:0.7684, R:0.0105)
Batch 100/537: Loss=0.7891 (C:0.7891, R:0.0105)
Batch 125/537: Loss=0.7993 (C:0.7993, R:0.0105)
Batch 150/537: Loss=0.8074 (C:0.8074, R:0.0105)
Batch 175/537: Loss=0.7710 (C:0.7710, R:0.0105)
Batch 200/537: Loss=0.7859 (C:0.7859, R:0.0105)
Batch 225/537: Loss=0.8162 (C:0.8162, R:0.0105)
Batch 250/537: Loss=0.7857 (C:0.7857, R:0.0105)
Batch 275/537: Loss=0.7972 (C:0.7972, R:0.0105)
Batch 300/537: Loss=0.7837 (C:0.7837, R:0.0105)
Batch 325/537: Loss=0.7875 (C:0.7875, R:0.0105)
Batch 350/537: Loss=0.8126 (C:0.8126, R:0.0105)
Batch 375/537: Loss=0.7924 (C:0.7924, R:0.0105)
Batch 400/537: Loss=0.7590 (C:0.7590, R:0.0105)
Batch 425/537: Loss=0.7891 (C:0.7891, R:0.0105)
Batch 450/537: Loss=0.8054 (C:0.8054, R:0.0105)
Batch 475/537: Loss=0.8055 (C:0.8055, R:0.0105)
Batch 500/537: Loss=0.8372 (C:0.8372, R:0.0105)
Batch 525/537: Loss=0.7629 (C:0.7629, R:0.0105)

============================================================
Epoch 27/300 completed in 21.5s
Train: Loss=0.7937 (C:0.7937, R:0.0105) Ratio=4.45x
Val:   Loss=0.9064 (C:0.9064, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.315 ± 0.516
    Neg distances: 1.991 ± 0.917
    Separation ratio: 6.32x
    Gap: -3.382
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.7560 (C:0.7560, R:0.0105)
Batch  25/537: Loss=0.7754 (C:0.7754, R:0.0105)
Batch  50/537: Loss=0.7653 (C:0.7653, R:0.0105)
Batch  75/537: Loss=0.7525 (C:0.7525, R:0.0105)
Batch 100/537: Loss=0.7720 (C:0.7720, R:0.0105)
Batch 125/537: Loss=0.7713 (C:0.7713, R:0.0106)
Batch 150/537: Loss=0.7783 (C:0.7783, R:0.0105)
Batch 175/537: Loss=0.7518 (C:0.7518, R:0.0105)
Batch 200/537: Loss=0.7806 (C:0.7806, R:0.0105)
Batch 225/537: Loss=0.7406 (C:0.7406, R:0.0105)
Batch 250/537: Loss=0.8025 (C:0.8025, R:0.0105)
Batch 275/537: Loss=0.7595 (C:0.7595, R:0.0105)
Batch 300/537: Loss=0.7576 (C:0.7576, R:0.0105)
Batch 325/537: Loss=0.7646 (C:0.7646, R:0.0105)
Batch 350/537: Loss=0.7735 (C:0.7735, R:0.0105)
Batch 375/537: Loss=0.7533 (C:0.7533, R:0.0105)
Batch 400/537: Loss=0.7501 (C:0.7501, R:0.0105)
Batch 425/537: Loss=0.7781 (C:0.7781, R:0.0105)
Batch 450/537: Loss=0.7890 (C:0.7890, R:0.0105)
Batch 475/537: Loss=0.7606 (C:0.7606, R:0.0105)
Batch 500/537: Loss=0.7513 (C:0.7513, R:0.0105)
Batch 525/537: Loss=0.7949 (C:0.7949, R:0.0105)

============================================================
Epoch 28/300 completed in 27.0s
Train: Loss=0.7716 (C:0.7716, R:0.0105) Ratio=4.46x
Val:   Loss=0.8843 (C:0.8843, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8843)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.7794 (C:0.7794, R:0.0105)
Batch  25/537: Loss=0.7622 (C:0.7622, R:0.0105)
Batch  50/537: Loss=0.8197 (C:0.8197, R:0.0105)
Batch  75/537: Loss=0.8122 (C:0.8122, R:0.0105)
Batch 100/537: Loss=0.7725 (C:0.7725, R:0.0105)
Batch 125/537: Loss=0.7382 (C:0.7382, R:0.0105)
Batch 150/537: Loss=0.7782 (C:0.7782, R:0.0106)
Batch 175/537: Loss=0.7430 (C:0.7430, R:0.0105)
Batch 200/537: Loss=0.7423 (C:0.7423, R:0.0105)
Batch 225/537: Loss=0.7820 (C:0.7820, R:0.0105)
Batch 250/537: Loss=0.7614 (C:0.7614, R:0.0105)
Batch 275/537: Loss=0.7514 (C:0.7514, R:0.0105)
Batch 300/537: Loss=0.7761 (C:0.7761, R:0.0105)
Batch 325/537: Loss=0.7258 (C:0.7258, R:0.0105)
Batch 350/537: Loss=0.7649 (C:0.7649, R:0.0105)
Batch 375/537: Loss=0.7897 (C:0.7897, R:0.0105)
Batch 400/537: Loss=0.7809 (C:0.7809, R:0.0105)
Batch 425/537: Loss=0.7510 (C:0.7510, R:0.0105)
Batch 450/537: Loss=0.7692 (C:0.7692, R:0.0105)
Batch 475/537: Loss=0.7923 (C:0.7923, R:0.0105)
Batch 500/537: Loss=0.7755 (C:0.7755, R:0.0105)
Batch 525/537: Loss=0.7970 (C:0.7970, R:0.0105)

============================================================
Epoch 29/300 completed in 21.2s
Train: Loss=0.7700 (C:0.7700, R:0.0105) Ratio=4.37x
Val:   Loss=0.8774 (C:0.8774, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8774)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7112 (C:0.7112, R:0.0105)
Batch  25/537: Loss=0.7538 (C:0.7538, R:0.0105)
Batch  50/537: Loss=0.7516 (C:0.7516, R:0.0105)
Batch  75/537: Loss=0.7730 (C:0.7730, R:0.0105)
Batch 100/537: Loss=0.7837 (C:0.7837, R:0.0105)
Batch 125/537: Loss=0.7738 (C:0.7738, R:0.0105)
Batch 150/537: Loss=0.7798 (C:0.7798, R:0.0105)
Batch 175/537: Loss=0.7801 (C:0.7801, R:0.0105)
Batch 200/537: Loss=0.8008 (C:0.8008, R:0.0105)
Batch 225/537: Loss=0.7564 (C:0.7564, R:0.0105)
Batch 250/537: Loss=0.7566 (C:0.7566, R:0.0105)
Batch 275/537: Loss=0.7648 (C:0.7648, R:0.0106)
Batch 300/537: Loss=0.7864 (C:0.7864, R:0.0106)
Batch 325/537: Loss=0.7690 (C:0.7690, R:0.0105)
Batch 350/537: Loss=0.7874 (C:0.7874, R:0.0105)
Batch 375/537: Loss=0.7707 (C:0.7707, R:0.0105)
Batch 400/537: Loss=0.7802 (C:0.7802, R:0.0105)
Batch 425/537: Loss=0.7493 (C:0.7493, R:0.0105)
Batch 450/537: Loss=0.7462 (C:0.7462, R:0.0105)
Batch 475/537: Loss=0.7893 (C:0.7893, R:0.0105)
Batch 500/537: Loss=0.7696 (C:0.7696, R:0.0105)
Batch 525/537: Loss=0.7635 (C:0.7635, R:0.0105)

============================================================
Epoch 30/300 completed in 21.4s
Train: Loss=0.7676 (C:0.7676, R:0.0105) Ratio=4.55x
Val:   Loss=0.8814 (C:0.8814, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.324 ± 0.533
    Neg distances: 2.087 ± 0.944
    Separation ratio: 6.45x
    Gap: -3.525
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.7379 (C:0.7379, R:0.0105)
Batch  25/537: Loss=0.7456 (C:0.7456, R:0.0105)
Batch  50/537: Loss=0.7482 (C:0.7482, R:0.0105)
Batch  75/537: Loss=0.7528 (C:0.7528, R:0.0105)
Batch 100/537: Loss=0.7163 (C:0.7163, R:0.0105)
Batch 125/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 150/537: Loss=0.7319 (C:0.7319, R:0.0105)
Batch 175/537: Loss=0.7282 (C:0.7282, R:0.0105)
Batch 200/537: Loss=0.7886 (C:0.7886, R:0.0105)
Batch 225/537: Loss=0.7528 (C:0.7528, R:0.0105)
Batch 250/537: Loss=0.7481 (C:0.7481, R:0.0105)
Batch 275/537: Loss=0.7545 (C:0.7545, R:0.0105)
Batch 300/537: Loss=0.7185 (C:0.7185, R:0.0105)
Batch 325/537: Loss=0.7779 (C:0.7779, R:0.0105)
Batch 350/537: Loss=0.7542 (C:0.7542, R:0.0105)
Batch 375/537: Loss=0.7378 (C:0.7378, R:0.0105)
Batch 400/537: Loss=0.7469 (C:0.7469, R:0.0105)
Batch 425/537: Loss=0.7662 (C:0.7662, R:0.0105)
Batch 450/537: Loss=0.7449 (C:0.7449, R:0.0105)
Batch 475/537: Loss=0.7803 (C:0.7803, R:0.0105)
Batch 500/537: Loss=0.7504 (C:0.7504, R:0.0105)
Batch 525/537: Loss=0.7398 (C:0.7398, R:0.0105)

============================================================
Epoch 31/300 completed in 27.0s
Train: Loss=0.7450 (C:0.7450, R:0.0105) Ratio=4.46x
Val:   Loss=0.8575 (C:0.8575, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8575)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.7354 (C:0.7354, R:0.0105)
Batch  25/537: Loss=0.6989 (C:0.6989, R:0.0105)
Batch  50/537: Loss=0.7715 (C:0.7715, R:0.0105)
Batch  75/537: Loss=0.7092 (C:0.7092, R:0.0106)
Batch 100/537: Loss=0.7469 (C:0.7469, R:0.0105)
Batch 125/537: Loss=0.7495 (C:0.7495, R:0.0105)
Batch 150/537: Loss=0.7479 (C:0.7479, R:0.0105)
Batch 175/537: Loss=0.7624 (C:0.7624, R:0.0105)
Batch 200/537: Loss=0.7146 (C:0.7146, R:0.0105)
Batch 225/537: Loss=0.7547 (C:0.7547, R:0.0105)
Batch 250/537: Loss=0.7490 (C:0.7490, R:0.0105)
Batch 275/537: Loss=0.7496 (C:0.7496, R:0.0105)
Batch 300/537: Loss=0.7599 (C:0.7599, R:0.0105)
Batch 325/537: Loss=0.7564 (C:0.7564, R:0.0105)
Batch 350/537: Loss=0.7268 (C:0.7268, R:0.0105)
Batch 375/537: Loss=0.7491 (C:0.7491, R:0.0105)
Batch 400/537: Loss=0.7197 (C:0.7197, R:0.0105)
Batch 425/537: Loss=0.7323 (C:0.7323, R:0.0105)
Batch 450/537: Loss=0.7477 (C:0.7477, R:0.0105)
Batch 475/537: Loss=0.7180 (C:0.7180, R:0.0105)
Batch 500/537: Loss=0.7401 (C:0.7401, R:0.0105)
Batch 525/537: Loss=0.7235 (C:0.7235, R:0.0105)

============================================================
Epoch 32/300 completed in 21.3s
Train: Loss=0.7425 (C:0.7425, R:0.0105) Ratio=4.53x
Val:   Loss=0.8602 (C:0.8602, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.7489 (C:0.7489, R:0.0105)
Batch  25/537: Loss=0.7837 (C:0.7837, R:0.0106)
Batch  50/537: Loss=0.6933 (C:0.6933, R:0.0105)
Batch  75/537: Loss=0.7525 (C:0.7525, R:0.0105)
Batch 100/537: Loss=0.7713 (C:0.7713, R:0.0106)
Batch 125/537: Loss=0.7226 (C:0.7226, R:0.0106)
Batch 150/537: Loss=0.7632 (C:0.7632, R:0.0105)
Batch 175/537: Loss=0.7471 (C:0.7471, R:0.0105)
Batch 200/537: Loss=0.7341 (C:0.7341, R:0.0105)
Batch 225/537: Loss=0.7334 (C:0.7334, R:0.0105)
Batch 250/537: Loss=0.7254 (C:0.7254, R:0.0105)
Batch 275/537: Loss=0.6996 (C:0.6996, R:0.0106)
Batch 300/537: Loss=0.7223 (C:0.7223, R:0.0105)
Batch 325/537: Loss=0.7499 (C:0.7499, R:0.0106)
Batch 350/537: Loss=0.7462 (C:0.7462, R:0.0106)
Batch 375/537: Loss=0.7327 (C:0.7327, R:0.0105)
Batch 400/537: Loss=0.7724 (C:0.7724, R:0.0105)
Batch 425/537: Loss=0.7099 (C:0.7099, R:0.0105)
Batch 450/537: Loss=0.7404 (C:0.7404, R:0.0105)
Batch 475/537: Loss=0.7357 (C:0.7357, R:0.0105)
Batch 500/537: Loss=0.7391 (C:0.7391, R:0.0105)
Batch 525/537: Loss=0.7495 (C:0.7495, R:0.0105)

============================================================
Epoch 33/300 completed in 21.1s
Train: Loss=0.7407 (C:0.7407, R:0.0105) Ratio=4.59x
Val:   Loss=0.8573 (C:0.8573, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.8573)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.335 ± 0.530
    Neg distances: 2.134 ± 0.957
    Separation ratio: 6.37x
    Gap: -3.663
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.6757 (C:0.6757, R:0.0106)
Batch  25/537: Loss=0.7149 (C:0.7149, R:0.0105)
Batch  50/537: Loss=0.7101 (C:0.7101, R:0.0106)
Batch  75/537: Loss=0.7049 (C:0.7049, R:0.0105)
Batch 100/537: Loss=0.7367 (C:0.7367, R:0.0105)
Batch 125/537: Loss=0.7146 (C:0.7146, R:0.0105)
Batch 150/537: Loss=0.7364 (C:0.7364, R:0.0105)
Batch 175/537: Loss=0.7338 (C:0.7338, R:0.0105)
Batch 200/537: Loss=0.7228 (C:0.7228, R:0.0105)
Batch 225/537: Loss=0.7440 (C:0.7440, R:0.0105)
Batch 250/537: Loss=0.7140 (C:0.7140, R:0.0105)
Batch 275/537: Loss=0.7401 (C:0.7401, R:0.0105)
Batch 300/537: Loss=0.7063 (C:0.7063, R:0.0105)
Batch 325/537: Loss=0.7398 (C:0.7398, R:0.0105)
Batch 350/537: Loss=0.6942 (C:0.6942, R:0.0105)
Batch 375/537: Loss=0.7186 (C:0.7186, R:0.0105)
Batch 400/537: Loss=0.7413 (C:0.7413, R:0.0105)
Batch 425/537: Loss=0.7488 (C:0.7488, R:0.0105)
Batch 450/537: Loss=0.7236 (C:0.7236, R:0.0105)
Batch 475/537: Loss=0.6971 (C:0.6971, R:0.0105)
Batch 500/537: Loss=0.7424 (C:0.7424, R:0.0105)
Batch 525/537: Loss=0.7168 (C:0.7168, R:0.0105)

============================================================
Epoch 34/300 completed in 27.0s
Train: Loss=0.7264 (C:0.7264, R:0.0105) Ratio=4.55x
Val:   Loss=0.8436 (C:0.8436, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8436)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.7213 (C:0.7213, R:0.0105)
Batch  25/537: Loss=0.6746 (C:0.6746, R:0.0105)
Batch  50/537: Loss=0.7018 (C:0.7018, R:0.0106)
Batch  75/537: Loss=0.7202 (C:0.7202, R:0.0106)
Batch 100/537: Loss=0.7204 (C:0.7204, R:0.0105)
Batch 125/537: Loss=0.7486 (C:0.7486, R:0.0105)
Batch 150/537: Loss=0.7239 (C:0.7239, R:0.0105)
Batch 175/537: Loss=0.7175 (C:0.7175, R:0.0105)
Batch 200/537: Loss=0.7632 (C:0.7632, R:0.0105)
Batch 225/537: Loss=0.7343 (C:0.7343, R:0.0105)
Batch 250/537: Loss=0.7132 (C:0.7132, R:0.0106)
Batch 275/537: Loss=0.7162 (C:0.7162, R:0.0105)
Batch 300/537: Loss=0.6894 (C:0.6894, R:0.0105)
Batch 325/537: Loss=0.7385 (C:0.7385, R:0.0105)
Batch 350/537: Loss=0.7634 (C:0.7634, R:0.0105)
Batch 375/537: Loss=0.7002 (C:0.7002, R:0.0105)
Batch 400/537: Loss=0.7283 (C:0.7283, R:0.0105)
Batch 425/537: Loss=0.7140 (C:0.7140, R:0.0105)
Batch 450/537: Loss=0.7282 (C:0.7282, R:0.0105)
Batch 475/537: Loss=0.7126 (C:0.7126, R:0.0105)
Batch 500/537: Loss=0.7226 (C:0.7226, R:0.0105)
Batch 525/537: Loss=0.7390 (C:0.7390, R:0.0105)

============================================================
Epoch 35/300 completed in 21.5s
Train: Loss=0.7250 (C:0.7250, R:0.0105) Ratio=4.65x
Val:   Loss=0.8485 (C:0.8485, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.075
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.7044 (C:0.7044, R:0.0105)
Batch  25/537: Loss=0.7195 (C:0.7195, R:0.0105)
Batch  50/537: Loss=0.7293 (C:0.7293, R:0.0105)
Batch  75/537: Loss=0.6878 (C:0.6878, R:0.0105)
Batch 100/537: Loss=0.7395 (C:0.7395, R:0.0105)
Batch 125/537: Loss=0.7167 (C:0.7167, R:0.0106)
Batch 150/537: Loss=0.7308 (C:0.7308, R:0.0105)
Batch 175/537: Loss=0.7190 (C:0.7190, R:0.0105)
Batch 200/537: Loss=0.7287 (C:0.7287, R:0.0105)
Batch 225/537: Loss=0.7132 (C:0.7132, R:0.0105)
Batch 250/537: Loss=0.7330 (C:0.7330, R:0.0105)
Batch 275/537: Loss=0.7241 (C:0.7241, R:0.0105)
Batch 300/537: Loss=0.6880 (C:0.6880, R:0.0105)
Batch 325/537: Loss=0.7045 (C:0.7045, R:0.0105)
Batch 350/537: Loss=0.7234 (C:0.7234, R:0.0105)
Batch 375/537: Loss=0.7263 (C:0.7263, R:0.0105)
Batch 400/537: Loss=0.7425 (C:0.7425, R:0.0105)
Batch 425/537: Loss=0.7153 (C:0.7153, R:0.0105)
Batch 450/537: Loss=0.7364 (C:0.7364, R:0.0105)
Batch 475/537: Loss=0.6885 (C:0.6885, R:0.0105)
Batch 500/537: Loss=0.7332 (C:0.7332, R:0.0106)
Batch 525/537: Loss=0.7477 (C:0.7477, R:0.0105)

============================================================
Epoch 36/300 completed in 21.3s
Train: Loss=0.7209 (C:0.7209, R:0.0105) Ratio=4.68x
Val:   Loss=0.8486 (C:0.8486, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.090
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.334 ± 0.570
    Neg distances: 2.240 ± 0.986
    Separation ratio: 6.71x
    Gap: -3.732
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.6784 (C:0.6784, R:0.0105)
Batch  25/537: Loss=0.6624 (C:0.6624, R:0.0105)
Batch  50/537: Loss=0.6791 (C:0.6791, R:0.0105)
Batch  75/537: Loss=0.6928 (C:0.6928, R:0.0105)
Batch 100/537: Loss=0.7072 (C:0.7072, R:0.0105)
Batch 125/537: Loss=0.7148 (C:0.7148, R:0.0105)
Batch 150/537: Loss=0.6767 (C:0.6767, R:0.0105)
Batch 175/537: Loss=0.7290 (C:0.7290, R:0.0105)
Batch 200/537: Loss=0.7051 (C:0.7051, R:0.0105)
Batch 225/537: Loss=0.7133 (C:0.7133, R:0.0105)
Batch 250/537: Loss=0.6933 (C:0.6933, R:0.0105)
Batch 275/537: Loss=0.7008 (C:0.7008, R:0.0105)
Batch 300/537: Loss=0.6766 (C:0.6766, R:0.0105)
Batch 325/537: Loss=0.6821 (C:0.6821, R:0.0106)
Batch 350/537: Loss=0.6859 (C:0.6859, R:0.0105)
Batch 375/537: Loss=0.7254 (C:0.7254, R:0.0105)
Batch 400/537: Loss=0.6862 (C:0.6862, R:0.0105)
Batch 425/537: Loss=0.6901 (C:0.6901, R:0.0105)
Batch 450/537: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 475/537: Loss=0.6779 (C:0.6779, R:0.0105)
Batch 500/537: Loss=0.6999 (C:0.6999, R:0.0105)
Batch 525/537: Loss=0.6916 (C:0.6916, R:0.0105)

============================================================
Epoch 37/300 completed in 27.0s
Train: Loss=0.6930 (C:0.6930, R:0.0105) Ratio=4.64x
Val:   Loss=0.8078 (C:0.8078, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8078)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.7216 (C:0.7216, R:0.0105)
Batch  25/537: Loss=0.6849 (C:0.6849, R:0.0105)
Batch  50/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch  75/537: Loss=0.6852 (C:0.6852, R:0.0105)
Batch 100/537: Loss=0.6682 (C:0.6682, R:0.0105)
Batch 125/537: Loss=0.6769 (C:0.6769, R:0.0105)
Batch 150/537: Loss=0.6834 (C:0.6834, R:0.0105)
Batch 175/537: Loss=0.6925 (C:0.6925, R:0.0105)
Batch 200/537: Loss=0.7044 (C:0.7044, R:0.0105)
Batch 225/537: Loss=0.6939 (C:0.6939, R:0.0106)
Batch 250/537: Loss=0.6969 (C:0.6969, R:0.0105)
Batch 275/537: Loss=0.6860 (C:0.6860, R:0.0105)
Batch 300/537: Loss=0.6989 (C:0.6989, R:0.0105)
Batch 325/537: Loss=0.7015 (C:0.7015, R:0.0105)
Batch 350/537: Loss=0.6356 (C:0.6356, R:0.0105)
Batch 375/537: Loss=0.6661 (C:0.6661, R:0.0105)
Batch 400/537: Loss=0.7180 (C:0.7180, R:0.0105)
Batch 425/537: Loss=0.7025 (C:0.7025, R:0.0105)
Batch 450/537: Loss=0.7085 (C:0.7085, R:0.0105)
Batch 475/537: Loss=0.7250 (C:0.7250, R:0.0105)
Batch 500/537: Loss=0.6867 (C:0.6867, R:0.0105)
Batch 525/537: Loss=0.6686 (C:0.6686, R:0.0105)

============================================================
Epoch 38/300 completed in 21.2s
Train: Loss=0.6910 (C:0.6910, R:0.0105) Ratio=4.76x
Val:   Loss=0.8131 (C:0.8131, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.7101 (C:0.7101, R:0.0105)
Batch  25/537: Loss=0.6491 (C:0.6491, R:0.0105)
Batch  50/537: Loss=0.7179 (C:0.7179, R:0.0105)
Batch  75/537: Loss=0.6733 (C:0.6733, R:0.0105)
Batch 100/537: Loss=0.6916 (C:0.6916, R:0.0105)
Batch 125/537: Loss=0.7027 (C:0.7027, R:0.0105)
Batch 150/537: Loss=0.6994 (C:0.6994, R:0.0105)
Batch 175/537: Loss=0.6802 (C:0.6802, R:0.0105)
Batch 200/537: Loss=0.6567 (C:0.6567, R:0.0105)
Batch 225/537: Loss=0.7035 (C:0.7035, R:0.0105)
Batch 250/537: Loss=0.6782 (C:0.6782, R:0.0105)
Batch 275/537: Loss=0.7029 (C:0.7029, R:0.0106)
Batch 300/537: Loss=0.7199 (C:0.7199, R:0.0105)
Batch 325/537: Loss=0.6742 (C:0.6742, R:0.0105)
Batch 350/537: Loss=0.6943 (C:0.6943, R:0.0105)
Batch 375/537: Loss=0.7074 (C:0.7074, R:0.0105)
Batch 400/537: Loss=0.7116 (C:0.7116, R:0.0105)
Batch 425/537: Loss=0.6641 (C:0.6641, R:0.0105)
Batch 450/537: Loss=0.7203 (C:0.7203, R:0.0105)
Batch 475/537: Loss=0.6836 (C:0.6836, R:0.0105)
Batch 500/537: Loss=0.7159 (C:0.7159, R:0.0105)
Batch 525/537: Loss=0.6825 (C:0.6825, R:0.0105)

============================================================
Epoch 39/300 completed in 21.3s
Train: Loss=0.6879 (C:0.6879, R:0.0105) Ratio=4.69x
Val:   Loss=0.8135 (C:0.8135, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.323 ± 0.549
    Neg distances: 2.284 ± 0.996
    Separation ratio: 7.07x
    Gap: -3.894
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.6663 (C:0.6663, R:0.0105)
Batch  25/537: Loss=0.6857 (C:0.6857, R:0.0105)
Batch  50/537: Loss=0.6621 (C:0.6621, R:0.0105)
Batch  75/537: Loss=0.6644 (C:0.6644, R:0.0105)
Batch 100/537: Loss=0.6846 (C:0.6846, R:0.0105)
Batch 125/537: Loss=0.7055 (C:0.7055, R:0.0106)
Batch 150/537: Loss=0.6579 (C:0.6579, R:0.0105)
Batch 175/537: Loss=0.6760 (C:0.6760, R:0.0105)
Batch 200/537: Loss=0.6824 (C:0.6824, R:0.0105)
Batch 225/537: Loss=0.6755 (C:0.6755, R:0.0105)
Batch 250/537: Loss=0.6635 (C:0.6635, R:0.0105)
Batch 275/537: Loss=0.6837 (C:0.6837, R:0.0105)
Batch 300/537: Loss=0.6638 (C:0.6638, R:0.0105)
Batch 325/537: Loss=0.6980 (C:0.6980, R:0.0105)
Batch 350/537: Loss=0.6615 (C:0.6615, R:0.0105)
Batch 375/537: Loss=0.6192 (C:0.6192, R:0.0105)
Batch 400/537: Loss=0.6799 (C:0.6799, R:0.0105)
Batch 425/537: Loss=0.6798 (C:0.6798, R:0.0105)
Batch 450/537: Loss=0.6722 (C:0.6722, R:0.0105)
Batch 475/537: Loss=0.6681 (C:0.6681, R:0.0105)
Batch 500/537: Loss=0.6525 (C:0.6525, R:0.0105)
Batch 525/537: Loss=0.7061 (C:0.7061, R:0.0105)

============================================================
Epoch 40/300 completed in 26.9s
Train: Loss=0.6673 (C:0.6673, R:0.0105) Ratio=4.72x
Val:   Loss=0.8014 (C:0.8014, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.8014)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6615 (C:0.6615, R:0.0105)
Batch  25/537: Loss=0.6756 (C:0.6756, R:0.0105)
Batch  50/537: Loss=0.6473 (C:0.6473, R:0.0105)
Batch  75/537: Loss=0.7042 (C:0.7042, R:0.0105)
Batch 100/537: Loss=0.6811 (C:0.6811, R:0.0105)
Batch 125/537: Loss=0.6573 (C:0.6573, R:0.0105)
Batch 150/537: Loss=0.6877 (C:0.6877, R:0.0105)
Batch 175/537: Loss=0.6928 (C:0.6928, R:0.0105)
Batch 200/537: Loss=0.6690 (C:0.6690, R:0.0105)
Batch 225/537: Loss=0.6748 (C:0.6748, R:0.0105)
Batch 250/537: Loss=0.6723 (C:0.6723, R:0.0105)
Batch 275/537: Loss=0.6703 (C:0.6703, R:0.0105)
Batch 300/537: Loss=0.6526 (C:0.6526, R:0.0105)
Batch 325/537: Loss=0.6346 (C:0.6346, R:0.0105)
Batch 350/537: Loss=0.6508 (C:0.6508, R:0.0105)
Batch 375/537: Loss=0.6859 (C:0.6859, R:0.0105)
Batch 400/537: Loss=0.6672 (C:0.6672, R:0.0105)
Batch 425/537: Loss=0.6846 (C:0.6846, R:0.0105)
Batch 450/537: Loss=0.6620 (C:0.6620, R:0.0105)
Batch 475/537: Loss=0.6846 (C:0.6846, R:0.0105)
Batch 500/537: Loss=0.6674 (C:0.6674, R:0.0105)
Batch 525/537: Loss=0.6907 (C:0.6907, R:0.0105)

============================================================
Epoch 41/300 completed in 21.2s
Train: Loss=0.6647 (C:0.6647, R:0.0105) Ratio=4.79x
Val:   Loss=0.8032 (C:0.8032, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.165
No improvement for 1 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.6653 (C:0.6653, R:0.0105)
Batch  25/537: Loss=0.6306 (C:0.6306, R:0.0105)
Batch  50/537: Loss=0.6611 (C:0.6611, R:0.0105)
Batch  75/537: Loss=0.6600 (C:0.6600, R:0.0105)
Batch 100/537: Loss=0.6524 (C:0.6524, R:0.0105)
Batch 125/537: Loss=0.6543 (C:0.6543, R:0.0105)
Batch 150/537: Loss=0.6319 (C:0.6319, R:0.0105)
Batch 175/537: Loss=0.6602 (C:0.6602, R:0.0105)
Batch 200/537: Loss=0.6803 (C:0.6803, R:0.0105)
Batch 225/537: Loss=0.6365 (C:0.6365, R:0.0106)
Batch 250/537: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 275/537: Loss=0.6584 (C:0.6584, R:0.0105)
Batch 300/537: Loss=0.6665 (C:0.6665, R:0.0105)
Batch 325/537: Loss=0.6208 (C:0.6208, R:0.0105)
Batch 350/537: Loss=0.6818 (C:0.6818, R:0.0105)
Batch 375/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 400/537: Loss=0.6532 (C:0.6532, R:0.0105)
Batch 425/537: Loss=0.6849 (C:0.6849, R:0.0105)
Batch 450/537: Loss=0.6522 (C:0.6522, R:0.0105)
Batch 475/537: Loss=0.6805 (C:0.6805, R:0.0105)
Batch 500/537: Loss=0.6961 (C:0.6961, R:0.0105)
Batch 525/537: Loss=0.6674 (C:0.6674, R:0.0105)

============================================================
Epoch 42/300 completed in 21.1s
Train: Loss=0.6629 (C:0.6629, R:0.0105) Ratio=4.93x
Val:   Loss=0.8028 (C:0.8028, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.180
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.336 ± 0.581
    Neg distances: 2.319 ± 1.008
    Separation ratio: 6.91x
    Gap: -3.934
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.6617 (C:0.6617, R:0.0105)
Batch  25/537: Loss=0.6473 (C:0.6473, R:0.0105)
Batch  50/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch  75/537: Loss=0.6599 (C:0.6599, R:0.0105)
Batch 100/537: Loss=0.6126 (C:0.6126, R:0.0106)
Batch 125/537: Loss=0.6350 (C:0.6350, R:0.0105)
Batch 150/537: Loss=0.6512 (C:0.6512, R:0.0105)
Batch 175/537: Loss=0.6489 (C:0.6489, R:0.0105)
Batch 200/537: Loss=0.6661 (C:0.6661, R:0.0105)
Batch 225/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch 250/537: Loss=0.6924 (C:0.6924, R:0.0105)
Batch 275/537: Loss=0.6551 (C:0.6551, R:0.0105)
Batch 300/537: Loss=0.6537 (C:0.6537, R:0.0105)
Batch 325/537: Loss=0.6727 (C:0.6727, R:0.0105)
Batch 350/537: Loss=0.6874 (C:0.6874, R:0.0105)
Batch 375/537: Loss=0.6540 (C:0.6540, R:0.0105)
Batch 400/537: Loss=0.6593 (C:0.6593, R:0.0105)
Batch 425/537: Loss=0.6532 (C:0.6532, R:0.0105)
Batch 450/537: Loss=0.6467 (C:0.6467, R:0.0105)
Batch 475/537: Loss=0.6729 (C:0.6729, R:0.0105)
Batch 500/537: Loss=0.7050 (C:0.7050, R:0.0105)
Batch 525/537: Loss=0.6377 (C:0.6377, R:0.0105)

============================================================
Epoch 43/300 completed in 28.3s
Train: Loss=0.6578 (C:0.6578, R:0.0105) Ratio=4.82x
Val:   Loss=0.7914 (C:0.7914, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.7914)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.6535 (C:0.6535, R:0.0105)
Batch  25/537: Loss=0.6227 (C:0.6227, R:0.0105)
Batch  50/537: Loss=0.6823 (C:0.6823, R:0.0105)
Batch  75/537: Loss=0.6211 (C:0.6211, R:0.0105)
Batch 100/537: Loss=0.6535 (C:0.6535, R:0.0105)
Batch 125/537: Loss=0.6434 (C:0.6434, R:0.0105)
Batch 150/537: Loss=0.6676 (C:0.6676, R:0.0105)
Batch 175/537: Loss=0.6564 (C:0.6564, R:0.0106)
Batch 200/537: Loss=0.6753 (C:0.6753, R:0.0105)
Batch 225/537: Loss=0.6109 (C:0.6109, R:0.0105)
Batch 250/537: Loss=0.6156 (C:0.6156, R:0.0105)
Batch 275/537: Loss=0.6729 (C:0.6729, R:0.0105)
Batch 300/537: Loss=0.6643 (C:0.6643, R:0.0105)
Batch 325/537: Loss=0.6680 (C:0.6680, R:0.0105)
Batch 350/537: Loss=0.6874 (C:0.6874, R:0.0105)
Batch 375/537: Loss=0.6475 (C:0.6475, R:0.0105)
Batch 400/537: Loss=0.6662 (C:0.6662, R:0.0106)
Batch 425/537: Loss=0.6887 (C:0.6887, R:0.0105)
Batch 450/537: Loss=0.6686 (C:0.6686, R:0.0105)
Batch 475/537: Loss=0.6764 (C:0.6764, R:0.0105)
Batch 500/537: Loss=0.6307 (C:0.6307, R:0.0106)
Batch 525/537: Loss=0.6224 (C:0.6224, R:0.0106)

============================================================
Epoch 44/300 completed in 21.9s
Train: Loss=0.6567 (C:0.6567, R:0.0105) Ratio=4.83x
Val:   Loss=0.7900 (C:0.7900, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.210
✅ New best model saved (Val Loss: 0.7900)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.6248 (C:0.6248, R:0.0105)
Batch  25/537: Loss=0.6448 (C:0.6448, R:0.0105)
Batch  50/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch  75/537: Loss=0.6776 (C:0.6776, R:0.0105)
Batch 100/537: Loss=0.6438 (C:0.6438, R:0.0105)
Batch 125/537: Loss=0.6623 (C:0.6623, R:0.0105)
Batch 150/537: Loss=0.6486 (C:0.6486, R:0.0105)
Batch 175/537: Loss=0.6359 (C:0.6359, R:0.0105)
Batch 200/537: Loss=0.6637 (C:0.6637, R:0.0105)
Batch 225/537: Loss=0.6310 (C:0.6310, R:0.0105)
Batch 250/537: Loss=0.7062 (C:0.7062, R:0.0105)
Batch 275/537: Loss=0.6366 (C:0.6366, R:0.0105)
Batch 300/537: Loss=0.6224 (C:0.6224, R:0.0105)
Batch 325/537: Loss=0.6371 (C:0.6371, R:0.0105)
Batch 350/537: Loss=0.6275 (C:0.6275, R:0.0105)
Batch 375/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 400/537: Loss=0.6817 (C:0.6817, R:0.0105)
Batch 425/537: Loss=0.6478 (C:0.6478, R:0.0105)
Batch 450/537: Loss=0.6702 (C:0.6702, R:0.0105)
Batch 475/537: Loss=0.6869 (C:0.6869, R:0.0105)
Batch 500/537: Loss=0.6508 (C:0.6508, R:0.0105)
Batch 525/537: Loss=0.6595 (C:0.6595, R:0.0105)

============================================================
Epoch 45/300 completed in 21.2s
Train: Loss=0.6545 (C:0.6545, R:0.0105) Ratio=4.91x
Val:   Loss=0.7857 (C:0.7857, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.225
✅ New best model saved (Val Loss: 0.7857)
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.313 ± 0.555
    Neg distances: 2.361 ± 1.006
    Separation ratio: 7.54x
    Gap: -4.154
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.6343 (C:0.6343, R:0.0106)
Batch  25/537: Loss=0.6223 (C:0.6223, R:0.0106)
Batch  50/537: Loss=0.6403 (C:0.6403, R:0.0105)
Batch  75/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch 100/537: Loss=0.6336 (C:0.6336, R:0.0105)
Batch 125/537: Loss=0.6204 (C:0.6204, R:0.0105)
Batch 150/537: Loss=0.6204 (C:0.6204, R:0.0105)
Batch 175/537: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 200/537: Loss=0.6640 (C:0.6640, R:0.0105)
Batch 225/537: Loss=0.6198 (C:0.6198, R:0.0105)
Batch 250/537: Loss=0.5879 (C:0.5879, R:0.0106)
Batch 275/537: Loss=0.6458 (C:0.6458, R:0.0106)
Batch 300/537: Loss=0.6113 (C:0.6113, R:0.0105)
Batch 325/537: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 350/537: Loss=0.6492 (C:0.6492, R:0.0105)
Batch 375/537: Loss=0.6513 (C:0.6513, R:0.0105)
Batch 400/537: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 425/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 450/537: Loss=0.6578 (C:0.6578, R:0.0105)
Batch 475/537: Loss=0.6324 (C:0.6324, R:0.0105)
Batch 500/537: Loss=0.6342 (C:0.6342, R:0.0105)
Batch 525/537: Loss=0.6786 (C:0.6786, R:0.0105)

============================================================
Epoch 46/300 completed in 27.7s
Train: Loss=0.6249 (C:0.6249, R:0.0105) Ratio=4.87x
Val:   Loss=0.7682 (C:0.7682, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.240
✅ New best model saved (Val Loss: 0.7682)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.6220 (C:0.6220, R:0.0106)
Batch  25/537: Loss=0.5896 (C:0.5896, R:0.0105)
Batch  50/537: Loss=0.6576 (C:0.6576, R:0.0105)
Batch  75/537: Loss=0.6232 (C:0.6232, R:0.0105)
Batch 100/537: Loss=0.6065 (C:0.6065, R:0.0105)
Batch 125/537: Loss=0.6267 (C:0.6267, R:0.0105)
Batch 150/537: Loss=0.6172 (C:0.6172, R:0.0105)
Batch 175/537: Loss=0.6418 (C:0.6418, R:0.0105)
Batch 200/537: Loss=0.6116 (C:0.6116, R:0.0105)
Batch 225/537: Loss=0.6490 (C:0.6490, R:0.0105)
Batch 250/537: Loss=0.6087 (C:0.6087, R:0.0105)
Batch 275/537: Loss=0.6523 (C:0.6523, R:0.0105)
Batch 300/537: Loss=0.6497 (C:0.6497, R:0.0105)
Batch 325/537: Loss=0.6044 (C:0.6044, R:0.0105)
Batch 350/537: Loss=0.6431 (C:0.6431, R:0.0105)
Batch 375/537: Loss=0.6147 (C:0.6147, R:0.0105)
Batch 400/537: Loss=0.6381 (C:0.6381, R:0.0105)
Batch 425/537: Loss=0.6285 (C:0.6285, R:0.0105)
Batch 450/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 475/537: Loss=0.6382 (C:0.6382, R:0.0105)
Batch 500/537: Loss=0.6148 (C:0.6148, R:0.0105)
Batch 525/537: Loss=0.6280 (C:0.6280, R:0.0105)

============================================================
Epoch 47/300 completed in 21.7s
Train: Loss=0.6237 (C:0.6237, R:0.0105) Ratio=4.88x
Val:   Loss=0.7666 (C:0.7666, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.255
✅ New best model saved (Val Loss: 0.7666)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.6190 (C:0.6190, R:0.0105)
Batch  25/537: Loss=0.6384 (C:0.6384, R:0.0105)
Batch  50/537: Loss=0.6392 (C:0.6392, R:0.0106)
Batch  75/537: Loss=0.6437 (C:0.6437, R:0.0105)
Batch 100/537: Loss=0.5979 (C:0.5979, R:0.0105)
Batch 125/537: Loss=0.6093 (C:0.6093, R:0.0105)
Batch 150/537: Loss=0.6260 (C:0.6260, R:0.0105)
Batch 175/537: Loss=0.5925 (C:0.5925, R:0.0105)
Batch 200/537: Loss=0.6343 (C:0.6343, R:0.0105)
Batch 225/537: Loss=0.6289 (C:0.6289, R:0.0105)
Batch 250/537: Loss=0.6418 (C:0.6418, R:0.0105)
Batch 275/537: Loss=0.6269 (C:0.6269, R:0.0105)
Batch 300/537: Loss=0.5960 (C:0.5960, R:0.0105)
Batch 325/537: Loss=0.6085 (C:0.6085, R:0.0105)
Batch 350/537: Loss=0.6116 (C:0.6116, R:0.0105)
Batch 375/537: Loss=0.6139 (C:0.6139, R:0.0105)
Batch 400/537: Loss=0.6514 (C:0.6514, R:0.0105)
Batch 425/537: Loss=0.6327 (C:0.6327, R:0.0105)
Batch 450/537: Loss=0.6420 (C:0.6420, R:0.0105)
Batch 475/537: Loss=0.6528 (C:0.6528, R:0.0105)
Batch 500/537: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 525/537: Loss=0.5864 (C:0.5864, R:0.0105)

============================================================
Epoch 48/300 completed in 21.4s
Train: Loss=0.6233 (C:0.6233, R:0.0105) Ratio=5.00x
Val:   Loss=0.7701 (C:0.7701, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.270
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.317 ± 0.558
    Neg distances: 2.397 ± 1.025
    Separation ratio: 7.57x
    Gap: -4.040
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.6193 (C:0.6193, R:0.0105)
Batch  25/537: Loss=0.6038 (C:0.6038, R:0.0105)
Batch  50/537: Loss=0.6223 (C:0.6223, R:0.0105)
Batch  75/537: Loss=0.5783 (C:0.5783, R:0.0105)
Batch 100/537: Loss=0.6063 (C:0.6063, R:0.0105)
Batch 125/537: Loss=0.6297 (C:0.6297, R:0.0105)
Batch 150/537: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 175/537: Loss=0.5931 (C:0.5931, R:0.0105)
Batch 200/537: Loss=0.6214 (C:0.6214, R:0.0105)
Batch 225/537: Loss=0.6057 (C:0.6057, R:0.0105)
Batch 250/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch 275/537: Loss=0.6285 (C:0.6285, R:0.0105)
Batch 300/537: Loss=0.6188 (C:0.6188, R:0.0105)
Batch 325/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 350/537: Loss=0.6187 (C:0.6187, R:0.0105)
Batch 375/537: Loss=0.6167 (C:0.6167, R:0.0105)
Batch 400/537: Loss=0.6205 (C:0.6205, R:0.0105)
Batch 425/537: Loss=0.6110 (C:0.6110, R:0.0105)
Batch 450/537: Loss=0.5954 (C:0.5954, R:0.0105)
Batch 475/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 500/537: Loss=0.6279 (C:0.6279, R:0.0105)
Batch 525/537: Loss=0.6301 (C:0.6301, R:0.0105)

============================================================
Epoch 49/300 completed in 27.6s
Train: Loss=0.6149 (C:0.6149, R:0.0105) Ratio=5.06x
Val:   Loss=0.7541 (C:0.7541, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.7541)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.5909 (C:0.5909, R:0.0105)
Batch  25/537: Loss=0.5916 (C:0.5916, R:0.0105)
Batch  50/537: Loss=0.6295 (C:0.6295, R:0.0105)
Batch  75/537: Loss=0.6198 (C:0.6198, R:0.0105)
Batch 100/537: Loss=0.6224 (C:0.6224, R:0.0105)
Batch 125/537: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 150/537: Loss=0.6323 (C:0.6323, R:0.0105)
Batch 175/537: Loss=0.6218 (C:0.6218, R:0.0105)
Batch 200/537: Loss=0.6131 (C:0.6131, R:0.0105)
Batch 225/537: Loss=0.6398 (C:0.6398, R:0.0105)
Batch 250/537: Loss=0.5994 (C:0.5994, R:0.0105)
Batch 275/537: Loss=0.6436 (C:0.6436, R:0.0105)
Batch 300/537: Loss=0.6401 (C:0.6401, R:0.0105)
Batch 325/537: Loss=0.5947 (C:0.5947, R:0.0105)
Batch 350/537: Loss=0.6070 (C:0.6070, R:0.0105)
Batch 375/537: Loss=0.5837 (C:0.5837, R:0.0105)
Batch 400/537: Loss=0.6209 (C:0.6209, R:0.0105)
Batch 425/537: Loss=0.6038 (C:0.6038, R:0.0105)
Batch 450/537: Loss=0.6409 (C:0.6409, R:0.0105)
Batch 475/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch 500/537: Loss=0.6500 (C:0.6500, R:0.0105)
Batch 525/537: Loss=0.6004 (C:0.6004, R:0.0105)

============================================================
Epoch 50/300 completed in 21.6s
Train: Loss=0.6128 (C:0.6128, R:0.0105) Ratio=4.96x
Val:   Loss=0.7652 (C:0.7652, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.6081 (C:0.6081, R:0.0105)
Batch  25/537: Loss=0.5965 (C:0.5965, R:0.0105)
Batch  50/537: Loss=0.6190 (C:0.6190, R:0.0105)
Batch  75/537: Loss=0.5977 (C:0.5977, R:0.0105)
Batch 100/537: Loss=0.6054 (C:0.6054, R:0.0105)
Batch 125/537: Loss=0.6020 (C:0.6020, R:0.0105)
Batch 150/537: Loss=0.6255 (C:0.6255, R:0.0105)
Batch 175/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch 200/537: Loss=0.6262 (C:0.6262, R:0.0106)
Batch 225/537: Loss=0.6036 (C:0.6036, R:0.0106)
Batch 250/537: Loss=0.6485 (C:0.6485, R:0.0105)
Batch 275/537: Loss=0.6232 (C:0.6232, R:0.0106)
Batch 300/537: Loss=0.6040 (C:0.6040, R:0.0105)
Batch 325/537: Loss=0.6331 (C:0.6331, R:0.0105)
Batch 350/537: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 375/537: Loss=0.6124 (C:0.6124, R:0.0105)
Batch 400/537: Loss=0.6160 (C:0.6160, R:0.0105)
Batch 425/537: Loss=0.5857 (C:0.5857, R:0.0105)
Batch 450/537: Loss=0.6045 (C:0.6045, R:0.0105)
Batch 475/537: Loss=0.6085 (C:0.6085, R:0.0105)
Batch 500/537: Loss=0.6115 (C:0.6115, R:0.0105)
Batch 525/537: Loss=0.6296 (C:0.6296, R:0.0105)

============================================================
Epoch 51/300 completed in 21.4s
Train: Loss=0.6123 (C:0.6123, R:0.0105) Ratio=5.09x
Val:   Loss=0.7693 (C:0.7693, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.332 ± 0.581
    Neg distances: 2.441 ± 1.048
    Separation ratio: 7.36x
    Gap: -4.074
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.6166 (C:0.6166, R:0.0105)
Batch  25/537: Loss=0.6147 (C:0.6147, R:0.0105)
Batch  50/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch  75/537: Loss=0.6116 (C:0.6116, R:0.0106)
Batch 100/537: Loss=0.5911 (C:0.5911, R:0.0105)
Batch 125/537: Loss=0.5779 (C:0.5779, R:0.0105)
Batch 150/537: Loss=0.6285 (C:0.6285, R:0.0105)
Batch 175/537: Loss=0.5932 (C:0.5932, R:0.0105)
Batch 200/537: Loss=0.6358 (C:0.6358, R:0.0105)
Batch 225/537: Loss=0.5884 (C:0.5884, R:0.0105)
Batch 250/537: Loss=0.6180 (C:0.6180, R:0.0105)
Batch 275/537: Loss=0.6311 (C:0.6311, R:0.0105)
Batch 300/537: Loss=0.6127 (C:0.6127, R:0.0105)
Batch 325/537: Loss=0.6366 (C:0.6366, R:0.0105)
Batch 350/537: Loss=0.6247 (C:0.6247, R:0.0105)
Batch 375/537: Loss=0.6191 (C:0.6191, R:0.0105)
Batch 400/537: Loss=0.5978 (C:0.5978, R:0.0105)
Batch 425/537: Loss=0.6414 (C:0.6414, R:0.0105)
Batch 450/537: Loss=0.5842 (C:0.5842, R:0.0105)
Batch 475/537: Loss=0.6297 (C:0.6297, R:0.0105)
Batch 500/537: Loss=0.6199 (C:0.6199, R:0.0105)
Batch 525/537: Loss=0.6362 (C:0.6362, R:0.0105)

============================================================
Epoch 52/300 completed in 27.7s
Train: Loss=0.6106 (C:0.6106, R:0.0105) Ratio=5.04x
Val:   Loss=0.7711 (C:0.7711, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.5810 (C:0.5810, R:0.0105)
Batch  25/537: Loss=0.6136 (C:0.6136, R:0.0105)
Batch  50/537: Loss=0.5954 (C:0.5954, R:0.0105)
Batch  75/537: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 100/537: Loss=0.6386 (C:0.6386, R:0.0105)
Batch 125/537: Loss=0.6012 (C:0.6012, R:0.0105)
Batch 150/537: Loss=0.5850 (C:0.5850, R:0.0105)
Batch 175/537: Loss=0.6261 (C:0.6261, R:0.0105)
Batch 200/537: Loss=0.6135 (C:0.6135, R:0.0105)
Batch 225/537: Loss=0.5996 (C:0.5996, R:0.0105)
Batch 250/537: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 275/537: Loss=0.6060 (C:0.6060, R:0.0106)
Batch 300/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 325/537: Loss=0.6284 (C:0.6284, R:0.0105)
Batch 350/537: Loss=0.6055 (C:0.6055, R:0.0106)
Batch 375/537: Loss=0.6236 (C:0.6236, R:0.0105)
Batch 400/537: Loss=0.6012 (C:0.6012, R:0.0105)
Batch 425/537: Loss=0.6506 (C:0.6506, R:0.0105)
Batch 450/537: Loss=0.6164 (C:0.6164, R:0.0105)
Batch 475/537: Loss=0.6399 (C:0.6399, R:0.0105)
Batch 500/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 525/537: Loss=0.6343 (C:0.6343, R:0.0105)

============================================================
Epoch 53/300 completed in 21.4s
Train: Loss=0.6092 (C:0.6092, R:0.0105) Ratio=5.06x
Val:   Loss=0.7587 (C:0.7587, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.6010 (C:0.6010, R:0.0105)
Batch  25/537: Loss=0.6191 (C:0.6191, R:0.0105)
Batch  50/537: Loss=0.5910 (C:0.5910, R:0.0105)
Batch  75/537: Loss=0.5995 (C:0.5995, R:0.0105)
Batch 100/537: Loss=0.6069 (C:0.6069, R:0.0105)
Batch 125/537: Loss=0.5983 (C:0.5983, R:0.0105)
Batch 150/537: Loss=0.6208 (C:0.6208, R:0.0105)
Batch 175/537: Loss=0.6131 (C:0.6131, R:0.0105)
Batch 200/537: Loss=0.6216 (C:0.6216, R:0.0105)
Batch 225/537: Loss=0.6040 (C:0.6040, R:0.0105)
Batch 250/537: Loss=0.6322 (C:0.6322, R:0.0105)
Batch 275/537: Loss=0.6497 (C:0.6497, R:0.0105)
Batch 300/537: Loss=0.6248 (C:0.6248, R:0.0105)
Batch 325/537: Loss=0.6176 (C:0.6176, R:0.0105)
Batch 350/537: Loss=0.6248 (C:0.6248, R:0.0105)
Batch 375/537: Loss=0.6227 (C:0.6227, R:0.0105)
Batch 400/537: Loss=0.6047 (C:0.6047, R:0.0105)
Batch 425/537: Loss=0.6073 (C:0.6073, R:0.0105)
Batch 450/537: Loss=0.5777 (C:0.5777, R:0.0105)
Batch 475/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 500/537: Loss=0.6196 (C:0.6196, R:0.0105)
Batch 525/537: Loss=0.6228 (C:0.6228, R:0.0105)

============================================================
Epoch 54/300 completed in 21.2s
Train: Loss=0.6078 (C:0.6078, R:0.0105) Ratio=5.00x
Val:   Loss=0.7638 (C:0.7638, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.323 ± 0.588
    Neg distances: 2.488 ± 1.060
    Separation ratio: 7.69x
    Gap: -4.118
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.5768 (C:0.5768, R:0.0105)
Batch  25/537: Loss=0.5980 (C:0.5980, R:0.0105)
Batch  50/537: Loss=0.5760 (C:0.5760, R:0.0105)
Batch  75/537: Loss=0.6115 (C:0.6115, R:0.0105)
Batch 100/537: Loss=0.6199 (C:0.6199, R:0.0105)
Batch 125/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 150/537: Loss=0.5926 (C:0.5926, R:0.0105)
Batch 175/537: Loss=0.5982 (C:0.5982, R:0.0105)
Batch 200/537: Loss=0.6113 (C:0.6113, R:0.0105)
Batch 225/537: Loss=0.5770 (C:0.5770, R:0.0106)
Batch 250/537: Loss=0.5825 (C:0.5825, R:0.0105)
Batch 275/537: Loss=0.5850 (C:0.5850, R:0.0105)
Batch 300/537: Loss=0.6111 (C:0.6111, R:0.0105)
Batch 325/537: Loss=0.6126 (C:0.6126, R:0.0105)
Batch 350/537: Loss=0.5846 (C:0.5846, R:0.0105)
Batch 375/537: Loss=0.5590 (C:0.5590, R:0.0105)
Batch 400/537: Loss=0.6400 (C:0.6400, R:0.0105)
Batch 425/537: Loss=0.5954 (C:0.5954, R:0.0105)
Batch 450/537: Loss=0.6007 (C:0.6007, R:0.0105)
Batch 475/537: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 500/537: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 525/537: Loss=0.5623 (C:0.5623, R:0.0105)

============================================================
Epoch 55/300 completed in 26.8s
Train: Loss=0.5922 (C:0.5922, R:0.0105) Ratio=5.15x
Val:   Loss=0.7491 (C:0.7491, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7491)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.5802 (C:0.5802, R:0.0105)
Batch  25/537: Loss=0.5977 (C:0.5977, R:0.0105)
Batch  50/537: Loss=0.5925 (C:0.5925, R:0.0105)
Batch  75/537: Loss=0.6118 (C:0.6118, R:0.0105)
Batch 100/537: Loss=0.5656 (C:0.5656, R:0.0105)
Batch 125/537: Loss=0.5778 (C:0.5778, R:0.0106)
Batch 150/537: Loss=0.5464 (C:0.5464, R:0.0105)
Batch 175/537: Loss=0.6290 (C:0.6290, R:0.0105)
Batch 200/537: Loss=0.5530 (C:0.5530, R:0.0105)
Batch 225/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 250/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 275/537: Loss=0.6074 (C:0.6074, R:0.0105)
Batch 300/537: Loss=0.5739 (C:0.5739, R:0.0105)
Batch 325/537: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 350/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch 375/537: Loss=0.6122 (C:0.6122, R:0.0105)
Batch 400/537: Loss=0.6037 (C:0.6037, R:0.0105)
Batch 425/537: Loss=0.5976 (C:0.5976, R:0.0105)
Batch 450/537: Loss=0.5738 (C:0.5738, R:0.0105)
Batch 475/537: Loss=0.5873 (C:0.5873, R:0.0105)
Batch 500/537: Loss=0.6369 (C:0.6369, R:0.0105)
Batch 525/537: Loss=0.5606 (C:0.5606, R:0.0105)

============================================================
Epoch 56/300 completed in 21.1s
Train: Loss=0.5916 (C:0.5916, R:0.0105) Ratio=5.10x
Val:   Loss=0.7406 (C:0.7406, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7406)
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.5929 (C:0.5929, R:0.0105)
Batch  25/537: Loss=0.5567 (C:0.5567, R:0.0105)
Batch  50/537: Loss=0.5745 (C:0.5745, R:0.0105)
Batch  75/537: Loss=0.5953 (C:0.5953, R:0.0105)
Batch 100/537: Loss=0.5808 (C:0.5808, R:0.0105)
Batch 125/537: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 150/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 175/537: Loss=0.5809 (C:0.5809, R:0.0105)
Batch 200/537: Loss=0.6198 (C:0.6198, R:0.0105)
Batch 225/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 250/537: Loss=0.6050 (C:0.6050, R:0.0105)
Batch 275/537: Loss=0.6092 (C:0.6092, R:0.0105)
Batch 300/537: Loss=0.6029 (C:0.6029, R:0.0105)
Batch 325/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 350/537: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 375/537: Loss=0.5675 (C:0.5675, R:0.0105)
Batch 400/537: Loss=0.5900 (C:0.5900, R:0.0105)
Batch 425/537: Loss=0.6187 (C:0.6187, R:0.0106)
Batch 450/537: Loss=0.6091 (C:0.6091, R:0.0105)
Batch 475/537: Loss=0.5998 (C:0.5998, R:0.0105)
Batch 500/537: Loss=0.5776 (C:0.5776, R:0.0105)
Batch 525/537: Loss=0.6114 (C:0.6114, R:0.0105)

============================================================
Epoch 57/300 completed in 21.2s
Train: Loss=0.5904 (C:0.5904, R:0.0105) Ratio=5.15x
Val:   Loss=0.7565 (C:0.7565, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.297 ± 0.556
    Neg distances: 2.528 ± 1.056
    Separation ratio: 8.50x
    Gap: -4.184
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.5605 (C:0.5605, R:0.0105)
Batch  25/537: Loss=0.5534 (C:0.5534, R:0.0106)
Batch  50/537: Loss=0.5658 (C:0.5658, R:0.0105)
Batch  75/537: Loss=0.5654 (C:0.5654, R:0.0105)
Batch 100/537: Loss=0.5295 (C:0.5295, R:0.0105)
Batch 125/537: Loss=0.5600 (C:0.5600, R:0.0105)
Batch 150/537: Loss=0.5788 (C:0.5788, R:0.0105)
Batch 175/537: Loss=0.5546 (C:0.5546, R:0.0105)
Batch 200/537: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 225/537: Loss=0.5548 (C:0.5548, R:0.0105)
Batch 250/537: Loss=0.5442 (C:0.5442, R:0.0105)
Batch 275/537: Loss=0.5654 (C:0.5654, R:0.0105)
Batch 300/537: Loss=0.5503 (C:0.5503, R:0.0106)
Batch 325/537: Loss=0.5463 (C:0.5463, R:0.0105)
Batch 350/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 375/537: Loss=0.5914 (C:0.5914, R:0.0105)
Batch 400/537: Loss=0.5588 (C:0.5588, R:0.0105)
Batch 425/537: Loss=0.5932 (C:0.5932, R:0.0105)
Batch 450/537: Loss=0.5686 (C:0.5686, R:0.0105)
Batch 475/537: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 500/537: Loss=0.5791 (C:0.5791, R:0.0105)
Batch 525/537: Loss=0.5543 (C:0.5543, R:0.0105)

============================================================
Epoch 58/300 completed in 27.2s
Train: Loss=0.5600 (C:0.5600, R:0.0105) Ratio=5.15x
Val:   Loss=0.7266 (C:0.7266, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7266)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch  25/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch  50/537: Loss=0.5288 (C:0.5288, R:0.0105)
Batch  75/537: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 100/537: Loss=0.5343 (C:0.5343, R:0.0105)
Batch 125/537: Loss=0.5083 (C:0.5083, R:0.0105)
Batch 150/537: Loss=0.5292 (C:0.5292, R:0.0105)
Batch 175/537: Loss=0.5727 (C:0.5727, R:0.0105)
Batch 200/537: Loss=0.5544 (C:0.5544, R:0.0105)
Batch 225/537: Loss=0.5457 (C:0.5457, R:0.0105)
Batch 250/537: Loss=0.5478 (C:0.5478, R:0.0105)
Batch 275/537: Loss=0.5786 (C:0.5786, R:0.0105)
Batch 300/537: Loss=0.5954 (C:0.5954, R:0.0105)
Batch 325/537: Loss=0.5587 (C:0.5587, R:0.0105)
Batch 350/537: Loss=0.5595 (C:0.5595, R:0.0105)
Batch 375/537: Loss=0.5757 (C:0.5757, R:0.0105)
Batch 400/537: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 425/537: Loss=0.5834 (C:0.5834, R:0.0105)
Batch 450/537: Loss=0.5605 (C:0.5605, R:0.0105)
Batch 475/537: Loss=0.5538 (C:0.5538, R:0.0105)
Batch 500/537: Loss=0.5826 (C:0.5826, R:0.0105)
Batch 525/537: Loss=0.6121 (C:0.6121, R:0.0105)

============================================================
Epoch 59/300 completed in 21.5s
Train: Loss=0.5604 (C:0.5604, R:0.0105) Ratio=5.26x
Val:   Loss=0.7188 (C:0.7188, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7188)
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.5501 (C:0.5501, R:0.0105)
Batch  25/537: Loss=0.5437 (C:0.5437, R:0.0105)
Batch  50/537: Loss=0.5677 (C:0.5677, R:0.0105)
Batch  75/537: Loss=0.5584 (C:0.5584, R:0.0105)
Batch 100/537: Loss=0.5444 (C:0.5444, R:0.0105)
Batch 125/537: Loss=0.6027 (C:0.6027, R:0.0105)
Batch 150/537: Loss=0.5782 (C:0.5782, R:0.0105)
Batch 175/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 200/537: Loss=0.5371 (C:0.5371, R:0.0105)
Batch 225/537: Loss=0.5497 (C:0.5497, R:0.0105)
Batch 250/537: Loss=0.5673 (C:0.5673, R:0.0105)
Batch 275/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 300/537: Loss=0.5600 (C:0.5600, R:0.0105)
Batch 325/537: Loss=0.5471 (C:0.5471, R:0.0105)
Batch 350/537: Loss=0.5375 (C:0.5375, R:0.0105)
Batch 375/537: Loss=0.5744 (C:0.5744, R:0.0106)
Batch 400/537: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 425/537: Loss=0.5489 (C:0.5489, R:0.0106)
Batch 450/537: Loss=0.5492 (C:0.5492, R:0.0105)
Batch 475/537: Loss=0.5655 (C:0.5655, R:0.0105)
Batch 500/537: Loss=0.5714 (C:0.5714, R:0.0105)
Batch 525/537: Loss=0.5457 (C:0.5457, R:0.0105)

============================================================
Epoch 60/300 completed in 21.6s
Train: Loss=0.5596 (C:0.5596, R:0.0105) Ratio=5.29x
Val:   Loss=0.7251 (C:0.7251, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.307 ± 0.559
    Neg distances: 2.551 ± 1.075
    Separation ratio: 8.30x
    Gap: -4.267
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.5866 (C:0.5866, R:0.0105)
Batch  25/537: Loss=0.5433 (C:0.5433, R:0.0105)
Batch  50/537: Loss=0.5767 (C:0.5767, R:0.0105)
Batch  75/537: Loss=0.5600 (C:0.5600, R:0.0105)
Batch 100/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 125/537: Loss=0.5594 (C:0.5594, R:0.0105)
Batch 150/537: Loss=0.5551 (C:0.5551, R:0.0105)
Batch 175/537: Loss=0.5954 (C:0.5954, R:0.0105)
Batch 200/537: Loss=0.5576 (C:0.5576, R:0.0105)
Batch 225/537: Loss=0.5412 (C:0.5412, R:0.0105)
Batch 250/537: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 275/537: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 300/537: Loss=0.5586 (C:0.5586, R:0.0105)
Batch 325/537: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 350/537: Loss=0.5511 (C:0.5511, R:0.0105)
Batch 375/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 400/537: Loss=0.5730 (C:0.5730, R:0.0106)
Batch 425/537: Loss=0.5715 (C:0.5715, R:0.0105)
Batch 450/537: Loss=0.5331 (C:0.5331, R:0.0105)
Batch 475/537: Loss=0.5755 (C:0.5755, R:0.0105)
Batch 500/537: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 525/537: Loss=0.5549 (C:0.5549, R:0.0105)

============================================================
Epoch 61/300 completed in 27.1s
Train: Loss=0.5615 (C:0.5615, R:0.0105) Ratio=5.28x
Val:   Loss=0.7182 (C:0.7182, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7182)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.5793 (C:0.5793, R:0.0105)
Batch  25/537: Loss=0.5336 (C:0.5336, R:0.0105)
Batch  50/537: Loss=0.5563 (C:0.5563, R:0.0105)
Batch  75/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 100/537: Loss=0.5590 (C:0.5590, R:0.0105)
Batch 125/537: Loss=0.6162 (C:0.6162, R:0.0105)
Batch 150/537: Loss=0.5442 (C:0.5442, R:0.0105)
Batch 175/537: Loss=0.5692 (C:0.5692, R:0.0105)
Batch 200/537: Loss=0.5454 (C:0.5454, R:0.0105)
Batch 225/537: Loss=0.5522 (C:0.5522, R:0.0105)
Batch 250/537: Loss=0.5812 (C:0.5812, R:0.0105)
Batch 275/537: Loss=0.5822 (C:0.5822, R:0.0105)
Batch 300/537: Loss=0.5668 (C:0.5668, R:0.0105)
Batch 325/537: Loss=0.5384 (C:0.5384, R:0.0105)
Batch 350/537: Loss=0.5540 (C:0.5540, R:0.0105)
Batch 375/537: Loss=0.5564 (C:0.5564, R:0.0105)
Batch 400/537: Loss=0.5669 (C:0.5669, R:0.0105)
Batch 425/537: Loss=0.5738 (C:0.5738, R:0.0105)
Batch 450/537: Loss=0.5106 (C:0.5106, R:0.0105)
Batch 475/537: Loss=0.5803 (C:0.5803, R:0.0105)
Batch 500/537: Loss=0.5700 (C:0.5700, R:0.0105)
Batch 525/537: Loss=0.5794 (C:0.5794, R:0.0105)

============================================================
Epoch 62/300 completed in 21.1s
Train: Loss=0.5597 (C:0.5597, R:0.0105) Ratio=5.29x
Val:   Loss=0.7205 (C:0.7205, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.5533 (C:0.5533, R:0.0105)
Batch  25/537: Loss=0.5168 (C:0.5168, R:0.0105)
Batch  50/537: Loss=0.5549 (C:0.5549, R:0.0105)
Batch  75/537: Loss=0.5260 (C:0.5260, R:0.0105)
Batch 100/537: Loss=0.5496 (C:0.5496, R:0.0105)
Batch 125/537: Loss=0.5754 (C:0.5754, R:0.0105)
Batch 150/537: Loss=0.5429 (C:0.5429, R:0.0105)
Batch 175/537: Loss=0.5459 (C:0.5459, R:0.0105)
Batch 200/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 225/537: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 250/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 275/537: Loss=0.5325 (C:0.5325, R:0.0105)
Batch 300/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 325/537: Loss=0.5255 (C:0.5255, R:0.0105)
Batch 350/537: Loss=0.5513 (C:0.5513, R:0.0105)
Batch 375/537: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 400/537: Loss=0.5797 (C:0.5797, R:0.0105)
Batch 425/537: Loss=0.5752 (C:0.5752, R:0.0105)
Batch 450/537: Loss=0.5288 (C:0.5288, R:0.0105)
Batch 475/537: Loss=0.5658 (C:0.5658, R:0.0105)
Batch 500/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 525/537: Loss=0.5619 (C:0.5619, R:0.0105)

============================================================
Epoch 63/300 completed in 21.3s
Train: Loss=0.5584 (C:0.5584, R:0.0105) Ratio=5.30x
Val:   Loss=0.7178 (C:0.7178, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7178)
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.324 ± 0.594
    Neg distances: 2.563 ± 1.085
    Separation ratio: 7.90x
    Gap: -4.312
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.5494 (C:0.5494, R:0.0105)
Batch  25/537: Loss=0.5807 (C:0.5807, R:0.0105)
Batch  50/537: Loss=0.5315 (C:0.5315, R:0.0105)
Batch  75/537: Loss=0.5508 (C:0.5508, R:0.0106)
Batch 100/537: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 125/537: Loss=0.5559 (C:0.5559, R:0.0105)
Batch 150/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 175/537: Loss=0.5628 (C:0.5628, R:0.0105)
Batch 200/537: Loss=0.5738 (C:0.5738, R:0.0105)
Batch 225/537: Loss=0.5633 (C:0.5633, R:0.0105)
Batch 250/537: Loss=0.5685 (C:0.5685, R:0.0105)
Batch 275/537: Loss=0.5257 (C:0.5257, R:0.0105)
Batch 300/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 325/537: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 350/537: Loss=0.5603 (C:0.5603, R:0.0106)
Batch 375/537: Loss=0.5575 (C:0.5575, R:0.0105)
Batch 400/537: Loss=0.5769 (C:0.5769, R:0.0105)
Batch 425/537: Loss=0.5525 (C:0.5525, R:0.0106)
Batch 450/537: Loss=0.5799 (C:0.5799, R:0.0105)
Batch 475/537: Loss=0.5756 (C:0.5756, R:0.0105)
Batch 500/537: Loss=0.5897 (C:0.5897, R:0.0105)
Batch 525/537: Loss=0.5642 (C:0.5642, R:0.0105)

============================================================
Epoch 64/300 completed in 27.4s
Train: Loss=0.5662 (C:0.5662, R:0.0105) Ratio=5.28x
Val:   Loss=0.7429 (C:0.7429, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.5644 (C:0.5644, R:0.0106)
Batch  25/537: Loss=0.5529 (C:0.5529, R:0.0105)
Batch  50/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch  75/537: Loss=0.5596 (C:0.5596, R:0.0105)
Batch 100/537: Loss=0.5195 (C:0.5195, R:0.0105)
Batch 125/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 150/537: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 175/537: Loss=0.5445 (C:0.5445, R:0.0105)
Batch 200/537: Loss=0.5823 (C:0.5823, R:0.0105)
Batch 225/537: Loss=0.5554 (C:0.5554, R:0.0105)
Batch 250/537: Loss=0.5076 (C:0.5076, R:0.0105)
Batch 275/537: Loss=0.5517 (C:0.5517, R:0.0105)
Batch 300/537: Loss=0.5809 (C:0.5809, R:0.0105)
Batch 325/537: Loss=0.6223 (C:0.6223, R:0.0105)
Batch 350/537: Loss=0.5685 (C:0.5685, R:0.0105)
Batch 375/537: Loss=0.5705 (C:0.5705, R:0.0105)
Batch 400/537: Loss=0.5491 (C:0.5491, R:0.0105)
Batch 425/537: Loss=0.5747 (C:0.5747, R:0.0106)
Batch 450/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 475/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 500/537: Loss=0.5555 (C:0.5555, R:0.0105)
Batch 525/537: Loss=0.5542 (C:0.5542, R:0.0105)

============================================================
Epoch 65/300 completed in 21.1s
Train: Loss=0.5645 (C:0.5645, R:0.0105) Ratio=5.37x
Val:   Loss=0.7410 (C:0.7410, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.5353 (C:0.5353, R:0.0105)
Batch  25/537: Loss=0.6087 (C:0.6087, R:0.0105)
Batch  50/537: Loss=0.5516 (C:0.5516, R:0.0105)
Batch  75/537: Loss=0.5737 (C:0.5737, R:0.0105)
Batch 100/537: Loss=0.5864 (C:0.5864, R:0.0105)
Batch 125/537: Loss=0.5519 (C:0.5519, R:0.0105)
Batch 150/537: Loss=0.5620 (C:0.5620, R:0.0104)
Batch 175/537: Loss=0.5700 (C:0.5700, R:0.0105)
Batch 200/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 225/537: Loss=0.5724 (C:0.5724, R:0.0105)
Batch 250/537: Loss=0.5631 (C:0.5631, R:0.0105)
Batch 275/537: Loss=0.5979 (C:0.5979, R:0.0105)
Batch 300/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 325/537: Loss=0.5595 (C:0.5595, R:0.0106)
Batch 350/537: Loss=0.5159 (C:0.5159, R:0.0105)
Batch 375/537: Loss=0.5578 (C:0.5578, R:0.0105)
Batch 400/537: Loss=0.5706 (C:0.5706, R:0.0105)
Batch 425/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 450/537: Loss=0.5377 (C:0.5377, R:0.0105)
Batch 475/537: Loss=0.5607 (C:0.5607, R:0.0105)
Batch 500/537: Loss=0.5593 (C:0.5593, R:0.0105)
Batch 525/537: Loss=0.5817 (C:0.5817, R:0.0105)

============================================================
Epoch 66/300 completed in 21.1s
Train: Loss=0.5638 (C:0.5638, R:0.0105) Ratio=5.36x
Val:   Loss=0.7305 (C:0.7305, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.314 ± 0.599
    Neg distances: 2.583 ± 1.085
    Separation ratio: 8.24x
    Gap: -4.283
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.5323 (C:0.5323, R:0.0105)
Batch  25/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch  50/537: Loss=0.5870 (C:0.5870, R:0.0105)
Batch  75/537: Loss=0.5258 (C:0.5258, R:0.0105)
Batch 100/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 125/537: Loss=0.5679 (C:0.5679, R:0.0105)
Batch 150/537: Loss=0.5000 (C:0.5000, R:0.0105)
Batch 175/537: Loss=0.5460 (C:0.5460, R:0.0105)
Batch 200/537: Loss=0.5578 (C:0.5578, R:0.0106)
Batch 225/537: Loss=0.5578 (C:0.5578, R:0.0105)
Batch 250/537: Loss=0.5789 (C:0.5789, R:0.0105)
Batch 275/537: Loss=0.5164 (C:0.5164, R:0.0105)
Batch 300/537: Loss=0.5684 (C:0.5684, R:0.0105)
Batch 325/537: Loss=0.5257 (C:0.5257, R:0.0105)
Batch 350/537: Loss=0.5477 (C:0.5477, R:0.0105)
Batch 375/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 400/537: Loss=0.5528 (C:0.5528, R:0.0105)
Batch 425/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 450/537: Loss=0.5764 (C:0.5764, R:0.0105)
Batch 475/537: Loss=0.5584 (C:0.5584, R:0.0105)
Batch 500/537: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 525/537: Loss=0.5374 (C:0.5374, R:0.0105)

============================================================
Epoch 67/300 completed in 26.9s
Train: Loss=0.5516 (C:0.5516, R:0.0105) Ratio=5.42x
Val:   Loss=0.7421 (C:0.7421, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.5362 (C:0.5362, R:0.0105)
Batch  25/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch  50/537: Loss=0.5626 (C:0.5626, R:0.0105)
Batch  75/537: Loss=0.5889 (C:0.5889, R:0.0105)
Batch 100/537: Loss=0.5273 (C:0.5273, R:0.0105)
Batch 125/537: Loss=0.5345 (C:0.5345, R:0.0105)
Batch 150/537: Loss=0.5709 (C:0.5709, R:0.0105)
Batch 175/537: Loss=0.5636 (C:0.5636, R:0.0105)
Batch 200/537: Loss=0.5580 (C:0.5580, R:0.0105)
Batch 225/537: Loss=0.5552 (C:0.5552, R:0.0105)
Batch 250/537: Loss=0.5477 (C:0.5477, R:0.0105)
Batch 275/537: Loss=0.5537 (C:0.5537, R:0.0105)
Batch 300/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 325/537: Loss=0.5262 (C:0.5262, R:0.0106)
Batch 350/537: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 375/537: Loss=0.5192 (C:0.5192, R:0.0105)
Batch 400/537: Loss=0.5471 (C:0.5471, R:0.0105)
Batch 425/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 450/537: Loss=0.5182 (C:0.5182, R:0.0105)
Batch 475/537: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 500/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 525/537: Loss=0.5458 (C:0.5458, R:0.0105)

============================================================
Epoch 68/300 completed in 21.1s
Train: Loss=0.5511 (C:0.5511, R:0.0105) Ratio=5.47x
Val:   Loss=0.7317 (C:0.7317, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch  25/537: Loss=0.5570 (C:0.5570, R:0.0105)
Batch  50/537: Loss=0.5516 (C:0.5516, R:0.0105)
Batch  75/537: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 100/537: Loss=0.5291 (C:0.5291, R:0.0105)
Batch 125/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 150/537: Loss=0.5268 (C:0.5268, R:0.0105)
Batch 175/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch 200/537: Loss=0.5797 (C:0.5797, R:0.0105)
Batch 225/537: Loss=0.5391 (C:0.5391, R:0.0105)
Batch 250/537: Loss=0.5397 (C:0.5397, R:0.0105)
Batch 275/537: Loss=0.5176 (C:0.5176, R:0.0105)
Batch 300/537: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 325/537: Loss=0.5383 (C:0.5383, R:0.0105)
Batch 350/537: Loss=0.5277 (C:0.5277, R:0.0105)
Batch 375/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 400/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 425/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 450/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 475/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 500/537: Loss=0.5415 (C:0.5415, R:0.0105)
Batch 525/537: Loss=0.5436 (C:0.5436, R:0.0105)

============================================================
Epoch 69/300 completed in 21.1s
Train: Loss=0.5488 (C:0.5488, R:0.0105) Ratio=5.60x
Val:   Loss=0.7245 (C:0.7245, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 70
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.290 ± 0.554
    Neg distances: 2.642 ± 1.095
    Separation ratio: 9.10x
    Gap: -4.380
    ✅ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.4880 (C:0.4880, R:0.0105)
Batch  25/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch  50/537: Loss=0.4796 (C:0.4796, R:0.0105)
Batch  75/537: Loss=0.5108 (C:0.5108, R:0.0105)
Batch 100/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 125/537: Loss=0.5263 (C:0.5263, R:0.0105)
Batch 150/537: Loss=0.4687 (C:0.4687, R:0.0105)
Batch 175/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 200/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch 225/537: Loss=0.5070 (C:0.5070, R:0.0105)
Batch 250/537: Loss=0.5158 (C:0.5158, R:0.0105)
Batch 275/537: Loss=0.5701 (C:0.5701, R:0.0106)
Batch 300/537: Loss=0.5372 (C:0.5372, R:0.0105)
Batch 325/537: Loss=0.5299 (C:0.5299, R:0.0105)
Batch 350/537: Loss=0.4873 (C:0.4873, R:0.0105)
Batch 375/537: Loss=0.5389 (C:0.5389, R:0.0105)
Batch 400/537: Loss=0.4971 (C:0.4971, R:0.0105)
Batch 425/537: Loss=0.5433 (C:0.5433, R:0.0105)
Batch 450/537: Loss=0.5424 (C:0.5424, R:0.0105)
Batch 475/537: Loss=0.5465 (C:0.5465, R:0.0105)
Batch 500/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 525/537: Loss=0.5297 (C:0.5297, R:0.0105)

============================================================
Epoch 70/300 completed in 26.9s
Train: Loss=0.5284 (C:0.5284, R:0.0105) Ratio=5.57x
Val:   Loss=0.7019 (C:0.7019, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7019)
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.4959 (C:0.4959, R:0.0105)
Batch  25/537: Loss=0.4804 (C:0.4804, R:0.0105)
Batch  50/537: Loss=0.5088 (C:0.5088, R:0.0105)
Batch  75/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 100/537: Loss=0.5413 (C:0.5413, R:0.0105)
Batch 125/537: Loss=0.5195 (C:0.5195, R:0.0105)
Batch 150/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 175/537: Loss=0.5389 (C:0.5389, R:0.0105)
Batch 200/537: Loss=0.4787 (C:0.4787, R:0.0105)
Batch 225/537: Loss=0.5284 (C:0.5284, R:0.0105)
Batch 250/537: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 275/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 300/537: Loss=0.5366 (C:0.5366, R:0.0105)
Batch 325/537: Loss=0.5586 (C:0.5586, R:0.0105)
Batch 350/537: Loss=0.5234 (C:0.5234, R:0.0105)
Batch 375/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 400/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 425/537: Loss=0.4852 (C:0.4852, R:0.0105)
Batch 450/537: Loss=0.5070 (C:0.5070, R:0.0105)
Batch 475/537: Loss=0.5009 (C:0.5009, R:0.0105)
Batch 500/537: Loss=0.5580 (C:0.5580, R:0.0105)
Batch 525/537: Loss=0.5069 (C:0.5069, R:0.0105)

============================================================
Epoch 71/300 completed in 21.1s
Train: Loss=0.5261 (C:0.5261, R:0.0105) Ratio=5.61x
Val:   Loss=0.7083 (C:0.7083, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.5158 (C:0.5158, R:0.0105)
Batch  25/537: Loss=0.5051 (C:0.5051, R:0.0106)
Batch  50/537: Loss=0.5247 (C:0.5247, R:0.0105)
Batch  75/537: Loss=0.5196 (C:0.5196, R:0.0105)
Batch 100/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 125/537: Loss=0.5428 (C:0.5428, R:0.0105)
Batch 150/537: Loss=0.5601 (C:0.5601, R:0.0105)
Batch 175/537: Loss=0.5166 (C:0.5166, R:0.0105)
Batch 200/537: Loss=0.5233 (C:0.5233, R:0.0106)
Batch 225/537: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 250/537: Loss=0.5109 (C:0.5109, R:0.0105)
Batch 275/537: Loss=0.5553 (C:0.5553, R:0.0105)
Batch 300/537: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 325/537: Loss=0.5092 (C:0.5092, R:0.0105)
Batch 350/537: Loss=0.4958 (C:0.4958, R:0.0105)
Batch 375/537: Loss=0.5245 (C:0.5245, R:0.0105)
Batch 400/537: Loss=0.5033 (C:0.5033, R:0.0105)
Batch 425/537: Loss=0.5244 (C:0.5244, R:0.0105)
Batch 450/537: Loss=0.5260 (C:0.5260, R:0.0105)
Batch 475/537: Loss=0.4884 (C:0.4884, R:0.0105)
Batch 500/537: Loss=0.5659 (C:0.5659, R:0.0105)
Batch 525/537: Loss=0.5266 (C:0.5266, R:0.0105)

============================================================
Epoch 72/300 completed in 21.1s
Train: Loss=0.5259 (C:0.5259, R:0.0105) Ratio=5.45x
Val:   Loss=0.7080 (C:0.7080, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 73
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.299 ± 0.582
    Neg distances: 2.638 ± 1.098
    Separation ratio: 8.82x
    Gap: -4.427
    ✅ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=0.5217 (C:0.5217, R:0.0105)
Batch  25/537: Loss=0.5469 (C:0.5469, R:0.0105)
Batch  50/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch  75/537: Loss=0.5260 (C:0.5260, R:0.0105)
Batch 100/537: Loss=0.5555 (C:0.5555, R:0.0105)
Batch 125/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 150/537: Loss=0.5215 (C:0.5215, R:0.0105)
Batch 175/537: Loss=0.5337 (C:0.5337, R:0.0106)
Batch 200/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 225/537: Loss=0.5710 (C:0.5710, R:0.0106)
Batch 250/537: Loss=0.5507 (C:0.5507, R:0.0105)
Batch 275/537: Loss=0.4925 (C:0.4925, R:0.0105)
Batch 300/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 325/537: Loss=0.5821 (C:0.5821, R:0.0105)
Batch 350/537: Loss=0.5449 (C:0.5449, R:0.0105)
Batch 375/537: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 400/537: Loss=0.5387 (C:0.5387, R:0.0105)
Batch 425/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch 450/537: Loss=0.5266 (C:0.5266, R:0.0105)
Batch 475/537: Loss=0.5054 (C:0.5054, R:0.0105)
Batch 500/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 525/537: Loss=0.5514 (C:0.5514, R:0.0105)

============================================================
Epoch 73/300 completed in 27.2s
Train: Loss=0.5283 (C:0.5283, R:0.0105) Ratio=5.38x
Val:   Loss=0.7123 (C:0.7123, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=0.4950 (C:0.4950, R:0.0105)
Batch  25/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch  50/537: Loss=0.5425 (C:0.5425, R:0.0105)
Batch  75/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 100/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 125/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 150/537: Loss=0.5163 (C:0.5163, R:0.0105)
Batch 175/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 200/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 225/537: Loss=0.5202 (C:0.5202, R:0.0105)
Batch 250/537: Loss=0.5545 (C:0.5545, R:0.0105)
Batch 275/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 300/537: Loss=0.5114 (C:0.5114, R:0.0105)
Batch 325/537: Loss=0.5096 (C:0.5096, R:0.0105)
Batch 350/537: Loss=0.5198 (C:0.5198, R:0.0105)
Batch 375/537: Loss=0.5326 (C:0.5326, R:0.0106)
Batch 400/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 425/537: Loss=0.5300 (C:0.5300, R:0.0106)
Batch 450/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 475/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 500/537: Loss=0.5348 (C:0.5348, R:0.0105)
Batch 525/537: Loss=0.5453 (C:0.5453, R:0.0105)

============================================================
Epoch 74/300 completed in 21.7s
Train: Loss=0.5277 (C:0.5277, R:0.0105) Ratio=5.61x
Val:   Loss=0.7187 (C:0.7187, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=0.4852 (C:0.4852, R:0.0105)
Batch  25/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch  50/537: Loss=0.5212 (C:0.5212, R:0.0105)
Batch  75/537: Loss=0.4843 (C:0.4843, R:0.0105)
Batch 100/537: Loss=0.4921 (C:0.4921, R:0.0106)
Batch 125/537: Loss=0.5648 (C:0.5648, R:0.0105)
Batch 150/537: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 175/537: Loss=0.4772 (C:0.4772, R:0.0105)
Batch 200/537: Loss=0.5381 (C:0.5381, R:0.0105)
Batch 225/537: Loss=0.5469 (C:0.5469, R:0.0105)
Batch 250/537: Loss=0.5203 (C:0.5203, R:0.0105)
Batch 275/537: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 300/537: Loss=0.5561 (C:0.5561, R:0.0105)
Batch 325/537: Loss=0.5335 (C:0.5335, R:0.0105)
Batch 350/537: Loss=0.5149 (C:0.5149, R:0.0105)
Batch 375/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 400/537: Loss=0.5523 (C:0.5523, R:0.0106)
Batch 425/537: Loss=0.5779 (C:0.5779, R:0.0105)
Batch 450/537: Loss=0.5294 (C:0.5294, R:0.0105)
Batch 475/537: Loss=0.5069 (C:0.5069, R:0.0106)
Batch 500/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 525/537: Loss=0.5606 (C:0.5606, R:0.0105)

============================================================
Epoch 75/300 completed in 21.5s
Train: Loss=0.5273 (C:0.5273, R:0.0105) Ratio=5.58x
Val:   Loss=0.7143 (C:0.7143, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 76
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.290 ± 0.549
    Neg distances: 2.664 ± 1.099
    Separation ratio: 9.19x
    Gap: -4.475
    ✅ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=0.4740 (C:0.4740, R:0.0105)
Batch  25/537: Loss=0.5236 (C:0.5236, R:0.0105)
Batch  50/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch  75/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch 100/537: Loss=0.5415 (C:0.5415, R:0.0105)
Batch 125/537: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 150/537: Loss=0.5113 (C:0.5113, R:0.0105)
Batch 175/537: Loss=0.5293 (C:0.5293, R:0.0105)
Batch 200/537: Loss=0.5036 (C:0.5036, R:0.0105)
Batch 225/537: Loss=0.5359 (C:0.5359, R:0.0105)
Batch 250/537: Loss=0.5140 (C:0.5140, R:0.0105)
Batch 275/537: Loss=0.5268 (C:0.5268, R:0.0105)
Batch 300/537: Loss=0.5066 (C:0.5066, R:0.0105)
Batch 325/537: Loss=0.4917 (C:0.4917, R:0.0105)
Batch 350/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 375/537: Loss=0.5379 (C:0.5379, R:0.0105)
Batch 400/537: Loss=0.5240 (C:0.5240, R:0.0106)
Batch 425/537: Loss=0.5313 (C:0.5313, R:0.0106)
Batch 450/537: Loss=0.5389 (C:0.5389, R:0.0105)
Batch 475/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 500/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch 525/537: Loss=0.5270 (C:0.5270, R:0.0105)

============================================================
Epoch 76/300 completed in 27.4s
Train: Loss=0.5186 (C:0.5186, R:0.0105) Ratio=5.58x
Val:   Loss=0.6989 (C:0.6989, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6989)
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=0.4848 (C:0.4848, R:0.0105)
Batch  25/537: Loss=0.5339 (C:0.5339, R:0.0105)
Batch  50/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch  75/537: Loss=0.5446 (C:0.5446, R:0.0105)
Batch 100/537: Loss=0.4820 (C:0.4820, R:0.0106)
Batch 125/537: Loss=0.5325 (C:0.5325, R:0.0105)
Batch 150/537: Loss=0.5163 (C:0.5163, R:0.0105)
Batch 175/537: Loss=0.4977 (C:0.4977, R:0.0105)
Batch 200/537: Loss=0.4938 (C:0.4938, R:0.0105)
Batch 225/537: Loss=0.4758 (C:0.4758, R:0.0105)
Batch 250/537: Loss=0.5402 (C:0.5402, R:0.0106)
Batch 275/537: Loss=0.5303 (C:0.5303, R:0.0105)
Batch 300/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch 325/537: Loss=0.5112 (C:0.5112, R:0.0105)
Batch 350/537: Loss=0.5485 (C:0.5485, R:0.0106)
Batch 375/537: Loss=0.5333 (C:0.5333, R:0.0105)
Batch 400/537: Loss=0.5099 (C:0.5099, R:0.0105)
Batch 425/537: Loss=0.5031 (C:0.5031, R:0.0105)
Batch 450/537: Loss=0.5137 (C:0.5137, R:0.0105)
Batch 475/537: Loss=0.5328 (C:0.5328, R:0.0105)
Batch 500/537: Loss=0.5323 (C:0.5323, R:0.0105)
Batch 525/537: Loss=0.5117 (C:0.5117, R:0.0105)

============================================================
Epoch 77/300 completed in 21.5s
Train: Loss=0.5162 (C:0.5162, R:0.0105) Ratio=5.61x
Val:   Loss=0.7120 (C:0.7120, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch  25/537: Loss=0.5018 (C:0.5018, R:0.0105)
Batch  50/537: Loss=0.4911 (C:0.4911, R:0.0105)
Batch  75/537: Loss=0.4946 (C:0.4946, R:0.0105)
Batch 100/537: Loss=0.5263 (C:0.5263, R:0.0105)
Batch 125/537: Loss=0.4962 (C:0.4962, R:0.0105)
Batch 150/537: Loss=0.5203 (C:0.5203, R:0.0105)
Batch 175/537: Loss=0.5625 (C:0.5625, R:0.0105)
Batch 200/537: Loss=0.4996 (C:0.4996, R:0.0105)
Batch 225/537: Loss=0.4976 (C:0.4976, R:0.0105)
Batch 250/537: Loss=0.4910 (C:0.4910, R:0.0105)
Batch 275/537: Loss=0.5054 (C:0.5054, R:0.0105)
Batch 300/537: Loss=0.5165 (C:0.5165, R:0.0105)
Batch 325/537: Loss=0.5147 (C:0.5147, R:0.0105)
Batch 350/537: Loss=0.5327 (C:0.5327, R:0.0105)
Batch 375/537: Loss=0.5314 (C:0.5314, R:0.0105)
Batch 400/537: Loss=0.4949 (C:0.4949, R:0.0106)
Batch 425/537: Loss=0.5149 (C:0.5149, R:0.0105)
Batch 450/537: Loss=0.5155 (C:0.5155, R:0.0105)
Batch 475/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 500/537: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 525/537: Loss=0.5339 (C:0.5339, R:0.0105)

============================================================
Epoch 78/300 completed in 21.5s
Train: Loss=0.5161 (C:0.5161, R:0.0105) Ratio=5.69x
Val:   Loss=0.7007 (C:0.7007, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 79
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.289 ± 0.579
    Neg distances: 2.666 ± 1.097
    Separation ratio: 9.22x
    Gap: -4.434
    ✅ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=0.5031 (C:0.5031, R:0.0105)
Batch  25/537: Loss=0.5264 (C:0.5264, R:0.0105)
Batch  50/537: Loss=0.4866 (C:0.4866, R:0.0105)
Batch  75/537: Loss=0.4825 (C:0.4825, R:0.0105)
Batch 100/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 125/537: Loss=0.4945 (C:0.4945, R:0.0105)
Batch 150/537: Loss=0.4900 (C:0.4900, R:0.0105)
Batch 175/537: Loss=0.5001 (C:0.5001, R:0.0105)
Batch 200/537: Loss=0.4899 (C:0.4899, R:0.0105)
Batch 225/537: Loss=0.5078 (C:0.5078, R:0.0105)
Batch 250/537: Loss=0.5333 (C:0.5333, R:0.0105)
Batch 275/537: Loss=0.5262 (C:0.5262, R:0.0106)
Batch 300/537: Loss=0.4839 (C:0.4839, R:0.0105)
Batch 325/537: Loss=0.5387 (C:0.5387, R:0.0105)
Batch 350/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 375/537: Loss=0.5213 (C:0.5213, R:0.0105)
Batch 400/537: Loss=0.5081 (C:0.5081, R:0.0105)
Batch 425/537: Loss=0.5018 (C:0.5018, R:0.0105)
Batch 450/537: Loss=0.5080 (C:0.5080, R:0.0105)
Batch 475/537: Loss=0.5070 (C:0.5070, R:0.0105)
Batch 500/537: Loss=0.5293 (C:0.5293, R:0.0105)
Batch 525/537: Loss=0.5452 (C:0.5452, R:0.0105)

============================================================
Epoch 79/300 completed in 27.5s
Train: Loss=0.5119 (C:0.5119, R:0.0105) Ratio=5.61x
Val:   Loss=0.7128 (C:0.7128, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=0.5096 (C:0.5096, R:0.0105)
Batch  25/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch  50/537: Loss=0.5111 (C:0.5111, R:0.0105)
Batch  75/537: Loss=0.5088 (C:0.5088, R:0.0106)
Batch 100/537: Loss=0.5276 (C:0.5276, R:0.0105)
Batch 125/537: Loss=0.5131 (C:0.5131, R:0.0105)
Batch 150/537: Loss=0.5210 (C:0.5210, R:0.0105)
Batch 175/537: Loss=0.5237 (C:0.5237, R:0.0106)
Batch 200/537: Loss=0.4827 (C:0.4827, R:0.0105)
Batch 225/537: Loss=0.4961 (C:0.4961, R:0.0105)
Batch 250/537: Loss=0.5153 (C:0.5153, R:0.0105)
Batch 275/537: Loss=0.5265 (C:0.5265, R:0.0105)
Batch 300/537: Loss=0.5023 (C:0.5023, R:0.0105)
Batch 325/537: Loss=0.5031 (C:0.5031, R:0.0106)
Batch 350/537: Loss=0.4863 (C:0.4863, R:0.0105)
Batch 375/537: Loss=0.5222 (C:0.5222, R:0.0105)
Batch 400/537: Loss=0.5015 (C:0.5015, R:0.0105)
Batch 425/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 450/537: Loss=0.4947 (C:0.4947, R:0.0105)
Batch 475/537: Loss=0.4947 (C:0.4947, R:0.0105)
Batch 500/537: Loss=0.4945 (C:0.4945, R:0.0105)
Batch 525/537: Loss=0.5442 (C:0.5442, R:0.0105)

============================================================
Epoch 80/300 completed in 21.2s
Train: Loss=0.5100 (C:0.5100, R:0.0105) Ratio=5.59x
Val:   Loss=0.7090 (C:0.7090, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 4 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=0.5053 (C:0.5053, R:0.0105)
Batch  25/537: Loss=0.4909 (C:0.4909, R:0.0105)
Batch  50/537: Loss=0.4766 (C:0.4766, R:0.0105)
Batch  75/537: Loss=0.5033 (C:0.5033, R:0.0105)
Batch 100/537: Loss=0.5125 (C:0.5125, R:0.0105)
Batch 125/537: Loss=0.5121 (C:0.5121, R:0.0105)
Batch 150/537: Loss=0.5123 (C:0.5123, R:0.0105)
Batch 175/537: Loss=0.5294 (C:0.5294, R:0.0105)
Batch 200/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 225/537: Loss=0.4970 (C:0.4970, R:0.0105)
Batch 250/537: Loss=0.4948 (C:0.4948, R:0.0105)
Batch 275/537: Loss=0.4932 (C:0.4932, R:0.0105)
Batch 300/537: Loss=0.4984 (C:0.4984, R:0.0105)
Batch 325/537: Loss=0.5309 (C:0.5309, R:0.0105)
Batch 350/537: Loss=0.5219 (C:0.5219, R:0.0105)
Batch 375/537: Loss=0.4907 (C:0.4907, R:0.0105)
Batch 400/537: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 425/537: Loss=0.5270 (C:0.5270, R:0.0105)
Batch 450/537: Loss=0.5259 (C:0.5259, R:0.0105)
Batch 475/537: Loss=0.5338 (C:0.5338, R:0.0105)
Batch 500/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch 525/537: Loss=0.4914 (C:0.4914, R:0.0105)

============================================================
Epoch 81/300 completed in 21.3s
Train: Loss=0.5103 (C:0.5103, R:0.0105) Ratio=5.65x
Val:   Loss=0.7062 (C:0.7062, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 82
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.279 ± 0.550
    Neg distances: 2.665 ± 1.091
    Separation ratio: 9.54x
    Gap: -4.437
    ✅ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=0.4994 (C:0.4994, R:0.0105)
Batch  25/537: Loss=0.4627 (C:0.4627, R:0.0105)
Batch  50/537: Loss=0.4607 (C:0.4607, R:0.0105)
Batch  75/537: Loss=0.5171 (C:0.5171, R:0.0105)
Batch 100/537: Loss=0.4780 (C:0.4780, R:0.0105)
Batch 125/537: Loss=0.4754 (C:0.4754, R:0.0105)
Batch 150/537: Loss=0.4840 (C:0.4840, R:0.0105)
Batch 175/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 200/537: Loss=0.4900 (C:0.4900, R:0.0105)
Batch 225/537: Loss=0.5158 (C:0.5158, R:0.0105)
Batch 250/537: Loss=0.4659 (C:0.4659, R:0.0105)
Batch 275/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch 300/537: Loss=0.4547 (C:0.4547, R:0.0105)
Batch 325/537: Loss=0.5421 (C:0.5421, R:0.0105)
Batch 350/537: Loss=0.5152 (C:0.5152, R:0.0105)
Batch 375/537: Loss=0.5149 (C:0.5149, R:0.0105)
Batch 400/537: Loss=0.5096 (C:0.5096, R:0.0105)
Batch 425/537: Loss=0.4863 (C:0.4863, R:0.0105)
Batch 450/537: Loss=0.4893 (C:0.4893, R:0.0105)
Batch 475/537: Loss=0.4877 (C:0.4877, R:0.0105)
Batch 500/537: Loss=0.4871 (C:0.4871, R:0.0105)
Batch 525/537: Loss=0.4819 (C:0.4819, R:0.0105)

============================================================
Epoch 82/300 completed in 27.0s
Train: Loss=0.5010 (C:0.5010, R:0.0105) Ratio=5.76x
Val:   Loss=0.7000 (C:0.7000, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=0.4980 (C:0.4980, R:0.0105)
Batch  25/537: Loss=0.4773 (C:0.4773, R:0.0105)
Batch  50/537: Loss=0.4625 (C:0.4625, R:0.0105)
Batch  75/537: Loss=0.4936 (C:0.4936, R:0.0105)
Batch 100/537: Loss=0.5054 (C:0.5054, R:0.0105)
Batch 125/537: Loss=0.4695 (C:0.4695, R:0.0105)
Batch 150/537: Loss=0.4794 (C:0.4794, R:0.0105)
Batch 175/537: Loss=0.5208 (C:0.5208, R:0.0106)
Batch 200/537: Loss=0.5298 (C:0.5298, R:0.0105)
Batch 225/537: Loss=0.4898 (C:0.4898, R:0.0105)
Batch 250/537: Loss=0.5093 (C:0.5093, R:0.0105)
Batch 275/537: Loss=0.4853 (C:0.4853, R:0.0105)
Batch 300/537: Loss=0.5137 (C:0.5137, R:0.0106)
Batch 325/537: Loss=0.4980 (C:0.4980, R:0.0106)
Batch 350/537: Loss=0.5023 (C:0.5023, R:0.0105)
Batch 375/537: Loss=0.4780 (C:0.4780, R:0.0105)
Batch 400/537: Loss=0.4937 (C:0.4937, R:0.0105)
Batch 425/537: Loss=0.5139 (C:0.5139, R:0.0105)
Batch 450/537: Loss=0.5195 (C:0.5195, R:0.0105)
Batch 475/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 500/537: Loss=0.4843 (C:0.4843, R:0.0105)
Batch 525/537: Loss=0.5074 (C:0.5074, R:0.0105)

============================================================
Epoch 83/300 completed in 21.1s
Train: Loss=0.5007 (C:0.5007, R:0.0105) Ratio=5.77x
Val:   Loss=0.6983 (C:0.6983, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6983)
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=0.4786 (C:0.4786, R:0.0105)
Batch  25/537: Loss=0.4691 (C:0.4691, R:0.0105)
Batch  50/537: Loss=0.5012 (C:0.5012, R:0.0105)
Batch  75/537: Loss=0.4594 (C:0.4594, R:0.0105)
Batch 100/537: Loss=0.5200 (C:0.5200, R:0.0105)
Batch 125/537: Loss=0.4893 (C:0.4893, R:0.0105)
Batch 150/537: Loss=0.5101 (C:0.5101, R:0.0105)
Batch 175/537: Loss=0.4528 (C:0.4528, R:0.0105)
Batch 200/537: Loss=0.5146 (C:0.5146, R:0.0105)
Batch 225/537: Loss=0.4921 (C:0.4921, R:0.0105)
Batch 250/537: Loss=0.4817 (C:0.4817, R:0.0105)
Batch 275/537: Loss=0.5073 (C:0.5073, R:0.0105)
Batch 300/537: Loss=0.4974 (C:0.4974, R:0.0105)
Batch 325/537: Loss=0.4975 (C:0.4975, R:0.0105)
Batch 350/537: Loss=0.4715 (C:0.4715, R:0.0105)
Batch 375/537: Loss=0.5023 (C:0.5023, R:0.0105)
Batch 400/537: Loss=0.5189 (C:0.5189, R:0.0105)
Batch 425/537: Loss=0.4767 (C:0.4767, R:0.0106)
Batch 450/537: Loss=0.5396 (C:0.5396, R:0.0105)
Batch 475/537: Loss=0.5135 (C:0.5135, R:0.0105)
Batch 500/537: Loss=0.4931 (C:0.4931, R:0.0105)
Batch 525/537: Loss=0.4965 (C:0.4965, R:0.0105)

============================================================
Epoch 84/300 completed in 21.1s
Train: Loss=0.4990 (C:0.4990, R:0.0105) Ratio=5.76x
Val:   Loss=0.6915 (C:0.6915, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6915)
============================================================

🌍 Updating global dataset at epoch 85
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.291 ± 0.565
    Neg distances: 2.702 ± 1.115
    Separation ratio: 9.29x
    Gap: -4.526
    ✅ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=0.5335 (C:0.5335, R:0.0105)
Batch  25/537: Loss=0.4743 (C:0.4743, R:0.0105)
Batch  50/537: Loss=0.5003 (C:0.5003, R:0.0105)
Batch  75/537: Loss=0.5175 (C:0.5175, R:0.0105)
Batch 100/537: Loss=0.4705 (C:0.4705, R:0.0105)
Batch 125/537: Loss=0.5155 (C:0.5155, R:0.0105)
Batch 150/537: Loss=0.4784 (C:0.4784, R:0.0105)
Batch 175/537: Loss=0.4958 (C:0.4958, R:0.0105)
Batch 200/537: Loss=0.4715 (C:0.4715, R:0.0105)
Batch 225/537: Loss=0.5089 (C:0.5089, R:0.0105)
Batch 250/537: Loss=0.5198 (C:0.5198, R:0.0105)
Batch 275/537: Loss=0.4797 (C:0.4797, R:0.0106)
Batch 300/537: Loss=0.5398 (C:0.5398, R:0.0106)
Batch 325/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 350/537: Loss=0.4775 (C:0.4775, R:0.0105)
Batch 375/537: Loss=0.5095 (C:0.5095, R:0.0105)
Batch 400/537: Loss=0.4929 (C:0.4929, R:0.0105)
Batch 425/537: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 450/537: Loss=0.5170 (C:0.5170, R:0.0106)
Batch 475/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch 500/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch 525/537: Loss=0.5476 (C:0.5476, R:0.0105)

============================================================
Epoch 85/300 completed in 26.8s
Train: Loss=0.5065 (C:0.5065, R:0.0105) Ratio=5.83x
Val:   Loss=0.7090 (C:0.7090, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=0.5086 (C:0.5086, R:0.0105)
Batch  25/537: Loss=0.5212 (C:0.5212, R:0.0105)
Batch  50/537: Loss=0.4798 (C:0.4798, R:0.0105)
Batch  75/537: Loss=0.4568 (C:0.4568, R:0.0105)
Batch 100/537: Loss=0.5169 (C:0.5169, R:0.0105)
Batch 125/537: Loss=0.5080 (C:0.5080, R:0.0105)
Batch 150/537: Loss=0.5403 (C:0.5403, R:0.0105)
Batch 175/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 200/537: Loss=0.4802 (C:0.4802, R:0.0105)
Batch 225/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch 250/537: Loss=0.5384 (C:0.5384, R:0.0105)
Batch 275/537: Loss=0.5397 (C:0.5397, R:0.0105)
Batch 300/537: Loss=0.4830 (C:0.4830, R:0.0105)
Batch 325/537: Loss=0.5489 (C:0.5489, R:0.0105)
Batch 350/537: Loss=0.4849 (C:0.4849, R:0.0105)
Batch 375/537: Loss=0.4844 (C:0.4844, R:0.0105)
Batch 400/537: Loss=0.4986 (C:0.4986, R:0.0105)
Batch 425/537: Loss=0.4915 (C:0.4915, R:0.0106)
Batch 450/537: Loss=0.5279 (C:0.5279, R:0.0105)
Batch 475/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 500/537: Loss=0.5009 (C:0.5009, R:0.0105)
Batch 525/537: Loss=0.4959 (C:0.4959, R:0.0105)

============================================================
Epoch 86/300 completed in 21.1s
Train: Loss=0.5057 (C:0.5057, R:0.0105) Ratio=5.77x
Val:   Loss=0.7034 (C:0.7034, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=0.4888 (C:0.4888, R:0.0105)
Batch  25/537: Loss=0.5048 (C:0.5048, R:0.0105)
Batch  50/537: Loss=0.4698 (C:0.4698, R:0.0105)
Batch  75/537: Loss=0.4793 (C:0.4793, R:0.0105)
Batch 100/537: Loss=0.4838 (C:0.4838, R:0.0105)
Batch 125/537: Loss=0.4644 (C:0.4644, R:0.0105)
Batch 150/537: Loss=0.4773 (C:0.4773, R:0.0105)
Batch 175/537: Loss=0.5024 (C:0.5024, R:0.0105)
Batch 200/537: Loss=0.5422 (C:0.5422, R:0.0105)
Batch 225/537: Loss=0.4874 (C:0.4874, R:0.0105)
Batch 250/537: Loss=0.5184 (C:0.5184, R:0.0105)
Batch 275/537: Loss=0.4984 (C:0.4984, R:0.0105)
Batch 300/537: Loss=0.5095 (C:0.5095, R:0.0105)
Batch 325/537: Loss=0.5206 (C:0.5206, R:0.0105)
Batch 350/537: Loss=0.5320 (C:0.5320, R:0.0105)
Batch 375/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch 400/537: Loss=0.4761 (C:0.4761, R:0.0105)
Batch 425/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 450/537: Loss=0.5093 (C:0.5093, R:0.0105)
Batch 475/537: Loss=0.5418 (C:0.5418, R:0.0105)
Batch 500/537: Loss=0.5317 (C:0.5317, R:0.0105)
Batch 525/537: Loss=0.5273 (C:0.5273, R:0.0105)

============================================================
Epoch 87/300 completed in 21.0s
Train: Loss=0.5057 (C:0.5057, R:0.0105) Ratio=5.82x
Val:   Loss=0.7010 (C:0.7010, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 88
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.277 ± 0.594
    Neg distances: 2.756 ± 1.125
    Separation ratio: 9.95x
    Gap: -4.551
    ✅ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=0.4824 (C:0.4824, R:0.0105)
Batch  25/537: Loss=0.4582 (C:0.4582, R:0.0105)
Batch  50/537: Loss=0.5073 (C:0.5073, R:0.0105)
Batch  75/537: Loss=0.4851 (C:0.4851, R:0.0105)
Batch 100/537: Loss=0.4804 (C:0.4804, R:0.0105)
Batch 125/537: Loss=0.5218 (C:0.5218, R:0.0105)
Batch 150/537: Loss=0.4913 (C:0.4913, R:0.0105)
Batch 175/537: Loss=0.4715 (C:0.4715, R:0.0105)
Batch 200/537: Loss=0.4951 (C:0.4951, R:0.0105)
Batch 225/537: Loss=0.4954 (C:0.4954, R:0.0105)
Batch 250/537: Loss=0.4938 (C:0.4938, R:0.0105)
Batch 275/537: Loss=0.4652 (C:0.4652, R:0.0105)
Batch 300/537: Loss=0.4832 (C:0.4832, R:0.0105)
Batch 325/537: Loss=0.4749 (C:0.4749, R:0.0105)
Batch 350/537: Loss=0.5002 (C:0.5002, R:0.0105)
Batch 375/537: Loss=0.4662 (C:0.4662, R:0.0105)
Batch 400/537: Loss=0.4486 (C:0.4486, R:0.0105)
Batch 425/537: Loss=0.5005 (C:0.5005, R:0.0105)
Batch 450/537: Loss=0.4580 (C:0.4580, R:0.0105)
Batch 475/537: Loss=0.4893 (C:0.4893, R:0.0105)
Batch 500/537: Loss=0.4756 (C:0.4756, R:0.0105)
Batch 525/537: Loss=0.5037 (C:0.5037, R:0.0105)

============================================================
Epoch 88/300 completed in 27.3s
Train: Loss=0.4895 (C:0.4895, R:0.0105) Ratio=5.83x
Val:   Loss=0.6884 (C:0.6884, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6884)
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=0.4926 (C:0.4926, R:0.0105)
Batch  25/537: Loss=0.4988 (C:0.4988, R:0.0105)
Batch  50/537: Loss=0.4975 (C:0.4975, R:0.0106)
Batch  75/537: Loss=0.4908 (C:0.4908, R:0.0105)
Batch 100/537: Loss=0.4784 (C:0.4784, R:0.0105)
Batch 125/537: Loss=0.4751 (C:0.4751, R:0.0105)
Batch 150/537: Loss=0.5123 (C:0.5123, R:0.0105)
Batch 175/537: Loss=0.4991 (C:0.4991, R:0.0105)
Batch 200/537: Loss=0.4990 (C:0.4990, R:0.0105)
Batch 225/537: Loss=0.4805 (C:0.4805, R:0.0105)
Batch 250/537: Loss=0.4681 (C:0.4681, R:0.0105)
Batch 275/537: Loss=0.5262 (C:0.5262, R:0.0105)
Batch 300/537: Loss=0.4777 (C:0.4777, R:0.0105)
Batch 325/537: Loss=0.5178 (C:0.5178, R:0.0105)
Batch 350/537: Loss=0.5021 (C:0.5021, R:0.0105)
Batch 375/537: Loss=0.4778 (C:0.4778, R:0.0105)
Batch 400/537: Loss=0.4793 (C:0.4793, R:0.0105)
Batch 425/537: Loss=0.4593 (C:0.4593, R:0.0105)
Batch 450/537: Loss=0.5146 (C:0.5146, R:0.0105)
Batch 475/537: Loss=0.4653 (C:0.4653, R:0.0105)
Batch 500/537: Loss=0.5215 (C:0.5215, R:0.0105)
Batch 525/537: Loss=0.5138 (C:0.5138, R:0.0105)

============================================================
Epoch 89/300 completed in 21.2s
Train: Loss=0.4909 (C:0.4909, R:0.0105) Ratio=5.80x
Val:   Loss=0.6914 (C:0.6914, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 90 Training
----------------------------------------
Batch   0/537: Loss=0.4636 (C:0.4636, R:0.0105)
Batch  25/537: Loss=0.4660 (C:0.4660, R:0.0105)
Batch  50/537: Loss=0.4905 (C:0.4905, R:0.0105)
Batch  75/537: Loss=0.4930 (C:0.4930, R:0.0105)
Batch 100/537: Loss=0.4891 (C:0.4891, R:0.0105)
Batch 125/537: Loss=0.5154 (C:0.5154, R:0.0105)
Batch 150/537: Loss=0.4988 (C:0.4988, R:0.0105)
Batch 175/537: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 200/537: Loss=0.4905 (C:0.4905, R:0.0105)
Batch 225/537: Loss=0.4715 (C:0.4715, R:0.0105)
Batch 250/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch 275/537: Loss=0.4816 (C:0.4816, R:0.0105)
Batch 300/537: Loss=0.4997 (C:0.4997, R:0.0105)
Batch 325/537: Loss=0.4682 (C:0.4682, R:0.0105)
Batch 350/537: Loss=0.4845 (C:0.4845, R:0.0105)
Batch 375/537: Loss=0.4767 (C:0.4767, R:0.0105)
Batch 400/537: Loss=0.5193 (C:0.5193, R:0.0105)
Batch 425/537: Loss=0.4845 (C:0.4845, R:0.0105)
Batch 450/537: Loss=0.5191 (C:0.5191, R:0.0105)
Batch 475/537: Loss=0.4733 (C:0.4733, R:0.0105)
Batch 500/537: Loss=0.4944 (C:0.4944, R:0.0105)
Batch 525/537: Loss=0.4754 (C:0.4754, R:0.0105)

============================================================
Epoch 90/300 completed in 21.3s
Train: Loss=0.4881 (C:0.4881, R:0.0105) Ratio=5.81x
Val:   Loss=0.7014 (C:0.7014, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 91
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.283 ± 0.562
    Neg distances: 2.728 ± 1.121
    Separation ratio: 9.63x
    Gap: -4.523
    ✅ Excellent global separation!

Epoch 91 Training
----------------------------------------
Batch   0/537: Loss=0.4713 (C:0.4713, R:0.0105)
Batch  25/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch  50/537: Loss=0.5006 (C:0.5006, R:0.0105)
Batch  75/537: Loss=0.4745 (C:0.4745, R:0.0105)
Batch 100/537: Loss=0.4869 (C:0.4869, R:0.0105)
Batch 125/537: Loss=0.5153 (C:0.5153, R:0.0105)
Batch 150/537: Loss=0.4464 (C:0.4464, R:0.0105)
Batch 175/537: Loss=0.5121 (C:0.5121, R:0.0105)
Batch 200/537: Loss=0.4644 (C:0.4644, R:0.0105)
Batch 225/537: Loss=0.4844 (C:0.4844, R:0.0105)
Batch 250/537: Loss=0.5271 (C:0.5271, R:0.0105)
Batch 275/537: Loss=0.5075 (C:0.5075, R:0.0105)
Batch 300/537: Loss=0.4783 (C:0.4783, R:0.0105)
Batch 325/537: Loss=0.4747 (C:0.4747, R:0.0106)
Batch 350/537: Loss=0.5086 (C:0.5086, R:0.0105)
Batch 375/537: Loss=0.4957 (C:0.4957, R:0.0105)
Batch 400/537: Loss=0.5011 (C:0.5011, R:0.0105)
Batch 425/537: Loss=0.5164 (C:0.5164, R:0.0105)
Batch 450/537: Loss=0.5013 (C:0.5013, R:0.0105)
Batch 475/537: Loss=0.5017 (C:0.5017, R:0.0105)
Batch 500/537: Loss=0.5114 (C:0.5114, R:0.0105)
Batch 525/537: Loss=0.5198 (C:0.5198, R:0.0105)

============================================================
Epoch 91/300 completed in 27.1s
Train: Loss=0.4956 (C:0.4956, R:0.0105) Ratio=5.82x
Val:   Loss=0.6954 (C:0.6954, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 92 Training
----------------------------------------
Batch   0/537: Loss=0.4681 (C:0.4681, R:0.0105)
Batch  25/537: Loss=0.4953 (C:0.4953, R:0.0105)
Batch  50/537: Loss=0.4896 (C:0.4896, R:0.0105)
Batch  75/537: Loss=0.4734 (C:0.4734, R:0.0105)
Batch 100/537: Loss=0.4514 (C:0.4514, R:0.0105)
Batch 125/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 150/537: Loss=0.4995 (C:0.4995, R:0.0105)
Batch 175/537: Loss=0.4742 (C:0.4742, R:0.0105)
Batch 200/537: Loss=0.4849 (C:0.4849, R:0.0105)
Batch 225/537: Loss=0.4852 (C:0.4852, R:0.0105)
Batch 250/537: Loss=0.5149 (C:0.5149, R:0.0105)
Batch 275/537: Loss=0.5099 (C:0.5099, R:0.0105)
Batch 300/537: Loss=0.4850 (C:0.4850, R:0.0105)
Batch 325/537: Loss=0.5110 (C:0.5110, R:0.0105)
Batch 350/537: Loss=0.5041 (C:0.5041, R:0.0105)
Batch 375/537: Loss=0.5161 (C:0.5161, R:0.0105)
Batch 400/537: Loss=0.4916 (C:0.4916, R:0.0105)
Batch 425/537: Loss=0.5147 (C:0.5147, R:0.0105)
Batch 450/537: Loss=0.4960 (C:0.4960, R:0.0105)
Batch 475/537: Loss=0.4851 (C:0.4851, R:0.0105)
Batch 500/537: Loss=0.4709 (C:0.4709, R:0.0105)
Batch 525/537: Loss=0.4726 (C:0.4726, R:0.0105)

============================================================
Epoch 92/300 completed in 21.1s
Train: Loss=0.4954 (C:0.4954, R:0.0105) Ratio=5.99x
Val:   Loss=0.7017 (C:0.7017, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 93 Training
----------------------------------------
Batch   0/537: Loss=0.4903 (C:0.4903, R:0.0105)
Batch  25/537: Loss=0.4488 (C:0.4488, R:0.0105)
Batch  50/537: Loss=0.5300 (C:0.5300, R:0.0105)
Batch  75/537: Loss=0.4690 (C:0.4690, R:0.0105)
Batch 100/537: Loss=0.5086 (C:0.5086, R:0.0105)
Batch 125/537: Loss=0.4918 (C:0.4918, R:0.0105)
Batch 150/537: Loss=0.5170 (C:0.5170, R:0.0105)
Batch 175/537: Loss=0.5684 (C:0.5684, R:0.0105)
Batch 200/537: Loss=0.4828 (C:0.4828, R:0.0105)
Batch 225/537: Loss=0.5131 (C:0.5131, R:0.0105)
Batch 250/537: Loss=0.5217 (C:0.5217, R:0.0106)
Batch 275/537: Loss=0.4785 (C:0.4785, R:0.0105)
Batch 300/537: Loss=0.5106 (C:0.5106, R:0.0105)
Batch 325/537: Loss=0.5132 (C:0.5132, R:0.0105)
Batch 350/537: Loss=0.5054 (C:0.5054, R:0.0105)
Batch 375/537: Loss=0.5021 (C:0.5021, R:0.0105)
Batch 400/537: Loss=0.5063 (C:0.5063, R:0.0105)
Batch 425/537: Loss=0.5067 (C:0.5067, R:0.0105)
Batch 450/537: Loss=0.4818 (C:0.4818, R:0.0105)
Batch 475/537: Loss=0.4613 (C:0.4613, R:0.0105)
Batch 500/537: Loss=0.4650 (C:0.4650, R:0.0105)
Batch 525/537: Loss=0.4927 (C:0.4927, R:0.0105)

============================================================
Epoch 93/300 completed in 21.1s
Train: Loss=0.4937 (C:0.4937, R:0.0105) Ratio=5.81x
Val:   Loss=0.7077 (C:0.7077, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 94
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.276 ± 0.560
    Neg distances: 2.732 ± 1.115
    Separation ratio: 9.88x
    Gap: -4.633
    ✅ Excellent global separation!

Epoch 94 Training
----------------------------------------
Batch   0/537: Loss=0.4570 (C:0.4570, R:0.0105)
Batch  25/537: Loss=0.5048 (C:0.5048, R:0.0105)
Batch  50/537: Loss=0.4687 (C:0.4687, R:0.0105)
Batch  75/537: Loss=0.4731 (C:0.4731, R:0.0105)
Batch 100/537: Loss=0.4978 (C:0.4978, R:0.0105)
Batch 125/537: Loss=0.4726 (C:0.4726, R:0.0105)
Batch 150/537: Loss=0.4749 (C:0.4749, R:0.0105)
Batch 175/537: Loss=0.4557 (C:0.4557, R:0.0105)
Batch 200/537: Loss=0.5053 (C:0.5053, R:0.0105)
Batch 225/537: Loss=0.4727 (C:0.4727, R:0.0105)
Batch 250/537: Loss=0.4504 (C:0.4504, R:0.0105)
Batch 275/537: Loss=0.4932 (C:0.4932, R:0.0105)
Batch 300/537: Loss=0.5046 (C:0.5046, R:0.0105)
Batch 325/537: Loss=0.4982 (C:0.4982, R:0.0105)
Batch 350/537: Loss=0.4835 (C:0.4835, R:0.0105)
Batch 375/537: Loss=0.4902 (C:0.4902, R:0.0105)
Batch 400/537: Loss=0.4892 (C:0.4892, R:0.0105)
Batch 425/537: Loss=0.4525 (C:0.4525, R:0.0105)
Batch 450/537: Loss=0.4692 (C:0.4692, R:0.0105)
Batch 475/537: Loss=0.4652 (C:0.4652, R:0.0105)
Batch 500/537: Loss=0.4782 (C:0.4782, R:0.0105)
Batch 525/537: Loss=0.5230 (C:0.5230, R:0.0105)

============================================================
Epoch 94/300 completed in 27.1s
Train: Loss=0.4885 (C:0.4885, R:0.0105) Ratio=5.95x
Val:   Loss=0.7014 (C:0.7014, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 95 Training
----------------------------------------
Batch   0/537: Loss=0.4998 (C:0.4998, R:0.0105)
Batch  25/537: Loss=0.4702 (C:0.4702, R:0.0105)
Batch  50/537: Loss=0.4762 (C:0.4762, R:0.0105)
Batch  75/537: Loss=0.4652 (C:0.4652, R:0.0105)
Batch 100/537: Loss=0.4468 (C:0.4468, R:0.0105)
Batch 125/537: Loss=0.4743 (C:0.4743, R:0.0105)
Batch 150/537: Loss=0.4572 (C:0.4572, R:0.0105)
Batch 175/537: Loss=0.4617 (C:0.4617, R:0.0105)
Batch 200/537: Loss=0.4969 (C:0.4969, R:0.0105)
Batch 225/537: Loss=0.4867 (C:0.4867, R:0.0105)
Batch 250/537: Loss=0.5297 (C:0.5297, R:0.0105)
Batch 275/537: Loss=0.4649 (C:0.4649, R:0.0105)
Batch 300/537: Loss=0.4521 (C:0.4521, R:0.0105)
Batch 325/537: Loss=0.4655 (C:0.4655, R:0.0105)
Batch 350/537: Loss=0.5158 (C:0.5158, R:0.0105)
Batch 375/537: Loss=0.4589 (C:0.4589, R:0.0105)
Batch 400/537: Loss=0.5168 (C:0.5168, R:0.0105)
Batch 425/537: Loss=0.4704 (C:0.4704, R:0.0105)
Batch 450/537: Loss=0.4910 (C:0.4910, R:0.0105)
Batch 475/537: Loss=0.5082 (C:0.5082, R:0.0105)
Batch 500/537: Loss=0.5040 (C:0.5040, R:0.0105)
Batch 525/537: Loss=0.5078 (C:0.5078, R:0.0105)

============================================================
Epoch 95/300 completed in 21.1s
Train: Loss=0.4865 (C:0.4865, R:0.0105) Ratio=5.91x
Val:   Loss=0.6968 (C:0.6968, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 96 Training
----------------------------------------
Batch   0/537: Loss=0.4840 (C:0.4840, R:0.0105)
Batch  25/537: Loss=0.4881 (C:0.4881, R:0.0105)
Batch  50/537: Loss=0.4360 (C:0.4360, R:0.0105)
Batch  75/537: Loss=0.4949 (C:0.4949, R:0.0105)
Batch 100/537: Loss=0.5033 (C:0.5033, R:0.0105)
Batch 125/537: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 150/537: Loss=0.4823 (C:0.4823, R:0.0105)
Batch 175/537: Loss=0.4707 (C:0.4707, R:0.0105)
Batch 200/537: Loss=0.4871 (C:0.4871, R:0.0105)
Batch 225/537: Loss=0.4738 (C:0.4738, R:0.0105)
Batch 250/537: Loss=0.5049 (C:0.5049, R:0.0105)
Batch 275/537: Loss=0.4841 (C:0.4841, R:0.0105)
Batch 300/537: Loss=0.5043 (C:0.5043, R:0.0105)
Batch 325/537: Loss=0.5055 (C:0.5055, R:0.0105)
Batch 350/537: Loss=0.4880 (C:0.4880, R:0.0105)
Batch 375/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 400/537: Loss=0.5052 (C:0.5052, R:0.0105)
Batch 425/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 450/537: Loss=0.4874 (C:0.4874, R:0.0105)
Batch 475/537: Loss=0.4910 (C:0.4910, R:0.0105)
Batch 500/537: Loss=0.4568 (C:0.4568, R:0.0105)
Batch 525/537: Loss=0.5087 (C:0.5087, R:0.0105)

============================================================
Epoch 96/300 completed in 21.3s
Train: Loss=0.4859 (C:0.4859, R:0.0105) Ratio=5.90x
Val:   Loss=0.6893 (C:0.6893, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 96 epochs
Best model was at epoch 88 with Val Loss: 0.6884

Global Dataset Training Completed!
Best epoch: 88
Best validation loss: 0.6884
Final separation ratios: Train=5.90x, Val=3.11x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4534
  Adjusted Rand Score: 0.5214
  Clustering Accuracy: 0.8087
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8142
  Per-class F1: [0.8264770611139326, 0.7499202551834131, 0.8675400291120815]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.799 ± 0.972
  Negative distances: 2.407 ± 1.289
  Separation ratio: 3.01x
  Gap: -4.570
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4534
  Clustering Accuracy: 0.8087
  Adjusted Rand Score: 0.5214

Classification Performance:
  Accuracy: 0.8142

Separation Quality:
  Separation Ratio: 3.01x
  Gap: -4.570
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758/results/evaluation_results_20250714_211537.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758/results/evaluation_results_20250714_211537.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1020_20250714_203758/final_results.json

Key Results:
  Separation ratio: 3.01x
  Perfect separation: False
  Classification accuracy: 0.8142
  Result: 0.8142% (improvement: +-80.86%)
  Cleaning up: coarse_lr2e-04_lat75_bs1020_20250714_203758

[10/12] Testing: coarse_lr2e-04_lat75_bs1536
  Learning rate: 0.0002
  Latent dim: 75
  Batch size: 1536
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 21:15:37.648814
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1536
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 356
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 6
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1536
  Balanced sampling: True
  Train batches: 356
  Val batches: 6
  Test batches: 7
Data loading completed!
  Train: 549367 samples, 356 batches
  Val: 9842 samples, 6 batches
  Test: 9824 samples, 7 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,876,555
Model created with 1,876,555 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0002)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,876,555
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.092 ± 0.011
    Neg distances: 0.092 ± 0.011
    Separation ratio: 1.00x
    Gap: -0.116
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/356: Loss=1.9999 (C:1.9999, R:0.0117)
Batch  25/356: Loss=1.9679 (C:1.9679, R:0.0113)
Batch  50/356: Loss=1.9525 (C:1.9525, R:0.0110)
Batch  75/356: Loss=1.9282 (C:1.9282, R:0.0108)
Batch 100/356: Loss=1.9170 (C:1.9170, R:0.0107)
Batch 125/356: Loss=1.9039 (C:1.9039, R:0.0106)
Batch 150/356: Loss=1.8813 (C:1.8813, R:0.0106)
Batch 175/356: Loss=1.8823 (C:1.8823, R:0.0105)
Batch 200/356: Loss=1.8595 (C:1.8595, R:0.0105)
Batch 225/356: Loss=1.8405 (C:1.8405, R:0.0105)
Batch 250/356: Loss=1.8530 (C:1.8530, R:0.0105)
Batch 275/356: Loss=1.8444 (C:1.8444, R:0.0105)
Batch 300/356: Loss=1.8427 (C:1.8427, R:0.0105)
Batch 325/356: Loss=1.8478 (C:1.8478, R:0.0105)
Batch 350/356: Loss=1.8574 (C:1.8574, R:0.0105)

============================================================
Epoch 1/300 completed in 26.8s
Train: Loss=1.8858 (C:1.8858, R:0.0107) Ratio=1.72x
Val:   Loss=1.8281 (C:1.8281, R:0.0104) Ratio=2.29x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8281)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/356: Loss=1.8316 (C:1.8316, R:0.0105)
Batch  25/356: Loss=1.8378 (C:1.8378, R:0.0105)
Batch  50/356: Loss=1.8219 (C:1.8219, R:0.0105)
Batch  75/356: Loss=1.8332 (C:1.8332, R:0.0105)
Batch 100/356: Loss=1.8411 (C:1.8411, R:0.0105)
Batch 125/356: Loss=1.8308 (C:1.8308, R:0.0105)
Batch 150/356: Loss=1.8269 (C:1.8269, R:0.0105)
Batch 175/356: Loss=1.8225 (C:1.8225, R:0.0105)
Batch 200/356: Loss=1.8174 (C:1.8174, R:0.0105)
Batch 225/356: Loss=1.8259 (C:1.8259, R:0.0105)
Batch 250/356: Loss=1.8138 (C:1.8138, R:0.0105)
Batch 275/356: Loss=1.8274 (C:1.8274, R:0.0105)
Batch 300/356: Loss=1.8136 (C:1.8136, R:0.0105)
Batch 325/356: Loss=1.8277 (C:1.8277, R:0.0105)
Batch 350/356: Loss=1.8222 (C:1.8222, R:0.0105)

============================================================
Epoch 2/300 completed in 20.6s
Train: Loss=1.8255 (C:1.8255, R:0.0105) Ratio=2.30x
Val:   Loss=1.8100 (C:1.8100, R:0.0104) Ratio=2.54x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8100)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/356: Loss=1.8147 (C:1.8147, R:0.0105)
Batch  25/356: Loss=1.8053 (C:1.8053, R:0.0105)
Batch  50/356: Loss=1.8094 (C:1.8094, R:0.0105)
Batch  75/356: Loss=1.8053 (C:1.8053, R:0.0105)
Batch 100/356: Loss=1.8150 (C:1.8150, R:0.0105)
Batch 125/356: Loss=1.8021 (C:1.8021, R:0.0105)
Batch 150/356: Loss=1.8160 (C:1.8160, R:0.0105)
Batch 175/356: Loss=1.8061 (C:1.8061, R:0.0105)
Batch 200/356: Loss=1.7972 (C:1.7972, R:0.0105)
Batch 225/356: Loss=1.8179 (C:1.8179, R:0.0105)
Batch 250/356: Loss=1.8191 (C:1.8191, R:0.0105)
Batch 275/356: Loss=1.7943 (C:1.7943, R:0.0105)
Batch 300/356: Loss=1.8061 (C:1.8061, R:0.0105)
Batch 325/356: Loss=1.7987 (C:1.7987, R:0.0105)
Batch 350/356: Loss=1.7940 (C:1.7940, R:0.0105)

============================================================
Epoch 3/300 completed in 21.2s
Train: Loss=1.8080 (C:1.8080, R:0.0105) Ratio=2.55x
Val:   Loss=1.7998 (C:1.7998, R:0.0104) Ratio=2.63x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.7998)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.560 ± 0.600
    Neg distances: 1.637 ± 0.906
    Separation ratio: 2.92x
    Gap: -3.556
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/356: Loss=1.1688 (C:1.1688, R:0.0105)
Batch  25/356: Loss=1.1967 (C:1.1967, R:0.0105)
Batch  50/356: Loss=1.1959 (C:1.1959, R:0.0105)
Batch  75/356: Loss=1.1957 (C:1.1957, R:0.0105)
Batch 100/356: Loss=1.2188 (C:1.2188, R:0.0105)
Batch 125/356: Loss=1.1513 (C:1.1513, R:0.0105)
Batch 150/356: Loss=1.1833 (C:1.1833, R:0.0105)
Batch 175/356: Loss=1.1730 (C:1.1730, R:0.0105)
Batch 200/356: Loss=1.1681 (C:1.1681, R:0.0105)
Batch 225/356: Loss=1.1820 (C:1.1820, R:0.0105)
Batch 250/356: Loss=1.1839 (C:1.1839, R:0.0105)
Batch 275/356: Loss=1.2068 (C:1.2068, R:0.0105)
Batch 300/356: Loss=1.1655 (C:1.1655, R:0.0105)
Batch 325/356: Loss=1.1924 (C:1.1924, R:0.0105)
Batch 350/356: Loss=1.2039 (C:1.2039, R:0.0105)

============================================================
Epoch 4/300 completed in 27.9s
Train: Loss=1.1833 (C:1.1833, R:0.0105) Ratio=2.65x
Val:   Loss=1.1836 (C:1.1836, R:0.0104) Ratio=2.70x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1836)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/356: Loss=1.1610 (C:1.1610, R:0.0105)
Batch  25/356: Loss=1.1331 (C:1.1331, R:0.0105)
Batch  50/356: Loss=1.1253 (C:1.1253, R:0.0106)
Batch  75/356: Loss=1.1469 (C:1.1469, R:0.0105)
Batch 100/356: Loss=1.1882 (C:1.1882, R:0.0105)
Batch 125/356: Loss=1.1470 (C:1.1470, R:0.0105)
Batch 150/356: Loss=1.1528 (C:1.1528, R:0.0105)
Batch 175/356: Loss=1.1820 (C:1.1820, R:0.0105)
Batch 200/356: Loss=1.1406 (C:1.1406, R:0.0105)
Batch 225/356: Loss=1.1605 (C:1.1605, R:0.0105)
Batch 250/356: Loss=1.1477 (C:1.1477, R:0.0105)
Batch 275/356: Loss=1.1557 (C:1.1557, R:0.0105)
Batch 300/356: Loss=1.1574 (C:1.1574, R:0.0105)
Batch 325/356: Loss=1.1856 (C:1.1856, R:0.0105)
Batch 350/356: Loss=1.1293 (C:1.1293, R:0.0105)

============================================================
Epoch 5/300 completed in 21.3s
Train: Loss=1.1556 (C:1.1556, R:0.0105) Ratio=2.84x
Val:   Loss=1.1625 (C:1.1625, R:0.0104) Ratio=2.82x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1625)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/356: Loss=1.1241 (C:1.1241, R:0.0105)
Batch  25/356: Loss=1.1518 (C:1.1518, R:0.0106)
Batch  50/356: Loss=1.1604 (C:1.1604, R:0.0105)
Batch  75/356: Loss=1.1125 (C:1.1125, R:0.0105)
Batch 100/356: Loss=1.1165 (C:1.1165, R:0.0105)
Batch 125/356: Loss=1.1469 (C:1.1469, R:0.0105)
Batch 150/356: Loss=1.1570 (C:1.1570, R:0.0105)
Batch 175/356: Loss=1.1266 (C:1.1266, R:0.0105)
Batch 200/356: Loss=1.1475 (C:1.1475, R:0.0105)
Batch 225/356: Loss=1.1261 (C:1.1261, R:0.0105)
Batch 250/356: Loss=1.1283 (C:1.1283, R:0.0105)
Batch 275/356: Loss=1.1345 (C:1.1345, R:0.0105)
Batch 300/356: Loss=1.1362 (C:1.1362, R:0.0105)
Batch 325/356: Loss=1.1330 (C:1.1330, R:0.0105)
Batch 350/356: Loss=1.1680 (C:1.1680, R:0.0105)

============================================================
Epoch 6/300 completed in 21.5s
Train: Loss=1.1389 (C:1.1389, R:0.0105) Ratio=3.02x
Val:   Loss=1.1542 (C:1.1542, R:0.0104) Ratio=2.86x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1542)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.492 ± 0.585
    Neg distances: 1.742 ± 0.905
    Separation ratio: 3.54x
    Gap: -3.322
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/356: Loss=1.0474 (C:1.0474, R:0.0105)
Batch  25/356: Loss=1.0683 (C:1.0683, R:0.0105)
Batch  50/356: Loss=1.0655 (C:1.0655, R:0.0105)
Batch  75/356: Loss=1.0514 (C:1.0514, R:0.0105)
Batch 100/356: Loss=1.0733 (C:1.0733, R:0.0105)
Batch 125/356: Loss=1.0546 (C:1.0546, R:0.0105)
Batch 150/356: Loss=1.0658 (C:1.0658, R:0.0105)
Batch 175/356: Loss=1.0608 (C:1.0608, R:0.0105)
Batch 200/356: Loss=1.0700 (C:1.0700, R:0.0105)
Batch 225/356: Loss=1.0631 (C:1.0631, R:0.0105)
Batch 250/356: Loss=1.0989 (C:1.0989, R:0.0105)
Batch 275/356: Loss=1.0723 (C:1.0723, R:0.0105)
Batch 300/356: Loss=1.0315 (C:1.0315, R:0.0105)
Batch 325/356: Loss=1.0842 (C:1.0842, R:0.0105)
Batch 350/356: Loss=1.0820 (C:1.0820, R:0.0105)

============================================================
Epoch 7/300 completed in 27.8s
Train: Loss=1.0615 (C:1.0615, R:0.0105) Ratio=3.10x
Val:   Loss=1.0755 (C:1.0755, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0755)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/356: Loss=1.0636 (C:1.0636, R:0.0105)
Batch  25/356: Loss=1.0201 (C:1.0201, R:0.0105)
Batch  50/356: Loss=1.0679 (C:1.0679, R:0.0105)
Batch  75/356: Loss=1.0006 (C:1.0006, R:0.0105)
Batch 100/356: Loss=1.0401 (C:1.0401, R:0.0105)
Batch 125/356: Loss=1.0776 (C:1.0776, R:0.0105)
Batch 150/356: Loss=1.0491 (C:1.0491, R:0.0105)
Batch 175/356: Loss=1.0098 (C:1.0098, R:0.0105)
Batch 200/356: Loss=1.0352 (C:1.0352, R:0.0105)
Batch 225/356: Loss=1.0093 (C:1.0093, R:0.0105)
Batch 250/356: Loss=1.0388 (C:1.0388, R:0.0105)
Batch 275/356: Loss=1.0599 (C:1.0599, R:0.0105)
Batch 300/356: Loss=1.0800 (C:1.0800, R:0.0105)
Batch 325/356: Loss=1.0135 (C:1.0135, R:0.0105)
Batch 350/356: Loss=1.0506 (C:1.0506, R:0.0105)

============================================================
Epoch 8/300 completed in 21.0s
Train: Loss=1.0478 (C:1.0478, R:0.0105) Ratio=3.23x
Val:   Loss=1.0807 (C:1.0807, R:0.0104) Ratio=2.92x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/356: Loss=1.0280 (C:1.0280, R:0.0105)
Batch  25/356: Loss=0.9923 (C:0.9923, R:0.0105)
Batch  50/356: Loss=0.9981 (C:0.9981, R:0.0105)
Batch  75/356: Loss=1.0337 (C:1.0337, R:0.0105)
Batch 100/356: Loss=1.0569 (C:1.0569, R:0.0105)
Batch 125/356: Loss=1.0352 (C:1.0352, R:0.0105)
Batch 150/356: Loss=1.0269 (C:1.0269, R:0.0106)
Batch 175/356: Loss=1.0533 (C:1.0533, R:0.0105)
Batch 200/356: Loss=1.0128 (C:1.0128, R:0.0105)
Batch 225/356: Loss=1.0666 (C:1.0666, R:0.0105)
Batch 250/356: Loss=1.0771 (C:1.0771, R:0.0105)
Batch 275/356: Loss=1.0576 (C:1.0576, R:0.0105)
Batch 300/356: Loss=1.0583 (C:1.0583, R:0.0105)
Batch 325/356: Loss=1.0525 (C:1.0525, R:0.0105)
Batch 350/356: Loss=1.0258 (C:1.0258, R:0.0105)

============================================================
Epoch 9/300 completed in 20.7s
Train: Loss=1.0375 (C:1.0375, R:0.0105) Ratio=3.32x
Val:   Loss=1.0763 (C:1.0763, R:0.0104) Ratio=2.94x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.455 ± 0.582
    Neg distances: 1.839 ± 0.905
    Separation ratio: 4.04x
    Gap: -3.424
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/356: Loss=0.9558 (C:0.9558, R:0.0105)
Batch  25/356: Loss=0.9769 (C:0.9769, R:0.0105)
Batch  50/356: Loss=0.9825 (C:0.9825, R:0.0105)
Batch  75/356: Loss=0.9688 (C:0.9688, R:0.0105)
Batch 100/356: Loss=0.9832 (C:0.9832, R:0.0105)
Batch 125/356: Loss=0.9826 (C:0.9826, R:0.0105)
Batch 150/356: Loss=0.9727 (C:0.9727, R:0.0105)
Batch 175/356: Loss=0.9723 (C:0.9723, R:0.0105)
Batch 200/356: Loss=0.9595 (C:0.9595, R:0.0105)
Batch 225/356: Loss=0.9855 (C:0.9855, R:0.0105)
Batch 250/356: Loss=0.9684 (C:0.9684, R:0.0105)
Batch 275/356: Loss=1.0101 (C:1.0101, R:0.0105)
Batch 300/356: Loss=1.0018 (C:1.0018, R:0.0105)
Batch 325/356: Loss=1.0120 (C:1.0120, R:0.0105)
Batch 350/356: Loss=1.0000 (C:1.0000, R:0.0105)

============================================================
Epoch 10/300 completed in 26.6s
Train: Loss=0.9816 (C:0.9816, R:0.0105) Ratio=3.42x
Val:   Loss=1.0297 (C:1.0297, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0297)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/356: Loss=0.9557 (C:0.9557, R:0.0105)
Batch  25/356: Loss=0.9433 (C:0.9433, R:0.0105)
Batch  50/356: Loss=0.9903 (C:0.9903, R:0.0105)
Batch  75/356: Loss=0.9443 (C:0.9443, R:0.0105)
Batch 100/356: Loss=0.9744 (C:0.9744, R:0.0105)
Batch 125/356: Loss=0.9829 (C:0.9829, R:0.0105)
Batch 150/356: Loss=0.9689 (C:0.9689, R:0.0105)
Batch 175/356: Loss=0.9674 (C:0.9674, R:0.0105)
Batch 200/356: Loss=0.9917 (C:0.9917, R:0.0106)
Batch 225/356: Loss=0.9905 (C:0.9905, R:0.0105)
Batch 250/356: Loss=1.0018 (C:1.0018, R:0.0105)
Batch 275/356: Loss=0.9631 (C:0.9631, R:0.0105)
Batch 300/356: Loss=0.9979 (C:0.9979, R:0.0105)
Batch 325/356: Loss=0.9756 (C:0.9756, R:0.0105)
Batch 350/356: Loss=0.9606 (C:0.9606, R:0.0105)

============================================================
Epoch 11/300 completed in 20.7s
Train: Loss=0.9713 (C:0.9713, R:0.0105) Ratio=3.50x
Val:   Loss=1.0311 (C:1.0311, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/356: Loss=0.9126 (C:0.9126, R:0.0105)
Batch  25/356: Loss=0.9638 (C:0.9638, R:0.0106)
Batch  50/356: Loss=0.9408 (C:0.9408, R:0.0105)
Batch  75/356: Loss=0.9582 (C:0.9582, R:0.0105)
Batch 100/356: Loss=0.9757 (C:0.9757, R:0.0105)
Batch 125/356: Loss=0.9702 (C:0.9702, R:0.0105)
Batch 150/356: Loss=0.9398 (C:0.9398, R:0.0105)
Batch 175/356: Loss=0.9461 (C:0.9461, R:0.0105)
Batch 200/356: Loss=0.9573 (C:0.9573, R:0.0105)
Batch 225/356: Loss=0.9924 (C:0.9924, R:0.0105)
Batch 250/356: Loss=0.9825 (C:0.9825, R:0.0105)
Batch 275/356: Loss=0.9529 (C:0.9529, R:0.0105)
Batch 300/356: Loss=0.9976 (C:0.9976, R:0.0105)
Batch 325/356: Loss=0.9629 (C:0.9629, R:0.0105)
Batch 350/356: Loss=0.9912 (C:0.9912, R:0.0105)

============================================================
Epoch 12/300 completed in 20.9s
Train: Loss=0.9631 (C:0.9631, R:0.0105) Ratio=3.57x
Val:   Loss=1.0312 (C:1.0312, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.420 ± 0.563
    Neg distances: 1.937 ± 0.924
    Separation ratio: 4.61x
    Gap: -3.416
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/356: Loss=0.8816 (C:0.8816, R:0.0106)
Batch  25/356: Loss=0.8768 (C:0.8768, R:0.0105)
Batch  50/356: Loss=0.9239 (C:0.9239, R:0.0105)
Batch  75/356: Loss=0.9013 (C:0.9013, R:0.0105)
Batch 100/356: Loss=0.8826 (C:0.8826, R:0.0105)
Batch 125/356: Loss=0.8916 (C:0.8916, R:0.0105)
Batch 150/356: Loss=0.9167 (C:0.9167, R:0.0105)
Batch 175/356: Loss=0.9305 (C:0.9305, R:0.0105)
Batch 200/356: Loss=0.9479 (C:0.9479, R:0.0105)
Batch 225/356: Loss=0.8968 (C:0.8968, R:0.0105)
Batch 250/356: Loss=0.9059 (C:0.9059, R:0.0105)
Batch 275/356: Loss=0.9437 (C:0.9437, R:0.0105)
Batch 300/356: Loss=0.9324 (C:0.9324, R:0.0105)
Batch 325/356: Loss=0.8963 (C:0.8963, R:0.0105)
Batch 350/356: Loss=0.9358 (C:0.9358, R:0.0105)

============================================================
Epoch 13/300 completed in 27.4s
Train: Loss=0.9153 (C:0.9153, R:0.0105) Ratio=3.66x
Val:   Loss=0.9851 (C:0.9851, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9851)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/356: Loss=0.8859 (C:0.8859, R:0.0105)
Batch  25/356: Loss=0.8501 (C:0.8501, R:0.0105)
Batch  50/356: Loss=0.8894 (C:0.8894, R:0.0105)
Batch  75/356: Loss=0.9235 (C:0.9235, R:0.0105)
Batch 100/356: Loss=0.9102 (C:0.9102, R:0.0105)
Batch 125/356: Loss=0.9024 (C:0.9024, R:0.0105)
Batch 150/356: Loss=0.8875 (C:0.8875, R:0.0105)
Batch 175/356: Loss=0.9094 (C:0.9094, R:0.0105)
Batch 200/356: Loss=0.9193 (C:0.9193, R:0.0105)
Batch 225/356: Loss=0.9092 (C:0.9092, R:0.0105)
Batch 250/356: Loss=0.8867 (C:0.8867, R:0.0105)
Batch 275/356: Loss=0.9320 (C:0.9320, R:0.0105)
Batch 300/356: Loss=0.8988 (C:0.8988, R:0.0105)
Batch 325/356: Loss=0.9215 (C:0.9215, R:0.0105)
Batch 350/356: Loss=0.9322 (C:0.9322, R:0.0105)

============================================================
Epoch 14/300 completed in 21.3s
Train: Loss=0.9079 (C:0.9079, R:0.0105) Ratio=3.71x
Val:   Loss=0.9875 (C:0.9875, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/356: Loss=0.8845 (C:0.8845, R:0.0105)
Batch  25/356: Loss=0.8857 (C:0.8857, R:0.0105)
Batch  50/356: Loss=0.8918 (C:0.8918, R:0.0105)
Batch  75/356: Loss=0.8865 (C:0.8865, R:0.0105)
Batch 100/356: Loss=0.8929 (C:0.8929, R:0.0105)
Batch 125/356: Loss=0.9057 (C:0.9057, R:0.0105)
Batch 150/356: Loss=0.8893 (C:0.8893, R:0.0105)
Batch 175/356: Loss=0.9156 (C:0.9156, R:0.0105)
Batch 200/356: Loss=0.9088 (C:0.9088, R:0.0105)
Batch 225/356: Loss=0.8737 (C:0.8737, R:0.0105)
Batch 250/356: Loss=0.8800 (C:0.8800, R:0.0105)
Batch 275/356: Loss=0.9020 (C:0.9020, R:0.0105)
Batch 300/356: Loss=0.9018 (C:0.9018, R:0.0105)
Batch 325/356: Loss=0.9423 (C:0.9423, R:0.0105)
Batch 350/356: Loss=0.9422 (C:0.9422, R:0.0105)

============================================================
Epoch 15/300 completed in 21.2s
Train: Loss=0.9007 (C:0.9007, R:0.0105) Ratio=3.79x
Val:   Loss=0.9895 (C:0.9895, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.398 ± 0.558
    Neg distances: 2.029 ± 0.938
    Separation ratio: 5.10x
    Gap: -3.550
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/356: Loss=0.8473 (C:0.8473, R:0.0105)
Batch  25/356: Loss=0.8628 (C:0.8628, R:0.0105)
Batch  50/356: Loss=0.8459 (C:0.8459, R:0.0105)
Batch  75/356: Loss=0.8460 (C:0.8460, R:0.0105)
Batch 100/356: Loss=0.8455 (C:0.8455, R:0.0105)
Batch 125/356: Loss=0.8724 (C:0.8724, R:0.0105)
Batch 150/356: Loss=0.8774 (C:0.8774, R:0.0105)
Batch 175/356: Loss=0.8508 (C:0.8508, R:0.0105)
Batch 200/356: Loss=0.8583 (C:0.8583, R:0.0105)
Batch 225/356: Loss=0.8863 (C:0.8863, R:0.0105)
Batch 250/356: Loss=0.8623 (C:0.8623, R:0.0105)
Batch 275/356: Loss=0.8795 (C:0.8795, R:0.0105)
Batch 300/356: Loss=0.8659 (C:0.8659, R:0.0105)
Batch 325/356: Loss=0.8736 (C:0.8736, R:0.0105)
Batch 350/356: Loss=0.8905 (C:0.8905, R:0.0105)

============================================================
Epoch 16/300 completed in 28.5s
Train: Loss=0.8603 (C:0.8603, R:0.0105) Ratio=3.81x
Val:   Loss=0.9521 (C:0.9521, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9521)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/356: Loss=0.8414 (C:0.8414, R:0.0105)
Batch  25/356: Loss=0.8258 (C:0.8258, R:0.0105)
Batch  50/356: Loss=0.8557 (C:0.8557, R:0.0105)
Batch  75/356: Loss=0.8701 (C:0.8701, R:0.0105)
Batch 100/356: Loss=0.8594 (C:0.8594, R:0.0105)
Batch 125/356: Loss=0.8702 (C:0.8702, R:0.0105)
Batch 150/356: Loss=0.8495 (C:0.8495, R:0.0105)
Batch 175/356: Loss=0.8529 (C:0.8529, R:0.0105)
Batch 200/356: Loss=0.8836 (C:0.8836, R:0.0105)
Batch 225/356: Loss=0.8645 (C:0.8645, R:0.0105)
Batch 250/356: Loss=0.8633 (C:0.8633, R:0.0105)
Batch 275/356: Loss=0.8862 (C:0.8862, R:0.0105)
Batch 300/356: Loss=0.8840 (C:0.8840, R:0.0105)
Batch 325/356: Loss=0.8698 (C:0.8698, R:0.0105)
Batch 350/356: Loss=0.8599 (C:0.8599, R:0.0105)

============================================================
Epoch 17/300 completed in 21.4s
Train: Loss=0.8546 (C:0.8546, R:0.0105) Ratio=3.87x
Val:   Loss=0.9524 (C:0.9524, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/356: Loss=0.8575 (C:0.8575, R:0.0105)
Batch  25/356: Loss=0.8985 (C:0.8985, R:0.0105)
Batch  50/356: Loss=0.8157 (C:0.8157, R:0.0105)
Batch  75/356: Loss=0.8908 (C:0.8908, R:0.0105)
Batch 100/356: Loss=0.8508 (C:0.8508, R:0.0105)
Batch 125/356: Loss=0.8290 (C:0.8290, R:0.0105)
Batch 150/356: Loss=0.8558 (C:0.8558, R:0.0105)
Batch 175/356: Loss=0.8629 (C:0.8629, R:0.0105)
Batch 200/356: Loss=0.8458 (C:0.8458, R:0.0105)
Batch 225/356: Loss=0.8347 (C:0.8347, R:0.0105)
Batch 250/356: Loss=0.8827 (C:0.8827, R:0.0105)
Batch 275/356: Loss=0.8397 (C:0.8397, R:0.0105)
Batch 300/356: Loss=0.8405 (C:0.8405, R:0.0105)
Batch 325/356: Loss=0.8176 (C:0.8176, R:0.0105)
Batch 350/356: Loss=0.8611 (C:0.8611, R:0.0105)

============================================================
Epoch 18/300 completed in 20.4s
Train: Loss=0.8490 (C:0.8490, R:0.0105) Ratio=3.98x
Val:   Loss=0.9561 (C:0.9561, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.411 ± 0.586
    Neg distances: 2.105 ± 0.969
    Separation ratio: 5.11x
    Gap: -3.638
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/356: Loss=0.7677 (C:0.7677, R:0.0105)
Batch  25/356: Loss=0.7953 (C:0.7953, R:0.0106)
Batch  50/356: Loss=0.7702 (C:0.7702, R:0.0105)
Batch  75/356: Loss=0.8171 (C:0.8171, R:0.0105)
Batch 100/356: Loss=0.8498 (C:0.8498, R:0.0105)
Batch 125/356: Loss=0.8402 (C:0.8402, R:0.0105)
Batch 150/356: Loss=0.8508 (C:0.8508, R:0.0105)
Batch 175/356: Loss=0.8482 (C:0.8482, R:0.0105)
Batch 200/356: Loss=0.8673 (C:0.8673, R:0.0105)
Batch 225/356: Loss=0.8563 (C:0.8563, R:0.0105)
Batch 250/356: Loss=0.8298 (C:0.8298, R:0.0105)
Batch 275/356: Loss=0.8502 (C:0.8502, R:0.0105)
Batch 300/356: Loss=0.8573 (C:0.8573, R:0.0105)
Batch 325/356: Loss=0.8464 (C:0.8464, R:0.0105)
Batch 350/356: Loss=0.8526 (C:0.8526, R:0.0105)

============================================================
Epoch 19/300 completed in 25.9s
Train: Loss=0.8335 (C:0.8335, R:0.0105) Ratio=4.00x
Val:   Loss=0.9402 (C:0.9402, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9402)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/356: Loss=0.8126 (C:0.8126, R:0.0105)
Batch  25/356: Loss=0.8277 (C:0.8277, R:0.0105)
Batch  50/356: Loss=0.8425 (C:0.8425, R:0.0105)
Batch  75/356: Loss=0.8356 (C:0.8356, R:0.0105)
Batch 100/356: Loss=0.8243 (C:0.8243, R:0.0105)
Batch 125/356: Loss=0.8387 (C:0.8387, R:0.0105)
Batch 150/356: Loss=0.8190 (C:0.8190, R:0.0105)
Batch 175/356: Loss=0.8331 (C:0.8331, R:0.0105)
Batch 200/356: Loss=0.8207 (C:0.8207, R:0.0105)
Batch 225/356: Loss=0.8308 (C:0.8308, R:0.0105)
Batch 250/356: Loss=0.8282 (C:0.8282, R:0.0105)
Batch 275/356: Loss=0.8045 (C:0.8045, R:0.0105)
Batch 300/356: Loss=0.8248 (C:0.8248, R:0.0105)
Batch 325/356: Loss=0.8258 (C:0.8258, R:0.0105)
Batch 350/356: Loss=0.8525 (C:0.8525, R:0.0105)

============================================================
Epoch 20/300 completed in 20.3s
Train: Loss=0.8301 (C:0.8301, R:0.0105) Ratio=4.13x
Val:   Loss=0.9383 (C:0.9383, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9383)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/356: Loss=0.8023 (C:0.8023, R:0.0106)
Batch  25/356: Loss=0.8309 (C:0.8309, R:0.0105)
Batch  50/356: Loss=0.8179 (C:0.8179, R:0.0105)
Batch  75/356: Loss=0.8514 (C:0.8514, R:0.0105)
Batch 100/356: Loss=0.8180 (C:0.8180, R:0.0105)
Batch 125/356: Loss=0.8215 (C:0.8215, R:0.0105)
Batch 150/356: Loss=0.8329 (C:0.8329, R:0.0105)
Batch 175/356: Loss=0.8041 (C:0.8041, R:0.0105)
Batch 200/356: Loss=0.8188 (C:0.8188, R:0.0105)
Batch 225/356: Loss=0.8305 (C:0.8305, R:0.0105)
Batch 250/356: Loss=0.8306 (C:0.8306, R:0.0105)
Batch 275/356: Loss=0.8051 (C:0.8051, R:0.0105)
Batch 300/356: Loss=0.8320 (C:0.8320, R:0.0105)
Batch 325/356: Loss=0.8490 (C:0.8490, R:0.0105)
Batch 350/356: Loss=0.7887 (C:0.7887, R:0.0105)

============================================================
Epoch 21/300 completed in 20.8s
Train: Loss=0.8255 (C:0.8255, R:0.0105) Ratio=4.15x
Val:   Loss=0.9437 (C:0.9437, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.379 ± 0.578
    Neg distances: 2.146 ± 0.966
    Separation ratio: 5.66x
    Gap: -3.657
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/356: Loss=0.8112 (C:0.8112, R:0.0105)
Batch  25/356: Loss=0.7750 (C:0.7750, R:0.0105)
Batch  50/356: Loss=0.7828 (C:0.7828, R:0.0105)
Batch  75/356: Loss=0.7824 (C:0.7824, R:0.0105)
Batch 100/356: Loss=0.8062 (C:0.8062, R:0.0105)
Batch 125/356: Loss=0.8004 (C:0.8004, R:0.0105)
Batch 150/356: Loss=0.8079 (C:0.8079, R:0.0105)
Batch 175/356: Loss=0.7895 (C:0.7895, R:0.0105)
Batch 200/356: Loss=0.7614 (C:0.7614, R:0.0105)
Batch 225/356: Loss=0.8094 (C:0.8094, R:0.0105)
Batch 250/356: Loss=0.8052 (C:0.8052, R:0.0105)
Batch 275/356: Loss=0.7721 (C:0.7721, R:0.0105)
Batch 300/356: Loss=0.8221 (C:0.8221, R:0.0105)
Batch 325/356: Loss=0.8289 (C:0.8289, R:0.0105)
Batch 350/356: Loss=0.8080 (C:0.8080, R:0.0105)

============================================================
Epoch 22/300 completed in 27.2s
Train: Loss=0.7936 (C:0.7936, R:0.0105) Ratio=4.10x
Val:   Loss=0.9059 (C:0.9059, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9059)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/356: Loss=0.7714 (C:0.7714, R:0.0105)
Batch  25/356: Loss=0.7852 (C:0.7852, R:0.0105)
Batch  50/356: Loss=0.7889 (C:0.7889, R:0.0105)
Batch  75/356: Loss=0.7659 (C:0.7659, R:0.0105)
Batch 100/356: Loss=0.7927 (C:0.7927, R:0.0105)
Batch 125/356: Loss=0.7935 (C:0.7935, R:0.0105)
Batch 150/356: Loss=0.7973 (C:0.7973, R:0.0105)
Batch 175/356: Loss=0.8074 (C:0.8074, R:0.0105)
Batch 200/356: Loss=0.7884 (C:0.7884, R:0.0105)
Batch 225/356: Loss=0.7989 (C:0.7989, R:0.0105)
Batch 250/356: Loss=0.7905 (C:0.7905, R:0.0105)
Batch 275/356: Loss=0.7779 (C:0.7779, R:0.0105)
Batch 300/356: Loss=0.7957 (C:0.7957, R:0.0105)
Batch 325/356: Loss=0.7578 (C:0.7578, R:0.0105)
Batch 350/356: Loss=0.8404 (C:0.8404, R:0.0105)

============================================================
Epoch 23/300 completed in 20.6s
Train: Loss=0.7896 (C:0.7896, R:0.0105) Ratio=4.16x
Val:   Loss=0.9013 (C:0.9013, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9013)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/356: Loss=0.7607 (C:0.7607, R:0.0105)
Batch  25/356: Loss=0.7762 (C:0.7762, R:0.0105)
Batch  50/356: Loss=0.7624 (C:0.7624, R:0.0105)
Batch  75/356: Loss=0.7941 (C:0.7941, R:0.0105)
Batch 100/356: Loss=0.7813 (C:0.7813, R:0.0105)
Batch 125/356: Loss=0.8190 (C:0.8190, R:0.0105)
Batch 150/356: Loss=0.7832 (C:0.7832, R:0.0105)
Batch 175/356: Loss=0.7953 (C:0.7953, R:0.0105)
Batch 200/356: Loss=0.7713 (C:0.7713, R:0.0105)
Batch 225/356: Loss=0.7870 (C:0.7870, R:0.0105)
Batch 250/356: Loss=0.7745 (C:0.7745, R:0.0105)
Batch 275/356: Loss=0.7976 (C:0.7976, R:0.0105)
Batch 300/356: Loss=0.7768 (C:0.7768, R:0.0105)
Batch 325/356: Loss=0.8145 (C:0.8145, R:0.0105)
Batch 350/356: Loss=0.8279 (C:0.8279, R:0.0105)

============================================================
Epoch 24/300 completed in 21.2s
Train: Loss=0.7834 (C:0.7834, R:0.0105) Ratio=4.22x
Val:   Loss=0.9036 (C:0.9036, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.351 ± 0.558
    Neg distances: 2.221 ± 0.975
    Separation ratio: 6.33x
    Gap: -3.744
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/356: Loss=0.7412 (C:0.7412, R:0.0105)
Batch  25/356: Loss=0.7463 (C:0.7463, R:0.0105)
Batch  50/356: Loss=0.7584 (C:0.7584, R:0.0105)
Batch  75/356: Loss=0.7642 (C:0.7642, R:0.0105)
Batch 100/356: Loss=0.7330 (C:0.7330, R:0.0105)
Batch 125/356: Loss=0.7402 (C:0.7402, R:0.0105)
Batch 150/356: Loss=0.7455 (C:0.7455, R:0.0105)
Batch 175/356: Loss=0.7421 (C:0.7421, R:0.0105)
Batch 200/356: Loss=0.7325 (C:0.7325, R:0.0105)
Batch 225/356: Loss=0.7392 (C:0.7392, R:0.0105)
Batch 250/356: Loss=0.7575 (C:0.7575, R:0.0106)
Batch 275/356: Loss=0.7343 (C:0.7343, R:0.0105)
Batch 300/356: Loss=0.8061 (C:0.8061, R:0.0105)
Batch 325/356: Loss=0.7757 (C:0.7757, R:0.0105)
Batch 350/356: Loss=0.7524 (C:0.7524, R:0.0105)

============================================================
Epoch 25/300 completed in 27.2s
Train: Loss=0.7496 (C:0.7496, R:0.0105) Ratio=4.23x
Val:   Loss=0.8761 (C:0.8761, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8761)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/356: Loss=0.7676 (C:0.7676, R:0.0105)
Batch  25/356: Loss=0.7347 (C:0.7347, R:0.0105)
Batch  50/356: Loss=0.7624 (C:0.7624, R:0.0105)
Batch  75/356: Loss=0.7497 (C:0.7497, R:0.0105)
Batch 100/356: Loss=0.7623 (C:0.7623, R:0.0105)
Batch 125/356: Loss=0.7643 (C:0.7643, R:0.0105)
Batch 150/356: Loss=0.7370 (C:0.7370, R:0.0106)
Batch 175/356: Loss=0.7604 (C:0.7604, R:0.0105)
Batch 200/356: Loss=0.7165 (C:0.7165, R:0.0105)
Batch 225/356: Loss=0.7222 (C:0.7222, R:0.0105)
Batch 250/356: Loss=0.7445 (C:0.7445, R:0.0105)
Batch 275/356: Loss=0.7449 (C:0.7449, R:0.0105)
Batch 300/356: Loss=0.7596 (C:0.7596, R:0.0105)
Batch 325/356: Loss=0.7415 (C:0.7415, R:0.0106)
Batch 350/356: Loss=0.7661 (C:0.7661, R:0.0105)

============================================================
Epoch 26/300 completed in 21.1s
Train: Loss=0.7460 (C:0.7460, R:0.0105) Ratio=4.30x
Val:   Loss=0.8676 (C:0.8676, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8676)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/356: Loss=0.7187 (C:0.7187, R:0.0105)
Batch  25/356: Loss=0.7418 (C:0.7418, R:0.0105)
Batch  50/356: Loss=0.7433 (C:0.7433, R:0.0105)
Batch  75/356: Loss=0.7490 (C:0.7490, R:0.0105)
Batch 100/356: Loss=0.7699 (C:0.7699, R:0.0105)
Batch 125/356: Loss=0.7451 (C:0.7451, R:0.0105)
Batch 150/356: Loss=0.7363 (C:0.7363, R:0.0105)
Batch 175/356: Loss=0.7483 (C:0.7483, R:0.0105)
Batch 200/356: Loss=0.7274 (C:0.7274, R:0.0105)
Batch 225/356: Loss=0.7421 (C:0.7421, R:0.0105)
Batch 250/356: Loss=0.7511 (C:0.7511, R:0.0105)
Batch 275/356: Loss=0.7618 (C:0.7618, R:0.0105)
Batch 300/356: Loss=0.7294 (C:0.7294, R:0.0105)
Batch 325/356: Loss=0.7344 (C:0.7344, R:0.0105)
Batch 350/356: Loss=0.7557 (C:0.7557, R:0.0105)

============================================================
Epoch 27/300 completed in 21.0s
Train: Loss=0.7421 (C:0.7421, R:0.0105) Ratio=4.34x
Val:   Loss=0.8653 (C:0.8653, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8653)
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.370 ± 0.599
    Neg distances: 2.254 ± 0.995
    Separation ratio: 6.09x
    Gap: -3.836
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/356: Loss=0.6962 (C:0.6962, R:0.0105)
Batch  25/356: Loss=0.7305 (C:0.7305, R:0.0105)
Batch  50/356: Loss=0.7444 (C:0.7444, R:0.0106)
Batch  75/356: Loss=0.7011 (C:0.7011, R:0.0105)
Batch 100/356: Loss=0.7178 (C:0.7178, R:0.0105)
Batch 125/356: Loss=0.7277 (C:0.7277, R:0.0105)
Batch 150/356: Loss=0.6839 (C:0.6839, R:0.0105)
Batch 175/356: Loss=0.7335 (C:0.7335, R:0.0105)
Batch 200/356: Loss=0.7594 (C:0.7594, R:0.0105)
Batch 225/356: Loss=0.7561 (C:0.7561, R:0.0105)
Batch 250/356: Loss=0.7686 (C:0.7686, R:0.0105)
Batch 275/356: Loss=0.7584 (C:0.7584, R:0.0105)
Batch 300/356: Loss=0.7428 (C:0.7428, R:0.0105)
Batch 325/356: Loss=0.7206 (C:0.7206, R:0.0105)
Batch 350/356: Loss=0.7109 (C:0.7109, R:0.0105)

============================================================
Epoch 28/300 completed in 26.9s
Train: Loss=0.7366 (C:0.7366, R:0.0105) Ratio=4.47x
Val:   Loss=0.8775 (C:0.8775, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/356: Loss=0.7399 (C:0.7399, R:0.0105)
Batch  25/356: Loss=0.7063 (C:0.7063, R:0.0105)
Batch  50/356: Loss=0.7231 (C:0.7231, R:0.0105)
Batch  75/356: Loss=0.7403 (C:0.7403, R:0.0105)
Batch 100/356: Loss=0.7517 (C:0.7517, R:0.0105)
Batch 125/356: Loss=0.7340 (C:0.7340, R:0.0105)
Batch 150/356: Loss=0.7436 (C:0.7436, R:0.0105)
Batch 175/356: Loss=0.7493 (C:0.7493, R:0.0105)
Batch 200/356: Loss=0.7299 (C:0.7299, R:0.0105)
Batch 225/356: Loss=0.7353 (C:0.7353, R:0.0105)
Batch 250/356: Loss=0.7514 (C:0.7514, R:0.0105)
Batch 275/356: Loss=0.7545 (C:0.7545, R:0.0105)
Batch 300/356: Loss=0.7620 (C:0.7620, R:0.0105)
Batch 325/356: Loss=0.7473 (C:0.7473, R:0.0105)
Batch 350/356: Loss=0.7408 (C:0.7408, R:0.0105)

============================================================
Epoch 29/300 completed in 21.2s
Train: Loss=0.7343 (C:0.7343, R:0.0105) Ratio=4.39x
Val:   Loss=0.8685 (C:0.8685, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/356: Loss=0.7258 (C:0.7258, R:0.0105)
Batch  25/356: Loss=0.7045 (C:0.7045, R:0.0105)
Batch  50/356: Loss=0.7498 (C:0.7498, R:0.0105)
Batch  75/356: Loss=0.7130 (C:0.7130, R:0.0105)
Batch 100/356: Loss=0.7039 (C:0.7039, R:0.0105)
Batch 125/356: Loss=0.7582 (C:0.7582, R:0.0105)
Batch 150/356: Loss=0.7387 (C:0.7387, R:0.0105)
Batch 175/356: Loss=0.7070 (C:0.7070, R:0.0105)
Batch 200/356: Loss=0.7597 (C:0.7597, R:0.0105)
Batch 225/356: Loss=0.7493 (C:0.7493, R:0.0105)
Batch 250/356: Loss=0.6851 (C:0.6851, R:0.0105)
Batch 275/356: Loss=0.7376 (C:0.7376, R:0.0105)
Batch 300/356: Loss=0.7330 (C:0.7330, R:0.0105)
Batch 325/356: Loss=0.7476 (C:0.7476, R:0.0105)
Batch 350/356: Loss=0.7577 (C:0.7577, R:0.0105)

============================================================
Epoch 30/300 completed in 20.7s
Train: Loss=0.7294 (C:0.7294, R:0.0105) Ratio=4.46x
Val:   Loss=0.8763 (C:0.8763, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.000
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.357 ± 0.574
    Neg distances: 2.327 ± 1.014
    Separation ratio: 6.52x
    Gap: -4.041
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/356: Loss=0.7209 (C:0.7209, R:0.0105)
Batch  25/356: Loss=0.6907 (C:0.6907, R:0.0105)
Batch  50/356: Loss=0.6905 (C:0.6905, R:0.0105)
Batch  75/356: Loss=0.6949 (C:0.6949, R:0.0105)
Batch 100/356: Loss=0.6915 (C:0.6915, R:0.0105)
Batch 125/356: Loss=0.7266 (C:0.7266, R:0.0105)
Batch 150/356: Loss=0.7056 (C:0.7056, R:0.0105)
Batch 175/356: Loss=0.6986 (C:0.6986, R:0.0105)
Batch 200/356: Loss=0.7157 (C:0.7157, R:0.0105)
Batch 225/356: Loss=0.7239 (C:0.7239, R:0.0105)
Batch 250/356: Loss=0.6818 (C:0.6818, R:0.0105)
Batch 275/356: Loss=0.7161 (C:0.7161, R:0.0105)
Batch 300/356: Loss=0.7252 (C:0.7252, R:0.0105)
Batch 325/356: Loss=0.7027 (C:0.7027, R:0.0105)
Batch 350/356: Loss=0.7004 (C:0.7004, R:0.0105)

============================================================
Epoch 31/300 completed in 27.2s
Train: Loss=0.7048 (C:0.7048, R:0.0105) Ratio=4.53x
Val:   Loss=0.8493 (C:0.8493, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8493)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/356: Loss=0.6750 (C:0.6750, R:0.0105)
Batch  25/356: Loss=0.7091 (C:0.7091, R:0.0105)
Batch  50/356: Loss=0.6939 (C:0.6939, R:0.0105)
Batch  75/356: Loss=0.6919 (C:0.6919, R:0.0105)
Batch 100/356: Loss=0.6897 (C:0.6897, R:0.0105)
Batch 125/356: Loss=0.6709 (C:0.6709, R:0.0105)
Batch 150/356: Loss=0.6891 (C:0.6891, R:0.0105)
Batch 175/356: Loss=0.6606 (C:0.6606, R:0.0105)
Batch 200/356: Loss=0.6875 (C:0.6875, R:0.0105)
Batch 225/356: Loss=0.7072 (C:0.7072, R:0.0105)
Batch 250/356: Loss=0.7050 (C:0.7050, R:0.0105)
Batch 275/356: Loss=0.6810 (C:0.6810, R:0.0105)
Batch 300/356: Loss=0.7086 (C:0.7086, R:0.0105)
Batch 325/356: Loss=0.6995 (C:0.6995, R:0.0105)
Batch 350/356: Loss=0.7293 (C:0.7293, R:0.0105)

============================================================
Epoch 32/300 completed in 20.6s
Train: Loss=0.7018 (C:0.7018, R:0.0105) Ratio=4.65x
Val:   Loss=0.8568 (C:0.8568, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/356: Loss=0.6744 (C:0.6744, R:0.0105)
Batch  25/356: Loss=0.6744 (C:0.6744, R:0.0105)
Batch  50/356: Loss=0.6678 (C:0.6678, R:0.0105)
Batch  75/356: Loss=0.7112 (C:0.7112, R:0.0105)
Batch 100/356: Loss=0.6587 (C:0.6587, R:0.0105)
Batch 125/356: Loss=0.7073 (C:0.7073, R:0.0105)
Batch 150/356: Loss=0.6812 (C:0.6812, R:0.0105)
Batch 175/356: Loss=0.6930 (C:0.6930, R:0.0105)
Batch 200/356: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 225/356: Loss=0.6799 (C:0.6799, R:0.0105)
Batch 250/356: Loss=0.7195 (C:0.7195, R:0.0105)
Batch 275/356: Loss=0.7303 (C:0.7303, R:0.0105)
Batch 300/356: Loss=0.6916 (C:0.6916, R:0.0105)
Batch 325/356: Loss=0.6911 (C:0.6911, R:0.0105)
Batch 350/356: Loss=0.7105 (C:0.7105, R:0.0105)

============================================================
Epoch 33/300 completed in 20.9s
Train: Loss=0.6986 (C:0.6986, R:0.0105) Ratio=4.65x
Val:   Loss=0.8406 (C:0.8406, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.8406)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.354 ± 0.595
    Neg distances: 2.376 ± 1.036
    Separation ratio: 6.71x
    Gap: -4.002
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/356: Loss=0.6677 (C:0.6677, R:0.0105)
Batch  25/356: Loss=0.6624 (C:0.6624, R:0.0105)
Batch  50/356: Loss=0.6736 (C:0.6736, R:0.0105)
Batch  75/356: Loss=0.6486 (C:0.6486, R:0.0105)
Batch 100/356: Loss=0.6938 (C:0.6938, R:0.0105)
Batch 125/356: Loss=0.6777 (C:0.6777, R:0.0105)
Batch 150/356: Loss=0.6750 (C:0.6750, R:0.0105)
Batch 175/356: Loss=0.6646 (C:0.6646, R:0.0105)
Batch 200/356: Loss=0.6933 (C:0.6933, R:0.0105)
Batch 225/356: Loss=0.7010 (C:0.7010, R:0.0105)
Batch 250/356: Loss=0.6863 (C:0.6863, R:0.0105)
Batch 275/356: Loss=0.6714 (C:0.6714, R:0.0105)
Batch 300/356: Loss=0.6998 (C:0.6998, R:0.0105)
Batch 325/356: Loss=0.6765 (C:0.6765, R:0.0105)
Batch 350/356: Loss=0.7292 (C:0.7292, R:0.0105)

============================================================
Epoch 34/300 completed in 27.0s
Train: Loss=0.6890 (C:0.6890, R:0.0105) Ratio=4.68x
Val:   Loss=0.8427 (C:0.8427, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.060
No improvement for 1 epochs
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/356: Loss=0.6800 (C:0.6800, R:0.0105)
Batch  25/356: Loss=0.6474 (C:0.6474, R:0.0105)
Batch  50/356: Loss=0.6991 (C:0.6991, R:0.0105)
Batch  75/356: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 100/356: Loss=0.6784 (C:0.6784, R:0.0105)
Batch 125/356: Loss=0.6789 (C:0.6789, R:0.0105)
Batch 150/356: Loss=0.7112 (C:0.7112, R:0.0105)
Batch 175/356: Loss=0.7381 (C:0.7381, R:0.0105)
Batch 200/356: Loss=0.7104 (C:0.7104, R:0.0105)
Batch 225/356: Loss=0.7122 (C:0.7122, R:0.0105)
Batch 250/356: Loss=0.7190 (C:0.7190, R:0.0105)
Batch 275/356: Loss=0.6695 (C:0.6695, R:0.0105)
Batch 300/356: Loss=0.7109 (C:0.7109, R:0.0105)
Batch 325/356: Loss=0.7018 (C:0.7018, R:0.0105)
Batch 350/356: Loss=0.7480 (C:0.7480, R:0.0105)

============================================================
Epoch 35/300 completed in 20.6s
Train: Loss=0.6866 (C:0.6866, R:0.0105) Ratio=4.61x
Val:   Loss=0.8368 (C:0.8368, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 0.8368)
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/356: Loss=0.6780 (C:0.6780, R:0.0105)
Batch  25/356: Loss=0.6878 (C:0.6878, R:0.0105)
Batch  50/356: Loss=0.6750 (C:0.6750, R:0.0105)
Batch  75/356: Loss=0.7487 (C:0.7487, R:0.0105)
Batch 100/356: Loss=0.7017 (C:0.7017, R:0.0105)
Batch 125/356: Loss=0.6542 (C:0.6542, R:0.0105)
Batch 150/356: Loss=0.6626 (C:0.6626, R:0.0105)
Batch 175/356: Loss=0.6694 (C:0.6694, R:0.0105)
Batch 200/356: Loss=0.6677 (C:0.6677, R:0.0105)
Batch 225/356: Loss=0.6752 (C:0.6752, R:0.0105)
Batch 250/356: Loss=0.6927 (C:0.6927, R:0.0105)
Batch 275/356: Loss=0.6889 (C:0.6889, R:0.0105)
Batch 300/356: Loss=0.7603 (C:0.7603, R:0.0105)
Batch 325/356: Loss=0.6683 (C:0.6683, R:0.0105)
Batch 350/356: Loss=0.7087 (C:0.7087, R:0.0105)

============================================================
Epoch 36/300 completed in 20.8s
Train: Loss=0.6825 (C:0.6825, R:0.0105) Ratio=4.62x
Val:   Loss=0.8367 (C:0.8367, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.090
✅ New best model saved (Val Loss: 0.8367)
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.352 ± 0.614
    Neg distances: 2.403 ± 1.043
    Separation ratio: 6.82x
    Gap: -4.162
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/356: Loss=0.6662 (C:0.6662, R:0.0105)
Batch  25/356: Loss=0.6523 (C:0.6523, R:0.0105)
Batch  50/356: Loss=0.6687 (C:0.6687, R:0.0105)
Batch  75/356: Loss=0.6453 (C:0.6453, R:0.0105)
Batch 100/356: Loss=0.6743 (C:0.6743, R:0.0105)
Batch 125/356: Loss=0.6771 (C:0.6771, R:0.0105)
Batch 150/356: Loss=0.6706 (C:0.6706, R:0.0105)
Batch 175/356: Loss=0.6935 (C:0.6935, R:0.0105)
Batch 200/356: Loss=0.6732 (C:0.6732, R:0.0105)
Batch 225/356: Loss=0.6682 (C:0.6682, R:0.0105)
Batch 250/356: Loss=0.6471 (C:0.6471, R:0.0105)
Batch 275/356: Loss=0.6484 (C:0.6484, R:0.0105)
Batch 300/356: Loss=0.7092 (C:0.7092, R:0.0105)
Batch 325/356: Loss=0.6735 (C:0.6735, R:0.0105)
Batch 350/356: Loss=0.6468 (C:0.6468, R:0.0106)

============================================================
Epoch 37/300 completed in 26.8s
Train: Loss=0.6711 (C:0.6711, R:0.0105) Ratio=4.76x
Val:   Loss=0.8294 (C:0.8294, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8294)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/356: Loss=0.6395 (C:0.6395, R:0.0105)
Batch  25/356: Loss=0.6716 (C:0.6716, R:0.0105)
Batch  50/356: Loss=0.6647 (C:0.6647, R:0.0105)
Batch  75/356: Loss=0.6670 (C:0.6670, R:0.0105)
Batch 100/356: Loss=0.6767 (C:0.6767, R:0.0105)
Batch 125/356: Loss=0.6983 (C:0.6983, R:0.0105)
Batch 150/356: Loss=0.6742 (C:0.6742, R:0.0105)
Batch 175/356: Loss=0.6452 (C:0.6452, R:0.0105)
Batch 200/356: Loss=0.6860 (C:0.6860, R:0.0105)
Batch 225/356: Loss=0.6980 (C:0.6980, R:0.0105)
Batch 250/356: Loss=0.6509 (C:0.6509, R:0.0106)
Batch 275/356: Loss=0.7258 (C:0.7258, R:0.0105)
Batch 300/356: Loss=0.6443 (C:0.6443, R:0.0105)
Batch 325/356: Loss=0.6775 (C:0.6775, R:0.0105)
Batch 350/356: Loss=0.6704 (C:0.6704, R:0.0105)

============================================================
Epoch 38/300 completed in 20.5s
Train: Loss=0.6682 (C:0.6682, R:0.0105) Ratio=4.69x
Val:   Loss=0.8302 (C:0.8302, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/356: Loss=0.6837 (C:0.6837, R:0.0105)
Batch  25/356: Loss=0.6747 (C:0.6747, R:0.0105)
Batch  50/356: Loss=0.6591 (C:0.6591, R:0.0105)
Batch  75/356: Loss=0.6693 (C:0.6693, R:0.0105)
Batch 100/356: Loss=0.7009 (C:0.7009, R:0.0105)
Batch 125/356: Loss=0.6912 (C:0.6912, R:0.0105)
Batch 150/356: Loss=0.6933 (C:0.6933, R:0.0105)
Batch 175/356: Loss=0.6668 (C:0.6668, R:0.0105)
Batch 200/356: Loss=0.6663 (C:0.6663, R:0.0105)
Batch 225/356: Loss=0.6956 (C:0.6956, R:0.0105)
Batch 250/356: Loss=0.6833 (C:0.6833, R:0.0105)
Batch 275/356: Loss=0.6531 (C:0.6531, R:0.0105)
Batch 300/356: Loss=0.6672 (C:0.6672, R:0.0105)
Batch 325/356: Loss=0.6245 (C:0.6245, R:0.0105)
Batch 350/356: Loss=0.6879 (C:0.6879, R:0.0105)

============================================================
Epoch 39/300 completed in 20.5s
Train: Loss=0.6670 (C:0.6670, R:0.0105) Ratio=4.75x
Val:   Loss=0.8224 (C:0.8224, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.135
✅ New best model saved (Val Loss: 0.8224)
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.322 ± 0.572
    Neg distances: 2.445 ± 1.038
    Separation ratio: 7.60x
    Gap: -4.070
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/356: Loss=0.6102 (C:0.6102, R:0.0105)
Batch  25/356: Loss=0.6157 (C:0.6157, R:0.0105)
Batch  50/356: Loss=0.6462 (C:0.6462, R:0.0105)
Batch  75/356: Loss=0.6244 (C:0.6244, R:0.0105)
Batch 100/356: Loss=0.6620 (C:0.6620, R:0.0105)
Batch 125/356: Loss=0.6341 (C:0.6341, R:0.0105)
Batch 150/356: Loss=0.6481 (C:0.6481, R:0.0105)
Batch 175/356: Loss=0.6410 (C:0.6410, R:0.0105)
Batch 200/356: Loss=0.6512 (C:0.6512, R:0.0105)
Batch 225/356: Loss=0.6329 (C:0.6329, R:0.0105)
Batch 250/356: Loss=0.6484 (C:0.6484, R:0.0105)
Batch 275/356: Loss=0.6610 (C:0.6610, R:0.0106)
Batch 300/356: Loss=0.6457 (C:0.6457, R:0.0105)
Batch 325/356: Loss=0.6469 (C:0.6469, R:0.0105)
Batch 350/356: Loss=0.6489 (C:0.6489, R:0.0105)

============================================================
Epoch 40/300 completed in 26.7s
Train: Loss=0.6371 (C:0.6371, R:0.0105) Ratio=4.83x
Val:   Loss=0.8111 (C:0.8111, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.8111)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/356: Loss=0.6335 (C:0.6335, R:0.0105)
Batch  25/356: Loss=0.6006 (C:0.6006, R:0.0105)
Batch  50/356: Loss=0.6128 (C:0.6128, R:0.0105)
Batch  75/356: Loss=0.6311 (C:0.6311, R:0.0105)
Batch 100/356: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 125/356: Loss=0.6403 (C:0.6403, R:0.0105)
Batch 150/356: Loss=0.6229 (C:0.6229, R:0.0106)
Batch 175/356: Loss=0.6786 (C:0.6786, R:0.0105)
Batch 200/356: Loss=0.6309 (C:0.6309, R:0.0105)
Batch 225/356: Loss=0.6545 (C:0.6545, R:0.0105)
Batch 250/356: Loss=0.6242 (C:0.6242, R:0.0105)
Batch 275/356: Loss=0.6471 (C:0.6471, R:0.0105)
Batch 300/356: Loss=0.6649 (C:0.6649, R:0.0105)
Batch 325/356: Loss=0.6833 (C:0.6833, R:0.0105)
Batch 350/356: Loss=0.6434 (C:0.6434, R:0.0105)

============================================================
Epoch 41/300 completed in 20.6s
Train: Loss=0.6361 (C:0.6361, R:0.0105) Ratio=4.84x
Val:   Loss=0.8012 (C:0.8012, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.8012)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/356: Loss=0.6191 (C:0.6191, R:0.0105)
Batch  25/356: Loss=0.6384 (C:0.6384, R:0.0105)
Batch  50/356: Loss=0.6176 (C:0.6176, R:0.0105)
Batch  75/356: Loss=0.6098 (C:0.6098, R:0.0105)
Batch 100/356: Loss=0.6180 (C:0.6180, R:0.0105)
Batch 125/356: Loss=0.6415 (C:0.6415, R:0.0105)
Batch 150/356: Loss=0.6324 (C:0.6324, R:0.0105)
Batch 175/356: Loss=0.6609 (C:0.6609, R:0.0105)
Batch 200/356: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 225/356: Loss=0.6686 (C:0.6686, R:0.0105)
Batch 250/356: Loss=0.6339 (C:0.6339, R:0.0105)
Batch 275/356: Loss=0.6475 (C:0.6475, R:0.0105)
Batch 300/356: Loss=0.6474 (C:0.6474, R:0.0105)
Batch 325/356: Loss=0.6474 (C:0.6474, R:0.0105)
Batch 350/356: Loss=0.6494 (C:0.6494, R:0.0105)

============================================================
Epoch 42/300 completed in 20.8s
Train: Loss=0.6322 (C:0.6322, R:0.0105) Ratio=4.88x
Val:   Loss=0.8135 (C:0.8135, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.180
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.331 ± 0.577
    Neg distances: 2.474 ± 1.056
    Separation ratio: 7.48x
    Gap: -4.199
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/356: Loss=0.6280 (C:0.6280, R:0.0105)
Batch  25/356: Loss=0.6209 (C:0.6209, R:0.0105)
Batch  50/356: Loss=0.6314 (C:0.6314, R:0.0105)
Batch  75/356: Loss=0.6281 (C:0.6281, R:0.0105)
Batch 100/356: Loss=0.6160 (C:0.6160, R:0.0105)
Batch 125/356: Loss=0.6477 (C:0.6477, R:0.0105)
Batch 150/356: Loss=0.6333 (C:0.6333, R:0.0105)
Batch 175/356: Loss=0.6469 (C:0.6469, R:0.0105)
Batch 200/356: Loss=0.6139 (C:0.6139, R:0.0105)
Batch 225/356: Loss=0.6370 (C:0.6370, R:0.0105)
Batch 250/356: Loss=0.6189 (C:0.6189, R:0.0105)
Batch 275/356: Loss=0.6098 (C:0.6098, R:0.0105)
Batch 300/356: Loss=0.6717 (C:0.6717, R:0.0105)
Batch 325/356: Loss=0.6240 (C:0.6240, R:0.0105)
Batch 350/356: Loss=0.6280 (C:0.6280, R:0.0105)

============================================================
Epoch 43/300 completed in 26.9s
Train: Loss=0.6331 (C:0.6331, R:0.0105) Ratio=4.87x
Val:   Loss=0.8068 (C:0.8068, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.195
No improvement for 2 epochs
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/356: Loss=0.6075 (C:0.6075, R:0.0105)
Batch  25/356: Loss=0.6068 (C:0.6068, R:0.0105)
Batch  50/356: Loss=0.6210 (C:0.6210, R:0.0105)
Batch  75/356: Loss=0.5949 (C:0.5949, R:0.0105)
Batch 100/356: Loss=0.6186 (C:0.6186, R:0.0105)
Batch 125/356: Loss=0.6279 (C:0.6279, R:0.0105)
Batch 150/356: Loss=0.6095 (C:0.6095, R:0.0105)
Batch 175/356: Loss=0.6509 (C:0.6509, R:0.0105)
Batch 200/356: Loss=0.6281 (C:0.6281, R:0.0105)
Batch 225/356: Loss=0.6252 (C:0.6252, R:0.0105)
Batch 250/356: Loss=0.6077 (C:0.6077, R:0.0105)
Batch 275/356: Loss=0.6495 (C:0.6495, R:0.0105)
Batch 300/356: Loss=0.6347 (C:0.6347, R:0.0105)
Batch 325/356: Loss=0.6086 (C:0.6086, R:0.0105)
Batch 350/356: Loss=0.6130 (C:0.6130, R:0.0105)

============================================================
Epoch 44/300 completed in 21.0s
Train: Loss=0.6299 (C:0.6299, R:0.0105) Ratio=5.07x
Val:   Loss=0.8076 (C:0.8076, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.210
No improvement for 3 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/356: Loss=0.6045 (C:0.6045, R:0.0105)
Batch  25/356: Loss=0.6133 (C:0.6133, R:0.0105)
Batch  50/356: Loss=0.6212 (C:0.6212, R:0.0105)
Batch  75/356: Loss=0.6082 (C:0.6082, R:0.0106)
Batch 100/356: Loss=0.6464 (C:0.6464, R:0.0105)
Batch 125/356: Loss=0.6087 (C:0.6087, R:0.0105)
Batch 150/356: Loss=0.6319 (C:0.6319, R:0.0105)
Batch 175/356: Loss=0.6329 (C:0.6329, R:0.0105)
Batch 200/356: Loss=0.6468 (C:0.6468, R:0.0105)
Batch 225/356: Loss=0.6011 (C:0.6011, R:0.0105)
Batch 250/356: Loss=0.6268 (C:0.6268, R:0.0105)
Batch 275/356: Loss=0.6537 (C:0.6537, R:0.0105)
Batch 300/356: Loss=0.6319 (C:0.6319, R:0.0105)
Batch 325/356: Loss=0.6359 (C:0.6359, R:0.0105)
Batch 350/356: Loss=0.6093 (C:0.6093, R:0.0105)

============================================================
Epoch 45/300 completed in 21.0s
Train: Loss=0.6292 (C:0.6292, R:0.0105) Ratio=4.92x
Val:   Loss=0.8106 (C:0.8106, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.225
No improvement for 4 epochs
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.347 ± 0.610
    Neg distances: 2.517 ± 1.076
    Separation ratio: 7.25x
    Gap: -4.274
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/356: Loss=0.5949 (C:0.5949, R:0.0105)
Batch  25/356: Loss=0.6264 (C:0.6264, R:0.0105)
Batch  50/356: Loss=0.6406 (C:0.6406, R:0.0105)
Batch  75/356: Loss=0.6115 (C:0.6115, R:0.0105)
Batch 100/356: Loss=0.6443 (C:0.6443, R:0.0105)
Batch 125/356: Loss=0.6175 (C:0.6175, R:0.0105)
Batch 150/356: Loss=0.6417 (C:0.6417, R:0.0105)
Batch 175/356: Loss=0.6338 (C:0.6338, R:0.0105)
Batch 200/356: Loss=0.5885 (C:0.5885, R:0.0105)
Batch 225/356: Loss=0.6412 (C:0.6412, R:0.0105)
Batch 250/356: Loss=0.6526 (C:0.6526, R:0.0105)
Batch 275/356: Loss=0.6376 (C:0.6376, R:0.0105)
Batch 300/356: Loss=0.6165 (C:0.6165, R:0.0105)
Batch 325/356: Loss=0.6446 (C:0.6446, R:0.0105)
Batch 350/356: Loss=0.6452 (C:0.6452, R:0.0105)

============================================================
Epoch 46/300 completed in 26.2s
Train: Loss=0.6299 (C:0.6299, R:0.0105) Ratio=5.03x
Val:   Loss=0.7994 (C:0.7994, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.240
✅ New best model saved (Val Loss: 0.7994)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/356: Loss=0.6278 (C:0.6278, R:0.0105)
Batch  25/356: Loss=0.6163 (C:0.6163, R:0.0105)
Batch  50/356: Loss=0.6269 (C:0.6269, R:0.0105)
Batch  75/356: Loss=0.6106 (C:0.6106, R:0.0105)
Batch 100/356: Loss=0.6373 (C:0.6373, R:0.0105)
Batch 125/356: Loss=0.5880 (C:0.5880, R:0.0105)
Batch 150/356: Loss=0.6034 (C:0.6034, R:0.0105)
Batch 175/356: Loss=0.6139 (C:0.6139, R:0.0105)
Batch 200/356: Loss=0.6249 (C:0.6249, R:0.0105)
Batch 225/356: Loss=0.6347 (C:0.6347, R:0.0105)
Batch 250/356: Loss=0.6210 (C:0.6210, R:0.0105)
Batch 275/356: Loss=0.6524 (C:0.6524, R:0.0105)
Batch 300/356: Loss=0.6415 (C:0.6415, R:0.0105)
Batch 325/356: Loss=0.6138 (C:0.6138, R:0.0105)
Batch 350/356: Loss=0.5992 (C:0.5992, R:0.0105)

============================================================
Epoch 47/300 completed in 20.5s
Train: Loss=0.6265 (C:0.6265, R:0.0105) Ratio=5.11x
Val:   Loss=0.8155 (C:0.8155, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.255
No improvement for 1 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/356: Loss=0.6767 (C:0.6767, R:0.0105)
Batch  25/356: Loss=0.5870 (C:0.5870, R:0.0105)
Batch  50/356: Loss=0.6199 (C:0.6199, R:0.0105)
Batch  75/356: Loss=0.6559 (C:0.6559, R:0.0105)
Batch 100/356: Loss=0.6305 (C:0.6305, R:0.0105)
Batch 125/356: Loss=0.6030 (C:0.6030, R:0.0105)
Batch 150/356: Loss=0.6557 (C:0.6557, R:0.0105)
Batch 175/356: Loss=0.6261 (C:0.6261, R:0.0105)
Batch 200/356: Loss=0.6459 (C:0.6459, R:0.0105)
Batch 225/356: Loss=0.6104 (C:0.6104, R:0.0105)
Batch 250/356: Loss=0.6359 (C:0.6359, R:0.0105)
Batch 275/356: Loss=0.6400 (C:0.6400, R:0.0105)
Batch 300/356: Loss=0.6696 (C:0.6696, R:0.0105)
Batch 325/356: Loss=0.6472 (C:0.6472, R:0.0105)
Batch 350/356: Loss=0.6228 (C:0.6228, R:0.0105)

============================================================
Epoch 48/300 completed in 21.2s
Train: Loss=0.6267 (C:0.6267, R:0.0105) Ratio=4.95x
Val:   Loss=0.7985 (C:0.7985, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.270
✅ New best model saved (Val Loss: 0.7985)
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.303 ± 0.570
    Neg distances: 2.545 ± 1.058
    Separation ratio: 8.40x
    Gap: -4.226
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/356: Loss=0.5606 (C:0.5606, R:0.0105)
Batch  25/356: Loss=0.5688 (C:0.5688, R:0.0105)
Batch  50/356: Loss=0.5962 (C:0.5962, R:0.0105)
Batch  75/356: Loss=0.6063 (C:0.6063, R:0.0105)
Batch 100/356: Loss=0.5517 (C:0.5517, R:0.0105)
Batch 125/356: Loss=0.6190 (C:0.6190, R:0.0105)
Batch 150/356: Loss=0.6050 (C:0.6050, R:0.0105)
Batch 175/356: Loss=0.5905 (C:0.5905, R:0.0105)
Batch 200/356: Loss=0.5839 (C:0.5839, R:0.0105)
Batch 225/356: Loss=0.6260 (C:0.6260, R:0.0105)
Batch 250/356: Loss=0.5707 (C:0.5707, R:0.0105)
Batch 275/356: Loss=0.5557 (C:0.5557, R:0.0105)
Batch 300/356: Loss=0.5556 (C:0.5556, R:0.0105)
Batch 325/356: Loss=0.5962 (C:0.5962, R:0.0106)
Batch 350/356: Loss=0.6183 (C:0.6183, R:0.0105)

============================================================
Epoch 49/300 completed in 27.1s
Train: Loss=0.5908 (C:0.5908, R:0.0105) Ratio=5.13x
Val:   Loss=0.7750 (C:0.7750, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.7750)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/356: Loss=0.6012 (C:0.6012, R:0.0105)
Batch  25/356: Loss=0.5994 (C:0.5994, R:0.0105)
Batch  50/356: Loss=0.5721 (C:0.5721, R:0.0105)
Batch  75/356: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 100/356: Loss=0.5587 (C:0.5587, R:0.0105)
Batch 125/356: Loss=0.5796 (C:0.5796, R:0.0105)
Batch 150/356: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 175/356: Loss=0.6155 (C:0.6155, R:0.0105)
Batch 200/356: Loss=0.5967 (C:0.5967, R:0.0105)
Batch 225/356: Loss=0.6139 (C:0.6139, R:0.0105)
Batch 250/356: Loss=0.5623 (C:0.5623, R:0.0105)
Batch 275/356: Loss=0.6171 (C:0.6171, R:0.0106)
Batch 300/356: Loss=0.5935 (C:0.5935, R:0.0105)
Batch 325/356: Loss=0.6177 (C:0.6177, R:0.0105)
Batch 350/356: Loss=0.5962 (C:0.5962, R:0.0105)

============================================================
Epoch 50/300 completed in 20.8s
Train: Loss=0.5896 (C:0.5896, R:0.0105) Ratio=5.11x
Val:   Loss=0.7759 (C:0.7759, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/356: Loss=0.5997 (C:0.5997, R:0.0105)
Batch  25/356: Loss=0.5847 (C:0.5847, R:0.0105)
Batch  50/356: Loss=0.5882 (C:0.5882, R:0.0105)
Batch  75/356: Loss=0.5968 (C:0.5968, R:0.0105)
Batch 100/356: Loss=0.5908 (C:0.5908, R:0.0105)
Batch 125/356: Loss=0.5920 (C:0.5920, R:0.0105)
Batch 150/356: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 175/356: Loss=0.5801 (C:0.5801, R:0.0105)
Batch 200/356: Loss=0.5492 (C:0.5492, R:0.0105)
Batch 225/356: Loss=0.6159 (C:0.6159, R:0.0105)
Batch 250/356: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 275/356: Loss=0.5709 (C:0.5709, R:0.0105)
Batch 300/356: Loss=0.6101 (C:0.6101, R:0.0105)
Batch 325/356: Loss=0.6065 (C:0.6065, R:0.0105)
Batch 350/356: Loss=0.6107 (C:0.6107, R:0.0105)

============================================================
Epoch 51/300 completed in 21.0s
Train: Loss=0.5881 (C:0.5881, R:0.0105) Ratio=5.02x
Val:   Loss=0.7865 (C:0.7865, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.305 ± 0.552
    Neg distances: 2.599 ± 1.089
    Separation ratio: 8.51x
    Gap: -4.222
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/356: Loss=0.5962 (C:0.5962, R:0.0105)
Batch  25/356: Loss=0.5548 (C:0.5548, R:0.0105)
Batch  50/356: Loss=0.5528 (C:0.5528, R:0.0105)
Batch  75/356: Loss=0.5837 (C:0.5837, R:0.0105)
Batch 100/356: Loss=0.5827 (C:0.5827, R:0.0105)
Batch 125/356: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 150/356: Loss=0.5906 (C:0.5906, R:0.0105)
Batch 175/356: Loss=0.6187 (C:0.6187, R:0.0105)
Batch 200/356: Loss=0.5722 (C:0.5722, R:0.0105)
Batch 225/356: Loss=0.5655 (C:0.5655, R:0.0105)
Batch 250/356: Loss=0.5797 (C:0.5797, R:0.0105)
Batch 275/356: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 300/356: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 325/356: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 350/356: Loss=0.5888 (C:0.5888, R:0.0105)

============================================================
Epoch 52/300 completed in 26.7s
Train: Loss=0.5870 (C:0.5870, R:0.0105) Ratio=5.25x
Val:   Loss=0.7725 (C:0.7725, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7725)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/356: Loss=0.5937 (C:0.5937, R:0.0105)
Batch  25/356: Loss=0.5702 (C:0.5702, R:0.0105)
Batch  50/356: Loss=0.6025 (C:0.6025, R:0.0105)
Batch  75/356: Loss=0.5861 (C:0.5861, R:0.0105)
Batch 100/356: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 125/356: Loss=0.5892 (C:0.5892, R:0.0105)
Batch 150/356: Loss=0.5832 (C:0.5832, R:0.0105)
Batch 175/356: Loss=0.5960 (C:0.5960, R:0.0105)
Batch 200/356: Loss=0.5667 (C:0.5667, R:0.0105)
Batch 225/356: Loss=0.5978 (C:0.5978, R:0.0105)
Batch 250/356: Loss=0.6060 (C:0.6060, R:0.0105)
Batch 275/356: Loss=0.5963 (C:0.5963, R:0.0105)
Batch 300/356: Loss=0.5892 (C:0.5892, R:0.0105)
Batch 325/356: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 350/356: Loss=0.6028 (C:0.6028, R:0.0105)

============================================================
Epoch 53/300 completed in 20.8s
Train: Loss=0.5841 (C:0.5841, R:0.0105) Ratio=5.18x
Val:   Loss=0.7895 (C:0.7895, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/356: Loss=0.5617 (C:0.5617, R:0.0105)
Batch  25/356: Loss=0.5360 (C:0.5360, R:0.0105)
Batch  50/356: Loss=0.5563 (C:0.5563, R:0.0105)
Batch  75/356: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 100/356: Loss=0.5356 (C:0.5356, R:0.0105)
Batch 125/356: Loss=0.5461 (C:0.5461, R:0.0105)
Batch 150/356: Loss=0.5826 (C:0.5826, R:0.0105)
Batch 175/356: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 200/356: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 225/356: Loss=0.6129 (C:0.6129, R:0.0105)
Batch 250/356: Loss=0.5886 (C:0.5886, R:0.0105)
Batch 275/356: Loss=0.5959 (C:0.5959, R:0.0105)
Batch 300/356: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 325/356: Loss=0.5737 (C:0.5737, R:0.0105)
Batch 350/356: Loss=0.5835 (C:0.5835, R:0.0105)

============================================================
Epoch 54/300 completed in 21.1s
Train: Loss=0.5821 (C:0.5821, R:0.0105) Ratio=5.27x
Val:   Loss=0.7839 (C:0.7839, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.309 ± 0.569
    Neg distances: 2.621 ± 1.097
    Separation ratio: 8.48x
    Gap: -4.475
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/356: Loss=0.6008 (C:0.6008, R:0.0105)
Batch  25/356: Loss=0.5691 (C:0.5691, R:0.0105)
Batch  50/356: Loss=0.5749 (C:0.5749, R:0.0105)
Batch  75/356: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 100/356: Loss=0.5993 (C:0.5993, R:0.0105)
Batch 125/356: Loss=0.6023 (C:0.6023, R:0.0105)
Batch 150/356: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 175/356: Loss=0.5728 (C:0.5728, R:0.0105)
Batch 200/356: Loss=0.5946 (C:0.5946, R:0.0106)
Batch 225/356: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 250/356: Loss=0.6283 (C:0.6283, R:0.0105)
Batch 275/356: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 300/356: Loss=0.5916 (C:0.5916, R:0.0105)
Batch 325/356: Loss=0.5756 (C:0.5756, R:0.0105)
Batch 350/356: Loss=0.5911 (C:0.5911, R:0.0105)

============================================================
Epoch 55/300 completed in 27.4s
Train: Loss=0.5808 (C:0.5808, R:0.0105) Ratio=5.15x
Val:   Loss=0.7789 (C:0.7789, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/356: Loss=0.5578 (C:0.5578, R:0.0105)
Batch  25/356: Loss=0.5956 (C:0.5956, R:0.0105)
Batch  50/356: Loss=0.5244 (C:0.5244, R:0.0105)
Batch  75/356: Loss=0.5500 (C:0.5500, R:0.0105)
Batch 100/356: Loss=0.5394 (C:0.5394, R:0.0105)
Batch 125/356: Loss=0.5780 (C:0.5780, R:0.0105)
Batch 150/356: Loss=0.5879 (C:0.5879, R:0.0105)
Batch 175/356: Loss=0.5930 (C:0.5930, R:0.0105)
Batch 200/356: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 225/356: Loss=0.5740 (C:0.5740, R:0.0105)
Batch 250/356: Loss=0.5969 (C:0.5969, R:0.0105)
Batch 275/356: Loss=0.5772 (C:0.5772, R:0.0105)
Batch 300/356: Loss=0.6010 (C:0.6010, R:0.0105)
Batch 325/356: Loss=0.6032 (C:0.6032, R:0.0105)
Batch 350/356: Loss=0.5606 (C:0.5606, R:0.0105)

============================================================
Epoch 56/300 completed in 21.3s
Train: Loss=0.5787 (C:0.5787, R:0.0105) Ratio=5.29x
Val:   Loss=0.7820 (C:0.7820, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/356: Loss=0.5966 (C:0.5966, R:0.0105)
Batch  25/356: Loss=0.5720 (C:0.5720, R:0.0105)
Batch  50/356: Loss=0.5787 (C:0.5787, R:0.0105)
Batch  75/356: Loss=0.6229 (C:0.6229, R:0.0105)
Batch 100/356: Loss=0.5703 (C:0.5703, R:0.0105)
Batch 125/356: Loss=0.5893 (C:0.5893, R:0.0105)
Batch 150/356: Loss=0.5930 (C:0.5930, R:0.0105)
Batch 175/356: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 200/356: Loss=0.6030 (C:0.6030, R:0.0105)
Batch 225/356: Loss=0.6034 (C:0.6034, R:0.0105)
Batch 250/356: Loss=0.5726 (C:0.5726, R:0.0105)
Batch 275/356: Loss=0.5940 (C:0.5940, R:0.0105)
Batch 300/356: Loss=0.5927 (C:0.5927, R:0.0105)
Batch 325/356: Loss=0.5895 (C:0.5895, R:0.0105)
Batch 350/356: Loss=0.6196 (C:0.6196, R:0.0105)

============================================================
Epoch 57/300 completed in 20.7s
Train: Loss=0.5755 (C:0.5755, R:0.0105) Ratio=5.13x
Val:   Loss=0.7743 (C:0.7743, R:0.0104) Ratio=3.19x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.310 ± 0.587
    Neg distances: 2.599 ± 1.088
    Separation ratio: 8.38x
    Gap: -4.411
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/356: Loss=0.5626 (C:0.5626, R:0.0105)
Batch  25/356: Loss=0.5663 (C:0.5663, R:0.0105)
Batch  50/356: Loss=0.5605 (C:0.5605, R:0.0105)
Batch  75/356: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 100/356: Loss=0.5768 (C:0.5768, R:0.0106)
Batch 125/356: Loss=0.5749 (C:0.5749, R:0.0105)
Batch 150/356: Loss=0.5981 (C:0.5981, R:0.0105)
Batch 175/356: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 200/356: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 225/356: Loss=0.5394 (C:0.5394, R:0.0105)
Batch 250/356: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 275/356: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 300/356: Loss=0.6105 (C:0.6105, R:0.0105)
Batch 325/356: Loss=0.5969 (C:0.5969, R:0.0105)
Batch 350/356: Loss=0.5847 (C:0.5847, R:0.0105)

============================================================
Epoch 58/300 completed in 26.6s
Train: Loss=0.5774 (C:0.5774, R:0.0105) Ratio=5.26x
Val:   Loss=0.7728 (C:0.7728, R:0.0104) Ratio=3.17x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/356: Loss=0.5729 (C:0.5729, R:0.0105)
Batch  25/356: Loss=0.5871 (C:0.5871, R:0.0105)
Batch  50/356: Loss=0.5591 (C:0.5591, R:0.0105)
Batch  75/356: Loss=0.5983 (C:0.5983, R:0.0105)
Batch 100/356: Loss=0.5345 (C:0.5345, R:0.0105)
Batch 125/356: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 150/356: Loss=0.5607 (C:0.5607, R:0.0105)
Batch 175/356: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 200/356: Loss=0.5812 (C:0.5812, R:0.0105)
Batch 225/356: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 250/356: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 275/356: Loss=0.5784 (C:0.5784, R:0.0105)
Batch 300/356: Loss=0.5780 (C:0.5780, R:0.0105)
Batch 325/356: Loss=0.5918 (C:0.5918, R:0.0105)
Batch 350/356: Loss=0.6203 (C:0.6203, R:0.0105)

============================================================
Epoch 59/300 completed in 20.6s
Train: Loss=0.5747 (C:0.5747, R:0.0105) Ratio=5.30x
Val:   Loss=0.7808 (C:0.7808, R:0.0104) Ratio=3.16x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/356: Loss=0.5710 (C:0.5710, R:0.0106)
Batch  25/356: Loss=0.5584 (C:0.5584, R:0.0105)
Batch  50/356: Loss=0.5908 (C:0.5908, R:0.0105)
Batch  75/356: Loss=0.5683 (C:0.5683, R:0.0105)
Batch 100/356: Loss=0.6041 (C:0.6041, R:0.0105)
Batch 125/356: Loss=0.5535 (C:0.5535, R:0.0105)
Batch 150/356: Loss=0.5807 (C:0.5807, R:0.0105)
Batch 175/356: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 200/356: Loss=0.5729 (C:0.5729, R:0.0105)
Batch 225/356: Loss=0.6026 (C:0.6026, R:0.0105)
Batch 250/356: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 275/356: Loss=0.5400 (C:0.5400, R:0.0105)
Batch 300/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 325/356: Loss=0.5636 (C:0.5636, R:0.0105)
Batch 350/356: Loss=0.5491 (C:0.5491, R:0.0105)

============================================================
Epoch 60/300 completed in 21.3s
Train: Loss=0.5742 (C:0.5742, R:0.0105) Ratio=5.36x
Val:   Loss=0.7943 (C:0.7943, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 8 epochs
Checkpoint saved at epoch 60

Early stopping triggered after 60 epochs
Best model was at epoch 52 with Val Loss: 0.7725

Global Dataset Training Completed!
Best epoch: 52
Best validation loss: 0.7725
Final separation ratios: Train=5.36x, Val=3.10x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/7 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4641
  Adjusted Rand Score: 0.5304
  Clustering Accuracy: 0.8131
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
Extracted representations: torch.Size([546816, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/6 batches
Extracted representations: torch.Size([9216, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9216 samples
Classification Results:
  Accuracy: 0.8153
  Per-class F1: [0.834850016852039, 0.7504351954423168, 0.8629228030425634]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.752 ± 0.927
  Negative distances: 2.320 ± 1.231
  Separation ratio: 3.09x
  Gap: -4.457
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4641
  Clustering Accuracy: 0.8131
  Adjusted Rand Score: 0.5304

Classification Performance:
  Accuracy: 0.8153

Separation Quality:
  Separation Ratio: 3.09x
  Gap: -4.457
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537/results/evaluation_results_20250714_213851.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537/results/evaluation_results_20250714_213851.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat75_bs1536_20250714_211537/final_results.json

Key Results:
  Separation ratio: 3.09x
  Perfect separation: False
  Classification accuracy: 0.8153
  Result: 0.8153% (improvement: +-80.85%)
  Cleaning up: coarse_lr2e-04_lat75_bs1536_20250714_211537

[11/12] Testing: coarse_lr2e-04_lat100_bs1020
  Learning rate: 0.0002
  Latent dim: 100
  Batch size: 1020
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 21:38:52.109116
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 100
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,889,380
Model created with 1,889,380 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0002)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,889,380
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.104 ± 0.012
    Neg distances: 0.105 ± 0.012
    Separation ratio: 1.00x
    Gap: -0.146
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9998 (C:1.9998, R:0.0117)
Batch  25/537: Loss=1.9756 (C:1.9756, R:0.0113)
Batch  50/537: Loss=1.9679 (C:1.9679, R:0.0110)
Batch  75/537: Loss=1.9533 (C:1.9533, R:0.0108)
Batch 100/537: Loss=1.9469 (C:1.9469, R:0.0107)
Batch 125/537: Loss=1.9378 (C:1.9378, R:0.0106)
Batch 150/537: Loss=1.9337 (C:1.9337, R:0.0106)
Batch 175/537: Loss=1.9280 (C:1.9280, R:0.0106)
Batch 200/537: Loss=1.9243 (C:1.9243, R:0.0105)
Batch 225/537: Loss=1.9120 (C:1.9120, R:0.0105)
Batch 250/537: Loss=1.9065 (C:1.9065, R:0.0105)
Batch 275/537: Loss=1.9132 (C:1.9132, R:0.0105)
Batch 300/537: Loss=1.9110 (C:1.9110, R:0.0105)
Batch 325/537: Loss=1.9045 (C:1.9045, R:0.0106)
Batch 350/537: Loss=1.9005 (C:1.9005, R:0.0105)
Batch 375/537: Loss=1.9027 (C:1.9027, R:0.0105)
Batch 400/537: Loss=1.8961 (C:1.8961, R:0.0105)
Batch 425/537: Loss=1.8949 (C:1.8949, R:0.0105)
Batch 450/537: Loss=1.9022 (C:1.9022, R:0.0105)
Batch 475/537: Loss=1.9040 (C:1.9040, R:0.0105)
Batch 500/537: Loss=1.8958 (C:1.8958, R:0.0105)
Batch 525/537: Loss=1.8971 (C:1.8971, R:0.0105)

============================================================
Epoch 1/300 completed in 27.3s
Train: Loss=1.9206 (C:1.9206, R:0.0106) Ratio=1.80x
Val:   Loss=1.8868 (C:1.8868, R:0.0104) Ratio=2.30x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8868)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.8968 (C:1.8968, R:0.0105)
Batch  25/537: Loss=1.8903 (C:1.8903, R:0.0105)
Batch  50/537: Loss=1.8903 (C:1.8903, R:0.0105)
Batch  75/537: Loss=1.8876 (C:1.8876, R:0.0105)
Batch 100/537: Loss=1.8950 (C:1.8950, R:0.0105)
Batch 125/537: Loss=1.8795 (C:1.8795, R:0.0105)
Batch 150/537: Loss=1.8875 (C:1.8875, R:0.0105)
Batch 175/537: Loss=1.8895 (C:1.8895, R:0.0105)
Batch 200/537: Loss=1.8862 (C:1.8862, R:0.0105)
Batch 225/537: Loss=1.8901 (C:1.8901, R:0.0105)
Batch 250/537: Loss=1.8923 (C:1.8923, R:0.0105)
Batch 275/537: Loss=1.8818 (C:1.8818, R:0.0105)
Batch 300/537: Loss=1.8968 (C:1.8968, R:0.0105)
Batch 325/537: Loss=1.8839 (C:1.8839, R:0.0105)
Batch 350/537: Loss=1.8904 (C:1.8904, R:0.0105)
Batch 375/537: Loss=1.8896 (C:1.8896, R:0.0106)
Batch 400/537: Loss=1.8881 (C:1.8881, R:0.0105)
Batch 425/537: Loss=1.8835 (C:1.8835, R:0.0106)
Batch 450/537: Loss=1.8851 (C:1.8851, R:0.0105)
Batch 475/537: Loss=1.8751 (C:1.8751, R:0.0105)
Batch 500/537: Loss=1.8884 (C:1.8884, R:0.0105)
Batch 525/537: Loss=1.8719 (C:1.8719, R:0.0105)

============================================================
Epoch 2/300 completed in 21.2s
Train: Loss=1.8837 (C:1.8837, R:0.0105) Ratio=2.29x
Val:   Loss=1.8772 (C:1.8772, R:0.0104) Ratio=2.50x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8772)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8733 (C:1.8733, R:0.0105)
Batch  25/537: Loss=1.8741 (C:1.8741, R:0.0105)
Batch  50/537: Loss=1.8823 (C:1.8823, R:0.0105)
Batch  75/537: Loss=1.8749 (C:1.8749, R:0.0105)
Batch 100/537: Loss=1.8800 (C:1.8800, R:0.0105)
Batch 125/537: Loss=1.8733 (C:1.8733, R:0.0105)
Batch 150/537: Loss=1.8706 (C:1.8706, R:0.0105)
Batch 175/537: Loss=1.8741 (C:1.8741, R:0.0105)
Batch 200/537: Loss=1.8677 (C:1.8677, R:0.0105)
Batch 225/537: Loss=1.8787 (C:1.8787, R:0.0105)
Batch 250/537: Loss=1.8734 (C:1.8734, R:0.0105)
Batch 275/537: Loss=1.8763 (C:1.8763, R:0.0105)
Batch 300/537: Loss=1.8852 (C:1.8852, R:0.0105)
Batch 325/537: Loss=1.8695 (C:1.8695, R:0.0105)
Batch 350/537: Loss=1.8765 (C:1.8765, R:0.0105)
Batch 375/537: Loss=1.8730 (C:1.8730, R:0.0105)
Batch 400/537: Loss=1.8794 (C:1.8794, R:0.0105)
Batch 425/537: Loss=1.8655 (C:1.8655, R:0.0105)
Batch 450/537: Loss=1.8722 (C:1.8722, R:0.0105)
Batch 475/537: Loss=1.8735 (C:1.8735, R:0.0105)
Batch 500/537: Loss=1.8731 (C:1.8731, R:0.0105)
Batch 525/537: Loss=1.8570 (C:1.8570, R:0.0105)

============================================================
Epoch 3/300 completed in 21.7s
Train: Loss=1.8724 (C:1.8724, R:0.0105) Ratio=2.54x
Val:   Loss=1.8719 (C:1.8719, R:0.0104) Ratio=2.64x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8719)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.542 ± 0.565
    Neg distances: 1.576 ± 0.848
    Separation ratio: 2.91x
    Gap: -3.355
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.1865 (C:1.1865, R:0.0105)
Batch  25/537: Loss=1.1500 (C:1.1500, R:0.0105)
Batch  50/537: Loss=1.1477 (C:1.1477, R:0.0105)
Batch  75/537: Loss=1.2168 (C:1.2168, R:0.0105)
Batch 100/537: Loss=1.1733 (C:1.1733, R:0.0105)
Batch 125/537: Loss=1.1771 (C:1.1771, R:0.0105)
Batch 150/537: Loss=1.1775 (C:1.1775, R:0.0106)
Batch 175/537: Loss=1.1928 (C:1.1928, R:0.0105)
Batch 200/537: Loss=1.1709 (C:1.1709, R:0.0105)
Batch 225/537: Loss=1.2044 (C:1.2044, R:0.0105)
Batch 250/537: Loss=1.1712 (C:1.1712, R:0.0105)
Batch 275/537: Loss=1.2175 (C:1.2175, R:0.0105)
Batch 300/537: Loss=1.2159 (C:1.2159, R:0.0105)
Batch 325/537: Loss=1.1480 (C:1.1480, R:0.0105)
Batch 350/537: Loss=1.1680 (C:1.1680, R:0.0105)
Batch 375/537: Loss=1.2087 (C:1.2087, R:0.0105)
Batch 400/537: Loss=1.2060 (C:1.2060, R:0.0105)
Batch 425/537: Loss=1.1925 (C:1.1925, R:0.0105)
Batch 450/537: Loss=1.1421 (C:1.1421, R:0.0105)
Batch 475/537: Loss=1.1593 (C:1.1593, R:0.0106)
Batch 500/537: Loss=1.1829 (C:1.1829, R:0.0105)
Batch 525/537: Loss=1.1931 (C:1.1931, R:0.0105)

============================================================
Epoch 4/300 completed in 28.0s
Train: Loss=1.1791 (C:1.1791, R:0.0105) Ratio=2.61x
Val:   Loss=1.1722 (C:1.1722, R:0.0104) Ratio=2.74x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1722)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.1289 (C:1.1289, R:0.0105)
Batch  25/537: Loss=1.1910 (C:1.1910, R:0.0105)
Batch  50/537: Loss=1.1325 (C:1.1325, R:0.0105)
Batch  75/537: Loss=1.1582 (C:1.1582, R:0.0105)
Batch 100/537: Loss=1.1341 (C:1.1341, R:0.0105)
Batch 125/537: Loss=1.1514 (C:1.1514, R:0.0105)
Batch 150/537: Loss=1.1568 (C:1.1568, R:0.0105)
Batch 175/537: Loss=1.1766 (C:1.1766, R:0.0105)
Batch 200/537: Loss=1.1670 (C:1.1670, R:0.0105)
Batch 225/537: Loss=1.1692 (C:1.1692, R:0.0105)
Batch 250/537: Loss=1.1486 (C:1.1486, R:0.0105)
Batch 275/537: Loss=1.1501 (C:1.1501, R:0.0105)
Batch 300/537: Loss=1.1686 (C:1.1686, R:0.0105)
Batch 325/537: Loss=1.1538 (C:1.1538, R:0.0105)
Batch 350/537: Loss=1.1485 (C:1.1485, R:0.0105)
Batch 375/537: Loss=1.1680 (C:1.1680, R:0.0105)
Batch 400/537: Loss=1.1357 (C:1.1357, R:0.0105)
Batch 425/537: Loss=1.1388 (C:1.1388, R:0.0105)
Batch 450/537: Loss=1.1427 (C:1.1427, R:0.0105)
Batch 475/537: Loss=1.1672 (C:1.1672, R:0.0105)
Batch 500/537: Loss=1.1611 (C:1.1611, R:0.0105)
Batch 525/537: Loss=1.1546 (C:1.1546, R:0.0105)

============================================================
Epoch 5/300 completed in 21.6s
Train: Loss=1.1554 (C:1.1554, R:0.0105) Ratio=2.87x
Val:   Loss=1.1609 (C:1.1609, R:0.0104) Ratio=2.80x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1609)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.1303 (C:1.1303, R:0.0105)
Batch  25/537: Loss=1.1184 (C:1.1184, R:0.0105)
Batch  50/537: Loss=1.1134 (C:1.1134, R:0.0106)
Batch  75/537: Loss=1.1464 (C:1.1464, R:0.0105)
Batch 100/537: Loss=1.1560 (C:1.1560, R:0.0105)
Batch 125/537: Loss=1.1383 (C:1.1383, R:0.0105)
Batch 150/537: Loss=1.0788 (C:1.0788, R:0.0105)
Batch 175/537: Loss=1.1268 (C:1.1268, R:0.0105)
Batch 200/537: Loss=1.1577 (C:1.1577, R:0.0105)
Batch 225/537: Loss=1.1374 (C:1.1374, R:0.0105)
Batch 250/537: Loss=1.1484 (C:1.1484, R:0.0105)
Batch 275/537: Loss=1.1478 (C:1.1478, R:0.0105)
Batch 300/537: Loss=1.1509 (C:1.1509, R:0.0105)
Batch 325/537: Loss=1.1476 (C:1.1476, R:0.0105)
Batch 350/537: Loss=1.0939 (C:1.0939, R:0.0105)
Batch 375/537: Loss=1.1211 (C:1.1211, R:0.0105)
Batch 400/537: Loss=1.1189 (C:1.1189, R:0.0106)
Batch 425/537: Loss=1.1410 (C:1.1410, R:0.0106)
Batch 450/537: Loss=1.1822 (C:1.1822, R:0.0105)
Batch 475/537: Loss=1.1327 (C:1.1327, R:0.0105)
Batch 500/537: Loss=1.1306 (C:1.1306, R:0.0105)
Batch 525/537: Loss=1.1401 (C:1.1401, R:0.0105)

============================================================
Epoch 6/300 completed in 21.1s
Train: Loss=1.1413 (C:1.1413, R:0.0105) Ratio=3.04x
Val:   Loss=1.1509 (C:1.1509, R:0.0104) Ratio=2.88x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1509)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.467 ± 0.576
    Neg distances: 1.699 ± 0.868
    Separation ratio: 3.63x
    Gap: -3.079
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.0406 (C:1.0406, R:0.0105)
Batch  25/537: Loss=1.0404 (C:1.0404, R:0.0105)
Batch  50/537: Loss=1.0875 (C:1.0875, R:0.0105)
Batch  75/537: Loss=1.0191 (C:1.0191, R:0.0105)
Batch 100/537: Loss=1.0550 (C:1.0550, R:0.0105)
Batch 125/537: Loss=1.0662 (C:1.0662, R:0.0105)
Batch 150/537: Loss=1.0370 (C:1.0370, R:0.0105)
Batch 175/537: Loss=1.0219 (C:1.0219, R:0.0105)
Batch 200/537: Loss=1.0431 (C:1.0431, R:0.0105)
Batch 225/537: Loss=1.0837 (C:1.0837, R:0.0105)
Batch 250/537: Loss=1.0606 (C:1.0606, R:0.0105)
Batch 275/537: Loss=1.0786 (C:1.0786, R:0.0105)
Batch 300/537: Loss=1.0257 (C:1.0257, R:0.0105)
Batch 325/537: Loss=1.0389 (C:1.0389, R:0.0105)
Batch 350/537: Loss=1.0658 (C:1.0658, R:0.0105)
Batch 375/537: Loss=1.0339 (C:1.0339, R:0.0105)
Batch 400/537: Loss=1.0575 (C:1.0575, R:0.0105)
Batch 425/537: Loss=1.0475 (C:1.0475, R:0.0105)
Batch 450/537: Loss=1.0618 (C:1.0618, R:0.0105)
Batch 475/537: Loss=1.0699 (C:1.0699, R:0.0105)
Batch 500/537: Loss=1.0946 (C:1.0946, R:0.0105)
Batch 525/537: Loss=1.0154 (C:1.0154, R:0.0105)

============================================================
Epoch 7/300 completed in 26.6s
Train: Loss=1.0480 (C:1.0480, R:0.0105) Ratio=3.11x
Val:   Loss=1.0667 (C:1.0667, R:0.0104) Ratio=2.92x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0667)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=0.9971 (C:0.9971, R:0.0105)
Batch  25/537: Loss=1.0147 (C:1.0147, R:0.0105)
Batch  50/537: Loss=1.0206 (C:1.0206, R:0.0105)
Batch  75/537: Loss=1.0558 (C:1.0558, R:0.0105)
Batch 100/537: Loss=1.0332 (C:1.0332, R:0.0105)
Batch 125/537: Loss=1.0140 (C:1.0140, R:0.0105)
Batch 150/537: Loss=1.0337 (C:1.0337, R:0.0105)
Batch 175/537: Loss=1.0340 (C:1.0340, R:0.0105)
Batch 200/537: Loss=0.9948 (C:0.9948, R:0.0105)
Batch 225/537: Loss=1.0379 (C:1.0379, R:0.0105)
Batch 250/537: Loss=1.0309 (C:1.0309, R:0.0105)
Batch 275/537: Loss=1.0337 (C:1.0337, R:0.0105)
Batch 300/537: Loss=1.0508 (C:1.0508, R:0.0105)
Batch 325/537: Loss=1.0436 (C:1.0436, R:0.0105)
Batch 350/537: Loss=1.0694 (C:1.0694, R:0.0105)
Batch 375/537: Loss=1.0471 (C:1.0471, R:0.0105)
Batch 400/537: Loss=1.0242 (C:1.0242, R:0.0105)
Batch 425/537: Loss=1.0505 (C:1.0505, R:0.0105)
Batch 450/537: Loss=1.0478 (C:1.0478, R:0.0105)
Batch 475/537: Loss=1.0639 (C:1.0639, R:0.0105)
Batch 500/537: Loss=1.0663 (C:1.0663, R:0.0105)
Batch 525/537: Loss=1.0313 (C:1.0313, R:0.0105)

============================================================
Epoch 8/300 completed in 20.9s
Train: Loss=1.0395 (C:1.0395, R:0.0105) Ratio=3.21x
Val:   Loss=1.0711 (C:1.0711, R:0.0104) Ratio=2.94x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0353 (C:1.0353, R:0.0105)
Batch  25/537: Loss=1.0299 (C:1.0299, R:0.0105)
Batch  50/537: Loss=1.0470 (C:1.0470, R:0.0105)
Batch  75/537: Loss=1.0386 (C:1.0386, R:0.0105)
Batch 100/537: Loss=1.0058 (C:1.0058, R:0.0105)
Batch 125/537: Loss=1.0406 (C:1.0406, R:0.0105)
Batch 150/537: Loss=1.0207 (C:1.0207, R:0.0105)
Batch 175/537: Loss=1.0094 (C:1.0094, R:0.0105)
Batch 200/537: Loss=1.0446 (C:1.0446, R:0.0105)
Batch 225/537: Loss=1.0554 (C:1.0554, R:0.0105)
Batch 250/537: Loss=1.0395 (C:1.0395, R:0.0106)
Batch 275/537: Loss=1.0566 (C:1.0566, R:0.0105)
Batch 300/537: Loss=1.0366 (C:1.0366, R:0.0105)
Batch 325/537: Loss=1.0027 (C:1.0027, R:0.0105)
Batch 350/537: Loss=1.0107 (C:1.0107, R:0.0105)
Batch 375/537: Loss=1.0191 (C:1.0191, R:0.0105)
Batch 400/537: Loss=1.0205 (C:1.0205, R:0.0105)
Batch 425/537: Loss=1.0397 (C:1.0397, R:0.0105)
Batch 450/537: Loss=1.0173 (C:1.0173, R:0.0105)
Batch 475/537: Loss=1.0618 (C:1.0618, R:0.0105)
Batch 500/537: Loss=1.0391 (C:1.0391, R:0.0105)
Batch 525/537: Loss=1.0262 (C:1.0262, R:0.0105)

============================================================
Epoch 9/300 completed in 21.3s
Train: Loss=1.0316 (C:1.0316, R:0.0105) Ratio=3.29x
Val:   Loss=1.0742 (C:1.0742, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.434 ± 0.548
    Neg distances: 1.790 ± 0.876
    Separation ratio: 4.13x
    Gap: -3.188
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=0.9719 (C:0.9719, R:0.0105)
Batch  25/537: Loss=0.9550 (C:0.9550, R:0.0105)
Batch  50/537: Loss=0.9742 (C:0.9742, R:0.0105)
Batch  75/537: Loss=1.0026 (C:1.0026, R:0.0105)
Batch 100/537: Loss=0.9994 (C:0.9994, R:0.0105)
Batch 125/537: Loss=0.9753 (C:0.9753, R:0.0105)
Batch 150/537: Loss=0.9838 (C:0.9838, R:0.0105)
Batch 175/537: Loss=0.9698 (C:0.9698, R:0.0105)
Batch 200/537: Loss=0.9659 (C:0.9659, R:0.0105)
Batch 225/537: Loss=0.9198 (C:0.9198, R:0.0105)
Batch 250/537: Loss=1.0316 (C:1.0316, R:0.0105)
Batch 275/537: Loss=0.9615 (C:0.9615, R:0.0105)
Batch 300/537: Loss=0.9613 (C:0.9613, R:0.0105)
Batch 325/537: Loss=0.9921 (C:0.9921, R:0.0105)
Batch 350/537: Loss=0.9847 (C:0.9847, R:0.0105)
Batch 375/537: Loss=0.9880 (C:0.9880, R:0.0106)
Batch 400/537: Loss=0.9722 (C:0.9722, R:0.0105)
Batch 425/537: Loss=0.9888 (C:0.9888, R:0.0105)
Batch 450/537: Loss=0.9703 (C:0.9703, R:0.0105)
Batch 475/537: Loss=0.9680 (C:0.9680, R:0.0105)
Batch 500/537: Loss=0.9745 (C:0.9745, R:0.0105)
Batch 525/537: Loss=0.9609 (C:0.9609, R:0.0106)

============================================================
Epoch 10/300 completed in 27.5s
Train: Loss=0.9758 (C:0.9758, R:0.0105) Ratio=3.30x
Val:   Loss=1.0132 (C:1.0132, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0132)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=0.9113 (C:0.9113, R:0.0105)
Batch  25/537: Loss=0.9471 (C:0.9471, R:0.0105)
Batch  50/537: Loss=0.9852 (C:0.9852, R:0.0106)
Batch  75/537: Loss=0.9668 (C:0.9668, R:0.0105)
Batch 100/537: Loss=0.9325 (C:0.9325, R:0.0105)
Batch 125/537: Loss=0.9761 (C:0.9761, R:0.0105)
Batch 150/537: Loss=0.9667 (C:0.9667, R:0.0105)
Batch 175/537: Loss=0.9702 (C:0.9702, R:0.0105)
Batch 200/537: Loss=0.9253 (C:0.9253, R:0.0105)
Batch 225/537: Loss=0.9315 (C:0.9315, R:0.0105)
Batch 250/537: Loss=0.9482 (C:0.9482, R:0.0105)
Batch 275/537: Loss=0.9722 (C:0.9722, R:0.0105)
Batch 300/537: Loss=0.9585 (C:0.9585, R:0.0105)
Batch 325/537: Loss=0.9365 (C:0.9365, R:0.0105)
Batch 350/537: Loss=0.9813 (C:0.9813, R:0.0105)
Batch 375/537: Loss=0.9487 (C:0.9487, R:0.0105)
Batch 400/537: Loss=0.9754 (C:0.9754, R:0.0105)
Batch 425/537: Loss=0.9774 (C:0.9774, R:0.0105)
Batch 450/537: Loss=0.9729 (C:0.9729, R:0.0105)
Batch 475/537: Loss=0.9706 (C:0.9706, R:0.0105)
Batch 500/537: Loss=0.9549 (C:0.9549, R:0.0105)
Batch 525/537: Loss=0.9759 (C:0.9759, R:0.0105)

============================================================
Epoch 11/300 completed in 21.6s
Train: Loss=0.9687 (C:0.9687, R:0.0105) Ratio=3.50x
Val:   Loss=1.0111 (C:1.0111, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0111)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=0.9423 (C:0.9423, R:0.0106)
Batch  25/537: Loss=0.9781 (C:0.9781, R:0.0105)
Batch  50/537: Loss=0.9477 (C:0.9477, R:0.0105)
Batch  75/537: Loss=0.9504 (C:0.9504, R:0.0105)
Batch 100/537: Loss=0.9569 (C:0.9569, R:0.0105)
Batch 125/537: Loss=0.9489 (C:0.9489, R:0.0105)
Batch 150/537: Loss=0.9716 (C:0.9716, R:0.0105)
Batch 175/537: Loss=0.9592 (C:0.9592, R:0.0105)
Batch 200/537: Loss=0.9938 (C:0.9938, R:0.0105)
Batch 225/537: Loss=0.9411 (C:0.9411, R:0.0105)
Batch 250/537: Loss=0.9540 (C:0.9540, R:0.0105)
Batch 275/537: Loss=1.0004 (C:1.0004, R:0.0105)
Batch 300/537: Loss=0.9571 (C:0.9571, R:0.0105)
Batch 325/537: Loss=0.9518 (C:0.9518, R:0.0106)
Batch 350/537: Loss=0.9890 (C:0.9890, R:0.0105)
Batch 375/537: Loss=0.9421 (C:0.9421, R:0.0105)
Batch 400/537: Loss=0.9752 (C:0.9752, R:0.0105)
Batch 425/537: Loss=0.9728 (C:0.9728, R:0.0105)
Batch 450/537: Loss=0.9780 (C:0.9780, R:0.0105)
Batch 475/537: Loss=0.9474 (C:0.9474, R:0.0105)
Batch 500/537: Loss=0.9935 (C:0.9935, R:0.0105)
Batch 525/537: Loss=0.9838 (C:0.9838, R:0.0105)

============================================================
Epoch 12/300 completed in 21.6s
Train: Loss=0.9630 (C:0.9630, R:0.0105) Ratio=3.50x
Val:   Loss=1.0153 (C:1.0153, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.414 ± 0.560
    Neg distances: 1.884 ± 0.889
    Separation ratio: 4.55x
    Gap: -3.339
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.8966 (C:0.8966, R:0.0105)
Batch  25/537: Loss=0.8924 (C:0.8924, R:0.0105)
Batch  50/537: Loss=0.8988 (C:0.8988, R:0.0105)
Batch  75/537: Loss=0.9031 (C:0.9031, R:0.0105)
Batch 100/537: Loss=0.8921 (C:0.8921, R:0.0105)
Batch 125/537: Loss=0.9281 (C:0.9281, R:0.0105)
Batch 150/537: Loss=0.9104 (C:0.9104, R:0.0105)
Batch 175/537: Loss=0.9071 (C:0.9071, R:0.0105)
Batch 200/537: Loss=0.9114 (C:0.9114, R:0.0105)
Batch 225/537: Loss=0.9203 (C:0.9203, R:0.0105)
Batch 250/537: Loss=0.9024 (C:0.9024, R:0.0105)
Batch 275/537: Loss=0.9249 (C:0.9249, R:0.0105)
Batch 300/537: Loss=0.9166 (C:0.9166, R:0.0105)
Batch 325/537: Loss=0.9102 (C:0.9102, R:0.0105)
Batch 350/537: Loss=0.9327 (C:0.9327, R:0.0105)
Batch 375/537: Loss=0.9178 (C:0.9178, R:0.0105)
Batch 400/537: Loss=0.9412 (C:0.9412, R:0.0105)
Batch 425/537: Loss=0.9167 (C:0.9167, R:0.0105)
Batch 450/537: Loss=0.9112 (C:0.9112, R:0.0105)
Batch 475/537: Loss=0.9365 (C:0.9365, R:0.0106)
Batch 500/537: Loss=0.8956 (C:0.8956, R:0.0105)
Batch 525/537: Loss=0.9066 (C:0.9066, R:0.0105)

============================================================
Epoch 13/300 completed in 27.7s
Train: Loss=0.9164 (C:0.9164, R:0.0105) Ratio=3.56x
Val:   Loss=0.9728 (C:0.9728, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9728)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9182 (C:0.9182, R:0.0105)
Batch  25/537: Loss=0.9031 (C:0.9031, R:0.0105)
Batch  50/537: Loss=0.9004 (C:0.9004, R:0.0105)
Batch  75/537: Loss=0.9197 (C:0.9197, R:0.0105)
Batch 100/537: Loss=0.9186 (C:0.9186, R:0.0105)
Batch 125/537: Loss=0.8780 (C:0.8780, R:0.0105)
Batch 150/537: Loss=0.9096 (C:0.9096, R:0.0105)
Batch 175/537: Loss=0.9267 (C:0.9267, R:0.0105)
Batch 200/537: Loss=0.9278 (C:0.9278, R:0.0105)
Batch 225/537: Loss=0.9325 (C:0.9325, R:0.0105)
Batch 250/537: Loss=0.9066 (C:0.9066, R:0.0105)
Batch 275/537: Loss=0.9055 (C:0.9055, R:0.0106)
Batch 300/537: Loss=0.8915 (C:0.8915, R:0.0105)
Batch 325/537: Loss=0.8999 (C:0.8999, R:0.0105)
Batch 350/537: Loss=0.9416 (C:0.9416, R:0.0105)
Batch 375/537: Loss=0.8971 (C:0.8971, R:0.0105)
Batch 400/537: Loss=0.9389 (C:0.9389, R:0.0105)
Batch 425/537: Loss=0.9227 (C:0.9227, R:0.0105)
Batch 450/537: Loss=0.9405 (C:0.9405, R:0.0105)
Batch 475/537: Loss=0.9166 (C:0.9166, R:0.0105)
Batch 500/537: Loss=0.9271 (C:0.9271, R:0.0105)
Batch 525/537: Loss=0.9098 (C:0.9098, R:0.0105)

============================================================
Epoch 14/300 completed in 22.1s
Train: Loss=0.9120 (C:0.9120, R:0.0105) Ratio=3.57x
Val:   Loss=0.9645 (C:0.9645, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9645)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.9137 (C:0.9137, R:0.0105)
Batch  25/537: Loss=0.8941 (C:0.8941, R:0.0105)
Batch  50/537: Loss=0.9042 (C:0.9042, R:0.0105)
Batch  75/537: Loss=0.8920 (C:0.8920, R:0.0105)
Batch 100/537: Loss=0.8784 (C:0.8784, R:0.0105)
Batch 125/537: Loss=0.9214 (C:0.9214, R:0.0106)
Batch 150/537: Loss=0.9126 (C:0.9126, R:0.0105)
Batch 175/537: Loss=0.9330 (C:0.9330, R:0.0105)
Batch 200/537: Loss=0.9181 (C:0.9181, R:0.0106)
Batch 225/537: Loss=0.9316 (C:0.9316, R:0.0105)
Batch 250/537: Loss=0.9303 (C:0.9303, R:0.0105)
Batch 275/537: Loss=0.9284 (C:0.9284, R:0.0105)
Batch 300/537: Loss=0.8850 (C:0.8850, R:0.0105)
Batch 325/537: Loss=0.8846 (C:0.8846, R:0.0105)
Batch 350/537: Loss=0.8999 (C:0.8999, R:0.0105)
Batch 375/537: Loss=0.9331 (C:0.9331, R:0.0105)
Batch 400/537: Loss=0.9455 (C:0.9455, R:0.0105)
Batch 425/537: Loss=0.8918 (C:0.8918, R:0.0105)
Batch 450/537: Loss=0.9222 (C:0.9222, R:0.0105)
Batch 475/537: Loss=0.9371 (C:0.9371, R:0.0105)
Batch 500/537: Loss=0.9267 (C:0.9267, R:0.0105)
Batch 525/537: Loss=0.9066 (C:0.9066, R:0.0105)

============================================================
Epoch 15/300 completed in 21.6s
Train: Loss=0.9058 (C:0.9058, R:0.0105) Ratio=3.64x
Val:   Loss=0.9672 (C:0.9672, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.406 ± 0.563
    Neg distances: 1.967 ± 0.909
    Separation ratio: 4.84x
    Gap: -3.427
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.8708 (C:0.8708, R:0.0105)
Batch  25/537: Loss=0.8793 (C:0.8793, R:0.0105)
Batch  50/537: Loss=0.8961 (C:0.8961, R:0.0105)
Batch  75/537: Loss=0.8782 (C:0.8782, R:0.0105)
Batch 100/537: Loss=0.8813 (C:0.8813, R:0.0105)
Batch 125/537: Loss=0.8714 (C:0.8714, R:0.0105)
Batch 150/537: Loss=0.8701 (C:0.8701, R:0.0105)
Batch 175/537: Loss=0.8453 (C:0.8453, R:0.0105)
Batch 200/537: Loss=0.8793 (C:0.8793, R:0.0105)
Batch 225/537: Loss=0.8633 (C:0.8633, R:0.0105)
Batch 250/537: Loss=0.8886 (C:0.8886, R:0.0105)
Batch 275/537: Loss=0.8939 (C:0.8939, R:0.0105)
Batch 300/537: Loss=0.8949 (C:0.8949, R:0.0105)
Batch 325/537: Loss=0.8915 (C:0.8915, R:0.0105)
Batch 350/537: Loss=0.8758 (C:0.8758, R:0.0105)
Batch 375/537: Loss=0.8728 (C:0.8728, R:0.0105)
Batch 400/537: Loss=0.8938 (C:0.8938, R:0.0105)
Batch 425/537: Loss=0.9079 (C:0.9079, R:0.0105)
Batch 450/537: Loss=0.8807 (C:0.8807, R:0.0105)
Batch 475/537: Loss=0.8768 (C:0.8768, R:0.0105)
Batch 500/537: Loss=0.8561 (C:0.8561, R:0.0105)
Batch 525/537: Loss=0.8802 (C:0.8802, R:0.0105)

============================================================
Epoch 16/300 completed in 28.1s
Train: Loss=0.8721 (C:0.8721, R:0.0105) Ratio=3.68x
Val:   Loss=0.9403 (C:0.9403, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9403)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.8595 (C:0.8595, R:0.0105)
Batch  25/537: Loss=0.8653 (C:0.8653, R:0.0105)
Batch  50/537: Loss=0.8559 (C:0.8559, R:0.0106)
Batch  75/537: Loss=0.8613 (C:0.8613, R:0.0105)
Batch 100/537: Loss=0.8788 (C:0.8788, R:0.0105)
Batch 125/537: Loss=0.8697 (C:0.8697, R:0.0105)
Batch 150/537: Loss=0.8558 (C:0.8558, R:0.0105)
Batch 175/537: Loss=0.8844 (C:0.8844, R:0.0105)
Batch 200/537: Loss=0.8706 (C:0.8706, R:0.0105)
Batch 225/537: Loss=0.8513 (C:0.8513, R:0.0105)
Batch 250/537: Loss=0.8640 (C:0.8640, R:0.0105)
Batch 275/537: Loss=0.8815 (C:0.8815, R:0.0105)
Batch 300/537: Loss=0.8848 (C:0.8848, R:0.0105)
Batch 325/537: Loss=0.8538 (C:0.8538, R:0.0105)
Batch 350/537: Loss=0.8878 (C:0.8878, R:0.0105)
Batch 375/537: Loss=0.8666 (C:0.8666, R:0.0105)
Batch 400/537: Loss=0.8562 (C:0.8562, R:0.0105)
Batch 425/537: Loss=0.8578 (C:0.8578, R:0.0105)
Batch 450/537: Loss=0.8594 (C:0.8594, R:0.0105)
Batch 475/537: Loss=0.8597 (C:0.8597, R:0.0105)
Batch 500/537: Loss=0.8679 (C:0.8679, R:0.0105)
Batch 525/537: Loss=0.8892 (C:0.8892, R:0.0105)

============================================================
Epoch 17/300 completed in 21.7s
Train: Loss=0.8660 (C:0.8660, R:0.0105) Ratio=3.85x
Val:   Loss=0.9408 (C:0.9408, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.9158 (C:0.9158, R:0.0105)
Batch  25/537: Loss=0.8611 (C:0.8611, R:0.0105)
Batch  50/537: Loss=0.8308 (C:0.8308, R:0.0105)
Batch  75/537: Loss=0.8541 (C:0.8541, R:0.0105)
Batch 100/537: Loss=0.8485 (C:0.8485, R:0.0105)
Batch 125/537: Loss=0.8727 (C:0.8727, R:0.0105)
Batch 150/537: Loss=0.8539 (C:0.8539, R:0.0104)
Batch 175/537: Loss=0.9219 (C:0.9219, R:0.0105)
Batch 200/537: Loss=0.9054 (C:0.9054, R:0.0105)
Batch 225/537: Loss=0.8759 (C:0.8759, R:0.0105)
Batch 250/537: Loss=0.8616 (C:0.8616, R:0.0105)
Batch 275/537: Loss=0.8763 (C:0.8763, R:0.0105)
Batch 300/537: Loss=0.8606 (C:0.8606, R:0.0105)
Batch 325/537: Loss=0.8685 (C:0.8685, R:0.0105)
Batch 350/537: Loss=0.8739 (C:0.8739, R:0.0105)
Batch 375/537: Loss=0.8466 (C:0.8466, R:0.0105)
Batch 400/537: Loss=0.8411 (C:0.8411, R:0.0105)
Batch 425/537: Loss=0.8633 (C:0.8633, R:0.0105)
Batch 450/537: Loss=0.8802 (C:0.8802, R:0.0105)
Batch 475/537: Loss=0.8474 (C:0.8474, R:0.0106)
Batch 500/537: Loss=0.8955 (C:0.8955, R:0.0105)
Batch 525/537: Loss=0.8476 (C:0.8476, R:0.0105)

============================================================
Epoch 18/300 completed in 21.2s
Train: Loss=0.8622 (C:0.8622, R:0.0105) Ratio=3.82x
Val:   Loss=0.9449 (C:0.9449, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.372 ± 0.518
    Neg distances: 2.026 ± 0.910
    Separation ratio: 5.45x
    Gap: -3.490
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.7849 (C:0.7849, R:0.0105)
Batch  25/537: Loss=0.8268 (C:0.8268, R:0.0105)
Batch  50/537: Loss=0.7908 (C:0.7908, R:0.0105)
Batch  75/537: Loss=0.7904 (C:0.7904, R:0.0105)
Batch 100/537: Loss=0.7738 (C:0.7738, R:0.0105)
Batch 125/537: Loss=0.8059 (C:0.8059, R:0.0105)
Batch 150/537: Loss=0.8069 (C:0.8069, R:0.0105)
Batch 175/537: Loss=0.7966 (C:0.7966, R:0.0105)
Batch 200/537: Loss=0.8063 (C:0.8063, R:0.0105)
Batch 225/537: Loss=0.8107 (C:0.8107, R:0.0105)
Batch 250/537: Loss=0.8237 (C:0.8237, R:0.0105)
Batch 275/537: Loss=0.8484 (C:0.8484, R:0.0105)
Batch 300/537: Loss=0.8379 (C:0.8379, R:0.0105)
Batch 325/537: Loss=0.8360 (C:0.8360, R:0.0105)
Batch 350/537: Loss=0.7879 (C:0.7879, R:0.0105)
Batch 375/537: Loss=0.8317 (C:0.8317, R:0.0105)
Batch 400/537: Loss=0.8374 (C:0.8374, R:0.0105)
Batch 425/537: Loss=0.8339 (C:0.8339, R:0.0105)
Batch 450/537: Loss=0.8048 (C:0.8048, R:0.0105)
Batch 475/537: Loss=0.8106 (C:0.8106, R:0.0105)
Batch 500/537: Loss=0.8136 (C:0.8136, R:0.0105)
Batch 525/537: Loss=0.8350 (C:0.8350, R:0.0105)

============================================================
Epoch 19/300 completed in 27.4s
Train: Loss=0.8198 (C:0.8198, R:0.0105) Ratio=3.91x
Val:   Loss=0.8958 (C:0.8958, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8958)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.8342 (C:0.8342, R:0.0105)
Batch  25/537: Loss=0.8342 (C:0.8342, R:0.0105)
Batch  50/537: Loss=0.8114 (C:0.8114, R:0.0105)
Batch  75/537: Loss=0.8080 (C:0.8080, R:0.0106)
Batch 100/537: Loss=0.8412 (C:0.8412, R:0.0105)
Batch 125/537: Loss=0.7928 (C:0.7928, R:0.0105)
Batch 150/537: Loss=0.8008 (C:0.8008, R:0.0105)
Batch 175/537: Loss=0.7883 (C:0.7883, R:0.0105)
Batch 200/537: Loss=0.8105 (C:0.8105, R:0.0105)
Batch 225/537: Loss=0.8199 (C:0.8199, R:0.0105)
Batch 250/537: Loss=0.8007 (C:0.8007, R:0.0105)
Batch 275/537: Loss=0.7919 (C:0.7919, R:0.0105)
Batch 300/537: Loss=0.8332 (C:0.8332, R:0.0105)
Batch 325/537: Loss=0.8294 (C:0.8294, R:0.0105)
Batch 350/537: Loss=0.8197 (C:0.8197, R:0.0106)
Batch 375/537: Loss=0.8094 (C:0.8094, R:0.0105)
Batch 400/537: Loss=0.8298 (C:0.8298, R:0.0105)
Batch 425/537: Loss=0.8325 (C:0.8325, R:0.0105)
Batch 450/537: Loss=0.8386 (C:0.8386, R:0.0105)
Batch 475/537: Loss=0.8326 (C:0.8326, R:0.0105)
Batch 500/537: Loss=0.8030 (C:0.8030, R:0.0105)
Batch 525/537: Loss=0.8039 (C:0.8039, R:0.0105)

============================================================
Epoch 20/300 completed in 21.3s
Train: Loss=0.8149 (C:0.8149, R:0.0105) Ratio=3.92x
Val:   Loss=0.8999 (C:0.8999, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8180 (C:0.8180, R:0.0105)
Batch  25/537: Loss=0.8092 (C:0.8092, R:0.0105)
Batch  50/537: Loss=0.8250 (C:0.8250, R:0.0105)
Batch  75/537: Loss=0.7849 (C:0.7849, R:0.0106)
Batch 100/537: Loss=0.8182 (C:0.8182, R:0.0105)
Batch 125/537: Loss=0.8262 (C:0.8262, R:0.0105)
Batch 150/537: Loss=0.7833 (C:0.7833, R:0.0105)
Batch 175/537: Loss=0.8038 (C:0.8038, R:0.0105)
Batch 200/537: Loss=0.8086 (C:0.8086, R:0.0105)
Batch 225/537: Loss=0.7793 (C:0.7793, R:0.0105)
Batch 250/537: Loss=0.8151 (C:0.8151, R:0.0105)
Batch 275/537: Loss=0.7820 (C:0.7820, R:0.0105)
Batch 300/537: Loss=0.7941 (C:0.7941, R:0.0105)
Batch 325/537: Loss=0.8605 (C:0.8605, R:0.0105)
Batch 350/537: Loss=0.7854 (C:0.7854, R:0.0105)
Batch 375/537: Loss=0.8251 (C:0.8251, R:0.0105)
Batch 400/537: Loss=0.7961 (C:0.7961, R:0.0106)
Batch 425/537: Loss=0.8107 (C:0.8107, R:0.0105)
Batch 450/537: Loss=0.7958 (C:0.7958, R:0.0106)
Batch 475/537: Loss=0.8238 (C:0.8238, R:0.0105)
Batch 500/537: Loss=0.8025 (C:0.8025, R:0.0106)
Batch 525/537: Loss=0.8141 (C:0.8141, R:0.0105)

============================================================
Epoch 21/300 completed in 21.2s
Train: Loss=0.8113 (C:0.8113, R:0.0105) Ratio=4.04x
Val:   Loss=0.8964 (C:0.8964, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.389 ± 0.553
    Neg distances: 2.112 ± 0.945
    Separation ratio: 5.43x
    Gap: -3.725
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.7903 (C:0.7903, R:0.0105)
Batch  25/537: Loss=0.7683 (C:0.7683, R:0.0105)
Batch  50/537: Loss=0.7775 (C:0.7775, R:0.0105)
Batch  75/537: Loss=0.8149 (C:0.8149, R:0.0105)
Batch 100/537: Loss=0.7589 (C:0.7589, R:0.0105)
Batch 125/537: Loss=0.8140 (C:0.8140, R:0.0105)
Batch 150/537: Loss=0.7781 (C:0.7781, R:0.0105)
Batch 175/537: Loss=0.7868 (C:0.7868, R:0.0105)
Batch 200/537: Loss=0.7748 (C:0.7748, R:0.0105)
Batch 225/537: Loss=0.7957 (C:0.7957, R:0.0105)
Batch 250/537: Loss=0.7667 (C:0.7667, R:0.0105)
Batch 275/537: Loss=0.8021 (C:0.8021, R:0.0105)
Batch 300/537: Loss=0.7644 (C:0.7644, R:0.0105)
Batch 325/537: Loss=0.7744 (C:0.7744, R:0.0104)
Batch 350/537: Loss=0.7692 (C:0.7692, R:0.0105)
Batch 375/537: Loss=0.7841 (C:0.7841, R:0.0105)
Batch 400/537: Loss=0.8386 (C:0.8386, R:0.0105)
Batch 425/537: Loss=0.7980 (C:0.7980, R:0.0105)
Batch 450/537: Loss=0.8095 (C:0.8095, R:0.0105)
Batch 475/537: Loss=0.8099 (C:0.8099, R:0.0105)
Batch 500/537: Loss=0.8319 (C:0.8319, R:0.0105)
Batch 525/537: Loss=0.7941 (C:0.7941, R:0.0105)

============================================================
Epoch 22/300 completed in 27.3s
Train: Loss=0.7933 (C:0.7933, R:0.0105) Ratio=4.00x
Val:   Loss=0.8868 (C:0.8868, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8868)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.7866 (C:0.7866, R:0.0105)
Batch  25/537: Loss=0.8029 (C:0.8029, R:0.0106)
Batch  50/537: Loss=0.7646 (C:0.7646, R:0.0105)
Batch  75/537: Loss=0.7801 (C:0.7801, R:0.0105)
Batch 100/537: Loss=0.7572 (C:0.7572, R:0.0105)
Batch 125/537: Loss=0.7786 (C:0.7786, R:0.0105)
Batch 150/537: Loss=0.7927 (C:0.7927, R:0.0105)
Batch 175/537: Loss=0.7785 (C:0.7785, R:0.0105)
Batch 200/537: Loss=0.7896 (C:0.7896, R:0.0105)
Batch 225/537: Loss=0.7782 (C:0.7782, R:0.0105)
Batch 250/537: Loss=0.7728 (C:0.7728, R:0.0105)
Batch 275/537: Loss=0.8124 (C:0.8124, R:0.0105)
Batch 300/537: Loss=0.7922 (C:0.7922, R:0.0105)
Batch 325/537: Loss=0.7598 (C:0.7598, R:0.0105)
Batch 350/537: Loss=0.7946 (C:0.7946, R:0.0105)
Batch 375/537: Loss=0.7672 (C:0.7672, R:0.0105)
Batch 400/537: Loss=0.8067 (C:0.8067, R:0.0105)
Batch 425/537: Loss=0.8195 (C:0.8195, R:0.0105)
Batch 450/537: Loss=0.7973 (C:0.7973, R:0.0105)
Batch 475/537: Loss=0.8257 (C:0.8257, R:0.0106)
Batch 500/537: Loss=0.7671 (C:0.7671, R:0.0105)
Batch 525/537: Loss=0.7948 (C:0.7948, R:0.0105)

============================================================
Epoch 23/300 completed in 21.3s
Train: Loss=0.7900 (C:0.7900, R:0.0105) Ratio=4.10x
Val:   Loss=0.8895 (C:0.8895, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.7812 (C:0.7812, R:0.0105)
Batch  25/537: Loss=0.8022 (C:0.8022, R:0.0105)
Batch  50/537: Loss=0.7420 (C:0.7420, R:0.0105)
Batch  75/537: Loss=0.7628 (C:0.7628, R:0.0105)
Batch 100/537: Loss=0.7608 (C:0.7608, R:0.0105)
Batch 125/537: Loss=0.7943 (C:0.7943, R:0.0105)
Batch 150/537: Loss=0.7846 (C:0.7846, R:0.0105)
Batch 175/537: Loss=0.7600 (C:0.7600, R:0.0105)
Batch 200/537: Loss=0.7524 (C:0.7524, R:0.0105)
Batch 225/537: Loss=0.7669 (C:0.7669, R:0.0105)
Batch 250/537: Loss=0.7874 (C:0.7874, R:0.0105)
Batch 275/537: Loss=0.8087 (C:0.8087, R:0.0105)
Batch 300/537: Loss=0.7858 (C:0.7858, R:0.0105)
Batch 325/537: Loss=0.8131 (C:0.8131, R:0.0105)
Batch 350/537: Loss=0.7892 (C:0.7892, R:0.0105)
Batch 375/537: Loss=0.7980 (C:0.7980, R:0.0105)
Batch 400/537: Loss=0.7986 (C:0.7986, R:0.0105)
Batch 425/537: Loss=0.8240 (C:0.8240, R:0.0105)
Batch 450/537: Loss=0.7706 (C:0.7706, R:0.0105)
Batch 475/537: Loss=0.7684 (C:0.7684, R:0.0105)
Batch 500/537: Loss=0.7617 (C:0.7617, R:0.0105)
Batch 525/537: Loss=0.8011 (C:0.8011, R:0.0105)

============================================================
Epoch 24/300 completed in 21.1s
Train: Loss=0.7873 (C:0.7873, R:0.0105) Ratio=4.15x
Val:   Loss=0.8809 (C:0.8809, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8809)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.393 ± 0.594
    Neg distances: 2.218 ± 0.998
    Separation ratio: 5.64x
    Gap: -3.931
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.7563 (C:0.7563, R:0.0105)
Batch  25/537: Loss=0.7520 (C:0.7520, R:0.0105)
Batch  50/537: Loss=0.7456 (C:0.7456, R:0.0105)
Batch  75/537: Loss=0.7578 (C:0.7578, R:0.0105)
Batch 100/537: Loss=0.7750 (C:0.7750, R:0.0105)
Batch 125/537: Loss=0.7506 (C:0.7506, R:0.0106)
Batch 150/537: Loss=0.7682 (C:0.7682, R:0.0105)
Batch 175/537: Loss=0.7470 (C:0.7470, R:0.0105)
Batch 200/537: Loss=0.7479 (C:0.7479, R:0.0105)
Batch 225/537: Loss=0.7342 (C:0.7342, R:0.0105)
Batch 250/537: Loss=0.7462 (C:0.7462, R:0.0105)
Batch 275/537: Loss=0.7320 (C:0.7320, R:0.0105)
Batch 300/537: Loss=0.7352 (C:0.7352, R:0.0105)
Batch 325/537: Loss=0.7817 (C:0.7817, R:0.0105)
Batch 350/537: Loss=0.7689 (C:0.7689, R:0.0105)
Batch 375/537: Loss=0.7574 (C:0.7574, R:0.0105)
Batch 400/537: Loss=0.7606 (C:0.7606, R:0.0105)
Batch 425/537: Loss=0.7670 (C:0.7670, R:0.0105)
Batch 450/537: Loss=0.7731 (C:0.7731, R:0.0105)
Batch 475/537: Loss=0.7592 (C:0.7592, R:0.0105)
Batch 500/537: Loss=0.7875 (C:0.7875, R:0.0105)
Batch 525/537: Loss=0.7534 (C:0.7534, R:0.0105)

============================================================
Epoch 25/300 completed in 26.8s
Train: Loss=0.7663 (C:0.7663, R:0.0105) Ratio=4.14x
Val:   Loss=0.8655 (C:0.8655, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8655)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.7341 (C:0.7341, R:0.0105)
Batch  25/537: Loss=0.7565 (C:0.7565, R:0.0105)
Batch  50/537: Loss=0.7480 (C:0.7480, R:0.0105)
Batch  75/537: Loss=0.7725 (C:0.7725, R:0.0105)
Batch 100/537: Loss=0.7861 (C:0.7861, R:0.0105)
Batch 125/537: Loss=0.7516 (C:0.7516, R:0.0105)
Batch 150/537: Loss=0.7998 (C:0.7998, R:0.0105)
Batch 175/537: Loss=0.7671 (C:0.7671, R:0.0105)
Batch 200/537: Loss=0.7403 (C:0.7403, R:0.0105)
Batch 225/537: Loss=0.7872 (C:0.7872, R:0.0105)
Batch 250/537: Loss=0.7342 (C:0.7342, R:0.0105)
Batch 275/537: Loss=0.7533 (C:0.7533, R:0.0105)
Batch 300/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 325/537: Loss=0.7597 (C:0.7597, R:0.0105)
Batch 350/537: Loss=0.7640 (C:0.7640, R:0.0105)
Batch 375/537: Loss=0.7576 (C:0.7576, R:0.0105)
Batch 400/537: Loss=0.7513 (C:0.7513, R:0.0105)
Batch 425/537: Loss=0.7592 (C:0.7592, R:0.0105)
Batch 450/537: Loss=0.7654 (C:0.7654, R:0.0105)
Batch 475/537: Loss=0.8011 (C:0.8011, R:0.0105)
Batch 500/537: Loss=0.7619 (C:0.7619, R:0.0105)
Batch 525/537: Loss=0.7811 (C:0.7811, R:0.0105)

============================================================
Epoch 26/300 completed in 21.2s
Train: Loss=0.7637 (C:0.7637, R:0.0105) Ratio=4.14x
Val:   Loss=0.8581 (C:0.8581, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8581)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.7691 (C:0.7691, R:0.0105)
Batch  25/537: Loss=0.7429 (C:0.7429, R:0.0105)
Batch  50/537: Loss=0.7755 (C:0.7755, R:0.0105)
Batch  75/537: Loss=0.7234 (C:0.7234, R:0.0105)
Batch 100/537: Loss=0.7566 (C:0.7566, R:0.0105)
Batch 125/537: Loss=0.7596 (C:0.7596, R:0.0105)
Batch 150/537: Loss=0.7473 (C:0.7473, R:0.0105)
Batch 175/537: Loss=0.7565 (C:0.7565, R:0.0105)
Batch 200/537: Loss=0.7588 (C:0.7588, R:0.0106)
Batch 225/537: Loss=0.7116 (C:0.7116, R:0.0105)
Batch 250/537: Loss=0.7686 (C:0.7686, R:0.0106)
Batch 275/537: Loss=0.7508 (C:0.7508, R:0.0105)
Batch 300/537: Loss=0.7347 (C:0.7347, R:0.0105)
Batch 325/537: Loss=0.7671 (C:0.7671, R:0.0105)
Batch 350/537: Loss=0.7633 (C:0.7633, R:0.0105)
Batch 375/537: Loss=0.7863 (C:0.7863, R:0.0105)
Batch 400/537: Loss=0.7512 (C:0.7512, R:0.0106)
Batch 425/537: Loss=0.7486 (C:0.7486, R:0.0105)
Batch 450/537: Loss=0.7772 (C:0.7772, R:0.0105)
Batch 475/537: Loss=0.7726 (C:0.7726, R:0.0105)
Batch 500/537: Loss=0.7488 (C:0.7488, R:0.0105)
Batch 525/537: Loss=0.7619 (C:0.7619, R:0.0105)

============================================================
Epoch 27/300 completed in 21.2s
Train: Loss=0.7601 (C:0.7601, R:0.0105) Ratio=4.27x
Val:   Loss=0.8732 (C:0.8732, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.361 ± 0.553
    Neg distances: 2.290 ± 0.992
    Separation ratio: 6.34x
    Gap: -3.937
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.6504 (C:0.6504, R:0.0105)
Batch  25/537: Loss=0.7271 (C:0.7271, R:0.0105)
Batch  50/537: Loss=0.7099 (C:0.7099, R:0.0106)
Batch  75/537: Loss=0.7004 (C:0.7004, R:0.0105)
Batch 100/537: Loss=0.6892 (C:0.6892, R:0.0105)
Batch 125/537: Loss=0.7050 (C:0.7050, R:0.0105)
Batch 150/537: Loss=0.6947 (C:0.6947, R:0.0105)
Batch 175/537: Loss=0.7002 (C:0.7002, R:0.0105)
Batch 200/537: Loss=0.6930 (C:0.6930, R:0.0105)
Batch 225/537: Loss=0.7209 (C:0.7209, R:0.0105)
Batch 250/537: Loss=0.7136 (C:0.7136, R:0.0105)
Batch 275/537: Loss=0.7120 (C:0.7120, R:0.0105)
Batch 300/537: Loss=0.6976 (C:0.6976, R:0.0105)
Batch 325/537: Loss=0.7049 (C:0.7049, R:0.0105)
Batch 350/537: Loss=0.7421 (C:0.7421, R:0.0105)
Batch 375/537: Loss=0.7293 (C:0.7293, R:0.0105)
Batch 400/537: Loss=0.7507 (C:0.7507, R:0.0105)
Batch 425/537: Loss=0.6985 (C:0.6985, R:0.0105)
Batch 450/537: Loss=0.7063 (C:0.7063, R:0.0106)
Batch 475/537: Loss=0.7235 (C:0.7235, R:0.0105)
Batch 500/537: Loss=0.7427 (C:0.7427, R:0.0105)
Batch 525/537: Loss=0.7035 (C:0.7035, R:0.0105)

============================================================
Epoch 28/300 completed in 27.2s
Train: Loss=0.7119 (C:0.7119, R:0.0105) Ratio=4.25x
Val:   Loss=0.8227 (C:0.8227, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8227)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.7120 (C:0.7120, R:0.0105)
Batch  25/537: Loss=0.6963 (C:0.6963, R:0.0105)
Batch  50/537: Loss=0.7226 (C:0.7226, R:0.0105)
Batch  75/537: Loss=0.6929 (C:0.6929, R:0.0105)
Batch 100/537: Loss=0.6953 (C:0.6953, R:0.0105)
Batch 125/537: Loss=0.7073 (C:0.7073, R:0.0105)
Batch 150/537: Loss=0.7198 (C:0.7198, R:0.0105)
Batch 175/537: Loss=0.6991 (C:0.6991, R:0.0105)
Batch 200/537: Loss=0.7309 (C:0.7309, R:0.0105)
Batch 225/537: Loss=0.6966 (C:0.6966, R:0.0105)
Batch 250/537: Loss=0.7212 (C:0.7212, R:0.0105)
Batch 275/537: Loss=0.6927 (C:0.6927, R:0.0105)
Batch 300/537: Loss=0.7149 (C:0.7149, R:0.0105)
Batch 325/537: Loss=0.7191 (C:0.7191, R:0.0105)
Batch 350/537: Loss=0.7078 (C:0.7078, R:0.0105)
Batch 375/537: Loss=0.7022 (C:0.7022, R:0.0105)
Batch 400/537: Loss=0.7117 (C:0.7117, R:0.0105)
Batch 425/537: Loss=0.7443 (C:0.7443, R:0.0105)
Batch 450/537: Loss=0.7556 (C:0.7556, R:0.0105)
Batch 475/537: Loss=0.7082 (C:0.7082, R:0.0105)
Batch 500/537: Loss=0.7485 (C:0.7485, R:0.0105)
Batch 525/537: Loss=0.7146 (C:0.7146, R:0.0105)

============================================================
Epoch 29/300 completed in 21.8s
Train: Loss=0.7080 (C:0.7080, R:0.0105) Ratio=4.24x
Val:   Loss=0.8200 (C:0.8200, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8200)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7147 (C:0.7147, R:0.0105)
Batch  25/537: Loss=0.6830 (C:0.6830, R:0.0105)
Batch  50/537: Loss=0.6965 (C:0.6965, R:0.0105)
Batch  75/537: Loss=0.6984 (C:0.6984, R:0.0105)
Batch 100/537: Loss=0.6850 (C:0.6850, R:0.0105)
Batch 125/537: Loss=0.6947 (C:0.6947, R:0.0105)
Batch 150/537: Loss=0.6917 (C:0.6917, R:0.0105)
Batch 175/537: Loss=0.6975 (C:0.6975, R:0.0105)
Batch 200/537: Loss=0.6514 (C:0.6514, R:0.0105)
Batch 225/537: Loss=0.6988 (C:0.6988, R:0.0105)
Batch 250/537: Loss=0.7016 (C:0.7016, R:0.0105)
Batch 275/537: Loss=0.7047 (C:0.7047, R:0.0105)
Batch 300/537: Loss=0.7364 (C:0.7364, R:0.0105)
Batch 325/537: Loss=0.6892 (C:0.6892, R:0.0105)
Batch 350/537: Loss=0.7113 (C:0.7113, R:0.0105)
Batch 375/537: Loss=0.6942 (C:0.6942, R:0.0105)
Batch 400/537: Loss=0.6910 (C:0.6910, R:0.0105)
Batch 425/537: Loss=0.6718 (C:0.6718, R:0.0105)
Batch 450/537: Loss=0.7250 (C:0.7250, R:0.0105)
Batch 475/537: Loss=0.7190 (C:0.7190, R:0.0105)
Batch 500/537: Loss=0.6381 (C:0.6381, R:0.0105)
Batch 525/537: Loss=0.7154 (C:0.7154, R:0.0105)

============================================================
Epoch 30/300 completed in 21.6s
Train: Loss=0.7067 (C:0.7067, R:0.0105) Ratio=4.47x
Val:   Loss=0.8200 (C:0.8200, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8200)
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.362 ± 0.549
    Neg distances: 2.301 ± 0.996
    Separation ratio: 6.35x
    Gap: -3.883
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.6818 (C:0.6818, R:0.0105)
Batch  25/537: Loss=0.7189 (C:0.7189, R:0.0105)
Batch  50/537: Loss=0.6805 (C:0.6805, R:0.0105)
Batch  75/537: Loss=0.6657 (C:0.6657, R:0.0106)
Batch 100/537: Loss=0.7060 (C:0.7060, R:0.0105)
Batch 125/537: Loss=0.7147 (C:0.7147, R:0.0105)
Batch 150/537: Loss=0.7011 (C:0.7011, R:0.0105)
Batch 175/537: Loss=0.7052 (C:0.7052, R:0.0105)
Batch 200/537: Loss=0.7059 (C:0.7059, R:0.0105)
Batch 225/537: Loss=0.7044 (C:0.7044, R:0.0105)
Batch 250/537: Loss=0.6789 (C:0.6789, R:0.0106)
Batch 275/537: Loss=0.7257 (C:0.7257, R:0.0105)
Batch 300/537: Loss=0.7104 (C:0.7104, R:0.0105)
Batch 325/537: Loss=0.7184 (C:0.7184, R:0.0105)
Batch 350/537: Loss=0.6908 (C:0.6908, R:0.0105)
Batch 375/537: Loss=0.6902 (C:0.6902, R:0.0105)
Batch 400/537: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 425/537: Loss=0.6986 (C:0.6986, R:0.0105)
Batch 450/537: Loss=0.7284 (C:0.7284, R:0.0105)
Batch 475/537: Loss=0.6633 (C:0.6633, R:0.0105)
Batch 500/537: Loss=0.6837 (C:0.6837, R:0.0105)
Batch 525/537: Loss=0.6876 (C:0.6876, R:0.0105)

============================================================
Epoch 31/300 completed in 28.1s
Train: Loss=0.6947 (C:0.6947, R:0.0105) Ratio=4.34x
Val:   Loss=0.8081 (C:0.8081, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8081)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.6826 (C:0.6826, R:0.0105)
Batch  25/537: Loss=0.6671 (C:0.6671, R:0.0105)
Batch  50/537: Loss=0.6723 (C:0.6723, R:0.0105)
Batch  75/537: Loss=0.7063 (C:0.7063, R:0.0105)
Batch 100/537: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 125/537: Loss=0.6797 (C:0.6797, R:0.0106)
Batch 150/537: Loss=0.6922 (C:0.6922, R:0.0105)
Batch 175/537: Loss=0.6770 (C:0.6770, R:0.0105)
Batch 200/537: Loss=0.6778 (C:0.6778, R:0.0105)
Batch 225/537: Loss=0.6620 (C:0.6620, R:0.0105)
Batch 250/537: Loss=0.6845 (C:0.6845, R:0.0105)
Batch 275/537: Loss=0.6836 (C:0.6836, R:0.0105)
Batch 300/537: Loss=0.6999 (C:0.6999, R:0.0105)
Batch 325/537: Loss=0.6882 (C:0.6882, R:0.0105)
Batch 350/537: Loss=0.7019 (C:0.7019, R:0.0105)
Batch 375/537: Loss=0.7247 (C:0.7247, R:0.0105)
Batch 400/537: Loss=0.6557 (C:0.6557, R:0.0105)
Batch 425/537: Loss=0.7015 (C:0.7015, R:0.0105)
Batch 450/537: Loss=0.7089 (C:0.7089, R:0.0105)
Batch 475/537: Loss=0.7068 (C:0.7068, R:0.0105)
Batch 500/537: Loss=0.7039 (C:0.7039, R:0.0105)
Batch 525/537: Loss=0.7053 (C:0.7053, R:0.0105)

============================================================
Epoch 32/300 completed in 21.9s
Train: Loss=0.6921 (C:0.6921, R:0.0105) Ratio=4.40x
Val:   Loss=0.8110 (C:0.8110, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.6896 (C:0.6896, R:0.0105)
Batch  25/537: Loss=0.6318 (C:0.6318, R:0.0105)
Batch  50/537: Loss=0.6815 (C:0.6815, R:0.0105)
Batch  75/537: Loss=0.7160 (C:0.7160, R:0.0105)
Batch 100/537: Loss=0.6780 (C:0.6780, R:0.0105)
Batch 125/537: Loss=0.6854 (C:0.6854, R:0.0105)
Batch 150/537: Loss=0.7009 (C:0.7009, R:0.0105)
Batch 175/537: Loss=0.6905 (C:0.6905, R:0.0105)
Batch 200/537: Loss=0.6895 (C:0.6895, R:0.0105)
Batch 225/537: Loss=0.7353 (C:0.7353, R:0.0105)
Batch 250/537: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 275/537: Loss=0.6962 (C:0.6962, R:0.0105)
Batch 300/537: Loss=0.6873 (C:0.6873, R:0.0105)
Batch 325/537: Loss=0.6977 (C:0.6977, R:0.0105)
Batch 350/537: Loss=0.6662 (C:0.6662, R:0.0105)
Batch 375/537: Loss=0.6893 (C:0.6893, R:0.0105)
Batch 400/537: Loss=0.7193 (C:0.7193, R:0.0105)
Batch 425/537: Loss=0.6997 (C:0.6997, R:0.0105)
Batch 450/537: Loss=0.6938 (C:0.6938, R:0.0105)
Batch 475/537: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 500/537: Loss=0.6425 (C:0.6425, R:0.0105)
Batch 525/537: Loss=0.6775 (C:0.6775, R:0.0105)

============================================================
Epoch 33/300 completed in 21.7s
Train: Loss=0.6903 (C:0.6903, R:0.0105) Ratio=4.46x
Val:   Loss=0.8078 (C:0.8078, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.045
✅ New best model saved (Val Loss: 0.8078)
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.378 ± 0.599
    Neg distances: 2.372 ± 1.031
    Separation ratio: 6.28x
    Gap: -4.065
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.6402 (C:0.6402, R:0.0105)
Batch  25/537: Loss=0.6768 (C:0.6768, R:0.0105)
Batch  50/537: Loss=0.6847 (C:0.6847, R:0.0105)
Batch  75/537: Loss=0.7046 (C:0.7046, R:0.0105)
Batch 100/537: Loss=0.6842 (C:0.6842, R:0.0105)
Batch 125/537: Loss=0.6480 (C:0.6480, R:0.0105)
Batch 150/537: Loss=0.7109 (C:0.7109, R:0.0105)
Batch 175/537: Loss=0.6835 (C:0.6835, R:0.0105)
Batch 200/537: Loss=0.7046 (C:0.7046, R:0.0105)
Batch 225/537: Loss=0.7104 (C:0.7104, R:0.0105)
Batch 250/537: Loss=0.7279 (C:0.7279, R:0.0105)
Batch 275/537: Loss=0.6697 (C:0.6697, R:0.0105)
Batch 300/537: Loss=0.6691 (C:0.6691, R:0.0105)
Batch 325/537: Loss=0.6795 (C:0.6795, R:0.0106)
Batch 350/537: Loss=0.6809 (C:0.6809, R:0.0105)
Batch 375/537: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 400/537: Loss=0.6707 (C:0.6707, R:0.0105)
Batch 425/537: Loss=0.6670 (C:0.6670, R:0.0106)
Batch 450/537: Loss=0.6916 (C:0.6916, R:0.0105)
Batch 475/537: Loss=0.6633 (C:0.6633, R:0.0105)
Batch 500/537: Loss=0.6817 (C:0.6817, R:0.0106)
Batch 525/537: Loss=0.7018 (C:0.7018, R:0.0105)

============================================================
Epoch 34/300 completed in 28.1s
Train: Loss=0.6841 (C:0.6841, R:0.0105) Ratio=4.48x
Val:   Loss=0.8042 (C:0.8042, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8042)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.6622 (C:0.6622, R:0.0105)
Batch  25/537: Loss=0.6804 (C:0.6804, R:0.0105)
Batch  50/537: Loss=0.7360 (C:0.7360, R:0.0105)
Batch  75/537: Loss=0.6653 (C:0.6653, R:0.0105)
Batch 100/537: Loss=0.6812 (C:0.6812, R:0.0105)
Batch 125/537: Loss=0.6535 (C:0.6535, R:0.0105)
Batch 150/537: Loss=0.6926 (C:0.6926, R:0.0105)
Batch 175/537: Loss=0.6991 (C:0.6991, R:0.0105)
Batch 200/537: Loss=0.7109 (C:0.7109, R:0.0106)
Batch 225/537: Loss=0.7143 (C:0.7143, R:0.0105)
Batch 250/537: Loss=0.7041 (C:0.7041, R:0.0105)
Batch 275/537: Loss=0.6762 (C:0.6762, R:0.0105)
Batch 300/537: Loss=0.6635 (C:0.6635, R:0.0105)
Batch 325/537: Loss=0.6935 (C:0.6935, R:0.0105)
Batch 350/537: Loss=0.7019 (C:0.7019, R:0.0105)
Batch 375/537: Loss=0.7093 (C:0.7093, R:0.0105)
Batch 400/537: Loss=0.6949 (C:0.6949, R:0.0105)
Batch 425/537: Loss=0.6544 (C:0.6544, R:0.0105)
Batch 450/537: Loss=0.7190 (C:0.7190, R:0.0105)
Batch 475/537: Loss=0.6741 (C:0.6741, R:0.0105)
Batch 500/537: Loss=0.6838 (C:0.6838, R:0.0105)
Batch 525/537: Loss=0.7006 (C:0.7006, R:0.0105)

============================================================
Epoch 35/300 completed in 21.5s
Train: Loss=0.6822 (C:0.6822, R:0.0105) Ratio=4.44x
Val:   Loss=0.8115 (C:0.8115, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.075
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.6853 (C:0.6853, R:0.0105)
Batch  25/537: Loss=0.6624 (C:0.6624, R:0.0105)
Batch  50/537: Loss=0.6855 (C:0.6855, R:0.0105)
Batch  75/537: Loss=0.7003 (C:0.7003, R:0.0105)
Batch 100/537: Loss=0.6625 (C:0.6625, R:0.0105)
Batch 125/537: Loss=0.6639 (C:0.6639, R:0.0105)
Batch 150/537: Loss=0.6923 (C:0.6923, R:0.0106)
Batch 175/537: Loss=0.6830 (C:0.6830, R:0.0105)
Batch 200/537: Loss=0.6559 (C:0.6559, R:0.0105)
Batch 225/537: Loss=0.6915 (C:0.6915, R:0.0105)
Batch 250/537: Loss=0.6991 (C:0.6991, R:0.0105)
Batch 275/537: Loss=0.7142 (C:0.7142, R:0.0105)
Batch 300/537: Loss=0.6921 (C:0.6921, R:0.0105)
Batch 325/537: Loss=0.6818 (C:0.6818, R:0.0105)
Batch 350/537: Loss=0.6585 (C:0.6585, R:0.0105)
Batch 375/537: Loss=0.7062 (C:0.7062, R:0.0105)
Batch 400/537: Loss=0.6719 (C:0.6719, R:0.0105)
Batch 425/537: Loss=0.6639 (C:0.6639, R:0.0105)
Batch 450/537: Loss=0.6365 (C:0.6365, R:0.0105)
Batch 475/537: Loss=0.6965 (C:0.6965, R:0.0105)
Batch 500/537: Loss=0.6882 (C:0.6882, R:0.0105)
Batch 525/537: Loss=0.6870 (C:0.6870, R:0.0105)

============================================================
Epoch 36/300 completed in 21.8s
Train: Loss=0.6800 (C:0.6800, R:0.0105) Ratio=4.51x
Val:   Loss=0.8150 (C:0.8150, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.090
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.367 ± 0.606
    Neg distances: 2.455 ± 1.058
    Separation ratio: 6.68x
    Gap: -4.183
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.6072 (C:0.6072, R:0.0105)
Batch  25/537: Loss=0.5993 (C:0.5993, R:0.0105)
Batch  50/537: Loss=0.6591 (C:0.6591, R:0.0105)
Batch  75/537: Loss=0.6369 (C:0.6369, R:0.0105)
Batch 100/537: Loss=0.6208 (C:0.6208, R:0.0105)
Batch 125/537: Loss=0.6680 (C:0.6680, R:0.0106)
Batch 150/537: Loss=0.6426 (C:0.6426, R:0.0105)
Batch 175/537: Loss=0.6667 (C:0.6667, R:0.0105)
Batch 200/537: Loss=0.6749 (C:0.6749, R:0.0105)
Batch 225/537: Loss=0.6369 (C:0.6369, R:0.0105)
Batch 250/537: Loss=0.6487 (C:0.6487, R:0.0105)
Batch 275/537: Loss=0.6543 (C:0.6543, R:0.0105)
Batch 300/537: Loss=0.6801 (C:0.6801, R:0.0105)
Batch 325/537: Loss=0.6526 (C:0.6526, R:0.0105)
Batch 350/537: Loss=0.6542 (C:0.6542, R:0.0105)
Batch 375/537: Loss=0.6769 (C:0.6769, R:0.0105)
Batch 400/537: Loss=0.6815 (C:0.6815, R:0.0105)
Batch 425/537: Loss=0.6295 (C:0.6295, R:0.0105)
Batch 450/537: Loss=0.6805 (C:0.6805, R:0.0105)
Batch 475/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 500/537: Loss=0.6330 (C:0.6330, R:0.0105)
Batch 525/537: Loss=0.6429 (C:0.6429, R:0.0105)

============================================================
Epoch 37/300 completed in 27.7s
Train: Loss=0.6560 (C:0.6560, R:0.0105) Ratio=4.62x
Val:   Loss=0.7871 (C:0.7871, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.7871)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.6269 (C:0.6269, R:0.0105)
Batch  25/537: Loss=0.6499 (C:0.6499, R:0.0105)
Batch  50/537: Loss=0.6512 (C:0.6512, R:0.0105)
Batch  75/537: Loss=0.6678 (C:0.6678, R:0.0105)
Batch 100/537: Loss=0.6668 (C:0.6668, R:0.0105)
Batch 125/537: Loss=0.6681 (C:0.6681, R:0.0105)
Batch 150/537: Loss=0.6686 (C:0.6686, R:0.0105)
Batch 175/537: Loss=0.6716 (C:0.6716, R:0.0105)
Batch 200/537: Loss=0.6911 (C:0.6911, R:0.0105)
Batch 225/537: Loss=0.6457 (C:0.6457, R:0.0105)
Batch 250/537: Loss=0.6503 (C:0.6503, R:0.0105)
Batch 275/537: Loss=0.6372 (C:0.6372, R:0.0105)
Batch 300/537: Loss=0.6364 (C:0.6364, R:0.0105)
Batch 325/537: Loss=0.6396 (C:0.6396, R:0.0105)
Batch 350/537: Loss=0.6562 (C:0.6562, R:0.0105)
Batch 375/537: Loss=0.6562 (C:0.6562, R:0.0105)
Batch 400/537: Loss=0.6544 (C:0.6544, R:0.0105)
Batch 425/537: Loss=0.6666 (C:0.6666, R:0.0105)
Batch 450/537: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 475/537: Loss=0.6685 (C:0.6685, R:0.0105)
Batch 500/537: Loss=0.6775 (C:0.6775, R:0.0105)
Batch 525/537: Loss=0.6710 (C:0.6710, R:0.0105)

============================================================
Epoch 38/300 completed in 21.7s
Train: Loss=0.6537 (C:0.6537, R:0.0105) Ratio=4.63x
Val:   Loss=0.7895 (C:0.7895, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.6360 (C:0.6360, R:0.0105)
Batch  25/537: Loss=0.6563 (C:0.6563, R:0.0105)
Batch  50/537: Loss=0.6399 (C:0.6399, R:0.0105)
Batch  75/537: Loss=0.6432 (C:0.6432, R:0.0105)
Batch 100/537: Loss=0.6822 (C:0.6822, R:0.0105)
Batch 125/537: Loss=0.6608 (C:0.6608, R:0.0106)
Batch 150/537: Loss=0.6624 (C:0.6624, R:0.0105)
Batch 175/537: Loss=0.6475 (C:0.6475, R:0.0105)
Batch 200/537: Loss=0.6377 (C:0.6377, R:0.0105)
Batch 225/537: Loss=0.6701 (C:0.6701, R:0.0105)
Batch 250/537: Loss=0.6578 (C:0.6578, R:0.0105)
Batch 275/537: Loss=0.6683 (C:0.6683, R:0.0105)
Batch 300/537: Loss=0.6733 (C:0.6733, R:0.0105)
Batch 325/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 350/537: Loss=0.6392 (C:0.6392, R:0.0105)
Batch 375/537: Loss=0.6555 (C:0.6555, R:0.0105)
Batch 400/537: Loss=0.6779 (C:0.6779, R:0.0105)
Batch 425/537: Loss=0.6396 (C:0.6396, R:0.0105)
Batch 450/537: Loss=0.6639 (C:0.6639, R:0.0105)
Batch 475/537: Loss=0.6623 (C:0.6623, R:0.0105)
Batch 500/537: Loss=0.6374 (C:0.6374, R:0.0105)
Batch 525/537: Loss=0.6402 (C:0.6402, R:0.0105)

============================================================
Epoch 39/300 completed in 21.9s
Train: Loss=0.6514 (C:0.6514, R:0.0105) Ratio=4.51x
Val:   Loss=0.7881 (C:0.7881, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.365 ± 0.562
    Neg distances: 2.503 ± 1.069
    Separation ratio: 6.86x
    Gap: -4.855
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.6370 (C:0.6370, R:0.0105)
Batch  25/537: Loss=0.6073 (C:0.6073, R:0.0105)
Batch  50/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch  75/537: Loss=0.6268 (C:0.6268, R:0.0105)
Batch 100/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 125/537: Loss=0.6656 (C:0.6656, R:0.0105)
Batch 150/537: Loss=0.6733 (C:0.6733, R:0.0105)
Batch 175/537: Loss=0.6271 (C:0.6271, R:0.0105)
Batch 200/537: Loss=0.6715 (C:0.6715, R:0.0105)
Batch 225/537: Loss=0.6240 (C:0.6240, R:0.0105)
Batch 250/537: Loss=0.6330 (C:0.6330, R:0.0105)
Batch 275/537: Loss=0.6710 (C:0.6710, R:0.0105)
Batch 300/537: Loss=0.6358 (C:0.6358, R:0.0105)
Batch 325/537: Loss=0.6182 (C:0.6182, R:0.0105)
Batch 350/537: Loss=0.6488 (C:0.6488, R:0.0105)
Batch 375/537: Loss=0.6434 (C:0.6434, R:0.0105)
Batch 400/537: Loss=0.6589 (C:0.6589, R:0.0105)
Batch 425/537: Loss=0.6568 (C:0.6568, R:0.0105)
Batch 450/537: Loss=0.6355 (C:0.6355, R:0.0105)
Batch 475/537: Loss=0.6520 (C:0.6520, R:0.0105)
Batch 500/537: Loss=0.6580 (C:0.6580, R:0.0105)
Batch 525/537: Loss=0.6062 (C:0.6062, R:0.0105)

============================================================
Epoch 40/300 completed in 28.2s
Train: Loss=0.6371 (C:0.6371, R:0.0105) Ratio=4.62x
Val:   Loss=0.7742 (C:0.7742, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.150
✅ New best model saved (Val Loss: 0.7742)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6255 (C:0.6255, R:0.0105)
Batch  25/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch  50/537: Loss=0.5989 (C:0.5989, R:0.0105)
Batch  75/537: Loss=0.6534 (C:0.6534, R:0.0105)
Batch 100/537: Loss=0.6092 (C:0.6092, R:0.0106)
Batch 125/537: Loss=0.6118 (C:0.6118, R:0.0105)
Batch 150/537: Loss=0.6587 (C:0.6587, R:0.0105)
Batch 175/537: Loss=0.6368 (C:0.6368, R:0.0105)
Batch 200/537: Loss=0.6280 (C:0.6280, R:0.0105)
Batch 225/537: Loss=0.6569 (C:0.6569, R:0.0105)
Batch 250/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 275/537: Loss=0.5998 (C:0.5998, R:0.0105)
Batch 300/537: Loss=0.6502 (C:0.6502, R:0.0105)
Batch 325/537: Loss=0.6437 (C:0.6437, R:0.0105)
Batch 350/537: Loss=0.6488 (C:0.6488, R:0.0105)
Batch 375/537: Loss=0.6231 (C:0.6231, R:0.0105)
Batch 400/537: Loss=0.6359 (C:0.6359, R:0.0105)
Batch 425/537: Loss=0.6466 (C:0.6466, R:0.0105)
Batch 450/537: Loss=0.6408 (C:0.6408, R:0.0105)
Batch 475/537: Loss=0.6177 (C:0.6177, R:0.0105)
Batch 500/537: Loss=0.6918 (C:0.6918, R:0.0105)
Batch 525/537: Loss=0.6131 (C:0.6131, R:0.0105)

============================================================
Epoch 41/300 completed in 22.3s
Train: Loss=0.6333 (C:0.6333, R:0.0105) Ratio=4.68x
Val:   Loss=0.7684 (C:0.7684, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.165
✅ New best model saved (Val Loss: 0.7684)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.6326 (C:0.6326, R:0.0105)
Batch  25/537: Loss=0.6179 (C:0.6179, R:0.0105)
Batch  50/537: Loss=0.6273 (C:0.6273, R:0.0105)
Batch  75/537: Loss=0.6727 (C:0.6727, R:0.0105)
Batch 100/537: Loss=0.6066 (C:0.6066, R:0.0105)
Batch 125/537: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 150/537: Loss=0.6191 (C:0.6191, R:0.0105)
Batch 175/537: Loss=0.6282 (C:0.6282, R:0.0105)
Batch 200/537: Loss=0.6389 (C:0.6389, R:0.0105)
Batch 225/537: Loss=0.6242 (C:0.6242, R:0.0105)
Batch 250/537: Loss=0.6469 (C:0.6469, R:0.0105)
Batch 275/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 300/537: Loss=0.6396 (C:0.6396, R:0.0105)
Batch 325/537: Loss=0.6349 (C:0.6349, R:0.0105)
Batch 350/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 375/537: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 400/537: Loss=0.6453 (C:0.6453, R:0.0105)
Batch 425/537: Loss=0.6430 (C:0.6430, R:0.0105)
Batch 450/537: Loss=0.6414 (C:0.6414, R:0.0105)
Batch 475/537: Loss=0.6458 (C:0.6458, R:0.0105)
Batch 500/537: Loss=0.6565 (C:0.6565, R:0.0105)
Batch 525/537: Loss=0.6585 (C:0.6585, R:0.0105)

============================================================
Epoch 42/300 completed in 22.2s
Train: Loss=0.6313 (C:0.6313, R:0.0105) Ratio=4.70x
Val:   Loss=0.7760 (C:0.7760, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.180
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.350 ± 0.558
    Neg distances: 2.523 ± 1.069
    Separation ratio: 7.21x
    Gap: -4.323
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.6287 (C:0.6287, R:0.0105)
Batch  25/537: Loss=0.5810 (C:0.5810, R:0.0105)
Batch  50/537: Loss=0.5745 (C:0.5745, R:0.0105)
Batch  75/537: Loss=0.6096 (C:0.6096, R:0.0105)
Batch 100/537: Loss=0.6088 (C:0.6088, R:0.0105)
Batch 125/537: Loss=0.6239 (C:0.6239, R:0.0105)
Batch 150/537: Loss=0.6142 (C:0.6142, R:0.0105)
Batch 175/537: Loss=0.6170 (C:0.6170, R:0.0105)
Batch 200/537: Loss=0.6449 (C:0.6449, R:0.0105)
Batch 225/537: Loss=0.6309 (C:0.6309, R:0.0105)
Batch 250/537: Loss=0.6165 (C:0.6165, R:0.0105)
Batch 275/537: Loss=0.6228 (C:0.6228, R:0.0105)
Batch 300/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch 325/537: Loss=0.6799 (C:0.6799, R:0.0105)
Batch 350/537: Loss=0.6666 (C:0.6666, R:0.0105)
Batch 375/537: Loss=0.6203 (C:0.6203, R:0.0105)
Batch 400/537: Loss=0.6089 (C:0.6089, R:0.0105)
Batch 425/537: Loss=0.6088 (C:0.6088, R:0.0105)
Batch 450/537: Loss=0.6110 (C:0.6110, R:0.0105)
Batch 475/537: Loss=0.6128 (C:0.6128, R:0.0105)
Batch 500/537: Loss=0.6186 (C:0.6186, R:0.0105)
Batch 525/537: Loss=0.6343 (C:0.6343, R:0.0105)

============================================================
Epoch 43/300 completed in 28.2s
Train: Loss=0.6146 (C:0.6146, R:0.0105) Ratio=4.67x
Val:   Loss=0.7603 (C:0.7603, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.7603)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.6202 (C:0.6202, R:0.0105)
Batch  25/537: Loss=0.5770 (C:0.5770, R:0.0105)
Batch  50/537: Loss=0.6502 (C:0.6502, R:0.0105)
Batch  75/537: Loss=0.6201 (C:0.6201, R:0.0105)
Batch 100/537: Loss=0.6198 (C:0.6198, R:0.0105)
Batch 125/537: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 150/537: Loss=0.5947 (C:0.5947, R:0.0105)
Batch 175/537: Loss=0.6356 (C:0.6356, R:0.0105)
Batch 200/537: Loss=0.6288 (C:0.6288, R:0.0105)
Batch 225/537: Loss=0.6193 (C:0.6193, R:0.0105)
Batch 250/537: Loss=0.6223 (C:0.6223, R:0.0106)
Batch 275/537: Loss=0.5773 (C:0.5773, R:0.0105)
Batch 300/537: Loss=0.6348 (C:0.6348, R:0.0105)
Batch 325/537: Loss=0.6044 (C:0.6044, R:0.0105)
Batch 350/537: Loss=0.6191 (C:0.6191, R:0.0105)
Batch 375/537: Loss=0.6161 (C:0.6161, R:0.0105)
Batch 400/537: Loss=0.6205 (C:0.6205, R:0.0105)
Batch 425/537: Loss=0.6342 (C:0.6342, R:0.0105)
Batch 450/537: Loss=0.6303 (C:0.6303, R:0.0105)
Batch 475/537: Loss=0.6396 (C:0.6396, R:0.0105)
Batch 500/537: Loss=0.6422 (C:0.6422, R:0.0105)
Batch 525/537: Loss=0.6220 (C:0.6220, R:0.0105)

============================================================
Epoch 44/300 completed in 22.2s
Train: Loss=0.6121 (C:0.6121, R:0.0105) Ratio=4.74x
Val:   Loss=0.7627 (C:0.7627, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.6043 (C:0.6043, R:0.0105)
Batch  25/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch  50/537: Loss=0.6077 (C:0.6077, R:0.0105)
Batch  75/537: Loss=0.5874 (C:0.5874, R:0.0105)
Batch 100/537: Loss=0.6354 (C:0.6354, R:0.0105)
Batch 125/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 150/537: Loss=0.6220 (C:0.6220, R:0.0105)
Batch 175/537: Loss=0.5899 (C:0.5899, R:0.0105)
Batch 200/537: Loss=0.5842 (C:0.5842, R:0.0105)
Batch 225/537: Loss=0.6580 (C:0.6580, R:0.0105)
Batch 250/537: Loss=0.5965 (C:0.5965, R:0.0105)
Batch 275/537: Loss=0.6031 (C:0.6031, R:0.0105)
Batch 300/537: Loss=0.6000 (C:0.6000, R:0.0105)
Batch 325/537: Loss=0.6489 (C:0.6489, R:0.0105)
Batch 350/537: Loss=0.6213 (C:0.6213, R:0.0105)
Batch 375/537: Loss=0.6269 (C:0.6269, R:0.0105)
Batch 400/537: Loss=0.6221 (C:0.6221, R:0.0105)
Batch 425/537: Loss=0.5851 (C:0.5851, R:0.0105)
Batch 450/537: Loss=0.6443 (C:0.6443, R:0.0105)
Batch 475/537: Loss=0.6099 (C:0.6099, R:0.0105)
Batch 500/537: Loss=0.6221 (C:0.6221, R:0.0105)
Batch 525/537: Loss=0.5864 (C:0.5864, R:0.0105)

============================================================
Epoch 45/300 completed in 21.8s
Train: Loss=0.6110 (C:0.6110, R:0.0105) Ratio=4.79x
Val:   Loss=0.7516 (C:0.7516, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.225
✅ New best model saved (Val Loss: 0.7516)
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.342 ± 0.589
    Neg distances: 2.582 ± 1.086
    Separation ratio: 7.55x
    Gap: -4.404
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.5973 (C:0.5973, R:0.0105)
Batch  25/537: Loss=0.5845 (C:0.5845, R:0.0105)
Batch  50/537: Loss=0.6037 (C:0.6037, R:0.0105)
Batch  75/537: Loss=0.5871 (C:0.5871, R:0.0105)
Batch 100/537: Loss=0.5908 (C:0.5908, R:0.0105)
Batch 125/537: Loss=0.5618 (C:0.5618, R:0.0105)
Batch 150/537: Loss=0.6049 (C:0.6049, R:0.0105)
Batch 175/537: Loss=0.6017 (C:0.6017, R:0.0105)
Batch 200/537: Loss=0.5952 (C:0.5952, R:0.0105)
Batch 225/537: Loss=0.5754 (C:0.5754, R:0.0105)
Batch 250/537: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 275/537: Loss=0.5991 (C:0.5991, R:0.0105)
Batch 300/537: Loss=0.5933 (C:0.5933, R:0.0105)
Batch 325/537: Loss=0.5697 (C:0.5697, R:0.0105)
Batch 350/537: Loss=0.6175 (C:0.6175, R:0.0105)
Batch 375/537: Loss=0.5990 (C:0.5990, R:0.0105)
Batch 400/537: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 425/537: Loss=0.5824 (C:0.5824, R:0.0105)
Batch 450/537: Loss=0.5793 (C:0.5793, R:0.0105)
Batch 475/537: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 500/537: Loss=0.5957 (C:0.5957, R:0.0105)
Batch 525/537: Loss=0.6065 (C:0.6065, R:0.0105)

============================================================
Epoch 46/300 completed in 27.8s
Train: Loss=0.5966 (C:0.5966, R:0.0105) Ratio=4.88x
Val:   Loss=0.7521 (C:0.7521, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.240
No improvement for 1 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.6006 (C:0.6006, R:0.0105)
Batch  25/537: Loss=0.5998 (C:0.5998, R:0.0105)
Batch  50/537: Loss=0.5647 (C:0.5647, R:0.0106)
Batch  75/537: Loss=0.6084 (C:0.6084, R:0.0105)
Batch 100/537: Loss=0.5820 (C:0.5820, R:0.0105)
Batch 125/537: Loss=0.5454 (C:0.5454, R:0.0105)
Batch 150/537: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 175/537: Loss=0.5788 (C:0.5788, R:0.0105)
Batch 200/537: Loss=0.6426 (C:0.6426, R:0.0105)
Batch 225/537: Loss=0.5992 (C:0.5992, R:0.0105)
Batch 250/537: Loss=0.5903 (C:0.5903, R:0.0105)
Batch 275/537: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 300/537: Loss=0.6019 (C:0.6019, R:0.0105)
Batch 325/537: Loss=0.5978 (C:0.5978, R:0.0105)
Batch 350/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 375/537: Loss=0.5722 (C:0.5722, R:0.0105)
Batch 400/537: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 425/537: Loss=0.6085 (C:0.6085, R:0.0105)
Batch 450/537: Loss=0.6177 (C:0.6177, R:0.0106)
Batch 475/537: Loss=0.6088 (C:0.6088, R:0.0105)
Batch 500/537: Loss=0.6097 (C:0.6097, R:0.0105)
Batch 525/537: Loss=0.6186 (C:0.6186, R:0.0105)

============================================================
Epoch 47/300 completed in 22.1s
Train: Loss=0.5953 (C:0.5953, R:0.0105) Ratio=4.93x
Val:   Loss=0.7409 (C:0.7409, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.255
✅ New best model saved (Val Loss: 0.7409)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch  25/537: Loss=0.5989 (C:0.5989, R:0.0105)
Batch  50/537: Loss=0.5680 (C:0.5680, R:0.0105)
Batch  75/537: Loss=0.5825 (C:0.5825, R:0.0105)
Batch 100/537: Loss=0.5820 (C:0.5820, R:0.0105)
Batch 125/537: Loss=0.5975 (C:0.5975, R:0.0105)
Batch 150/537: Loss=0.5974 (C:0.5974, R:0.0105)
Batch 175/537: Loss=0.6124 (C:0.6124, R:0.0105)
Batch 200/537: Loss=0.5810 (C:0.5810, R:0.0105)
Batch 225/537: Loss=0.6088 (C:0.6088, R:0.0105)
Batch 250/537: Loss=0.6061 (C:0.6061, R:0.0105)
Batch 275/537: Loss=0.5804 (C:0.5804, R:0.0105)
Batch 300/537: Loss=0.6069 (C:0.6069, R:0.0105)
Batch 325/537: Loss=0.5820 (C:0.5820, R:0.0105)
Batch 350/537: Loss=0.6157 (C:0.6157, R:0.0105)
Batch 375/537: Loss=0.5971 (C:0.5971, R:0.0105)
Batch 400/537: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 425/537: Loss=0.5695 (C:0.5695, R:0.0105)
Batch 450/537: Loss=0.5979 (C:0.5979, R:0.0106)
Batch 475/537: Loss=0.6105 (C:0.6105, R:0.0105)
Batch 500/537: Loss=0.6252 (C:0.6252, R:0.0105)
Batch 525/537: Loss=0.6182 (C:0.6182, R:0.0105)

============================================================
Epoch 48/300 completed in 21.4s
Train: Loss=0.5929 (C:0.5929, R:0.0105) Ratio=4.84x
Val:   Loss=0.7453 (C:0.7453, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.270
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.331 ± 0.572
    Neg distances: 2.591 ± 1.080
    Separation ratio: 7.84x
    Gap: -4.366
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.5841 (C:0.5841, R:0.0105)
Batch  25/537: Loss=0.5496 (C:0.5496, R:0.0105)
Batch  50/537: Loss=0.5608 (C:0.5608, R:0.0105)
Batch  75/537: Loss=0.5452 (C:0.5452, R:0.0105)
Batch 100/537: Loss=0.5864 (C:0.5864, R:0.0105)
Batch 125/537: Loss=0.5831 (C:0.5831, R:0.0105)
Batch 150/537: Loss=0.5618 (C:0.5618, R:0.0105)
Batch 175/537: Loss=0.5321 (C:0.5321, R:0.0105)
Batch 200/537: Loss=0.5951 (C:0.5951, R:0.0105)
Batch 225/537: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 250/537: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 275/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 300/537: Loss=0.5693 (C:0.5693, R:0.0105)
Batch 325/537: Loss=0.6342 (C:0.6342, R:0.0106)
Batch 350/537: Loss=0.5466 (C:0.5466, R:0.0105)
Batch 375/537: Loss=0.5411 (C:0.5411, R:0.0105)
Batch 400/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 425/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 450/537: Loss=0.6173 (C:0.6173, R:0.0105)
Batch 475/537: Loss=0.6285 (C:0.6285, R:0.0105)
Batch 500/537: Loss=0.5692 (C:0.5692, R:0.0105)
Batch 525/537: Loss=0.5924 (C:0.5924, R:0.0105)

============================================================
Epoch 49/300 completed in 27.8s
Train: Loss=0.5782 (C:0.5782, R:0.0105) Ratio=4.88x
Val:   Loss=0.7260 (C:0.7260, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.7260)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.5549 (C:0.5549, R:0.0105)
Batch  25/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch  50/537: Loss=0.5839 (C:0.5839, R:0.0105)
Batch  75/537: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 100/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 125/537: Loss=0.5305 (C:0.5305, R:0.0105)
Batch 150/537: Loss=0.6027 (C:0.6027, R:0.0105)
Batch 175/537: Loss=0.6001 (C:0.6001, R:0.0105)
Batch 200/537: Loss=0.5810 (C:0.5810, R:0.0105)
Batch 225/537: Loss=0.5786 (C:0.5786, R:0.0105)
Batch 250/537: Loss=0.5835 (C:0.5835, R:0.0105)
Batch 275/537: Loss=0.6027 (C:0.6027, R:0.0105)
Batch 300/537: Loss=0.5650 (C:0.5650, R:0.0105)
Batch 325/537: Loss=0.5982 (C:0.5982, R:0.0105)
Batch 350/537: Loss=0.6016 (C:0.6016, R:0.0105)
Batch 375/537: Loss=0.5935 (C:0.5935, R:0.0106)
Batch 400/537: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 425/537: Loss=0.5491 (C:0.5491, R:0.0105)
Batch 450/537: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 475/537: Loss=0.5762 (C:0.5762, R:0.0105)
Batch 500/537: Loss=0.5863 (C:0.5863, R:0.0105)
Batch 525/537: Loss=0.5525 (C:0.5525, R:0.0105)

============================================================
Epoch 50/300 completed in 21.5s
Train: Loss=0.5781 (C:0.5781, R:0.0105) Ratio=4.98x
Val:   Loss=0.7409 (C:0.7409, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.5789 (C:0.5789, R:0.0105)
Batch  25/537: Loss=0.5750 (C:0.5750, R:0.0105)
Batch  50/537: Loss=0.5347 (C:0.5347, R:0.0105)
Batch  75/537: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 100/537: Loss=0.5592 (C:0.5592, R:0.0105)
Batch 125/537: Loss=0.5487 (C:0.5487, R:0.0105)
Batch 150/537: Loss=0.5678 (C:0.5678, R:0.0105)
Batch 175/537: Loss=0.5715 (C:0.5715, R:0.0105)
Batch 200/537: Loss=0.5657 (C:0.5657, R:0.0105)
Batch 225/537: Loss=0.5747 (C:0.5747, R:0.0105)
Batch 250/537: Loss=0.5832 (C:0.5832, R:0.0106)
Batch 275/537: Loss=0.5492 (C:0.5492, R:0.0105)
Batch 300/537: Loss=0.5917 (C:0.5917, R:0.0105)
Batch 325/537: Loss=0.5822 (C:0.5822, R:0.0105)
Batch 350/537: Loss=0.5464 (C:0.5464, R:0.0105)
Batch 375/537: Loss=0.5862 (C:0.5862, R:0.0105)
Batch 400/537: Loss=0.5571 (C:0.5571, R:0.0105)
Batch 425/537: Loss=0.5869 (C:0.5869, R:0.0105)
Batch 450/537: Loss=0.5617 (C:0.5617, R:0.0105)
Batch 475/537: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 500/537: Loss=0.5878 (C:0.5878, R:0.0105)
Batch 525/537: Loss=0.5735 (C:0.5735, R:0.0105)

============================================================
Epoch 51/300 completed in 21.2s
Train: Loss=0.5755 (C:0.5755, R:0.0105) Ratio=5.03x
Val:   Loss=0.7409 (C:0.7409, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.319 ± 0.555
    Neg distances: 2.619 ± 1.084
    Separation ratio: 8.21x
    Gap: -4.957
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.5797 (C:0.5797, R:0.0105)
Batch  25/537: Loss=0.5551 (C:0.5551, R:0.0105)
Batch  50/537: Loss=0.5682 (C:0.5682, R:0.0105)
Batch  75/537: Loss=0.5579 (C:0.5579, R:0.0105)
Batch 100/537: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 125/537: Loss=0.5300 (C:0.5300, R:0.0105)
Batch 150/537: Loss=0.5480 (C:0.5480, R:0.0105)
Batch 175/537: Loss=0.5689 (C:0.5689, R:0.0105)
Batch 200/537: Loss=0.5903 (C:0.5903, R:0.0105)
Batch 225/537: Loss=0.5510 (C:0.5510, R:0.0105)
Batch 250/537: Loss=0.5442 (C:0.5442, R:0.0105)
Batch 275/537: Loss=0.5947 (C:0.5947, R:0.0105)
Batch 300/537: Loss=0.5650 (C:0.5650, R:0.0105)
Batch 325/537: Loss=0.5167 (C:0.5167, R:0.0105)
Batch 350/537: Loss=0.5907 (C:0.5907, R:0.0105)
Batch 375/537: Loss=0.5774 (C:0.5774, R:0.0105)
Batch 400/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 425/537: Loss=0.5808 (C:0.5808, R:0.0105)
Batch 450/537: Loss=0.5776 (C:0.5776, R:0.0105)
Batch 475/537: Loss=0.5730 (C:0.5730, R:0.0105)
Batch 500/537: Loss=0.5736 (C:0.5736, R:0.0106)
Batch 525/537: Loss=0.6001 (C:0.6001, R:0.0105)

============================================================
Epoch 52/300 completed in 27.4s
Train: Loss=0.5623 (C:0.5623, R:0.0105) Ratio=4.95x
Val:   Loss=0.7295 (C:0.7295, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.5140 (C:0.5140, R:0.0105)
Batch  25/537: Loss=0.5507 (C:0.5507, R:0.0105)
Batch  50/537: Loss=0.5533 (C:0.5533, R:0.0105)
Batch  75/537: Loss=0.5479 (C:0.5479, R:0.0105)
Batch 100/537: Loss=0.5606 (C:0.5606, R:0.0105)
Batch 125/537: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 150/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 175/537: Loss=0.5597 (C:0.5597, R:0.0105)
Batch 200/537: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 225/537: Loss=0.5994 (C:0.5994, R:0.0105)
Batch 250/537: Loss=0.5396 (C:0.5396, R:0.0105)
Batch 275/537: Loss=0.5819 (C:0.5819, R:0.0105)
Batch 300/537: Loss=0.5771 (C:0.5771, R:0.0105)
Batch 325/537: Loss=0.5851 (C:0.5851, R:0.0105)
Batch 350/537: Loss=0.5587 (C:0.5587, R:0.0105)
Batch 375/537: Loss=0.5668 (C:0.5668, R:0.0105)
Batch 400/537: Loss=0.5466 (C:0.5466, R:0.0105)
Batch 425/537: Loss=0.5391 (C:0.5391, R:0.0105)
Batch 450/537: Loss=0.5461 (C:0.5461, R:0.0105)
Batch 475/537: Loss=0.5562 (C:0.5562, R:0.0105)
Batch 500/537: Loss=0.5865 (C:0.5865, R:0.0105)
Batch 525/537: Loss=0.5763 (C:0.5763, R:0.0105)

============================================================
Epoch 53/300 completed in 21.7s
Train: Loss=0.5618 (C:0.5618, R:0.0105) Ratio=5.09x
Val:   Loss=0.7272 (C:0.7272, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.5699 (C:0.5699, R:0.0105)
Batch  25/537: Loss=0.5902 (C:0.5902, R:0.0105)
Batch  50/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch  75/537: Loss=0.5283 (C:0.5283, R:0.0105)
Batch 100/537: Loss=0.5464 (C:0.5464, R:0.0105)
Batch 125/537: Loss=0.5594 (C:0.5594, R:0.0105)
Batch 150/537: Loss=0.5790 (C:0.5790, R:0.0105)
Batch 175/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 200/537: Loss=0.5535 (C:0.5535, R:0.0105)
Batch 225/537: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 250/537: Loss=0.5537 (C:0.5537, R:0.0105)
Batch 275/537: Loss=0.5663 (C:0.5663, R:0.0105)
Batch 300/537: Loss=0.5931 (C:0.5931, R:0.0105)
Batch 325/537: Loss=0.5153 (C:0.5153, R:0.0105)
Batch 350/537: Loss=0.5197 (C:0.5197, R:0.0105)
Batch 375/537: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 400/537: Loss=0.5500 (C:0.5500, R:0.0105)
Batch 425/537: Loss=0.5501 (C:0.5501, R:0.0105)
Batch 450/537: Loss=0.5543 (C:0.5543, R:0.0105)
Batch 475/537: Loss=0.5893 (C:0.5893, R:0.0105)
Batch 500/537: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 525/537: Loss=0.5581 (C:0.5581, R:0.0105)

============================================================
Epoch 54/300 completed in 21.6s
Train: Loss=0.5592 (C:0.5592, R:0.0105) Ratio=5.02x
Val:   Loss=0.7281 (C:0.7281, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.306 ± 0.531
    Neg distances: 2.637 ± 1.086
    Separation ratio: 8.62x
    Gap: -4.514
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.5196 (C:0.5196, R:0.0105)
Batch  25/537: Loss=0.5207 (C:0.5207, R:0.0105)
Batch  50/537: Loss=0.5337 (C:0.5337, R:0.0105)
Batch  75/537: Loss=0.5480 (C:0.5480, R:0.0105)
Batch 100/537: Loss=0.5230 (C:0.5230, R:0.0105)
Batch 125/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 150/537: Loss=0.5421 (C:0.5421, R:0.0105)
Batch 175/537: Loss=0.5341 (C:0.5341, R:0.0105)
Batch 200/537: Loss=0.5249 (C:0.5249, R:0.0105)
Batch 225/537: Loss=0.5604 (C:0.5604, R:0.0105)
Batch 250/537: Loss=0.5444 (C:0.5444, R:0.0105)
Batch 275/537: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 300/537: Loss=0.5396 (C:0.5396, R:0.0105)
Batch 325/537: Loss=0.5470 (C:0.5470, R:0.0105)
Batch 350/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 375/537: Loss=0.5573 (C:0.5573, R:0.0105)
Batch 400/537: Loss=0.5219 (C:0.5219, R:0.0105)
Batch 425/537: Loss=0.5838 (C:0.5838, R:0.0105)
Batch 450/537: Loss=0.5702 (C:0.5702, R:0.0105)
Batch 475/537: Loss=0.5589 (C:0.5589, R:0.0105)
Batch 500/537: Loss=0.5707 (C:0.5707, R:0.0105)
Batch 525/537: Loss=0.5737 (C:0.5737, R:0.0105)

============================================================
Epoch 55/300 completed in 27.4s
Train: Loss=0.5474 (C:0.5474, R:0.0105) Ratio=5.02x
Val:   Loss=0.7103 (C:0.7103, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7103)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.5645 (C:0.5645, R:0.0105)
Batch  25/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch  50/537: Loss=0.5150 (C:0.5150, R:0.0105)
Batch  75/537: Loss=0.5041 (C:0.5041, R:0.0105)
Batch 100/537: Loss=0.5538 (C:0.5538, R:0.0105)
Batch 125/537: Loss=0.5143 (C:0.5143, R:0.0105)
Batch 150/537: Loss=0.5753 (C:0.5753, R:0.0105)
Batch 175/537: Loss=0.5515 (C:0.5515, R:0.0105)
Batch 200/537: Loss=0.5509 (C:0.5509, R:0.0105)
Batch 225/537: Loss=0.5820 (C:0.5820, R:0.0105)
Batch 250/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 275/537: Loss=0.5532 (C:0.5532, R:0.0105)
Batch 300/537: Loss=0.5703 (C:0.5703, R:0.0105)
Batch 325/537: Loss=0.5457 (C:0.5457, R:0.0105)
Batch 350/537: Loss=0.5672 (C:0.5672, R:0.0105)
Batch 375/537: Loss=0.5678 (C:0.5678, R:0.0105)
Batch 400/537: Loss=0.5759 (C:0.5759, R:0.0105)
Batch 425/537: Loss=0.5688 (C:0.5688, R:0.0105)
Batch 450/537: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 475/537: Loss=0.5598 (C:0.5598, R:0.0105)
Batch 500/537: Loss=0.5660 (C:0.5660, R:0.0105)
Batch 525/537: Loss=0.5541 (C:0.5541, R:0.0105)

============================================================
Epoch 56/300 completed in 21.2s
Train: Loss=0.5460 (C:0.5460, R:0.0105) Ratio=5.08x
Val:   Loss=0.7138 (C:0.7138, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.5126 (C:0.5126, R:0.0105)
Batch  25/537: Loss=0.5243 (C:0.5243, R:0.0106)
Batch  50/537: Loss=0.5301 (C:0.5301, R:0.0105)
Batch  75/537: Loss=0.5208 (C:0.5208, R:0.0105)
Batch 100/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 125/537: Loss=0.5299 (C:0.5299, R:0.0105)
Batch 150/537: Loss=0.5167 (C:0.5167, R:0.0105)
Batch 175/537: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 200/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 225/537: Loss=0.5648 (C:0.5648, R:0.0105)
Batch 250/537: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 275/537: Loss=0.5105 (C:0.5105, R:0.0105)
Batch 300/537: Loss=0.5623 (C:0.5623, R:0.0105)
Batch 325/537: Loss=0.5411 (C:0.5411, R:0.0105)
Batch 350/537: Loss=0.5454 (C:0.5454, R:0.0105)
Batch 375/537: Loss=0.5469 (C:0.5469, R:0.0105)
Batch 400/537: Loss=0.5634 (C:0.5634, R:0.0105)
Batch 425/537: Loss=0.5455 (C:0.5455, R:0.0106)
Batch 450/537: Loss=0.5605 (C:0.5605, R:0.0106)
Batch 475/537: Loss=0.5347 (C:0.5347, R:0.0106)
Batch 500/537: Loss=0.5719 (C:0.5719, R:0.0105)
Batch 525/537: Loss=0.5562 (C:0.5562, R:0.0105)

============================================================
Epoch 57/300 completed in 21.1s
Train: Loss=0.5441 (C:0.5441, R:0.0105) Ratio=5.09x
Val:   Loss=0.7137 (C:0.7137, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.307 ± 0.561
    Neg distances: 2.649 ± 1.093
    Separation ratio: 8.62x
    Gap: -4.446
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.5383 (C:0.5383, R:0.0105)
Batch  25/537: Loss=0.5089 (C:0.5089, R:0.0105)
Batch  50/537: Loss=0.5358 (C:0.5358, R:0.0105)
Batch  75/537: Loss=0.5286 (C:0.5286, R:0.0106)
Batch 100/537: Loss=0.5592 (C:0.5592, R:0.0105)
Batch 125/537: Loss=0.5310 (C:0.5310, R:0.0105)
Batch 150/537: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 175/537: Loss=0.5392 (C:0.5392, R:0.0105)
Batch 200/537: Loss=0.5022 (C:0.5022, R:0.0105)
Batch 225/537: Loss=0.5178 (C:0.5178, R:0.0105)
Batch 250/537: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 275/537: Loss=0.5234 (C:0.5234, R:0.0105)
Batch 300/537: Loss=0.5191 (C:0.5191, R:0.0105)
Batch 325/537: Loss=0.5643 (C:0.5643, R:0.0105)
Batch 350/537: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 375/537: Loss=0.5594 (C:0.5594, R:0.0105)
Batch 400/537: Loss=0.5737 (C:0.5737, R:0.0105)
Batch 425/537: Loss=0.5214 (C:0.5214, R:0.0105)
Batch 450/537: Loss=0.5283 (C:0.5283, R:0.0105)
Batch 475/537: Loss=0.5377 (C:0.5377, R:0.0105)
Batch 500/537: Loss=0.5190 (C:0.5190, R:0.0105)
Batch 525/537: Loss=0.5500 (C:0.5500, R:0.0105)

============================================================
Epoch 58/300 completed in 27.0s
Train: Loss=0.5411 (C:0.5411, R:0.0105) Ratio=5.12x
Val:   Loss=0.7100 (C:0.7100, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7100)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.5439 (C:0.5439, R:0.0105)
Batch  25/537: Loss=0.5101 (C:0.5101, R:0.0105)
Batch  50/537: Loss=0.5685 (C:0.5685, R:0.0105)
Batch  75/537: Loss=0.5235 (C:0.5235, R:0.0105)
Batch 100/537: Loss=0.5254 (C:0.5254, R:0.0105)
Batch 125/537: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 150/537: Loss=0.5390 (C:0.5390, R:0.0105)
Batch 175/537: Loss=0.5351 (C:0.5351, R:0.0105)
Batch 200/537: Loss=0.4799 (C:0.4799, R:0.0105)
Batch 225/537: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 250/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch 275/537: Loss=0.5459 (C:0.5459, R:0.0105)
Batch 300/537: Loss=0.5313 (C:0.5313, R:0.0105)
Batch 325/537: Loss=0.5848 (C:0.5848, R:0.0105)
Batch 350/537: Loss=0.5497 (C:0.5497, R:0.0105)
Batch 375/537: Loss=0.5685 (C:0.5685, R:0.0105)
Batch 400/537: Loss=0.5365 (C:0.5365, R:0.0105)
Batch 425/537: Loss=0.5408 (C:0.5408, R:0.0105)
Batch 450/537: Loss=0.5177 (C:0.5177, R:0.0105)
Batch 475/537: Loss=0.5300 (C:0.5300, R:0.0105)
Batch 500/537: Loss=0.5504 (C:0.5504, R:0.0105)
Batch 525/537: Loss=0.5595 (C:0.5595, R:0.0105)

============================================================
Epoch 59/300 completed in 21.2s
Train: Loss=0.5405 (C:0.5405, R:0.0105) Ratio=5.19x
Val:   Loss=0.7158 (C:0.7158, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch  25/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch  50/537: Loss=0.5525 (C:0.5525, R:0.0105)
Batch  75/537: Loss=0.5474 (C:0.5474, R:0.0105)
Batch 100/537: Loss=0.5641 (C:0.5641, R:0.0105)
Batch 125/537: Loss=0.5330 (C:0.5330, R:0.0105)
Batch 150/537: Loss=0.5123 (C:0.5123, R:0.0105)
Batch 175/537: Loss=0.5377 (C:0.5377, R:0.0105)
Batch 200/537: Loss=0.6170 (C:0.6170, R:0.0105)
Batch 225/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 250/537: Loss=0.5395 (C:0.5395, R:0.0105)
Batch 275/537: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 300/537: Loss=0.5494 (C:0.5494, R:0.0105)
Batch 325/537: Loss=0.5420 (C:0.5420, R:0.0105)
Batch 350/537: Loss=0.5116 (C:0.5116, R:0.0105)
Batch 375/537: Loss=0.5535 (C:0.5535, R:0.0105)
Batch 400/537: Loss=0.5648 (C:0.5648, R:0.0105)
Batch 425/537: Loss=0.5566 (C:0.5566, R:0.0105)
Batch 450/537: Loss=0.4986 (C:0.4986, R:0.0105)
Batch 475/537: Loss=0.5345 (C:0.5345, R:0.0105)
Batch 500/537: Loss=0.5353 (C:0.5353, R:0.0105)
Batch 525/537: Loss=0.5534 (C:0.5534, R:0.0105)

============================================================
Epoch 60/300 completed in 21.2s
Train: Loss=0.5389 (C:0.5389, R:0.0105) Ratio=5.12x
Val:   Loss=0.7127 (C:0.7127, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.284 ± 0.531
    Neg distances: 2.633 ± 1.073
    Separation ratio: 9.26x
    Gap: -4.439
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.4971 (C:0.4971, R:0.0105)
Batch  25/537: Loss=0.5062 (C:0.5062, R:0.0105)
Batch  50/537: Loss=0.4786 (C:0.4786, R:0.0105)
Batch  75/537: Loss=0.5227 (C:0.5227, R:0.0105)
Batch 100/537: Loss=0.5191 (C:0.5191, R:0.0105)
Batch 125/537: Loss=0.5364 (C:0.5364, R:0.0105)
Batch 150/537: Loss=0.4862 (C:0.4862, R:0.0105)
Batch 175/537: Loss=0.5108 (C:0.5108, R:0.0105)
Batch 200/537: Loss=0.4766 (C:0.4766, R:0.0105)
Batch 225/537: Loss=0.5322 (C:0.5322, R:0.0105)
Batch 250/537: Loss=0.5165 (C:0.5165, R:0.0105)
Batch 275/537: Loss=0.5196 (C:0.5196, R:0.0105)
Batch 300/537: Loss=0.4923 (C:0.4923, R:0.0105)
Batch 325/537: Loss=0.5187 (C:0.5187, R:0.0105)
Batch 350/537: Loss=0.4836 (C:0.4836, R:0.0105)
Batch 375/537: Loss=0.5142 (C:0.5142, R:0.0105)
Batch 400/537: Loss=0.5221 (C:0.5221, R:0.0105)
Batch 425/537: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 450/537: Loss=0.5555 (C:0.5555, R:0.0105)
Batch 475/537: Loss=0.4865 (C:0.4865, R:0.0105)
Batch 500/537: Loss=0.5182 (C:0.5182, R:0.0105)
Batch 525/537: Loss=0.5340 (C:0.5340, R:0.0105)

============================================================
Epoch 61/300 completed in 27.2s
Train: Loss=0.5215 (C:0.5215, R:0.0105) Ratio=5.29x
Val:   Loss=0.6936 (C:0.6936, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.6936)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.4808 (C:0.4808, R:0.0105)
Batch  25/537: Loss=0.5060 (C:0.5060, R:0.0105)
Batch  50/537: Loss=0.4919 (C:0.4919, R:0.0105)
Batch  75/537: Loss=0.5173 (C:0.5173, R:0.0105)
Batch 100/537: Loss=0.4886 (C:0.4886, R:0.0105)
Batch 125/537: Loss=0.5109 (C:0.5109, R:0.0105)
Batch 150/537: Loss=0.5091 (C:0.5091, R:0.0105)
Batch 175/537: Loss=0.5603 (C:0.5603, R:0.0105)
Batch 200/537: Loss=0.5485 (C:0.5485, R:0.0105)
Batch 225/537: Loss=0.5286 (C:0.5286, R:0.0105)
Batch 250/537: Loss=0.5411 (C:0.5411, R:0.0105)
Batch 275/537: Loss=0.5116 (C:0.5116, R:0.0105)
Batch 300/537: Loss=0.5008 (C:0.5008, R:0.0105)
Batch 325/537: Loss=0.4798 (C:0.4798, R:0.0105)
Batch 350/537: Loss=0.5230 (C:0.5230, R:0.0105)
Batch 375/537: Loss=0.5120 (C:0.5120, R:0.0105)
Batch 400/537: Loss=0.5330 (C:0.5330, R:0.0105)
Batch 425/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 450/537: Loss=0.4680 (C:0.4680, R:0.0105)
Batch 475/537: Loss=0.5491 (C:0.5491, R:0.0105)
Batch 500/537: Loss=0.5553 (C:0.5553, R:0.0105)
Batch 525/537: Loss=0.5221 (C:0.5221, R:0.0105)

============================================================
Epoch 62/300 completed in 21.2s
Train: Loss=0.5202 (C:0.5202, R:0.0105) Ratio=5.24x
Val:   Loss=0.6987 (C:0.6987, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.5122 (C:0.5122, R:0.0105)
Batch  25/537: Loss=0.4955 (C:0.4955, R:0.0105)
Batch  50/537: Loss=0.5387 (C:0.5387, R:0.0105)
Batch  75/537: Loss=0.5298 (C:0.5298, R:0.0105)
Batch 100/537: Loss=0.5051 (C:0.5051, R:0.0105)
Batch 125/537: Loss=0.5202 (C:0.5202, R:0.0105)
Batch 150/537: Loss=0.4796 (C:0.4796, R:0.0105)
Batch 175/537: Loss=0.5127 (C:0.5127, R:0.0105)
Batch 200/537: Loss=0.5141 (C:0.5141, R:0.0105)
Batch 225/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 250/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 275/537: Loss=0.5106 (C:0.5106, R:0.0105)
Batch 300/537: Loss=0.5353 (C:0.5353, R:0.0105)
Batch 325/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 350/537: Loss=0.5206 (C:0.5206, R:0.0105)
Batch 375/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 400/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch 425/537: Loss=0.5276 (C:0.5276, R:0.0105)
Batch 450/537: Loss=0.5122 (C:0.5122, R:0.0105)
Batch 475/537: Loss=0.5045 (C:0.5045, R:0.0105)
Batch 500/537: Loss=0.5093 (C:0.5093, R:0.0105)
Batch 525/537: Loss=0.5305 (C:0.5305, R:0.0105)

============================================================
Epoch 63/300 completed in 21.4s
Train: Loss=0.5189 (C:0.5189, R:0.0105) Ratio=5.22x
Val:   Loss=0.7105 (C:0.7105, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.316 ± 0.571
    Neg distances: 2.652 ± 1.098
    Separation ratio: 8.39x
    Gap: -4.460
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.5361 (C:0.5361, R:0.0105)
Batch  25/537: Loss=0.5153 (C:0.5153, R:0.0105)
Batch  50/537: Loss=0.5393 (C:0.5393, R:0.0106)
Batch  75/537: Loss=0.5448 (C:0.5448, R:0.0105)
Batch 100/537: Loss=0.5510 (C:0.5510, R:0.0105)
Batch 125/537: Loss=0.5396 (C:0.5396, R:0.0105)
Batch 150/537: Loss=0.5380 (C:0.5380, R:0.0105)
Batch 175/537: Loss=0.5229 (C:0.5229, R:0.0105)
Batch 200/537: Loss=0.5527 (C:0.5527, R:0.0105)
Batch 225/537: Loss=0.5145 (C:0.5145, R:0.0105)
Batch 250/537: Loss=0.5479 (C:0.5479, R:0.0106)
Batch 275/537: Loss=0.5647 (C:0.5647, R:0.0105)
Batch 300/537: Loss=0.5506 (C:0.5506, R:0.0105)
Batch 325/537: Loss=0.5151 (C:0.5151, R:0.0105)
Batch 350/537: Loss=0.5578 (C:0.5578, R:0.0105)
Batch 375/537: Loss=0.5372 (C:0.5372, R:0.0105)
Batch 400/537: Loss=0.5537 (C:0.5537, R:0.0105)
Batch 425/537: Loss=0.5570 (C:0.5570, R:0.0105)
Batch 450/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 475/537: Loss=0.5519 (C:0.5519, R:0.0105)
Batch 500/537: Loss=0.5494 (C:0.5494, R:0.0105)
Batch 525/537: Loss=0.5314 (C:0.5314, R:0.0105)

============================================================
Epoch 64/300 completed in 27.2s
Train: Loss=0.5417 (C:0.5417, R:0.0105) Ratio=5.24x
Val:   Loss=0.7223 (C:0.7223, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.5068 (C:0.5068, R:0.0105)
Batch  25/537: Loss=0.5448 (C:0.5448, R:0.0105)
Batch  50/537: Loss=0.5070 (C:0.5070, R:0.0105)
Batch  75/537: Loss=0.5250 (C:0.5250, R:0.0106)
Batch 100/537: Loss=0.5668 (C:0.5668, R:0.0105)
Batch 125/537: Loss=0.5571 (C:0.5571, R:0.0105)
Batch 150/537: Loss=0.5255 (C:0.5255, R:0.0105)
Batch 175/537: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 200/537: Loss=0.5359 (C:0.5359, R:0.0105)
Batch 225/537: Loss=0.5408 (C:0.5408, R:0.0105)
Batch 250/537: Loss=0.5405 (C:0.5405, R:0.0105)
Batch 275/537: Loss=0.5177 (C:0.5177, R:0.0105)
Batch 300/537: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 325/537: Loss=0.5396 (C:0.5396, R:0.0105)
Batch 350/537: Loss=0.5734 (C:0.5734, R:0.0105)
Batch 375/537: Loss=0.5512 (C:0.5512, R:0.0105)
Batch 400/537: Loss=0.5113 (C:0.5113, R:0.0105)
Batch 425/537: Loss=0.5505 (C:0.5505, R:0.0105)
Batch 450/537: Loss=0.5478 (C:0.5478, R:0.0105)
Batch 475/537: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 500/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 525/537: Loss=0.5402 (C:0.5402, R:0.0105)

============================================================
Epoch 65/300 completed in 21.2s
Train: Loss=0.5405 (C:0.5405, R:0.0105) Ratio=5.25x
Val:   Loss=0.7194 (C:0.7194, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.4924 (C:0.4924, R:0.0105)
Batch  25/537: Loss=0.5082 (C:0.5082, R:0.0105)
Batch  50/537: Loss=0.5587 (C:0.5587, R:0.0105)
Batch  75/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 100/537: Loss=0.5536 (C:0.5536, R:0.0105)
Batch 125/537: Loss=0.5183 (C:0.5183, R:0.0105)
Batch 150/537: Loss=0.5398 (C:0.5398, R:0.0105)
Batch 175/537: Loss=0.5592 (C:0.5592, R:0.0105)
Batch 200/537: Loss=0.5733 (C:0.5733, R:0.0105)
Batch 225/537: Loss=0.5378 (C:0.5378, R:0.0105)
Batch 250/537: Loss=0.5090 (C:0.5090, R:0.0105)
Batch 275/537: Loss=0.5123 (C:0.5123, R:0.0105)
Batch 300/537: Loss=0.5325 (C:0.5325, R:0.0105)
Batch 325/537: Loss=0.5475 (C:0.5475, R:0.0105)
Batch 350/537: Loss=0.5514 (C:0.5514, R:0.0105)
Batch 375/537: Loss=0.5413 (C:0.5413, R:0.0105)
Batch 400/537: Loss=0.5609 (C:0.5609, R:0.0106)
Batch 425/537: Loss=0.5757 (C:0.5757, R:0.0105)
Batch 450/537: Loss=0.5255 (C:0.5255, R:0.0105)
Batch 475/537: Loss=0.5310 (C:0.5310, R:0.0105)
Batch 500/537: Loss=0.5888 (C:0.5888, R:0.0105)
Batch 525/537: Loss=0.5353 (C:0.5353, R:0.0105)

============================================================
Epoch 66/300 completed in 21.2s
Train: Loss=0.5397 (C:0.5397, R:0.0105) Ratio=5.29x
Val:   Loss=0.7221 (C:0.7221, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.296 ± 0.562
    Neg distances: 2.643 ± 1.083
    Separation ratio: 8.92x
    Gap: -4.579
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.5120 (C:0.5120, R:0.0105)
Batch  25/537: Loss=0.5205 (C:0.5205, R:0.0105)
Batch  50/537: Loss=0.4608 (C:0.4608, R:0.0105)
Batch  75/537: Loss=0.5028 (C:0.5028, R:0.0105)
Batch 100/537: Loss=0.5089 (C:0.5089, R:0.0105)
Batch 125/537: Loss=0.5041 (C:0.5041, R:0.0105)
Batch 150/537: Loss=0.5292 (C:0.5292, R:0.0105)
Batch 175/537: Loss=0.5094 (C:0.5094, R:0.0105)
Batch 200/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 225/537: Loss=0.5155 (C:0.5155, R:0.0105)
Batch 250/537: Loss=0.5262 (C:0.5262, R:0.0105)
Batch 275/537: Loss=0.5247 (C:0.5247, R:0.0105)
Batch 300/537: Loss=0.5366 (C:0.5366, R:0.0105)
Batch 325/537: Loss=0.5272 (C:0.5272, R:0.0105)
Batch 350/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 375/537: Loss=0.4995 (C:0.4995, R:0.0105)
Batch 400/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 425/537: Loss=0.5146 (C:0.5146, R:0.0105)
Batch 450/537: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 475/537: Loss=0.5512 (C:0.5512, R:0.0105)
Batch 500/537: Loss=0.5202 (C:0.5202, R:0.0105)
Batch 525/537: Loss=0.5060 (C:0.5060, R:0.0105)

============================================================
Epoch 67/300 completed in 27.2s
Train: Loss=0.5225 (C:0.5225, R:0.0105) Ratio=5.41x
Val:   Loss=0.7049 (C:0.7049, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.5202 (C:0.5202, R:0.0105)
Batch  25/537: Loss=0.4758 (C:0.4758, R:0.0105)
Batch  50/537: Loss=0.5072 (C:0.5072, R:0.0105)
Batch  75/537: Loss=0.4970 (C:0.4970, R:0.0105)
Batch 100/537: Loss=0.5253 (C:0.5253, R:0.0105)
Batch 125/537: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 150/537: Loss=0.5092 (C:0.5092, R:0.0106)
Batch 175/537: Loss=0.5239 (C:0.5239, R:0.0105)
Batch 200/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 225/537: Loss=0.5342 (C:0.5342, R:0.0105)
Batch 250/537: Loss=0.5224 (C:0.5224, R:0.0105)
Batch 275/537: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 300/537: Loss=0.5651 (C:0.5651, R:0.0105)
Batch 325/537: Loss=0.5260 (C:0.5260, R:0.0105)
Batch 350/537: Loss=0.5384 (C:0.5384, R:0.0105)
Batch 375/537: Loss=0.5174 (C:0.5174, R:0.0105)
Batch 400/537: Loss=0.5319 (C:0.5319, R:0.0105)
Batch 425/537: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 450/537: Loss=0.5334 (C:0.5334, R:0.0105)
Batch 475/537: Loss=0.4973 (C:0.4973, R:0.0105)
Batch 500/537: Loss=0.5412 (C:0.5412, R:0.0105)
Batch 525/537: Loss=0.4887 (C:0.4887, R:0.0105)

============================================================
Epoch 68/300 completed in 21.9s
Train: Loss=0.5221 (C:0.5221, R:0.0105) Ratio=5.34x
Val:   Loss=0.7133 (C:0.7133, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.4653 (C:0.4653, R:0.0105)
Batch  25/537: Loss=0.5278 (C:0.5278, R:0.0105)
Batch  50/537: Loss=0.5260 (C:0.5260, R:0.0105)
Batch  75/537: Loss=0.5442 (C:0.5442, R:0.0105)
Batch 100/537: Loss=0.5433 (C:0.5433, R:0.0105)
Batch 125/537: Loss=0.5175 (C:0.5175, R:0.0105)
Batch 150/537: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 175/537: Loss=0.5553 (C:0.5553, R:0.0105)
Batch 200/537: Loss=0.5108 (C:0.5108, R:0.0106)
Batch 225/537: Loss=0.5223 (C:0.5223, R:0.0105)
Batch 250/537: Loss=0.5017 (C:0.5017, R:0.0105)
Batch 275/537: Loss=0.5278 (C:0.5278, R:0.0105)
Batch 300/537: Loss=0.5221 (C:0.5221, R:0.0105)
Batch 325/537: Loss=0.5044 (C:0.5044, R:0.0106)
Batch 350/537: Loss=0.4842 (C:0.4842, R:0.0105)
Batch 375/537: Loss=0.5294 (C:0.5294, R:0.0105)
Batch 400/537: Loss=0.5446 (C:0.5446, R:0.0105)
Batch 425/537: Loss=0.5064 (C:0.5064, R:0.0105)
Batch 450/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 475/537: Loss=0.5376 (C:0.5376, R:0.0105)
Batch 500/537: Loss=0.5355 (C:0.5355, R:0.0105)
Batch 525/537: Loss=0.5257 (C:0.5257, R:0.0105)

============================================================
Epoch 69/300 completed in 21.3s
Train: Loss=0.5196 (C:0.5196, R:0.0105) Ratio=5.30x
Val:   Loss=0.7057 (C:0.7057, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 69 epochs
Best model was at epoch 61 with Val Loss: 0.6936

Global Dataset Training Completed!
Best epoch: 61
Best validation loss: 0.6936
Final separation ratios: Train=5.30x, Val=3.01x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 100])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4576
  Adjusted Rand Score: 0.5284
  Clustering Accuracy: 0.8136
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 100])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 100])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8154
  Per-class F1: [0.8350532904753848, 0.7538194991337219, 0.8603278688524589]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.778 ± 0.918
  Negative distances: 2.341 ± 1.247
  Separation ratio: 3.01x
  Gap: -4.554
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4576
  Clustering Accuracy: 0.8136
  Adjusted Rand Score: 0.5284

Classification Performance:
  Accuracy: 0.8154

Separation Quality:
  Separation Ratio: 3.01x
  Gap: -4.554
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852/results/evaluation_results_20250714_220614.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852/results/evaluation_results_20250714_220614.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1020_20250714_213852/final_results.json

Key Results:
  Separation ratio: 3.01x
  Perfect separation: False
  Classification accuracy: 0.8154
  Result: 0.8154% (improvement: +-80.85%)
  Cleaning up: coarse_lr2e-04_lat100_bs1020_20250714_213852

[12/12] Testing: coarse_lr2e-04_lat100_bs1536
  Learning rate: 0.0002
  Latent dim: 100
  Batch size: 1536
  Reconstruction weight: 0.3 (fixed)
GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-14 22:06:14.452453
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1536
  Global update frequency: 3 epochs
  Training epochs: 300
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 356
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 512
  Effective batch size: 1536
  Number of batches: 6
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1536
  Balanced sampling: True
  Train batches: 356
  Val batches: 6
  Test batches: 7
Data loading completed!
  Train: 549367 samples, 356 batches
  Val: 9842 samples, 6 batches
  Test: 9824 samples, 7 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 100
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,889,380
Model created with 1,889,380 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0002)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,889,380
Starting training...
========================================
Starting Global Dataset Training...
============================================================

🌍 Updating global dataset at epoch 1
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.102 ± 0.011
    Neg distances: 0.102 ± 0.011
    Separation ratio: 1.00x
    Gap: -0.133
    ❌ Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/356: Loss=1.9998 (C:1.9998, R:0.0116)
Batch  25/356: Loss=1.9693 (C:1.9693, R:0.0112)
Batch  50/356: Loss=1.9460 (C:1.9460, R:0.0109)
Batch  75/356: Loss=1.9301 (C:1.9301, R:0.0108)
Batch 100/356: Loss=1.9100 (C:1.9100, R:0.0107)
Batch 125/356: Loss=1.8827 (C:1.8827, R:0.0106)
Batch 150/356: Loss=1.8802 (C:1.8802, R:0.0106)
Batch 175/356: Loss=1.8736 (C:1.8736, R:0.0105)
Batch 200/356: Loss=1.8679 (C:1.8679, R:0.0105)
Batch 225/356: Loss=1.8493 (C:1.8493, R:0.0105)
Batch 250/356: Loss=1.8500 (C:1.8500, R:0.0105)
Batch 275/356: Loss=1.8447 (C:1.8447, R:0.0105)
Batch 300/356: Loss=1.8509 (C:1.8509, R:0.0105)
Batch 325/356: Loss=1.8524 (C:1.8524, R:0.0105)
Batch 350/356: Loss=1.8280 (C:1.8280, R:0.0105)

============================================================
Epoch 1/300 completed in 26.7s
Train: Loss=1.8863 (C:1.8863, R:0.0107) Ratio=1.73x
Val:   Loss=1.8288 (C:1.8288, R:0.0104) Ratio=2.24x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8288)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/356: Loss=1.8210 (C:1.8210, R:0.0105)
Batch  25/356: Loss=1.8312 (C:1.8312, R:0.0105)
Batch  50/356: Loss=1.8382 (C:1.8382, R:0.0105)
Batch  75/356: Loss=1.8520 (C:1.8520, R:0.0105)
Batch 100/356: Loss=1.8335 (C:1.8335, R:0.0105)
Batch 125/356: Loss=1.8189 (C:1.8189, R:0.0105)
Batch 150/356: Loss=1.8205 (C:1.8205, R:0.0105)
Batch 175/356: Loss=1.8229 (C:1.8229, R:0.0105)
Batch 200/356: Loss=1.8255 (C:1.8255, R:0.0105)
Batch 225/356: Loss=1.8116 (C:1.8116, R:0.0105)
Batch 250/356: Loss=1.8159 (C:1.8159, R:0.0105)
Batch 275/356: Loss=1.8028 (C:1.8028, R:0.0105)
Batch 300/356: Loss=1.8152 (C:1.8152, R:0.0105)
Batch 325/356: Loss=1.8172 (C:1.8172, R:0.0105)
Batch 350/356: Loss=1.8133 (C:1.8133, R:0.0105)

============================================================
Epoch 2/300 completed in 20.6s
Train: Loss=1.8242 (C:1.8242, R:0.0105) Ratio=2.28x
Val:   Loss=1.8035 (C:1.8035, R:0.0104) Ratio=2.45x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.8035)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/356: Loss=1.8047 (C:1.8047, R:0.0105)
Batch  25/356: Loss=1.7874 (C:1.7874, R:0.0105)
Batch  50/356: Loss=1.7844 (C:1.7844, R:0.0105)
Batch  75/356: Loss=1.8188 (C:1.8188, R:0.0105)
Batch 100/356: Loss=1.8240 (C:1.8240, R:0.0105)
Batch 125/356: Loss=1.8029 (C:1.8029, R:0.0105)
Batch 150/356: Loss=1.7976 (C:1.7976, R:0.0105)
Batch 175/356: Loss=1.8167 (C:1.8167, R:0.0105)
Batch 200/356: Loss=1.8114 (C:1.8114, R:0.0105)
Batch 225/356: Loss=1.8173 (C:1.8173, R:0.0105)
Batch 250/356: Loss=1.8241 (C:1.8241, R:0.0105)
Batch 275/356: Loss=1.8015 (C:1.8015, R:0.0105)
Batch 300/356: Loss=1.8077 (C:1.8077, R:0.0105)
Batch 325/356: Loss=1.8162 (C:1.8162, R:0.0105)
Batch 350/356: Loss=1.7893 (C:1.7893, R:0.0105)

============================================================
Epoch 3/300 completed in 21.0s
Train: Loss=1.8042 (C:1.8042, R:0.0105) Ratio=2.48x
Val:   Loss=1.7966 (C:1.7966, R:0.0104) Ratio=2.61x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.7966)
============================================================

🌍 Updating global dataset at epoch 4
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.539 ± 0.563
    Neg distances: 1.611 ± 0.860
    Separation ratio: 2.99x
    Gap: -3.338
    ✅ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/356: Loss=1.1492 (C:1.1492, R:0.0105)
Batch  25/356: Loss=1.1450 (C:1.1450, R:0.0105)
Batch  50/356: Loss=1.1540 (C:1.1540, R:0.0105)
Batch  75/356: Loss=1.1662 (C:1.1662, R:0.0105)
Batch 100/356: Loss=1.1765 (C:1.1765, R:0.0105)
Batch 125/356: Loss=1.1681 (C:1.1681, R:0.0105)
Batch 150/356: Loss=1.1604 (C:1.1604, R:0.0105)
Batch 175/356: Loss=1.1502 (C:1.1502, R:0.0105)
Batch 200/356: Loss=1.1853 (C:1.1853, R:0.0105)
Batch 225/356: Loss=1.1682 (C:1.1682, R:0.0105)
Batch 250/356: Loss=1.1840 (C:1.1840, R:0.0105)
Batch 275/356: Loss=1.1796 (C:1.1796, R:0.0105)
Batch 300/356: Loss=1.1988 (C:1.1988, R:0.0105)
Batch 325/356: Loss=1.1678 (C:1.1678, R:0.0105)
Batch 350/356: Loss=1.1930 (C:1.1930, R:0.0105)

============================================================
Epoch 4/300 completed in 27.8s
Train: Loss=1.1752 (C:1.1752, R:0.0105) Ratio=2.63x
Val:   Loss=1.1730 (C:1.1730, R:0.0104) Ratio=2.70x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1730)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/356: Loss=1.1454 (C:1.1454, R:0.0105)
Batch  25/356: Loss=1.1493 (C:1.1493, R:0.0105)
Batch  50/356: Loss=1.1411 (C:1.1411, R:0.0105)
Batch  75/356: Loss=1.1366 (C:1.1366, R:0.0105)
Batch 100/356: Loss=1.1443 (C:1.1443, R:0.0105)
Batch 125/356: Loss=1.1413 (C:1.1413, R:0.0105)
Batch 150/356: Loss=1.1393 (C:1.1393, R:0.0105)
Batch 175/356: Loss=1.1507 (C:1.1507, R:0.0105)
Batch 200/356: Loss=1.1481 (C:1.1481, R:0.0105)
Batch 225/356: Loss=1.1478 (C:1.1478, R:0.0105)
Batch 250/356: Loss=1.1358 (C:1.1358, R:0.0105)
Batch 275/356: Loss=1.1755 (C:1.1755, R:0.0105)
Batch 300/356: Loss=1.1535 (C:1.1535, R:0.0105)
Batch 325/356: Loss=1.1407 (C:1.1407, R:0.0105)
Batch 350/356: Loss=1.1504 (C:1.1504, R:0.0105)

============================================================
Epoch 5/300 completed in 20.6s
Train: Loss=1.1491 (C:1.1491, R:0.0105) Ratio=2.80x
Val:   Loss=1.1545 (C:1.1545, R:0.0104) Ratio=2.81x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1545)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/356: Loss=1.1080 (C:1.1080, R:0.0105)
Batch  25/356: Loss=1.1360 (C:1.1360, R:0.0105)
Batch  50/356: Loss=1.1282 (C:1.1282, R:0.0105)
Batch  75/356: Loss=1.1293 (C:1.1293, R:0.0105)
Batch 100/356: Loss=1.1299 (C:1.1299, R:0.0105)
Batch 125/356: Loss=1.1369 (C:1.1369, R:0.0105)
Batch 150/356: Loss=1.1517 (C:1.1517, R:0.0105)
Batch 175/356: Loss=1.1290 (C:1.1290, R:0.0105)
Batch 200/356: Loss=1.1198 (C:1.1198, R:0.0105)
Batch 225/356: Loss=1.1318 (C:1.1318, R:0.0105)
Batch 250/356: Loss=1.1341 (C:1.1341, R:0.0105)
Batch 275/356: Loss=1.1230 (C:1.1230, R:0.0105)
Batch 300/356: Loss=1.1566 (C:1.1566, R:0.0105)
Batch 325/356: Loss=1.1346 (C:1.1346, R:0.0105)
Batch 350/356: Loss=1.1727 (C:1.1727, R:0.0105)

============================================================
Epoch 6/300 completed in 20.5s
Train: Loss=1.1332 (C:1.1332, R:0.0105) Ratio=2.93x
Val:   Loss=1.1454 (C:1.1454, R:0.0104) Ratio=2.85x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.1454)
============================================================

🌍 Updating global dataset at epoch 7
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.473 ± 0.571
    Neg distances: 1.734 ± 0.879
    Separation ratio: 3.67x
    Gap: -3.153
    ✅ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/356: Loss=1.0508 (C:1.0508, R:0.0105)
Batch  25/356: Loss=1.0519 (C:1.0519, R:0.0105)
Batch  50/356: Loss=1.0403 (C:1.0403, R:0.0105)
Batch  75/356: Loss=1.0404 (C:1.0404, R:0.0105)
Batch 100/356: Loss=1.0401 (C:1.0401, R:0.0105)
Batch 125/356: Loss=1.0186 (C:1.0186, R:0.0105)
Batch 150/356: Loss=1.0521 (C:1.0521, R:0.0105)
Batch 175/356: Loss=1.0632 (C:1.0632, R:0.0105)
Batch 200/356: Loss=1.0932 (C:1.0932, R:0.0105)
Batch 225/356: Loss=1.0713 (C:1.0713, R:0.0105)
Batch 250/356: Loss=1.0315 (C:1.0315, R:0.0105)
Batch 275/356: Loss=1.0760 (C:1.0760, R:0.0105)
Batch 300/356: Loss=1.0843 (C:1.0843, R:0.0105)
Batch 325/356: Loss=1.0516 (C:1.0516, R:0.0105)
Batch 350/356: Loss=1.0656 (C:1.0656, R:0.0105)

============================================================
Epoch 7/300 completed in 26.4s
Train: Loss=1.0562 (C:1.0562, R:0.0105) Ratio=3.00x
Val:   Loss=1.0796 (C:1.0796, R:0.0104) Ratio=2.85x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0796)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/356: Loss=1.0364 (C:1.0364, R:0.0105)
Batch  25/356: Loss=1.0522 (C:1.0522, R:0.0105)
Batch  50/356: Loss=1.0395 (C:1.0395, R:0.0105)
Batch  75/356: Loss=1.0435 (C:1.0435, R:0.0105)
Batch 100/356: Loss=1.0397 (C:1.0397, R:0.0105)
Batch 125/356: Loss=1.0441 (C:1.0441, R:0.0105)
Batch 150/356: Loss=1.0395 (C:1.0395, R:0.0105)
Batch 175/356: Loss=1.0451 (C:1.0451, R:0.0105)
Batch 200/356: Loss=1.0568 (C:1.0568, R:0.0105)
Batch 225/356: Loss=1.0175 (C:1.0175, R:0.0105)
Batch 250/356: Loss=1.0390 (C:1.0390, R:0.0105)
Batch 275/356: Loss=1.0732 (C:1.0732, R:0.0105)
Batch 300/356: Loss=1.0546 (C:1.0546, R:0.0105)
Batch 325/356: Loss=1.0546 (C:1.0546, R:0.0105)
Batch 350/356: Loss=1.0623 (C:1.0623, R:0.0105)

============================================================
Epoch 8/300 completed in 20.8s
Train: Loss=1.0451 (C:1.0451, R:0.0105) Ratio=3.13x
Val:   Loss=1.0680 (C:1.0680, R:0.0104) Ratio=2.92x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0680)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/356: Loss=1.0405 (C:1.0405, R:0.0105)
Batch  25/356: Loss=1.0226 (C:1.0226, R:0.0105)
Batch  50/356: Loss=1.0372 (C:1.0372, R:0.0105)
Batch  75/356: Loss=1.0628 (C:1.0628, R:0.0105)
Batch 100/356: Loss=1.0200 (C:1.0200, R:0.0106)
Batch 125/356: Loss=1.0540 (C:1.0540, R:0.0105)
Batch 150/356: Loss=1.0437 (C:1.0437, R:0.0105)
Batch 175/356: Loss=1.0346 (C:1.0346, R:0.0105)
Batch 200/356: Loss=1.0497 (C:1.0497, R:0.0105)
Batch 225/356: Loss=1.0558 (C:1.0558, R:0.0105)
Batch 250/356: Loss=1.0553 (C:1.0553, R:0.0105)
Batch 275/356: Loss=1.0387 (C:1.0387, R:0.0105)
Batch 300/356: Loss=1.0706 (C:1.0706, R:0.0105)
Batch 325/356: Loss=1.0403 (C:1.0403, R:0.0104)
Batch 350/356: Loss=1.0212 (C:1.0212, R:0.0105)

============================================================
Epoch 9/300 completed in 20.6s
Train: Loss=1.0350 (C:1.0350, R:0.0105) Ratio=3.20x
Val:   Loss=1.0669 (C:1.0669, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0669)
============================================================

🌍 Updating global dataset at epoch 10
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.466 ± 0.582
    Neg distances: 1.815 ± 0.898
    Separation ratio: 3.90x
    Gap: -3.211
    ✅ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/356: Loss=0.9989 (C:0.9989, R:0.0105)
Batch  25/356: Loss=0.9776 (C:0.9776, R:0.0105)
Batch  50/356: Loss=0.9944 (C:0.9944, R:0.0105)
Batch  75/356: Loss=0.9917 (C:0.9917, R:0.0105)
Batch 100/356: Loss=1.0047 (C:1.0047, R:0.0105)
Batch 125/356: Loss=1.0149 (C:1.0149, R:0.0106)
Batch 150/356: Loss=0.9862 (C:0.9862, R:0.0105)
Batch 175/356: Loss=1.0125 (C:1.0125, R:0.0105)
Batch 200/356: Loss=0.9894 (C:0.9894, R:0.0105)
Batch 225/356: Loss=1.0183 (C:1.0183, R:0.0105)
Batch 250/356: Loss=1.0110 (C:1.0110, R:0.0105)
Batch 275/356: Loss=1.0265 (C:1.0265, R:0.0105)
Batch 300/356: Loss=0.9819 (C:0.9819, R:0.0105)
Batch 325/356: Loss=1.0337 (C:1.0337, R:0.0105)
Batch 350/356: Loss=0.9773 (C:0.9773, R:0.0105)

============================================================
Epoch 10/300 completed in 27.2s
Train: Loss=1.0032 (C:1.0032, R:0.0105) Ratio=3.31x
Val:   Loss=1.0429 (C:1.0429, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 1.0429)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/356: Loss=0.9611 (C:0.9611, R:0.0105)
Batch  25/356: Loss=1.0401 (C:1.0401, R:0.0105)
Batch  50/356: Loss=0.9801 (C:0.9801, R:0.0105)
Batch  75/356: Loss=0.9995 (C:0.9995, R:0.0105)
Batch 100/356: Loss=1.0135 (C:1.0135, R:0.0105)
Batch 125/356: Loss=0.9922 (C:0.9922, R:0.0105)
Batch 150/356: Loss=1.0234 (C:1.0234, R:0.0105)
Batch 175/356: Loss=0.9925 (C:0.9925, R:0.0105)
Batch 200/356: Loss=0.9817 (C:0.9817, R:0.0105)
Batch 225/356: Loss=0.9998 (C:0.9998, R:0.0105)
Batch 250/356: Loss=0.9880 (C:0.9880, R:0.0105)
Batch 275/356: Loss=0.9795 (C:0.9795, R:0.0105)
Batch 300/356: Loss=0.9862 (C:0.9862, R:0.0105)
Batch 325/356: Loss=0.9846 (C:0.9846, R:0.0105)
Batch 350/356: Loss=0.9906 (C:0.9906, R:0.0105)

============================================================
Epoch 11/300 completed in 20.7s
Train: Loss=0.9941 (C:0.9941, R:0.0105) Ratio=3.36x
Val:   Loss=1.0529 (C:1.0529, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/356: Loss=0.9799 (C:0.9799, R:0.0105)
Batch  25/356: Loss=0.9737 (C:0.9737, R:0.0105)
Batch  50/356: Loss=0.9469 (C:0.9469, R:0.0105)
Batch  75/356: Loss=1.0038 (C:1.0038, R:0.0105)
Batch 100/356: Loss=0.9592 (C:0.9592, R:0.0105)
Batch 125/356: Loss=1.0036 (C:1.0036, R:0.0105)
Batch 150/356: Loss=0.9640 (C:0.9640, R:0.0105)
Batch 175/356: Loss=0.9748 (C:0.9748, R:0.0105)
Batch 200/356: Loss=0.9819 (C:0.9819, R:0.0105)
Batch 225/356: Loss=0.9761 (C:0.9761, R:0.0105)
Batch 250/356: Loss=1.0112 (C:1.0112, R:0.0105)
Batch 275/356: Loss=0.9697 (C:0.9697, R:0.0105)
Batch 300/356: Loss=0.9963 (C:0.9963, R:0.0105)
Batch 325/356: Loss=1.0057 (C:1.0057, R:0.0105)
Batch 350/356: Loss=1.0212 (C:1.0212, R:0.0105)

============================================================
Epoch 12/300 completed in 20.4s
Train: Loss=0.9860 (C:0.9860, R:0.0105) Ratio=3.43x
Val:   Loss=1.0528 (C:1.0528, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 13
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.425 ± 0.554
    Neg distances: 1.908 ± 0.892
    Separation ratio: 4.49x
    Gap: -3.429
    ✅ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/356: Loss=0.9122 (C:0.9122, R:0.0105)
Batch  25/356: Loss=0.9369 (C:0.9369, R:0.0105)
Batch  50/356: Loss=0.9493 (C:0.9493, R:0.0105)
Batch  75/356: Loss=0.9258 (C:0.9258, R:0.0105)
Batch 100/356: Loss=0.9513 (C:0.9513, R:0.0105)
Batch 125/356: Loss=0.9914 (C:0.9914, R:0.0105)
Batch 150/356: Loss=0.9468 (C:0.9468, R:0.0105)
Batch 175/356: Loss=0.9296 (C:0.9296, R:0.0105)
Batch 200/356: Loss=0.9409 (C:0.9409, R:0.0105)
Batch 225/356: Loss=0.9268 (C:0.9268, R:0.0105)
Batch 250/356: Loss=0.9074 (C:0.9074, R:0.0105)
Batch 275/356: Loss=0.9215 (C:0.9215, R:0.0105)
Batch 300/356: Loss=0.9214 (C:0.9214, R:0.0105)
Batch 325/356: Loss=0.9241 (C:0.9241, R:0.0105)
Batch 350/356: Loss=0.9404 (C:0.9404, R:0.0105)

============================================================
Epoch 13/300 completed in 26.1s
Train: Loss=0.9305 (C:0.9305, R:0.0105) Ratio=3.48x
Val:   Loss=0.9912 (C:0.9912, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9912)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/356: Loss=0.9095 (C:0.9095, R:0.0105)
Batch  25/356: Loss=0.8949 (C:0.8949, R:0.0105)
Batch  50/356: Loss=0.9074 (C:0.9074, R:0.0105)
Batch  75/356: Loss=0.9052 (C:0.9052, R:0.0105)
Batch 100/356: Loss=0.9054 (C:0.9054, R:0.0105)
Batch 125/356: Loss=0.9306 (C:0.9306, R:0.0105)
Batch 150/356: Loss=0.9120 (C:0.9120, R:0.0105)
Batch 175/356: Loss=0.9413 (C:0.9413, R:0.0105)
Batch 200/356: Loss=0.9603 (C:0.9603, R:0.0105)
Batch 225/356: Loss=0.9387 (C:0.9387, R:0.0105)
Batch 250/356: Loss=0.9414 (C:0.9414, R:0.0105)
Batch 275/356: Loss=0.9353 (C:0.9353, R:0.0106)
Batch 300/356: Loss=0.8996 (C:0.8996, R:0.0105)
Batch 325/356: Loss=0.9440 (C:0.9440, R:0.0105)
Batch 350/356: Loss=0.9407 (C:0.9407, R:0.0105)

============================================================
Epoch 14/300 completed in 20.4s
Train: Loss=0.9228 (C:0.9228, R:0.0105) Ratio=3.52x
Val:   Loss=0.9905 (C:0.9905, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9905)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/356: Loss=0.8767 (C:0.8767, R:0.0105)
Batch  25/356: Loss=0.9155 (C:0.9155, R:0.0105)
Batch  50/356: Loss=0.8998 (C:0.8998, R:0.0105)
Batch  75/356: Loss=0.9064 (C:0.9064, R:0.0105)
Batch 100/356: Loss=0.9104 (C:0.9104, R:0.0105)
Batch 125/356: Loss=0.9103 (C:0.9103, R:0.0105)
Batch 150/356: Loss=0.9157 (C:0.9157, R:0.0105)
Batch 175/356: Loss=0.9174 (C:0.9174, R:0.0105)
Batch 200/356: Loss=0.8985 (C:0.8985, R:0.0105)
Batch 225/356: Loss=0.9225 (C:0.9225, R:0.0105)
Batch 250/356: Loss=0.9244 (C:0.9244, R:0.0105)
Batch 275/356: Loss=0.9098 (C:0.9098, R:0.0105)
Batch 300/356: Loss=0.9249 (C:0.9249, R:0.0105)
Batch 325/356: Loss=0.9039 (C:0.9039, R:0.0105)
Batch 350/356: Loss=0.9324 (C:0.9324, R:0.0105)

============================================================
Epoch 15/300 completed in 20.3s
Train: Loss=0.9174 (C:0.9174, R:0.0105) Ratio=3.61x
Val:   Loss=0.9957 (C:0.9957, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 16
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.427 ± 0.565
    Neg distances: 1.994 ± 0.927
    Separation ratio: 4.67x
    Gap: -3.493
    ✅ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/356: Loss=0.8661 (C:0.8661, R:0.0105)
Batch  25/356: Loss=0.8968 (C:0.8968, R:0.0105)
Batch  50/356: Loss=0.8996 (C:0.8996, R:0.0106)
Batch  75/356: Loss=0.9028 (C:0.9028, R:0.0105)
Batch 100/356: Loss=0.8794 (C:0.8794, R:0.0105)
Batch 125/356: Loss=0.9246 (C:0.9246, R:0.0105)
Batch 150/356: Loss=0.9207 (C:0.9207, R:0.0105)
Batch 175/356: Loss=0.9061 (C:0.9061, R:0.0105)
Batch 200/356: Loss=0.9041 (C:0.9041, R:0.0105)
Batch 225/356: Loss=0.8968 (C:0.8968, R:0.0105)
Batch 250/356: Loss=0.8735 (C:0.8735, R:0.0105)
Batch 275/356: Loss=0.8763 (C:0.8763, R:0.0105)
Batch 300/356: Loss=0.9005 (C:0.9005, R:0.0105)
Batch 325/356: Loss=0.8911 (C:0.8911, R:0.0105)
Batch 350/356: Loss=0.8978 (C:0.8978, R:0.0105)

============================================================
Epoch 16/300 completed in 26.1s
Train: Loss=0.8944 (C:0.8944, R:0.0105) Ratio=3.66x
Val:   Loss=0.9812 (C:0.9812, R:0.0104) Ratio=2.98x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9812)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/356: Loss=0.8963 (C:0.8963, R:0.0105)
Batch  25/356: Loss=0.8892 (C:0.8892, R:0.0105)
Batch  50/356: Loss=0.8754 (C:0.8754, R:0.0105)
Batch  75/356: Loss=0.9102 (C:0.9102, R:0.0105)
Batch 100/356: Loss=0.8920 (C:0.8920, R:0.0105)
Batch 125/356: Loss=0.8949 (C:0.8949, R:0.0105)
Batch 150/356: Loss=0.8901 (C:0.8901, R:0.0105)
Batch 175/356: Loss=0.8705 (C:0.8705, R:0.0105)
Batch 200/356: Loss=0.8866 (C:0.8866, R:0.0105)
Batch 225/356: Loss=0.8717 (C:0.8717, R:0.0105)
Batch 250/356: Loss=0.9125 (C:0.9125, R:0.0105)
Batch 275/356: Loss=0.9023 (C:0.9023, R:0.0105)
Batch 300/356: Loss=0.8774 (C:0.8774, R:0.0105)
Batch 325/356: Loss=0.9057 (C:0.9057, R:0.0105)
Batch 350/356: Loss=0.8875 (C:0.8875, R:0.0105)

============================================================
Epoch 17/300 completed in 20.4s
Train: Loss=0.8874 (C:0.8874, R:0.0105) Ratio=3.76x
Val:   Loss=0.9737 (C:0.9737, R:0.0104) Ratio=2.99x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9737)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/356: Loss=0.8827 (C:0.8827, R:0.0105)
Batch  25/356: Loss=0.8939 (C:0.8939, R:0.0105)
Batch  50/356: Loss=0.8766 (C:0.8766, R:0.0105)
Batch  75/356: Loss=0.8733 (C:0.8733, R:0.0105)
Batch 100/356: Loss=0.8897 (C:0.8897, R:0.0105)
Batch 125/356: Loss=0.9130 (C:0.9130, R:0.0105)
Batch 150/356: Loss=0.8787 (C:0.8787, R:0.0105)
Batch 175/356: Loss=0.9007 (C:0.9007, R:0.0105)
Batch 200/356: Loss=0.8781 (C:0.8781, R:0.0105)
Batch 225/356: Loss=0.8792 (C:0.8792, R:0.0105)
Batch 250/356: Loss=0.8912 (C:0.8912, R:0.0105)
Batch 275/356: Loss=0.9121 (C:0.9121, R:0.0105)
Batch 300/356: Loss=0.8873 (C:0.8873, R:0.0105)
Batch 325/356: Loss=0.8447 (C:0.8447, R:0.0105)
Batch 350/356: Loss=0.8869 (C:0.8869, R:0.0105)

============================================================
Epoch 18/300 completed in 20.5s
Train: Loss=0.8826 (C:0.8826, R:0.0105) Ratio=3.81x
Val:   Loss=0.9723 (C:0.9723, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9723)
============================================================

🌍 Updating global dataset at epoch 19
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.409 ± 0.559
    Neg distances: 2.088 ± 0.948
    Separation ratio: 5.11x
    Gap: -3.661
    ✅ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/356: Loss=0.8582 (C:0.8582, R:0.0105)
Batch  25/356: Loss=0.8466 (C:0.8466, R:0.0105)
Batch  50/356: Loss=0.8037 (C:0.8037, R:0.0105)
Batch  75/356: Loss=0.8189 (C:0.8189, R:0.0106)
Batch 100/356: Loss=0.8717 (C:0.8717, R:0.0105)
Batch 125/356: Loss=0.8269 (C:0.8269, R:0.0105)
Batch 150/356: Loss=0.8588 (C:0.8588, R:0.0105)
Batch 175/356: Loss=0.8316 (C:0.8316, R:0.0105)
Batch 200/356: Loss=0.8430 (C:0.8430, R:0.0105)
Batch 225/356: Loss=0.8346 (C:0.8346, R:0.0105)
Batch 250/356: Loss=0.7942 (C:0.7942, R:0.0105)
Batch 275/356: Loss=0.8804 (C:0.8804, R:0.0105)
Batch 300/356: Loss=0.8333 (C:0.8333, R:0.0105)
Batch 325/356: Loss=0.8561 (C:0.8561, R:0.0105)
Batch 350/356: Loss=0.8449 (C:0.8449, R:0.0105)

============================================================
Epoch 19/300 completed in 26.1s
Train: Loss=0.8444 (C:0.8444, R:0.0105) Ratio=3.81x
Val:   Loss=0.9310 (C:0.9310, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9310)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/356: Loss=0.8402 (C:0.8402, R:0.0105)
Batch  25/356: Loss=0.8113 (C:0.8113, R:0.0105)
Batch  50/356: Loss=0.8240 (C:0.8240, R:0.0105)
Batch  75/356: Loss=0.8495 (C:0.8495, R:0.0105)
Batch 100/356: Loss=0.8099 (C:0.8099, R:0.0105)
Batch 125/356: Loss=0.7988 (C:0.7988, R:0.0105)
Batch 150/356: Loss=0.8445 (C:0.8445, R:0.0105)
Batch 175/356: Loss=0.8685 (C:0.8685, R:0.0105)
Batch 200/356: Loss=0.8318 (C:0.8318, R:0.0105)
Batch 225/356: Loss=0.8564 (C:0.8564, R:0.0105)
Batch 250/356: Loss=0.8551 (C:0.8551, R:0.0105)
Batch 275/356: Loss=0.8408 (C:0.8408, R:0.0105)
Batch 300/356: Loss=0.8301 (C:0.8301, R:0.0105)
Batch 325/356: Loss=0.8142 (C:0.8142, R:0.0105)
Batch 350/356: Loss=0.8457 (C:0.8457, R:0.0105)

============================================================
Epoch 20/300 completed in 20.4s
Train: Loss=0.8392 (C:0.8392, R:0.0105) Ratio=3.89x
Val:   Loss=0.9388 (C:0.9388, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/356: Loss=0.8132 (C:0.8132, R:0.0105)
Batch  25/356: Loss=0.8261 (C:0.8261, R:0.0105)
Batch  50/356: Loss=0.8101 (C:0.8101, R:0.0105)
Batch  75/356: Loss=0.8264 (C:0.8264, R:0.0105)
Batch 100/356: Loss=0.8215 (C:0.8215, R:0.0105)
Batch 125/356: Loss=0.8614 (C:0.8614, R:0.0105)
Batch 150/356: Loss=0.8226 (C:0.8226, R:0.0105)
Batch 175/356: Loss=0.8438 (C:0.8438, R:0.0105)
Batch 200/356: Loss=0.8294 (C:0.8294, R:0.0105)
Batch 225/356: Loss=0.8218 (C:0.8218, R:0.0105)
Batch 250/356: Loss=0.8230 (C:0.8230, R:0.0105)
Batch 275/356: Loss=0.8404 (C:0.8404, R:0.0105)
Batch 300/356: Loss=0.8359 (C:0.8359, R:0.0105)
Batch 325/356: Loss=0.8398 (C:0.8398, R:0.0105)
Batch 350/356: Loss=0.8341 (C:0.8341, R:0.0105)

============================================================
Epoch 21/300 completed in 20.4s
Train: Loss=0.8334 (C:0.8334, R:0.0105) Ratio=3.96x
Val:   Loss=0.9482 (C:0.9482, R:0.0104) Ratio=3.00x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 22
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.418 ± 0.598
    Neg distances: 2.182 ± 0.985
    Separation ratio: 5.21x
    Gap: -3.821
    ✅ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/356: Loss=0.8198 (C:0.8198, R:0.0105)
Batch  25/356: Loss=0.7879 (C:0.7879, R:0.0105)
Batch  50/356: Loss=0.7933 (C:0.7933, R:0.0105)
Batch  75/356: Loss=0.8286 (C:0.8286, R:0.0105)
Batch 100/356: Loss=0.8101 (C:0.8101, R:0.0105)
Batch 125/356: Loss=0.7811 (C:0.7811, R:0.0105)
Batch 150/356: Loss=0.8168 (C:0.8168, R:0.0105)
Batch 175/356: Loss=0.8539 (C:0.8539, R:0.0105)
Batch 200/356: Loss=0.8534 (C:0.8534, R:0.0105)
Batch 225/356: Loss=0.7993 (C:0.7993, R:0.0105)
Batch 250/356: Loss=0.7926 (C:0.7926, R:0.0105)
Batch 275/356: Loss=0.8142 (C:0.8142, R:0.0105)
Batch 300/356: Loss=0.8146 (C:0.8146, R:0.0105)
Batch 325/356: Loss=0.8227 (C:0.8227, R:0.0105)
Batch 350/356: Loss=0.8486 (C:0.8486, R:0.0105)

============================================================
Epoch 22/300 completed in 26.2s
Train: Loss=0.8151 (C:0.8151, R:0.0105) Ratio=3.96x
Val:   Loss=0.9275 (C:0.9275, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9275)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/356: Loss=0.8116 (C:0.8116, R:0.0105)
Batch  25/356: Loss=0.7935 (C:0.7935, R:0.0105)
Batch  50/356: Loss=0.8002 (C:0.8002, R:0.0105)
Batch  75/356: Loss=0.7898 (C:0.7898, R:0.0105)
Batch 100/356: Loss=0.8181 (C:0.8181, R:0.0105)
Batch 125/356: Loss=0.8144 (C:0.8144, R:0.0105)
Batch 150/356: Loss=0.8279 (C:0.8279, R:0.0105)
Batch 175/356: Loss=0.8535 (C:0.8535, R:0.0106)
Batch 200/356: Loss=0.8203 (C:0.8203, R:0.0105)
Batch 225/356: Loss=0.8008 (C:0.8008, R:0.0105)
Batch 250/356: Loss=0.8281 (C:0.8281, R:0.0105)
Batch 275/356: Loss=0.8440 (C:0.8440, R:0.0105)
Batch 300/356: Loss=0.8232 (C:0.8232, R:0.0105)
Batch 325/356: Loss=0.8147 (C:0.8147, R:0.0105)
Batch 350/356: Loss=0.8432 (C:0.8432, R:0.0105)

============================================================
Epoch 23/300 completed in 20.6s
Train: Loss=0.8126 (C:0.8126, R:0.0105) Ratio=3.95x
Val:   Loss=0.9231 (C:0.9231, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9231)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/356: Loss=0.8485 (C:0.8485, R:0.0105)
Batch  25/356: Loss=0.8075 (C:0.8075, R:0.0105)
Batch  50/356: Loss=0.7740 (C:0.7740, R:0.0105)
Batch  75/356: Loss=0.8078 (C:0.8078, R:0.0105)
Batch 100/356: Loss=0.8284 (C:0.8284, R:0.0105)
Batch 125/356: Loss=0.7839 (C:0.7839, R:0.0105)
Batch 150/356: Loss=0.8160 (C:0.8160, R:0.0105)
Batch 175/356: Loss=0.8052 (C:0.8052, R:0.0105)
Batch 200/356: Loss=0.8040 (C:0.8040, R:0.0105)
Batch 225/356: Loss=0.8035 (C:0.8035, R:0.0105)
Batch 250/356: Loss=0.7717 (C:0.7717, R:0.0105)
Batch 275/356: Loss=0.8224 (C:0.8224, R:0.0105)
Batch 300/356: Loss=0.8062 (C:0.8062, R:0.0105)
Batch 325/356: Loss=0.8416 (C:0.8416, R:0.0105)
Batch 350/356: Loss=0.8001 (C:0.8001, R:0.0105)

============================================================
Epoch 24/300 completed in 20.4s
Train: Loss=0.8091 (C:0.8091, R:0.0105) Ratio=4.07x
Val:   Loss=0.9195 (C:0.9195, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.9195)
============================================================

🌍 Updating global dataset at epoch 25
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.408 ± 0.587
    Neg distances: 2.252 ± 1.007
    Separation ratio: 5.53x
    Gap: -4.101
    ✅ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/356: Loss=0.7970 (C:0.7970, R:0.0105)
Batch  25/356: Loss=0.7688 (C:0.7688, R:0.0105)
Batch  50/356: Loss=0.7517 (C:0.7517, R:0.0105)
Batch  75/356: Loss=0.7621 (C:0.7621, R:0.0105)
Batch 100/356: Loss=0.7903 (C:0.7903, R:0.0105)
Batch 125/356: Loss=0.7542 (C:0.7542, R:0.0105)
Batch 150/356: Loss=0.8052 (C:0.8052, R:0.0105)
Batch 175/356: Loss=0.7888 (C:0.7888, R:0.0105)
Batch 200/356: Loss=0.7707 (C:0.7707, R:0.0105)
Batch 225/356: Loss=0.7820 (C:0.7820, R:0.0105)
Batch 250/356: Loss=0.7766 (C:0.7766, R:0.0105)
Batch 275/356: Loss=0.8340 (C:0.8340, R:0.0106)
Batch 300/356: Loss=0.7743 (C:0.7743, R:0.0105)
Batch 325/356: Loss=0.7906 (C:0.7906, R:0.0105)
Batch 350/356: Loss=0.7856 (C:0.7856, R:0.0105)

============================================================
Epoch 25/300 completed in 26.4s
Train: Loss=0.7823 (C:0.7823, R:0.0105) Ratio=4.16x
Val:   Loss=0.8921 (C:0.8921, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8921)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/356: Loss=0.7886 (C:0.7886, R:0.0105)
Batch  25/356: Loss=0.8035 (C:0.8035, R:0.0105)
Batch  50/356: Loss=0.7453 (C:0.7453, R:0.0105)
Batch  75/356: Loss=0.7395 (C:0.7395, R:0.0105)
Batch 100/356: Loss=0.7711 (C:0.7711, R:0.0105)
Batch 125/356: Loss=0.7698 (C:0.7698, R:0.0105)
Batch 150/356: Loss=0.7775 (C:0.7775, R:0.0105)
Batch 175/356: Loss=0.7720 (C:0.7720, R:0.0105)
Batch 200/356: Loss=0.7787 (C:0.7787, R:0.0105)
Batch 225/356: Loss=0.7775 (C:0.7775, R:0.0105)
Batch 250/356: Loss=0.7681 (C:0.7681, R:0.0105)
Batch 275/356: Loss=0.7854 (C:0.7854, R:0.0105)
Batch 300/356: Loss=0.7897 (C:0.7897, R:0.0105)
Batch 325/356: Loss=0.7810 (C:0.7810, R:0.0105)
Batch 350/356: Loss=0.7944 (C:0.7944, R:0.0105)

============================================================
Epoch 26/300 completed in 21.2s
Train: Loss=0.7794 (C:0.7794, R:0.0105) Ratio=4.14x
Val:   Loss=0.8957 (C:0.8957, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/356: Loss=0.7591 (C:0.7591, R:0.0105)
Batch  25/356: Loss=0.7869 (C:0.7869, R:0.0105)
Batch  50/356: Loss=0.7606 (C:0.7606, R:0.0105)
Batch  75/356: Loss=0.7848 (C:0.7848, R:0.0105)
Batch 100/356: Loss=0.7775 (C:0.7775, R:0.0105)
Batch 125/356: Loss=0.7656 (C:0.7656, R:0.0105)
Batch 150/356: Loss=0.7509 (C:0.7509, R:0.0105)
Batch 175/356: Loss=0.7758 (C:0.7758, R:0.0105)
Batch 200/356: Loss=0.7413 (C:0.7413, R:0.0105)
Batch 225/356: Loss=0.7685 (C:0.7685, R:0.0105)
Batch 250/356: Loss=0.7973 (C:0.7973, R:0.0105)
Batch 275/356: Loss=0.7720 (C:0.7720, R:0.0105)
Batch 300/356: Loss=0.7765 (C:0.7765, R:0.0105)
Batch 325/356: Loss=0.7782 (C:0.7782, R:0.0105)
Batch 350/356: Loss=0.8135 (C:0.8135, R:0.0105)

============================================================
Epoch 27/300 completed in 20.9s
Train: Loss=0.7737 (C:0.7737, R:0.0105) Ratio=4.16x
Val:   Loss=0.9010 (C:0.9010, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 28
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.410 ± 0.611
    Neg distances: 2.322 ± 1.026
    Separation ratio: 5.66x
    Gap: -4.097
    ✅ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/356: Loss=0.7424 (C:0.7424, R:0.0105)
Batch  25/356: Loss=0.7291 (C:0.7291, R:0.0105)
Batch  50/356: Loss=0.7843 (C:0.7843, R:0.0105)
Batch  75/356: Loss=0.7248 (C:0.7248, R:0.0105)
Batch 100/356: Loss=0.7442 (C:0.7442, R:0.0105)
Batch 125/356: Loss=0.7737 (C:0.7737, R:0.0105)
Batch 150/356: Loss=0.7063 (C:0.7063, R:0.0105)
Batch 175/356: Loss=0.7443 (C:0.7443, R:0.0105)
Batch 200/356: Loss=0.7828 (C:0.7828, R:0.0105)
Batch 225/356: Loss=0.7489 (C:0.7489, R:0.0105)
Batch 250/356: Loss=0.7467 (C:0.7467, R:0.0105)
Batch 275/356: Loss=0.7790 (C:0.7790, R:0.0105)
Batch 300/356: Loss=0.7867 (C:0.7867, R:0.0105)
Batch 325/356: Loss=0.7556 (C:0.7556, R:0.0105)
Batch 350/356: Loss=0.7683 (C:0.7683, R:0.0105)

============================================================
Epoch 28/300 completed in 26.4s
Train: Loss=0.7560 (C:0.7560, R:0.0105) Ratio=4.20x
Val:   Loss=0.8878 (C:0.8878, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8878)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/356: Loss=0.7490 (C:0.7490, R:0.0105)
Batch  25/356: Loss=0.7726 (C:0.7726, R:0.0105)
Batch  50/356: Loss=0.7360 (C:0.7360, R:0.0105)
Batch  75/356: Loss=0.7350 (C:0.7350, R:0.0105)
Batch 100/356: Loss=0.7078 (C:0.7078, R:0.0105)
Batch 125/356: Loss=0.7427 (C:0.7427, R:0.0105)
Batch 150/356: Loss=0.7595 (C:0.7595, R:0.0105)
Batch 175/356: Loss=0.7458 (C:0.7458, R:0.0105)
Batch 200/356: Loss=0.7604 (C:0.7604, R:0.0105)
Batch 225/356: Loss=0.7922 (C:0.7922, R:0.0105)
Batch 250/356: Loss=0.7816 (C:0.7816, R:0.0105)
Batch 275/356: Loss=0.7383 (C:0.7383, R:0.0105)
Batch 300/356: Loss=0.7782 (C:0.7782, R:0.0105)
Batch 325/356: Loss=0.7531 (C:0.7531, R:0.0105)
Batch 350/356: Loss=0.7575 (C:0.7575, R:0.0105)

============================================================
Epoch 29/300 completed in 21.3s
Train: Loss=0.7506 (C:0.7506, R:0.0105) Ratio=4.19x
Val:   Loss=0.8856 (C:0.8856, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8856)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/356: Loss=0.7309 (C:0.7309, R:0.0105)
Batch  25/356: Loss=0.7523 (C:0.7523, R:0.0105)
Batch  50/356: Loss=0.7762 (C:0.7762, R:0.0105)
Batch  75/356: Loss=0.7675 (C:0.7675, R:0.0105)
Batch 100/356: Loss=0.7423 (C:0.7423, R:0.0105)
Batch 125/356: Loss=0.7508 (C:0.7508, R:0.0105)
Batch 150/356: Loss=0.7102 (C:0.7102, R:0.0105)
Batch 175/356: Loss=0.7341 (C:0.7341, R:0.0105)
Batch 200/356: Loss=0.7312 (C:0.7312, R:0.0105)
Batch 225/356: Loss=0.7599 (C:0.7599, R:0.0105)
Batch 250/356: Loss=0.7579 (C:0.7579, R:0.0105)
Batch 275/356: Loss=0.7987 (C:0.7987, R:0.0105)
Batch 300/356: Loss=0.7325 (C:0.7325, R:0.0105)
Batch 325/356: Loss=0.7363 (C:0.7363, R:0.0105)
Batch 350/356: Loss=0.7518 (C:0.7518, R:0.0105)

============================================================
Epoch 30/300 completed in 20.8s
Train: Loss=0.7473 (C:0.7473, R:0.0105) Ratio=4.28x
Val:   Loss=0.8854 (C:0.8854, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
✅ New best model saved (Val Loss: 0.8854)
============================================================

🌍 Updating global dataset at epoch 31
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.382 ± 0.589
    Neg distances: 2.402 ± 1.047
    Separation ratio: 6.29x
    Gap: -4.119
    ✅ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/356: Loss=0.6930 (C:0.6930, R:0.0105)
Batch  25/356: Loss=0.7058 (C:0.7058, R:0.0105)
Batch  50/356: Loss=0.7229 (C:0.7229, R:0.0105)
Batch  75/356: Loss=0.6735 (C:0.6735, R:0.0105)
Batch 100/356: Loss=0.7211 (C:0.7211, R:0.0105)
Batch 125/356: Loss=0.6744 (C:0.6744, R:0.0105)
Batch 150/356: Loss=0.6926 (C:0.6926, R:0.0105)
Batch 175/356: Loss=0.7134 (C:0.7134, R:0.0105)
Batch 200/356: Loss=0.7271 (C:0.7271, R:0.0105)
Batch 225/356: Loss=0.7280 (C:0.7280, R:0.0105)
Batch 250/356: Loss=0.6861 (C:0.6861, R:0.0105)
Batch 275/356: Loss=0.6886 (C:0.6886, R:0.0105)
Batch 300/356: Loss=0.7209 (C:0.7209, R:0.0105)
Batch 325/356: Loss=0.7290 (C:0.7290, R:0.0105)
Batch 350/356: Loss=0.7048 (C:0.7048, R:0.0105)

============================================================
Epoch 31/300 completed in 26.9s
Train: Loss=0.7160 (C:0.7160, R:0.0105) Ratio=4.37x
Val:   Loss=0.8477 (C:0.8477, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.015
✅ New best model saved (Val Loss: 0.8477)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/356: Loss=0.7104 (C:0.7104, R:0.0105)
Batch  25/356: Loss=0.7060 (C:0.7060, R:0.0106)
Batch  50/356: Loss=0.7017 (C:0.7017, R:0.0105)
Batch  75/356: Loss=0.7281 (C:0.7281, R:0.0105)
Batch 100/356: Loss=0.7020 (C:0.7020, R:0.0105)
Batch 125/356: Loss=0.7150 (C:0.7150, R:0.0105)
Batch 150/356: Loss=0.7260 (C:0.7260, R:0.0105)
Batch 175/356: Loss=0.6974 (C:0.6974, R:0.0105)
Batch 200/356: Loss=0.7176 (C:0.7176, R:0.0105)
Batch 225/356: Loss=0.7071 (C:0.7071, R:0.0105)
Batch 250/356: Loss=0.7320 (C:0.7320, R:0.0105)
Batch 275/356: Loss=0.7002 (C:0.7002, R:0.0105)
Batch 300/356: Loss=0.7406 (C:0.7406, R:0.0105)
Batch 325/356: Loss=0.7333 (C:0.7333, R:0.0105)
Batch 350/356: Loss=0.7541 (C:0.7541, R:0.0105)

============================================================
Epoch 32/300 completed in 20.8s
Train: Loss=0.7132 (C:0.7132, R:0.0105) Ratio=4.30x
Val:   Loss=0.8568 (C:0.8568, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/356: Loss=0.7001 (C:0.7001, R:0.0105)
Batch  25/356: Loss=0.6762 (C:0.6762, R:0.0105)
Batch  50/356: Loss=0.7005 (C:0.7005, R:0.0105)
Batch  75/356: Loss=0.7417 (C:0.7417, R:0.0105)
Batch 100/356: Loss=0.7096 (C:0.7096, R:0.0105)
Batch 125/356: Loss=0.7362 (C:0.7362, R:0.0105)
Batch 150/356: Loss=0.7262 (C:0.7262, R:0.0105)
Batch 175/356: Loss=0.7309 (C:0.7309, R:0.0105)
Batch 200/356: Loss=0.7312 (C:0.7312, R:0.0105)
Batch 225/356: Loss=0.7410 (C:0.7410, R:0.0105)
Batch 250/356: Loss=0.7399 (C:0.7399, R:0.0105)
Batch 275/356: Loss=0.7203 (C:0.7203, R:0.0105)
Batch 300/356: Loss=0.7196 (C:0.7196, R:0.0105)
Batch 325/356: Loss=0.7146 (C:0.7146, R:0.0105)
Batch 350/356: Loss=0.7236 (C:0.7236, R:0.0105)

============================================================
Epoch 33/300 completed in 20.4s
Train: Loss=0.7083 (C:0.7083, R:0.0105) Ratio=4.26x
Val:   Loss=0.8570 (C:0.8570, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.045
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 34
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.372 ± 0.589
    Neg distances: 2.444 ± 1.051
    Separation ratio: 6.57x
    Gap: -4.242
    ✅ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/356: Loss=0.6801 (C:0.6801, R:0.0105)
Batch  25/356: Loss=0.6802 (C:0.6802, R:0.0105)
Batch  50/356: Loss=0.6898 (C:0.6898, R:0.0105)
Batch  75/356: Loss=0.7030 (C:0.7030, R:0.0105)
Batch 100/356: Loss=0.6835 (C:0.6835, R:0.0105)
Batch 125/356: Loss=0.7190 (C:0.7190, R:0.0105)
Batch 150/356: Loss=0.6783 (C:0.6783, R:0.0105)
Batch 175/356: Loss=0.6795 (C:0.6795, R:0.0105)
Batch 200/356: Loss=0.7163 (C:0.7163, R:0.0105)
Batch 225/356: Loss=0.6595 (C:0.6595, R:0.0105)
Batch 250/356: Loss=0.6736 (C:0.6736, R:0.0105)
Batch 275/356: Loss=0.6799 (C:0.6799, R:0.0105)
Batch 300/356: Loss=0.7037 (C:0.7037, R:0.0105)
Batch 325/356: Loss=0.7032 (C:0.7032, R:0.0105)
Batch 350/356: Loss=0.6764 (C:0.6764, R:0.0105)

============================================================
Epoch 34/300 completed in 26.6s
Train: Loss=0.6931 (C:0.6931, R:0.0105) Ratio=4.42x
Val:   Loss=0.8464 (C:0.8464, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.060
✅ New best model saved (Val Loss: 0.8464)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/356: Loss=0.6514 (C:0.6514, R:0.0105)
Batch  25/356: Loss=0.6670 (C:0.6670, R:0.0105)
Batch  50/356: Loss=0.6906 (C:0.6906, R:0.0105)
Batch  75/356: Loss=0.6767 (C:0.6767, R:0.0105)
Batch 100/356: Loss=0.6817 (C:0.6817, R:0.0105)
Batch 125/356: Loss=0.6645 (C:0.6645, R:0.0105)
Batch 150/356: Loss=0.6914 (C:0.6914, R:0.0105)
Batch 175/356: Loss=0.7099 (C:0.7099, R:0.0105)
Batch 200/356: Loss=0.6761 (C:0.6761, R:0.0105)
Batch 225/356: Loss=0.6800 (C:0.6800, R:0.0105)
Batch 250/356: Loss=0.6709 (C:0.6709, R:0.0105)
Batch 275/356: Loss=0.6872 (C:0.6872, R:0.0105)
Batch 300/356: Loss=0.7118 (C:0.7118, R:0.0105)
Batch 325/356: Loss=0.7197 (C:0.7197, R:0.0105)
Batch 350/356: Loss=0.7123 (C:0.7123, R:0.0105)

============================================================
Epoch 35/300 completed in 20.3s
Train: Loss=0.6882 (C:0.6882, R:0.0105) Ratio=4.46x
Val:   Loss=0.8314 (C:0.8314, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.075
✅ New best model saved (Val Loss: 0.8314)
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/356: Loss=0.6891 (C:0.6891, R:0.0105)
Batch  25/356: Loss=0.6710 (C:0.6710, R:0.0105)
Batch  50/356: Loss=0.6662 (C:0.6662, R:0.0105)
Batch  75/356: Loss=0.7136 (C:0.7136, R:0.0106)
Batch 100/356: Loss=0.6913 (C:0.6913, R:0.0105)
Batch 125/356: Loss=0.6768 (C:0.6768, R:0.0105)
Batch 150/356: Loss=0.7090 (C:0.7090, R:0.0105)
Batch 175/356: Loss=0.7189 (C:0.7189, R:0.0105)
Batch 200/356: Loss=0.6861 (C:0.6861, R:0.0105)
Batch 225/356: Loss=0.6842 (C:0.6842, R:0.0105)
Batch 250/356: Loss=0.7053 (C:0.7053, R:0.0105)
Batch 275/356: Loss=0.6865 (C:0.6865, R:0.0105)
Batch 300/356: Loss=0.7068 (C:0.7068, R:0.0105)
Batch 325/356: Loss=0.7496 (C:0.7496, R:0.0105)
Batch 350/356: Loss=0.7050 (C:0.7050, R:0.0105)

============================================================
Epoch 36/300 completed in 20.8s
Train: Loss=0.6855 (C:0.6855, R:0.0105) Ratio=4.35x
Val:   Loss=0.8431 (C:0.8431, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.090
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 37
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.361 ± 0.569
    Neg distances: 2.524 ± 1.074
    Separation ratio: 6.99x
    Gap: -4.337
    ✅ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/356: Loss=0.6499 (C:0.6499, R:0.0105)
Batch  25/356: Loss=0.6434 (C:0.6434, R:0.0105)
Batch  50/356: Loss=0.6782 (C:0.6782, R:0.0105)
Batch  75/356: Loss=0.6548 (C:0.6548, R:0.0105)
Batch 100/356: Loss=0.6498 (C:0.6498, R:0.0105)
Batch 125/356: Loss=0.6561 (C:0.6561, R:0.0105)
Batch 150/356: Loss=0.6769 (C:0.6769, R:0.0105)
Batch 175/356: Loss=0.6716 (C:0.6716, R:0.0105)
Batch 200/356: Loss=0.6699 (C:0.6699, R:0.0105)
Batch 225/356: Loss=0.6904 (C:0.6904, R:0.0105)
Batch 250/356: Loss=0.6345 (C:0.6345, R:0.0105)
Batch 275/356: Loss=0.6563 (C:0.6563, R:0.0105)
Batch 300/356: Loss=0.6781 (C:0.6781, R:0.0105)
Batch 325/356: Loss=0.6920 (C:0.6920, R:0.0105)
Batch 350/356: Loss=0.6841 (C:0.6841, R:0.0105)

============================================================
Epoch 37/300 completed in 26.9s
Train: Loss=0.6683 (C:0.6683, R:0.0105) Ratio=4.49x
Val:   Loss=0.8234 (C:0.8234, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.105
✅ New best model saved (Val Loss: 0.8234)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/356: Loss=0.6482 (C:0.6482, R:0.0105)
Batch  25/356: Loss=0.6222 (C:0.6222, R:0.0105)
Batch  50/356: Loss=0.6476 (C:0.6476, R:0.0105)
Batch  75/356: Loss=0.6420 (C:0.6420, R:0.0105)
Batch 100/356: Loss=0.6948 (C:0.6948, R:0.0105)
Batch 125/356: Loss=0.6702 (C:0.6702, R:0.0105)
Batch 150/356: Loss=0.6616 (C:0.6616, R:0.0105)
Batch 175/356: Loss=0.6816 (C:0.6816, R:0.0105)
Batch 200/356: Loss=0.6634 (C:0.6634, R:0.0105)
Batch 225/356: Loss=0.6986 (C:0.6986, R:0.0105)
Batch 250/356: Loss=0.6796 (C:0.6796, R:0.0105)
Batch 275/356: Loss=0.6715 (C:0.6715, R:0.0105)
Batch 300/356: Loss=0.6863 (C:0.6863, R:0.0105)
Batch 325/356: Loss=0.7036 (C:0.7036, R:0.0105)
Batch 350/356: Loss=0.6554 (C:0.6554, R:0.0105)

============================================================
Epoch 38/300 completed in 20.4s
Train: Loss=0.6653 (C:0.6653, R:0.0105) Ratio=4.48x
Val:   Loss=0.8243 (C:0.8243, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/356: Loss=0.6597 (C:0.6597, R:0.0105)
Batch  25/356: Loss=0.6146 (C:0.6146, R:0.0105)
Batch  50/356: Loss=0.6469 (C:0.6469, R:0.0105)
Batch  75/356: Loss=0.6518 (C:0.6518, R:0.0105)
Batch 100/356: Loss=0.6487 (C:0.6487, R:0.0105)
Batch 125/356: Loss=0.6563 (C:0.6563, R:0.0105)
Batch 150/356: Loss=0.6507 (C:0.6507, R:0.0105)
Batch 175/356: Loss=0.6637 (C:0.6637, R:0.0105)
Batch 200/356: Loss=0.6792 (C:0.6792, R:0.0106)
Batch 225/356: Loss=0.6532 (C:0.6532, R:0.0105)
Batch 250/356: Loss=0.6440 (C:0.6440, R:0.0105)
Batch 275/356: Loss=0.6333 (C:0.6333, R:0.0105)
Batch 300/356: Loss=0.6683 (C:0.6683, R:0.0105)
Batch 325/356: Loss=0.6384 (C:0.6384, R:0.0105)
Batch 350/356: Loss=0.6461 (C:0.6461, R:0.0105)

============================================================
Epoch 39/300 completed in 21.3s
Train: Loss=0.6615 (C:0.6615, R:0.0105) Ratio=4.67x
Val:   Loss=0.8284 (C:0.8284, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 40
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.377 ± 0.595
    Neg distances: 2.553 ± 1.091
    Separation ratio: 6.77x
    Gap: -4.442
    ✅ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/356: Loss=0.6603 (C:0.6603, R:0.0105)
Batch  25/356: Loss=0.6394 (C:0.6394, R:0.0105)
Batch  50/356: Loss=0.6567 (C:0.6567, R:0.0105)
Batch  75/356: Loss=0.6746 (C:0.6746, R:0.0105)
Batch 100/356: Loss=0.6568 (C:0.6568, R:0.0105)
Batch 125/356: Loss=0.6714 (C:0.6714, R:0.0105)
Batch 150/356: Loss=0.6386 (C:0.6386, R:0.0105)
Batch 175/356: Loss=0.6613 (C:0.6613, R:0.0105)
Batch 200/356: Loss=0.6697 (C:0.6697, R:0.0105)
Batch 225/356: Loss=0.6720 (C:0.6720, R:0.0105)
Batch 250/356: Loss=0.6966 (C:0.6966, R:0.0105)
Batch 275/356: Loss=0.6607 (C:0.6607, R:0.0105)
Batch 300/356: Loss=0.6493 (C:0.6493, R:0.0105)
Batch 325/356: Loss=0.6516 (C:0.6516, R:0.0105)
Batch 350/356: Loss=0.6523 (C:0.6523, R:0.0105)

============================================================
Epoch 40/300 completed in 27.5s
Train: Loss=0.6641 (C:0.6641, R:0.0105) Ratio=4.62x
Val:   Loss=0.8320 (C:0.8320, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.150
No improvement for 3 epochs
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/356: Loss=0.6251 (C:0.6251, R:0.0105)
Batch  25/356: Loss=0.6583 (C:0.6583, R:0.0105)
Batch  50/356: Loss=0.6361 (C:0.6361, R:0.0105)
Batch  75/356: Loss=0.7073 (C:0.7073, R:0.0105)
Batch 100/356: Loss=0.6730 (C:0.6730, R:0.0105)
Batch 125/356: Loss=0.6455 (C:0.6455, R:0.0105)
Batch 150/356: Loss=0.6193 (C:0.6193, R:0.0105)
Batch 175/356: Loss=0.6687 (C:0.6687, R:0.0105)
Batch 200/356: Loss=0.7000 (C:0.7000, R:0.0105)
Batch 225/356: Loss=0.6605 (C:0.6605, R:0.0105)
Batch 250/356: Loss=0.6400 (C:0.6400, R:0.0105)
Batch 275/356: Loss=0.6941 (C:0.6941, R:0.0105)
Batch 300/356: Loss=0.6810 (C:0.6810, R:0.0105)
Batch 325/356: Loss=0.6446 (C:0.6446, R:0.0105)
Batch 350/356: Loss=0.6575 (C:0.6575, R:0.0105)

============================================================
Epoch 41/300 completed in 20.7s
Train: Loss=0.6627 (C:0.6627, R:0.0105) Ratio=4.62x
Val:   Loss=0.8344 (C:0.8344, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.165
No improvement for 4 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/356: Loss=0.6284 (C:0.6284, R:0.0105)
Batch  25/356: Loss=0.6371 (C:0.6371, R:0.0105)
Batch  50/356: Loss=0.6432 (C:0.6432, R:0.0105)
Batch  75/356: Loss=0.6758 (C:0.6758, R:0.0105)
Batch 100/356: Loss=0.6660 (C:0.6660, R:0.0105)
Batch 125/356: Loss=0.6308 (C:0.6308, R:0.0105)
Batch 150/356: Loss=0.6800 (C:0.6800, R:0.0105)
Batch 175/356: Loss=0.6409 (C:0.6409, R:0.0105)
Batch 200/356: Loss=0.6775 (C:0.6775, R:0.0105)
Batch 225/356: Loss=0.6606 (C:0.6606, R:0.0105)
Batch 250/356: Loss=0.6676 (C:0.6676, R:0.0105)
Batch 275/356: Loss=0.6599 (C:0.6599, R:0.0105)
Batch 300/356: Loss=0.6710 (C:0.6710, R:0.0105)
Batch 325/356: Loss=0.6855 (C:0.6855, R:0.0105)
Batch 350/356: Loss=0.6576 (C:0.6576, R:0.0105)

============================================================
Epoch 42/300 completed in 20.8s
Train: Loss=0.6589 (C:0.6589, R:0.0105) Ratio=4.63x
Val:   Loss=0.8270 (C:0.8270, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.180
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 43
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.348 ± 0.556
    Neg distances: 2.544 ± 1.076
    Separation ratio: 7.32x
    Gap: -4.251
    ✅ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/356: Loss=0.6379 (C:0.6379, R:0.0105)
Batch  25/356: Loss=0.5806 (C:0.5806, R:0.0105)
Batch  50/356: Loss=0.6209 (C:0.6209, R:0.0105)
Batch  75/356: Loss=0.6504 (C:0.6504, R:0.0105)
Batch 100/356: Loss=0.6188 (C:0.6188, R:0.0105)
Batch 125/356: Loss=0.6170 (C:0.6170, R:0.0105)
Batch 150/356: Loss=0.6734 (C:0.6734, R:0.0105)
Batch 175/356: Loss=0.6570 (C:0.6570, R:0.0105)
Batch 200/356: Loss=0.6343 (C:0.6343, R:0.0105)
Batch 225/356: Loss=0.6607 (C:0.6607, R:0.0105)
Batch 250/356: Loss=0.6761 (C:0.6761, R:0.0105)
Batch 275/356: Loss=0.6210 (C:0.6210, R:0.0105)
Batch 300/356: Loss=0.6640 (C:0.6640, R:0.0105)
Batch 325/356: Loss=0.6346 (C:0.6346, R:0.0105)
Batch 350/356: Loss=0.6236 (C:0.6236, R:0.0105)

============================================================
Epoch 43/300 completed in 26.5s
Train: Loss=0.6388 (C:0.6388, R:0.0105) Ratio=4.69x
Val:   Loss=0.8155 (C:0.8155, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.195
✅ New best model saved (Val Loss: 0.8155)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/356: Loss=0.6258 (C:0.6258, R:0.0105)
Batch  25/356: Loss=0.6197 (C:0.6197, R:0.0105)
Batch  50/356: Loss=0.6367 (C:0.6367, R:0.0105)
Batch  75/356: Loss=0.6355 (C:0.6355, R:0.0105)
Batch 100/356: Loss=0.6626 (C:0.6626, R:0.0105)
Batch 125/356: Loss=0.6365 (C:0.6365, R:0.0105)
Batch 150/356: Loss=0.6347 (C:0.6347, R:0.0105)
Batch 175/356: Loss=0.6368 (C:0.6368, R:0.0105)
Batch 200/356: Loss=0.6257 (C:0.6257, R:0.0105)
Batch 225/356: Loss=0.6023 (C:0.6023, R:0.0105)
Batch 250/356: Loss=0.6374 (C:0.6374, R:0.0105)
Batch 275/356: Loss=0.6154 (C:0.6154, R:0.0105)
Batch 300/356: Loss=0.6757 (C:0.6757, R:0.0105)
Batch 325/356: Loss=0.6308 (C:0.6308, R:0.0105)
Batch 350/356: Loss=0.6660 (C:0.6660, R:0.0105)

============================================================
Epoch 44/300 completed in 20.8s
Train: Loss=0.6356 (C:0.6356, R:0.0105) Ratio=4.72x
Val:   Loss=0.8222 (C:0.8222, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/356: Loss=0.6325 (C:0.6325, R:0.0105)
Batch  25/356: Loss=0.6423 (C:0.6423, R:0.0105)
Batch  50/356: Loss=0.6290 (C:0.6290, R:0.0105)
Batch  75/356: Loss=0.6426 (C:0.6426, R:0.0105)
Batch 100/356: Loss=0.6609 (C:0.6609, R:0.0105)
Batch 125/356: Loss=0.6145 (C:0.6145, R:0.0105)
Batch 150/356: Loss=0.6327 (C:0.6327, R:0.0105)
Batch 175/356: Loss=0.6475 (C:0.6475, R:0.0105)
Batch 200/356: Loss=0.6034 (C:0.6034, R:0.0105)
Batch 225/356: Loss=0.6139 (C:0.6139, R:0.0105)
Batch 250/356: Loss=0.6525 (C:0.6525, R:0.0105)
Batch 275/356: Loss=0.6452 (C:0.6452, R:0.0105)
Batch 300/356: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 325/356: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 350/356: Loss=0.6514 (C:0.6514, R:0.0105)

============================================================
Epoch 45/300 completed in 20.8s
Train: Loss=0.6326 (C:0.6326, R:0.0105) Ratio=4.70x
Val:   Loss=0.8187 (C:0.8187, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.225
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 46
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.358 ± 0.595
    Neg distances: 2.549 ± 1.083
    Separation ratio: 7.12x
    Gap: -4.328
    ✅ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/356: Loss=0.6116 (C:0.6116, R:0.0105)
Batch  25/356: Loss=0.6333 (C:0.6333, R:0.0105)
Batch  50/356: Loss=0.6117 (C:0.6117, R:0.0105)
Batch  75/356: Loss=0.6278 (C:0.6278, R:0.0105)
Batch 100/356: Loss=0.6221 (C:0.6221, R:0.0105)
Batch 125/356: Loss=0.6276 (C:0.6276, R:0.0105)
Batch 150/356: Loss=0.6210 (C:0.6210, R:0.0105)
Batch 175/356: Loss=0.6250 (C:0.6250, R:0.0105)
Batch 200/356: Loss=0.6273 (C:0.6273, R:0.0105)
Batch 225/356: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 250/356: Loss=0.6431 (C:0.6431, R:0.0105)
Batch 275/356: Loss=0.6272 (C:0.6272, R:0.0106)
Batch 300/356: Loss=0.6417 (C:0.6417, R:0.0105)
Batch 325/356: Loss=0.6242 (C:0.6242, R:0.0105)
Batch 350/356: Loss=0.6401 (C:0.6401, R:0.0105)

============================================================
Epoch 46/300 completed in 26.1s
Train: Loss=0.6349 (C:0.6349, R:0.0105) Ratio=4.84x
Val:   Loss=0.8241 (C:0.8241, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.240
No improvement for 3 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/356: Loss=0.6358 (C:0.6358, R:0.0105)
Batch  25/356: Loss=0.6570 (C:0.6570, R:0.0105)
Batch  50/356: Loss=0.5937 (C:0.5937, R:0.0105)
Batch  75/356: Loss=0.6270 (C:0.6270, R:0.0105)
Batch 100/356: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 125/356: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 150/356: Loss=0.6572 (C:0.6572, R:0.0105)
Batch 175/356: Loss=0.6039 (C:0.6039, R:0.0105)
Batch 200/356: Loss=0.6504 (C:0.6504, R:0.0105)
Batch 225/356: Loss=0.6270 (C:0.6270, R:0.0105)
Batch 250/356: Loss=0.6389 (C:0.6389, R:0.0105)
Batch 275/356: Loss=0.6271 (C:0.6271, R:0.0105)
Batch 300/356: Loss=0.6394 (C:0.6394, R:0.0105)
Batch 325/356: Loss=0.6946 (C:0.6946, R:0.0105)
Batch 350/356: Loss=0.6246 (C:0.6246, R:0.0105)

============================================================
Epoch 47/300 completed in 21.1s
Train: Loss=0.6320 (C:0.6320, R:0.0105) Ratio=4.76x
Val:   Loss=0.8185 (C:0.8185, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.255
No improvement for 4 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/356: Loss=0.6205 (C:0.6205, R:0.0105)
Batch  25/356: Loss=0.6200 (C:0.6200, R:0.0105)
Batch  50/356: Loss=0.6385 (C:0.6385, R:0.0105)
Batch  75/356: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 100/356: Loss=0.6297 (C:0.6297, R:0.0105)
Batch 125/356: Loss=0.6541 (C:0.6541, R:0.0105)
Batch 150/356: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 175/356: Loss=0.6530 (C:0.6530, R:0.0105)
Batch 200/356: Loss=0.5963 (C:0.5963, R:0.0105)
Batch 225/356: Loss=0.6452 (C:0.6452, R:0.0105)
Batch 250/356: Loss=0.6410 (C:0.6410, R:0.0105)
Batch 275/356: Loss=0.6286 (C:0.6286, R:0.0105)
Batch 300/356: Loss=0.6239 (C:0.6239, R:0.0105)
Batch 325/356: Loss=0.6378 (C:0.6378, R:0.0105)
Batch 350/356: Loss=0.6567 (C:0.6567, R:0.0105)

============================================================
Epoch 48/300 completed in 21.4s
Train: Loss=0.6298 (C:0.6298, R:0.0105) Ratio=4.81x
Val:   Loss=0.8175 (C:0.8175, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.270
No improvement for 5 epochs
============================================================

🌍 Updating global dataset at epoch 49
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.347 ± 0.613
    Neg distances: 2.594 ± 1.097
    Separation ratio: 7.47x
    Gap: -4.443
    ✅ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/356: Loss=0.5949 (C:0.5949, R:0.0105)
Batch  25/356: Loss=0.6247 (C:0.6247, R:0.0105)
Batch  50/356: Loss=0.6024 (C:0.6024, R:0.0106)
Batch  75/356: Loss=0.5949 (C:0.5949, R:0.0105)
Batch 100/356: Loss=0.5877 (C:0.5877, R:0.0105)
Batch 125/356: Loss=0.5986 (C:0.5986, R:0.0105)
Batch 150/356: Loss=0.6172 (C:0.6172, R:0.0105)
Batch 175/356: Loss=0.5872 (C:0.5872, R:0.0105)
Batch 200/356: Loss=0.6276 (C:0.6276, R:0.0105)
Batch 225/356: Loss=0.6400 (C:0.6400, R:0.0105)
Batch 250/356: Loss=0.6346 (C:0.6346, R:0.0105)
Batch 275/356: Loss=0.6271 (C:0.6271, R:0.0105)
Batch 300/356: Loss=0.6209 (C:0.6209, R:0.0105)
Batch 325/356: Loss=0.6143 (C:0.6143, R:0.0105)
Batch 350/356: Loss=0.6005 (C:0.6005, R:0.0105)

============================================================
Epoch 49/300 completed in 27.4s
Train: Loss=0.6179 (C:0.6179, R:0.0105) Ratio=4.92x
Val:   Loss=0.8084 (C:0.8084, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.285
✅ New best model saved (Val Loss: 0.8084)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/356: Loss=0.5910 (C:0.5910, R:0.0105)
Batch  25/356: Loss=0.6112 (C:0.6112, R:0.0105)
Batch  50/356: Loss=0.6045 (C:0.6045, R:0.0105)
Batch  75/356: Loss=0.6153 (C:0.6153, R:0.0105)
Batch 100/356: Loss=0.6449 (C:0.6449, R:0.0105)
Batch 125/356: Loss=0.6330 (C:0.6330, R:0.0106)
Batch 150/356: Loss=0.6031 (C:0.6031, R:0.0105)
Batch 175/356: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 200/356: Loss=0.5964 (C:0.5964, R:0.0105)
Batch 225/356: Loss=0.6517 (C:0.6517, R:0.0105)
Batch 250/356: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 275/356: Loss=0.6306 (C:0.6306, R:0.0105)
Batch 300/356: Loss=0.6111 (C:0.6111, R:0.0105)
Batch 325/356: Loss=0.6071 (C:0.6071, R:0.0105)
Batch 350/356: Loss=0.6590 (C:0.6590, R:0.0105)

============================================================
Epoch 50/300 completed in 20.5s
Train: Loss=0.6156 (C:0.6156, R:0.0105) Ratio=4.85x
Val:   Loss=0.8049 (C:0.8049, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.8049)
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/356: Loss=0.6125 (C:0.6125, R:0.0105)
Batch  25/356: Loss=0.6021 (C:0.6021, R:0.0105)
Batch  50/356: Loss=0.6025 (C:0.6025, R:0.0105)
Batch  75/356: Loss=0.6122 (C:0.6122, R:0.0105)
Batch 100/356: Loss=0.5948 (C:0.5948, R:0.0105)
Batch 125/356: Loss=0.6170 (C:0.6170, R:0.0105)
Batch 150/356: Loss=0.6013 (C:0.6013, R:0.0105)
Batch 175/356: Loss=0.6369 (C:0.6369, R:0.0105)
Batch 200/356: Loss=0.6164 (C:0.6164, R:0.0105)
Batch 225/356: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 250/356: Loss=0.6231 (C:0.6231, R:0.0105)
Batch 275/356: Loss=0.6283 (C:0.6283, R:0.0105)
Batch 300/356: Loss=0.6107 (C:0.6107, R:0.0105)
Batch 325/356: Loss=0.6302 (C:0.6302, R:0.0105)
Batch 350/356: Loss=0.6230 (C:0.6230, R:0.0105)

============================================================
Epoch 51/300 completed in 20.5s
Train: Loss=0.6150 (C:0.6150, R:0.0105) Ratio=4.93x
Val:   Loss=0.8136 (C:0.8136, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

🌍 Updating global dataset at epoch 52
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.321 ± 0.545
    Neg distances: 2.603 ± 1.082
    Separation ratio: 8.11x
    Gap: -4.333
    ✅ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/356: Loss=0.5927 (C:0.5927, R:0.0105)
Batch  25/356: Loss=0.5891 (C:0.5891, R:0.0105)
Batch  50/356: Loss=0.6313 (C:0.6313, R:0.0105)
Batch  75/356: Loss=0.5839 (C:0.5839, R:0.0105)
Batch 100/356: Loss=0.6212 (C:0.6212, R:0.0105)
Batch 125/356: Loss=0.5871 (C:0.5871, R:0.0105)
Batch 150/356: Loss=0.5661 (C:0.5661, R:0.0105)
Batch 175/356: Loss=0.5949 (C:0.5949, R:0.0105)
Batch 200/356: Loss=0.6098 (C:0.6098, R:0.0105)
Batch 225/356: Loss=0.6038 (C:0.6038, R:0.0105)
Batch 250/356: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 275/356: Loss=0.6001 (C:0.6001, R:0.0105)
Batch 300/356: Loss=0.6219 (C:0.6219, R:0.0105)
Batch 325/356: Loss=0.6023 (C:0.6023, R:0.0105)
Batch 350/356: Loss=0.6307 (C:0.6307, R:0.0105)

============================================================
Epoch 52/300 completed in 27.5s
Train: Loss=0.5938 (C:0.5938, R:0.0105) Ratio=4.89x
Val:   Loss=0.7895 (C:0.7895, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7895)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/356: Loss=0.5467 (C:0.5467, R:0.0105)
Batch  25/356: Loss=0.5516 (C:0.5516, R:0.0105)
Batch  50/356: Loss=0.6179 (C:0.6179, R:0.0105)
Batch  75/356: Loss=0.5844 (C:0.5844, R:0.0105)
Batch 100/356: Loss=0.5683 (C:0.5683, R:0.0105)
Batch 125/356: Loss=0.5659 (C:0.5659, R:0.0105)
Batch 150/356: Loss=0.5755 (C:0.5755, R:0.0105)
Batch 175/356: Loss=0.5727 (C:0.5727, R:0.0105)
Batch 200/356: Loss=0.5889 (C:0.5889, R:0.0105)
Batch 225/356: Loss=0.5675 (C:0.5675, R:0.0105)
Batch 250/356: Loss=0.6184 (C:0.6184, R:0.0105)
Batch 275/356: Loss=0.6022 (C:0.6022, R:0.0105)
Batch 300/356: Loss=0.6151 (C:0.6151, R:0.0105)
Batch 325/356: Loss=0.6133 (C:0.6133, R:0.0105)
Batch 350/356: Loss=0.6311 (C:0.6311, R:0.0106)

============================================================
Epoch 53/300 completed in 20.5s
Train: Loss=0.5915 (C:0.5915, R:0.0105) Ratio=5.02x
Val:   Loss=0.7965 (C:0.7965, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/356: Loss=0.6144 (C:0.6144, R:0.0105)
Batch  25/356: Loss=0.5808 (C:0.5808, R:0.0105)
Batch  50/356: Loss=0.5906 (C:0.5906, R:0.0105)
Batch  75/356: Loss=0.5930 (C:0.5930, R:0.0105)
Batch 100/356: Loss=0.5676 (C:0.5676, R:0.0105)
Batch 125/356: Loss=0.6098 (C:0.6098, R:0.0105)
Batch 150/356: Loss=0.5841 (C:0.5841, R:0.0105)
Batch 175/356: Loss=0.5881 (C:0.5881, R:0.0105)
Batch 200/356: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 225/356: Loss=0.6413 (C:0.6413, R:0.0105)
Batch 250/356: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 275/356: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 300/356: Loss=0.6061 (C:0.6061, R:0.0105)
Batch 325/356: Loss=0.6156 (C:0.6156, R:0.0105)
Batch 350/356: Loss=0.6004 (C:0.6004, R:0.0105)

============================================================
Epoch 54/300 completed in 20.6s
Train: Loss=0.5908 (C:0.5908, R:0.0105) Ratio=4.96x
Val:   Loss=0.7938 (C:0.7938, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 55
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.313 ± 0.560
    Neg distances: 2.634 ± 1.086
    Separation ratio: 8.42x
    Gap: -4.559
    ✅ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/356: Loss=0.5693 (C:0.5693, R:0.0105)
Batch  25/356: Loss=0.5702 (C:0.5702, R:0.0105)
Batch  50/356: Loss=0.5760 (C:0.5760, R:0.0105)
Batch  75/356: Loss=0.5714 (C:0.5714, R:0.0105)
Batch 100/356: Loss=0.5786 (C:0.5786, R:0.0105)
Batch 125/356: Loss=0.5837 (C:0.5837, R:0.0105)
Batch 150/356: Loss=0.5868 (C:0.5868, R:0.0105)
Batch 175/356: Loss=0.5560 (C:0.5560, R:0.0105)
Batch 200/356: Loss=0.6063 (C:0.6063, R:0.0105)
Batch 225/356: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 250/356: Loss=0.5796 (C:0.5796, R:0.0105)
Batch 275/356: Loss=0.5786 (C:0.5786, R:0.0105)
Batch 300/356: Loss=0.6361 (C:0.6361, R:0.0105)
Batch 325/356: Loss=0.6378 (C:0.6378, R:0.0105)
Batch 350/356: Loss=0.6081 (C:0.6081, R:0.0105)

============================================================
Epoch 55/300 completed in 27.1s
Train: Loss=0.5808 (C:0.5808, R:0.0105) Ratio=4.89x
Val:   Loss=0.7857 (C:0.7857, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7857)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/356: Loss=0.5603 (C:0.5603, R:0.0105)
Batch  25/356: Loss=0.5506 (C:0.5506, R:0.0105)
Batch  50/356: Loss=0.5718 (C:0.5718, R:0.0105)
Batch  75/356: Loss=0.5583 (C:0.5583, R:0.0105)
Batch 100/356: Loss=0.5369 (C:0.5369, R:0.0105)
Batch 125/356: Loss=0.5645 (C:0.5645, R:0.0105)
Batch 150/356: Loss=0.5807 (C:0.5807, R:0.0105)
Batch 175/356: Loss=0.5723 (C:0.5723, R:0.0105)
Batch 200/356: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 225/356: Loss=0.5842 (C:0.5842, R:0.0105)
Batch 250/356: Loss=0.5987 (C:0.5987, R:0.0105)
Batch 275/356: Loss=0.5847 (C:0.5847, R:0.0105)
Batch 300/356: Loss=0.6097 (C:0.6097, R:0.0105)
Batch 325/356: Loss=0.6089 (C:0.6089, R:0.0105)
Batch 350/356: Loss=0.5500 (C:0.5500, R:0.0105)

============================================================
Epoch 56/300 completed in 20.9s
Train: Loss=0.5796 (C:0.5796, R:0.0105) Ratio=5.08x
Val:   Loss=0.7931 (C:0.7931, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/356: Loss=0.5865 (C:0.5865, R:0.0105)
Batch  25/356: Loss=0.5668 (C:0.5668, R:0.0105)
Batch  50/356: Loss=0.5760 (C:0.5760, R:0.0105)
Batch  75/356: Loss=0.5706 (C:0.5706, R:0.0105)
Batch 100/356: Loss=0.5744 (C:0.5744, R:0.0105)
Batch 125/356: Loss=0.5474 (C:0.5474, R:0.0105)
Batch 150/356: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 175/356: Loss=0.5746 (C:0.5746, R:0.0105)
Batch 200/356: Loss=0.5822 (C:0.5822, R:0.0105)
Batch 225/356: Loss=0.6011 (C:0.6011, R:0.0105)
Batch 250/356: Loss=0.5925 (C:0.5925, R:0.0105)
Batch 275/356: Loss=0.5807 (C:0.5807, R:0.0105)
Batch 300/356: Loss=0.5709 (C:0.5709, R:0.0105)
Batch 325/356: Loss=0.5617 (C:0.5617, R:0.0105)
Batch 350/356: Loss=0.5780 (C:0.5780, R:0.0105)

============================================================
Epoch 57/300 completed in 20.8s
Train: Loss=0.5764 (C:0.5764, R:0.0105) Ratio=5.08x
Val:   Loss=0.7894 (C:0.7894, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

🌍 Updating global dataset at epoch 58
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.292 ± 0.541
    Neg distances: 2.656 ± 1.086
    Separation ratio: 9.10x
    Gap: -4.409
    ✅ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/356: Loss=0.5476 (C:0.5476, R:0.0105)
Batch  25/356: Loss=0.5310 (C:0.5310, R:0.0105)
Batch  50/356: Loss=0.5434 (C:0.5434, R:0.0105)
Batch  75/356: Loss=0.5674 (C:0.5674, R:0.0105)
Batch 100/356: Loss=0.5391 (C:0.5391, R:0.0105)
Batch 125/356: Loss=0.5585 (C:0.5585, R:0.0105)
Batch 150/356: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 175/356: Loss=0.5478 (C:0.5478, R:0.0105)
Batch 200/356: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 225/356: Loss=0.5662 (C:0.5662, R:0.0105)
Batch 250/356: Loss=0.5904 (C:0.5904, R:0.0105)
Batch 275/356: Loss=0.5483 (C:0.5483, R:0.0105)
Batch 300/356: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 325/356: Loss=0.5621 (C:0.5621, R:0.0105)
Batch 350/356: Loss=0.5527 (C:0.5527, R:0.0105)

============================================================
Epoch 58/300 completed in 27.1s
Train: Loss=0.5619 (C:0.5619, R:0.0105) Ratio=5.11x
Val:   Loss=0.7679 (C:0.7679, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7679)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/356: Loss=0.5808 (C:0.5808, R:0.0105)
Batch  25/356: Loss=0.5346 (C:0.5346, R:0.0105)
Batch  50/356: Loss=0.5319 (C:0.5319, R:0.0105)
Batch  75/356: Loss=0.5609 (C:0.5609, R:0.0105)
Batch 100/356: Loss=0.4943 (C:0.4943, R:0.0105)
Batch 125/356: Loss=0.5458 (C:0.5458, R:0.0105)
Batch 150/356: Loss=0.5785 (C:0.5785, R:0.0105)
Batch 175/356: Loss=0.6052 (C:0.6052, R:0.0105)
Batch 200/356: Loss=0.5385 (C:0.5385, R:0.0105)
Batch 225/356: Loss=0.5591 (C:0.5591, R:0.0105)
Batch 250/356: Loss=0.5663 (C:0.5663, R:0.0105)
Batch 275/356: Loss=0.5394 (C:0.5394, R:0.0105)
Batch 300/356: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 325/356: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 350/356: Loss=0.5715 (C:0.5715, R:0.0105)

============================================================
Epoch 59/300 completed in 20.6s
Train: Loss=0.5608 (C:0.5608, R:0.0105) Ratio=5.14x
Val:   Loss=0.7750 (C:0.7750, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/356: Loss=0.5249 (C:0.5249, R:0.0105)
Batch  25/356: Loss=0.5621 (C:0.5621, R:0.0105)
Batch  50/356: Loss=0.5422 (C:0.5422, R:0.0105)
Batch  75/356: Loss=0.5425 (C:0.5425, R:0.0105)
Batch 100/356: Loss=0.5057 (C:0.5057, R:0.0105)
Batch 125/356: Loss=0.5282 (C:0.5282, R:0.0106)
Batch 150/356: Loss=0.5680 (C:0.5680, R:0.0105)
Batch 175/356: Loss=0.5369 (C:0.5369, R:0.0106)
Batch 200/356: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 225/356: Loss=0.5742 (C:0.5742, R:0.0105)
Batch 250/356: Loss=0.5572 (C:0.5572, R:0.0105)
Batch 275/356: Loss=0.5586 (C:0.5586, R:0.0105)
Batch 300/356: Loss=0.5570 (C:0.5570, R:0.0105)
Batch 325/356: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 350/356: Loss=0.5587 (C:0.5587, R:0.0105)

============================================================
Epoch 60/300 completed in 20.7s
Train: Loss=0.5573 (C:0.5573, R:0.0105) Ratio=5.22x
Val:   Loss=0.7670 (C:0.7670, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
✅ New best model saved (Val Loss: 0.7670)
Checkpoint saved at epoch 60
============================================================

🌍 Updating global dataset at epoch 61
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.301 ± 0.551
    Neg distances: 2.655 ± 1.090
    Separation ratio: 8.82x
    Gap: -4.435
    ✅ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/356: Loss=0.5628 (C:0.5628, R:0.0105)
Batch  25/356: Loss=0.5564 (C:0.5564, R:0.0105)
Batch  50/356: Loss=0.5717 (C:0.5717, R:0.0105)
Batch  75/356: Loss=0.5620 (C:0.5620, R:0.0105)
Batch 100/356: Loss=0.5417 (C:0.5417, R:0.0105)
Batch 125/356: Loss=0.5227 (C:0.5227, R:0.0105)
Batch 150/356: Loss=0.5319 (C:0.5319, R:0.0105)
Batch 175/356: Loss=0.5679 (C:0.5679, R:0.0105)
Batch 200/356: Loss=0.5879 (C:0.5879, R:0.0105)
Batch 225/356: Loss=0.5750 (C:0.5750, R:0.0105)
Batch 250/356: Loss=0.5739 (C:0.5739, R:0.0105)
Batch 275/356: Loss=0.5717 (C:0.5717, R:0.0105)
Batch 300/356: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 325/356: Loss=0.5833 (C:0.5833, R:0.0105)
Batch 350/356: Loss=0.5420 (C:0.5420, R:0.0105)

============================================================
Epoch 61/300 completed in 26.7s
Train: Loss=0.5602 (C:0.5602, R:0.0105) Ratio=5.19x
Val:   Loss=0.7865 (C:0.7865, R:0.0104) Ratio=3.03x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/356: Loss=0.5419 (C:0.5419, R:0.0105)
Batch  25/356: Loss=0.5540 (C:0.5540, R:0.0105)
Batch  50/356: Loss=0.5750 (C:0.5750, R:0.0105)
Batch  75/356: Loss=0.5317 (C:0.5317, R:0.0105)
Batch 100/356: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 125/356: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 150/356: Loss=0.5356 (C:0.5356, R:0.0105)
Batch 175/356: Loss=0.5463 (C:0.5463, R:0.0105)
Batch 200/356: Loss=0.5470 (C:0.5470, R:0.0105)
Batch 225/356: Loss=0.5342 (C:0.5342, R:0.0105)
Batch 250/356: Loss=0.5564 (C:0.5564, R:0.0105)
Batch 275/356: Loss=0.5647 (C:0.5647, R:0.0105)
Batch 300/356: Loss=0.5685 (C:0.5685, R:0.0105)
Batch 325/356: Loss=0.5625 (C:0.5625, R:0.0105)
Batch 350/356: Loss=0.5854 (C:0.5854, R:0.0105)

============================================================
Epoch 62/300 completed in 20.8s
Train: Loss=0.5565 (C:0.5565, R:0.0105) Ratio=5.18x
Val:   Loss=0.7812 (C:0.7812, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/356: Loss=0.5466 (C:0.5466, R:0.0105)
Batch  25/356: Loss=0.5324 (C:0.5324, R:0.0105)
Batch  50/356: Loss=0.5639 (C:0.5639, R:0.0105)
Batch  75/356: Loss=0.5294 (C:0.5294, R:0.0105)
Batch 100/356: Loss=0.5386 (C:0.5386, R:0.0105)
Batch 125/356: Loss=0.5758 (C:0.5758, R:0.0105)
Batch 150/356: Loss=0.5675 (C:0.5675, R:0.0105)
Batch 175/356: Loss=0.5552 (C:0.5552, R:0.0106)
Batch 200/356: Loss=0.5416 (C:0.5416, R:0.0105)
Batch 225/356: Loss=0.5461 (C:0.5461, R:0.0106)
Batch 250/356: Loss=0.5704 (C:0.5704, R:0.0105)
Batch 275/356: Loss=0.5697 (C:0.5697, R:0.0106)
Batch 300/356: Loss=0.5581 (C:0.5581, R:0.0105)
Batch 325/356: Loss=0.5682 (C:0.5682, R:0.0105)
Batch 350/356: Loss=0.5795 (C:0.5795, R:0.0105)

============================================================
Epoch 63/300 completed in 21.3s
Train: Loss=0.5554 (C:0.5554, R:0.0105) Ratio=5.23x
Val:   Loss=0.7871 (C:0.7871, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

🌍 Updating global dataset at epoch 64
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.289 ± 0.532
    Neg distances: 2.694 ± 1.101
    Separation ratio: 9.32x
    Gap: -4.539
    ✅ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/356: Loss=0.5553 (C:0.5553, R:0.0105)
Batch  25/356: Loss=0.5288 (C:0.5288, R:0.0105)
Batch  50/356: Loss=0.5482 (C:0.5482, R:0.0105)
Batch  75/356: Loss=0.5210 (C:0.5210, R:0.0105)
Batch 100/356: Loss=0.5300 (C:0.5300, R:0.0105)
Batch 125/356: Loss=0.5035 (C:0.5035, R:0.0105)
Batch 150/356: Loss=0.5472 (C:0.5472, R:0.0105)
Batch 175/356: Loss=0.5233 (C:0.5233, R:0.0105)
Batch 200/356: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 225/356: Loss=0.5760 (C:0.5760, R:0.0105)
Batch 250/356: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 275/356: Loss=0.5436 (C:0.5436, R:0.0105)
Batch 300/356: Loss=0.5379 (C:0.5379, R:0.0105)
Batch 325/356: Loss=0.5508 (C:0.5508, R:0.0105)
Batch 350/356: Loss=0.5461 (C:0.5461, R:0.0105)

============================================================
Epoch 64/300 completed in 27.4s
Train: Loss=0.5458 (C:0.5458, R:0.0105) Ratio=5.26x
Val:   Loss=0.7769 (C:0.7769, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/356: Loss=0.5257 (C:0.5257, R:0.0105)
Batch  25/356: Loss=0.5442 (C:0.5442, R:0.0105)
Batch  50/356: Loss=0.5479 (C:0.5479, R:0.0105)
Batch  75/356: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 100/356: Loss=0.5509 (C:0.5509, R:0.0105)
Batch 125/356: Loss=0.5254 (C:0.5254, R:0.0105)
Batch 150/356: Loss=0.5558 (C:0.5558, R:0.0105)
Batch 175/356: Loss=0.5682 (C:0.5682, R:0.0105)
Batch 200/356: Loss=0.5198 (C:0.5198, R:0.0105)
Batch 225/356: Loss=0.5604 (C:0.5604, R:0.0106)
Batch 250/356: Loss=0.5347 (C:0.5347, R:0.0105)
Batch 275/356: Loss=0.5742 (C:0.5742, R:0.0105)
Batch 300/356: Loss=0.5378 (C:0.5378, R:0.0105)
Batch 325/356: Loss=0.5713 (C:0.5713, R:0.0105)
Batch 350/356: Loss=0.5577 (C:0.5577, R:0.0105)

============================================================
Epoch 65/300 completed in 21.9s
Train: Loss=0.5445 (C:0.5445, R:0.0105) Ratio=5.20x
Val:   Loss=0.7789 (C:0.7789, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/356: Loss=0.4903 (C:0.4903, R:0.0105)
Batch  25/356: Loss=0.5449 (C:0.5449, R:0.0105)
Batch  50/356: Loss=0.5480 (C:0.5480, R:0.0105)
Batch  75/356: Loss=0.5066 (C:0.5066, R:0.0105)
Batch 100/356: Loss=0.5215 (C:0.5215, R:0.0105)
Batch 125/356: Loss=0.5323 (C:0.5323, R:0.0105)
Batch 150/356: Loss=0.5102 (C:0.5102, R:0.0105)
Batch 175/356: Loss=0.5190 (C:0.5190, R:0.0105)
Batch 200/356: Loss=0.5711 (C:0.5711, R:0.0105)
Batch 225/356: Loss=0.5182 (C:0.5182, R:0.0105)
Batch 250/356: Loss=0.5914 (C:0.5914, R:0.0105)
Batch 275/356: Loss=0.5220 (C:0.5220, R:0.0105)
Batch 300/356: Loss=0.5671 (C:0.5671, R:0.0105)
Batch 325/356: Loss=0.5709 (C:0.5709, R:0.0105)
Batch 350/356: Loss=0.5294 (C:0.5294, R:0.0105)

============================================================
Epoch 66/300 completed in 21.3s
Train: Loss=0.5438 (C:0.5438, R:0.0105) Ratio=5.41x
Val:   Loss=0.7692 (C:0.7692, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

🌍 Updating global dataset at epoch 67
🌍 Extracting features for entire dataset...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.301 ± 0.568
    Neg distances: 2.724 ± 1.118
    Separation ratio: 9.06x
    Gap: -4.490
    ✅ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/356: Loss=0.5421 (C:0.5421, R:0.0105)
Batch  25/356: Loss=0.5228 (C:0.5228, R:0.0105)
Batch  50/356: Loss=0.5236 (C:0.5236, R:0.0105)
Batch  75/356: Loss=0.5300 (C:0.5300, R:0.0105)
Batch 100/356: Loss=0.5631 (C:0.5631, R:0.0106)
Batch 125/356: Loss=0.5388 (C:0.5388, R:0.0105)
Batch 150/356: Loss=0.5655 (C:0.5655, R:0.0105)
Batch 175/356: Loss=0.5307 (C:0.5307, R:0.0105)
Batch 200/356: Loss=0.5662 (C:0.5662, R:0.0105)
Batch 225/356: Loss=0.5425 (C:0.5425, R:0.0105)
Batch 250/356: Loss=0.5373 (C:0.5373, R:0.0105)
Batch 275/356: Loss=0.5602 (C:0.5602, R:0.0105)
Batch 300/356: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 325/356: Loss=0.5686 (C:0.5686, R:0.0105)
Batch 350/356: Loss=0.5461 (C:0.5461, R:0.0105)

============================================================
Epoch 67/300 completed in 27.1s
Train: Loss=0.5496 (C:0.5496, R:0.0105) Ratio=5.36x
Val:   Loss=0.7830 (C:0.7830, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/356: Loss=0.5190 (C:0.5190, R:0.0105)
Batch  25/356: Loss=0.5619 (C:0.5619, R:0.0105)
Batch  50/356: Loss=0.5655 (C:0.5655, R:0.0105)
Batch  75/356: Loss=0.5629 (C:0.5629, R:0.0105)
Batch 100/356: Loss=0.5406 (C:0.5406, R:0.0105)
Batch 125/356: Loss=0.5441 (C:0.5441, R:0.0105)
Batch 150/356: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 175/356: Loss=0.5457 (C:0.5457, R:0.0105)
Batch 200/356: Loss=0.5564 (C:0.5564, R:0.0105)
Batch 225/356: Loss=0.5415 (C:0.5415, R:0.0105)
Batch 250/356: Loss=0.5472 (C:0.5472, R:0.0105)
Batch 275/356: Loss=0.5374 (C:0.5374, R:0.0105)
Batch 300/356: Loss=0.5338 (C:0.5338, R:0.0105)
Batch 325/356: Loss=0.5928 (C:0.5928, R:0.0105)
Batch 350/356: Loss=0.5582 (C:0.5582, R:0.0105)

============================================================
Epoch 68/300 completed in 20.6s
Train: Loss=0.5476 (C:0.5476, R:0.0105) Ratio=5.27x
Val:   Loss=0.7854 (C:0.7854, R:0.0104) Ratio=3.05x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 68 epochs
Best model was at epoch 60 with Val Loss: 0.7670

Global Dataset Training Completed!
Best epoch: 60
Best validation loss: 0.7670
Final separation ratios: Train=5.27x, Val=3.05x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/7 batches
Extracted representations: torch.Size([9824, 100])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4636
  Adjusted Rand Score: 0.5368
  Clustering Accuracy: 0.8178
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/356 batches
  Processed 51/356 batches
  Processed 101/356 batches
  Processed 151/356 batches
  Processed 201/356 batches
  Processed 251/356 batches
  Processed 301/356 batches
  Processed 351/356 batches
Extracted representations: torch.Size([546816, 100])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/6 batches
Extracted representations: torch.Size([9216, 100])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9216 samples
Classification Results:
  Accuracy: 0.8145
  Per-class F1: [0.8310594576385381, 0.7539086929330832, 0.8617806197737334]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.763 ± 0.894
  Negative distances: 2.325 ± 1.237
  Separation ratio: 3.05x
  Gap: -4.541
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4636
  Clustering Accuracy: 0.8178
  Adjusted Rand Score: 0.5368

Classification Performance:
  Accuracy: 0.8145

Separation Quality:
  Separation Ratio: 3.05x
  Gap: -4.541
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614/results/evaluation_results_20250714_223221.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614/results/evaluation_results_20250714_223221.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_lr2e-04_lat100_bs1536_20250714_220614/final_results.json

Key Results:
  Separation ratio: 3.05x
  Perfect separation: False
  Classification accuracy: 0.8145
  Result: 0.8145% (improvement: +-80.86%)
  Cleaning up: coarse_lr2e-04_lat100_bs1536_20250714_220614

======================================================================
COARSE SEARCH COMPLETED
======================================================================
Best accuracy: 0.8197%
Total improvement: +-80.85%
Best config: {'learning_rate': 0.0001, 'latent_dim': 75, 'batch_size': 1020}
Best model saved in: coarse_lr1e-04_lat75_bs1020_20250714_172135

Detailed results saved to: coarse_hyperparameter_search_20250714_223221.json
Readable summary saved to: coarse_search_summary_20250714_223221.txt

Search completed. Review coarse_search_summary_20250714_223221.txt for easy analysis.
Best model saved in: experiments/coarse_lr1e-04_lat75_bs1020_20250714_172135
All other experiment directories were cleaned up to save space.

Analysis completed with exit code: 0
Time: Mon 14 Jul 22:32:26 BST 2025

=== ANALYSIS SUCCESSFUL ===
Hyperparam search successful!


Job finished.
