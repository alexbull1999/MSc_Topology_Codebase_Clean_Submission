Starting Surface Distance Metric Analysis job...
Job ID: 184389
Node: gpuvm16
Time: Mon 21 Jul 11:45:03 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Mon Jul 21 11:45:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_114819
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_114819/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 200
  Effective batch size: 600
  Number of batches: 913
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 200
  Effective batch size: 600
  Number of batches: 913
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 600
  Balanced sampling: True
  Train batches: 913
  Val batches: 913
  Test batches: 916
Data loading completed!
  Train: 549367 samples, 913 batches
  Val: 549367 samples, 913 batches
  Test: 549367 samples, 916 batches
AttentionAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Attention Heads: 5
  Total parameters: 5,881,841
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,881,841
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.01
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=2.8554 (C:2.8543, R:0.0112, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.9274 (C:1.9264, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.7362 (C:1.7352, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.6382 (C:1.6372, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.7239 (C:1.7229, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.6605 (C:1.6595, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.5534 (C:1.5524, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.6853 (C:1.6843, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.5963 (C:1.5953, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.5660 (C:1.5650, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.5501 (C:1.5491, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.6496 (C:1.6486, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.5301 (C:1.5291, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.5188 (C:1.5178, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.4851 (C:1.4841, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.5069 (C:1.5059, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.4511 (C:1.4501, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.5368 (C:1.5358, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.4428 (C:1.4418, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.4772 (C:1.4762, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.4916 (C:1.4907, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.5200 (C:1.5190, R:0.0099, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.6119 (C:1.6109, R:0.0100, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.4430 (C:1.4420, R:0.0099, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.4321 (C:1.4311, R:0.0099, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.5479 (C:1.5469, R:0.0100, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.4211 (C:1.4201, R:0.0100, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.5229 (C:1.5219, R:0.0100, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.4157 (C:1.4147, R:0.0100, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.4339 (C:1.4329, R:0.0099, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.4819 (C:1.4809, R:0.0099, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.5150 (C:1.5140, R:0.0099, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.5262 (C:1.5252, R:0.0099, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.5866 (C:1.5856, R:0.0099, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.5212 (C:1.5202, R:0.0100, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.5571 (C:1.5561, R:0.0100, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.4853 (C:1.4843, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5537
  Contrastive: 1.5527
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4543
  Contrastive: 1.4534
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (86.9s)
Train Loss: 1.5537 (C:1.5527, R:0.0100, T:0.0000)
Val Loss:   1.4543 (C:1.4534, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.4896 (C:1.4886, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.4150 (C:1.4140, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.4019 (C:1.4009, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.5220 (C:1.5210, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4971 (C:1.4961, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.6266 (C:1.6256, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.5210 (C:1.5200, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.4931 (C:1.4921, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.4965 (C:1.4955, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.4959 (C:1.4949, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.3920 (C:1.3910, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.3920 (C:1.3910, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.4866 (C:1.4856, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.4907 (C:1.4897, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.4467 (C:1.4457, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.5171 (C:1.5161, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.4446 (C:1.4437, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.4817 (C:1.4807, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.4379 (C:1.4369, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.4805 (C:1.4795, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.3546 (C:1.3536, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.3762 (C:1.3752, R:0.0100, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.5696 (C:1.5686, R:0.0100, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.5250 (C:1.5240, R:0.0099, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.4955 (C:1.4945, R:0.0100, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.4229 (C:1.4219, R:0.0100, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.4828 (C:1.4818, R:0.0099, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.4659 (C:1.4649, R:0.0100, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.5073 (C:1.5064, R:0.0099, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.5358 (C:1.5348, R:0.0100, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.4952 (C:1.4942, R:0.0099, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.5185 (C:1.5175, R:0.0099, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.5335 (C:1.5325, R:0.0099, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.4107 (C:1.4097, R:0.0100, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.6048 (C:1.6038, R:0.0099, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.4288 (C:1.4278, R:0.0100, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.4651 (C:1.4641, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.4790
  Contrastive: 1.4780
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5178
  Contrastive: 1.5168
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

🎯 EPOCH 2/50 COMPLETE (20.0s)
Train Loss: 1.4790 (C:1.4780, R:0.0100, T:0.0000)
Val Loss:   1.5178 (C:1.5168, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.5115 (C:1.5105, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.4617 (C:1.4607, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.5140 (C:1.5130, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.4240 (C:1.4230, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4475 (C:1.4465, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.4024 (C:1.4014, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.4970 (C:1.4960, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.4558 (C:1.4548, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.4315 (C:1.4305, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.4898 (C:1.4888, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.6042 (C:1.6032, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.5369 (C:1.5359, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.5953 (C:1.5943, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.4680 (C:1.4670, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.6173 (C:1.6163, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.5028 (C:1.5018, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.5421 (C:1.5411, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.5676 (C:1.5666, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.4066 (C:1.4056, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.5103 (C:1.5093, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.5589 (C:1.5579, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.3910 (C:1.3900, R:0.0099, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.5294 (C:1.5284, R:0.0099, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.5637 (C:1.5627, R:0.0099, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.5220 (C:1.5210, R:0.0099, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.3941 (C:1.3931, R:0.0099, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.5532 (C:1.5522, R:0.0100, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.5823 (C:1.5813, R:0.0100, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.4966 (C:1.4956, R:0.0099, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.4450 (C:1.4440, R:0.0100, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.5360 (C:1.5350, R:0.0100, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.4836 (C:1.4826, R:0.0099, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.4322 (C:1.4312, R:0.0100, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.3931 (C:1.3921, R:0.0100, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.4228 (C:1.4218, R:0.0100, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.5119 (C:1.5109, R:0.0100, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.5254 (C:1.5244, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.4987
  Contrastive: 1.4977
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5152
  Contrastive: 1.5142
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

🎯 EPOCH 3/50 COMPLETE (20.8s)
Train Loss: 1.4987 (C:1.4977, R:0.0100, T:0.0000)
Val Loss:   1.5152 (C:1.5142, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.5571 (C:1.5561, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.4505 (C:1.4495, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.4695 (C:1.4685, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.3705 (C:1.3695, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4325 (C:1.4315, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.4279 (C:1.4270, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.4063 (C:1.4053, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.3929 (C:1.3919, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.4182 (C:1.4172, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.3800 (C:1.3790, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.4068 (C:1.4058, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.4809 (C:1.4799, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.4897 (C:1.4887, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.5600 (C:1.5590, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.5716 (C:1.5706, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.3960 (C:1.3950, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.6027 (C:1.6017, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.4566 (C:1.4556, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.4500 (C:1.4490, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.3911 (C:1.3901, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.4054 (C:1.4044, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.4215 (C:1.4205, R:0.0100, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.4604 (C:1.4594, R:0.0100, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.4569 (C:1.4559, R:0.0099, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.5544 (C:1.5534, R:0.0099, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.3513 (C:1.3503, R:0.0100, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.4997 (C:1.4987, R:0.0100, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.5152 (C:1.5142, R:0.0100, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.5257 (C:1.5247, R:0.0099, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.5043 (C:1.5033, R:0.0100, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.6076 (C:1.6066, R:0.0100, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.5725 (C:1.5715, R:0.0100, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.4799 (C:1.4789, R:0.0099, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.4986 (C:1.4976, R:0.0100, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.4974 (C:1.4964, R:0.0100, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.3994 (C:1.3984, R:0.0100, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.4563 (C:1.4553, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.4765
  Contrastive: 1.4755
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4520
  Contrastive: 1.4510
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (20.8s)
Train Loss: 1.4765 (C:1.4755, R:0.0100, T:0.0000)
Val Loss:   1.4520 (C:1.4510, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.5413 (C:1.5403, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.3946 (C:1.3936, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.4334 (C:1.4324, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.4305 (C:1.4295, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4891 (C:1.4881, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.4456 (C:1.4446, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.4367 (C:1.4357, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.5382 (C:1.5372, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.6547 (C:1.6538, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.6370 (C:1.6360, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.6325 (C:1.6315, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.5315 (C:1.5305, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.4743 (C:1.4733, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.5222 (C:1.5212, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.5457 (C:1.5447, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.5071 (C:1.5061, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.3803 (C:1.3793, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.4384 (C:1.4374, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.5194 (C:1.5184, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.5283 (C:1.5273, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.3882 (C:1.3872, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.4058 (C:1.4048, R:0.0100, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.5429 (C:1.5419, R:0.0099, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.5074 (C:1.5064, R:0.0099, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.4835 (C:1.4825, R:0.0099, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.4488 (C:1.4478, R:0.0100, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.5060 (C:1.5050, R:0.0099, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.4372 (C:1.4362, R:0.0099, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.4538 (C:1.4528, R:0.0100, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.4934 (C:1.4924, R:0.0099, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.4595 (C:1.4585, R:0.0100, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.3738 (C:1.3728, R:0.0100, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.4555 (C:1.4545, R:0.0099, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.4842 (C:1.4832, R:0.0100, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.4930 (C:1.4920, R:0.0100, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.4298 (C:1.4288, R:0.0099, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.4928 (C:1.4918, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.4822
  Contrastive: 1.4812
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4124
  Contrastive: 1.4115
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (20.5s)
Train Loss: 1.4822 (C:1.4812, R:0.0100, T:0.0000)
Val Loss:   1.4124 (C:1.4115, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.4331 (C:1.4321, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.3534 (C:1.3524, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.4343 (C:1.4333, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.4561 (C:1.4551, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4459 (C:1.4449, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.3399 (C:1.3389, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.3515 (C:1.3505, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.5104 (C:1.5094, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.4303 (C:1.4293, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.5163 (C:1.5153, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.5593 (C:1.5583, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.6260 (C:1.6250, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.4723 (C:1.4713, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.4783 (C:1.4773, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.4140 (C:1.4131, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.5176 (C:1.5166, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.5898 (C:1.5888, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.4606 (C:1.4596, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.5054 (C:1.5044, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.4304 (C:1.4294, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.4531 (C:1.4521, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.5073 (C:1.5063, R:0.0100, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.4396 (C:1.4386, R:0.0100, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.5651 (C:1.5641, R:0.0100, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.4664 (C:1.4654, R:0.0100, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.5422 (C:1.5413, R:0.0099, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.5544 (C:1.5534, R:0.0099, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.4349 (C:1.4339, R:0.0099, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.5124 (C:1.5114, R:0.0100, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.4605 (C:1.4595, R:0.0099, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.6072 (C:1.6062, R:0.0099, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.4983 (C:1.4973, R:0.0099, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.5561 (C:1.5551, R:0.0100, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.3342 (C:1.3332, R:0.0100, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.4750 (C:1.4740, R:0.0100, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.4978 (C:1.4968, R:0.0099, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.5246 (C:1.5236, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.4949
  Contrastive: 1.4939
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4706
  Contrastive: 1.4696
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

🎯 EPOCH 6/50 COMPLETE (20.6s)
Train Loss: 1.4949 (C:1.4939, R:0.0100, T:0.0000)
Val Loss:   1.4706 (C:1.4696, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.5079 (C:1.5069, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.5025 (C:1.5015, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.5079 (C:1.5069, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.5685 (C:1.5675, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4773 (C:1.4763, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.5195 (C:1.5185, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.5928 (C:1.5918, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.5511 (C:1.5501, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.4231 (C:1.4221, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.4213 (C:1.4203, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.5278 (C:1.5268, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.3594 (C:1.3584, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.4332 (C:1.4322, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.5293 (C:1.5284, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.4716 (C:1.4707, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.4944 (C:1.4934, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.4916 (C:1.4906, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.5101 (C:1.5091, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.4796 (C:1.4786, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.5016 (C:1.5006, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.5280 (C:1.5270, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.4770 (C:1.4760, R:0.0100, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.4795 (C:1.4786, R:0.0100, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.4694 (C:1.4684, R:0.0099, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.4943 (C:1.4933, R:0.0099, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.5361 (C:1.5351, R:0.0100, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.5899 (C:1.5889, R:0.0100, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.3957 (C:1.3947, R:0.0100, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.5322 (C:1.5312, R:0.0099, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.4837 (C:1.4827, R:0.0099, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.4934 (C:1.4924, R:0.0100, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.5696 (C:1.5686, R:0.0099, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.4483 (C:1.4473, R:0.0100, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.4954 (C:1.4944, R:0.0099, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.4764 (C:1.4754, R:0.0100, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.5752 (C:1.5743, R:0.0099, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.5441 (C:1.5431, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.4918
  Contrastive: 1.4908
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4753
  Contrastive: 1.4743
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

🎯 EPOCH 7/50 COMPLETE (20.4s)
Train Loss: 1.4918 (C:1.4908, R:0.0100, T:0.0000)
Val Loss:   1.4753 (C:1.4743, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.5036 (C:1.5026, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.5784 (C:1.5774, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.5224 (C:1.5214, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.4642 (C:1.4632, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4961 (C:1.4951, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.5064 (C:1.5054, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.4928 (C:1.4918, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.5255 (C:1.5245, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.3745 (C:1.3735, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.5832 (C:1.5822, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.5172 (C:1.5162, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.4471 (C:1.4461, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.5326 (C:1.5316, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.4626 (C:1.4616, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.4225 (C:1.4215, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.5480 (C:1.5470, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.4883 (C:1.4873, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.5310 (C:1.5300, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.4907 (C:1.4897, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.5621 (C:1.5611, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.5577 (C:1.5567, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.6051 (C:1.6041, R:0.0100, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.5733 (C:1.5723, R:0.0099, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.5559 (C:1.5549, R:0.0100, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.5907 (C:1.5897, R:0.0100, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.4658 (C:1.4649, R:0.0099, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.6649 (C:1.6639, R:0.0100, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.6058 (C:1.6048, R:0.0100, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.4640 (C:1.4630, R:0.0099, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.5381 (C:1.5371, R:0.0100, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.4526 (C:1.4516, R:0.0100, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.4101 (C:1.4091, R:0.0100, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.6045 (C:1.6035, R:0.0100, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.5807 (C:1.5797, R:0.0100, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.5329 (C:1.5319, R:0.0099, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.5114 (C:1.5104, R:0.0100, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.4419 (C:1.4409, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.5096
  Contrastive: 1.5086
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4854
  Contrastive: 1.4844
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

🎯 EPOCH 8/50 COMPLETE (19.5s)
Train Loss: 1.5096 (C:1.5086, R:0.0100, T:0.0000)
Val Loss:   1.4854 (C:1.4844, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.4589 (C:1.4579, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.5094 (C:1.5084, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.4916 (C:1.4906, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.5528 (C:1.5518, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.4699 (C:1.4689, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.6000 (C:1.5990, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.5371 (C:1.5361, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.5118 (C:1.5108, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.4317 (C:1.4307, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.5011 (C:1.5001, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.5082 (C:1.5072, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.4830 (C:1.4820, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.5405 (C:1.5395, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.4800 (C:1.4790, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.4444 (C:1.4434, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.6670 (C:1.6660, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.4821 (C:1.4811, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.4786 (C:1.4776, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.5131 (C:1.5121, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.5020 (C:1.5010, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.6057 (C:1.6047, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.5287 (C:1.5277, R:0.0099, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.4942 (C:1.4932, R:0.0099, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.3526 (C:1.3516, R:0.0099, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.4520 (C:1.4510, R:0.0100, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.4412 (C:1.4402, R:0.0100, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.4762 (C:1.4752, R:0.0100, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.5873 (C:1.5863, R:0.0099, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.5538 (C:1.5528, R:0.0099, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.4514 (C:1.4504, R:0.0099, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.4299 (C:1.4289, R:0.0099, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.5146 (C:1.5136, R:0.0100, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.5013 (C:1.5003, R:0.0100, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.6083 (C:1.6073, R:0.0099, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.4997 (C:1.4987, R:0.0099, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.4419 (C:1.4409, R:0.0100, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.5082 (C:1.5072, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.4995
  Contrastive: 1.4985
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4575
  Contrastive: 1.4565
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

🎯 EPOCH 9/50 COMPLETE (20.1s)
Train Loss: 1.4995 (C:1.4985, R:0.0100, T:0.0000)
Val Loss:   1.4575 (C:1.4565, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 913 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/913: Loss=1.5399 (C:1.5389, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/913: Loss=1.4313 (C:1.4303, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/913: Loss=1.4841 (C:1.4831, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/913: Loss=1.5376 (C:1.5366, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/913: Loss=1.5181 (C:1.5171, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/913: Loss=1.5702 (C:1.5692, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/913: Loss=1.4106 (C:1.4096, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/913: Loss=1.4050 (C:1.4040, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/913: Loss=1.5443 (C:1.5433, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/913: Loss=1.6990 (C:1.6980, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/913: Loss=1.5600 (C:1.5590, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/913: Loss=1.4496 (C:1.4486, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/913: Loss=1.4895 (C:1.4885, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/913: Loss=1.5284 (C:1.5274, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/913: Loss=1.6202 (C:1.6192, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/913: Loss=1.6113 (C:1.6103, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/913: Loss=1.5359 (C:1.5349, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/913: Loss=1.3910 (C:1.3900, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/913: Loss=1.5245 (C:1.5235, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/913: Loss=1.4552 (C:1.4542, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/913: Loss=1.5003 (C:1.4993, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/913: Loss=1.4555 (C:1.4545, R:0.0100, T:0.0000(w:0.000)❌)
Batch 550/913: Loss=1.5630 (C:1.5620, R:0.0099, T:0.0000(w:0.000)❌)
Batch 575/913: Loss=1.5289 (C:1.5279, R:0.0100, T:0.0000(w:0.000)❌)
Batch 600/913: Loss=1.4915 (C:1.4905, R:0.0100, T:0.0000(w:0.000)❌)
Batch 625/913: Loss=1.4014 (C:1.4004, R:0.0100, T:0.0000(w:0.000)❌)
Batch 650/913: Loss=1.4872 (C:1.4862, R:0.0099, T:0.0000(w:0.000)❌)
Batch 675/913: Loss=1.5336 (C:1.5326, R:0.0100, T:0.0000(w:0.000)❌)
Batch 700/913: Loss=1.4958 (C:1.4948, R:0.0100, T:0.0000(w:0.000)❌)
Batch 725/913: Loss=1.5859 (C:1.5849, R:0.0099, T:0.0000(w:0.000)❌)
Batch 750/913: Loss=1.4805 (C:1.4795, R:0.0099, T:0.0000(w:0.000)❌)
Batch 775/913: Loss=1.6243 (C:1.6233, R:0.0099, T:0.0000(w:0.000)❌)
Batch 800/913: Loss=1.5798 (C:1.5788, R:0.0099, T:0.0000(w:0.000)❌)
Batch 825/913: Loss=1.5109 (C:1.5099, R:0.0100, T:0.0000(w:0.000)❌)
Batch 850/913: Loss=1.5084 (C:1.5074, R:0.0099, T:0.0000(w:0.000)❌)
Batch 875/913: Loss=1.5029 (C:1.5019, R:0.0099, T:0.0000(w:0.000)❌)
Batch 900/913: Loss=1.4868 (C:1.4858, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.4966
  Contrastive: 1.4956
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4574
  Contrastive: 1.4565
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/913 (0.0%)

🎯 EPOCH 10/50 COMPLETE (20.7s)
Train Loss: 1.4966 (C:1.4956, R:0.0100, T:0.0000)
Val Loss:   1.4574 (C:1.4565, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 913 | Topological Weight: 0.0100
🌱 Early topological learning
============================================================
Batch   0/913: Loss=72.4990 (C:1.4778, R:0.0100, T:71.0203(w:0.010)⚠️)
Batch  25/913: Loss=16.2321 (C:5.9645, R:0.0100, T:10.2666(w:0.010)⚠️)
Batch  50/913: Loss=10.1379 (C:7.6721, R:0.0100, T:2.4648(w:0.010)🚀)
Batch  75/913: Loss=19.5869 (C:8.7607, R:0.0100, T:10.8252(w:0.010)⚠️)
Batch 100/913: Loss=15.0746 (C:8.9003, R:0.0100, T:6.1733(w:0.010)🚀)
Batch 125/913: Loss=14.9170 (C:8.7625, R:0.0100, T:6.1535(w:0.010)🚀)
Batch 150/913: Loss=10.8719 (C:7.5412, R:0.0099, T:3.3298(w:0.010)🚀)
Batch 175/913: Loss=7.5688 (C:5.6073, R:0.0099, T:1.9604(w:0.010)🚀)
Batch 200/913: Loss=8.8527 (C:5.9344, R:0.0100, T:2.9173(w:0.010)🚀)
Batch 225/913: Loss=6.2062 (C:5.7575, R:0.0100, T:0.4477(w:0.010)🎉)
Batch 250/913: Loss=9.4198 (C:5.4338, R:0.0099, T:3.9850(w:0.010)🚀)
Batch 275/913: Loss=8.9494 (C:5.5547, R:0.0100, T:3.3937(w:0.010)🚀)
Batch 300/913: Loss=5.9230 (C:5.5877, R:0.0099, T:0.3343(w:0.010)🎉)
Batch 325/913: Loss=13.6876 (C:4.9163, R:0.0100, T:8.7703(w:0.010)🚀)
Batch 350/913: Loss=6.3432 (C:5.8474, R:0.0100, T:0.4948(w:0.010)🎉)
Batch 375/913: Loss=10.7181 (C:5.1860, R:0.0099, T:5.5311(w:0.010)🚀)
Batch 400/913: Loss=8.7446 (C:5.6728, R:0.0100, T:3.0707(w:0.010)🚀)
Batch 425/913: Loss=6.1342 (C:5.3936, R:0.0099, T:0.7396(w:0.010)🎉)
Batch 450/913: Loss=5.8212 (C:5.6353, R:0.0099, T:0.1849(w:0.010)🎉)
Batch 475/913: Loss=7.8105 (C:5.4132, R:0.0100, T:2.3964(w:0.010)🚀)
Batch 500/913: Loss=9.5113 (C:5.1246, R:0.0099, T:4.3857(w:0.010)🚀)
Batch 525/913: Loss=12.0651 (C:5.4540, R:0.0099, T:6.6101(w:0.010)🚀)
Batch 550/913: Loss=14.0187 (C:5.4989, R:0.0100, T:8.5189(w:0.010)🚀)
Batch 575/913: Loss=9.1483 (C:5.2664, R:0.0099, T:3.8809(w:0.010)🚀)
Batch 600/913: Loss=7.7173 (C:5.3961, R:0.0099, T:2.3201(w:0.010)🚀)
Batch 625/913: Loss=9.2950 (C:4.9363, R:0.0100, T:4.3577(w:0.010)🚀)
Batch 650/913: Loss=8.3338 (C:5.2254, R:0.0100, T:3.1075(w:0.010)🚀)
Batch 675/913: Loss=9.3043 (C:5.7587, R:0.0099, T:3.5446(w:0.010)🚀)
Batch 700/913: Loss=8.1928 (C:5.6605, R:0.0100, T:2.5313(w:0.010)🚀)
Batch 725/913: Loss=11.9213 (C:5.2331, R:0.0100, T:6.6873(w:0.010)🚀)
Batch 750/913: Loss=5.9037 (C:5.2250, R:0.0100, T:0.6777(w:0.010)🎉)
Batch 775/913: Loss=8.8625 (C:5.3125, R:0.0099, T:3.5490(w:0.010)🚀)
Batch 800/913: Loss=6.7355 (C:5.2649, R:0.0100, T:1.4697(w:0.010)🚀)
Batch 825/913: Loss=7.0431 (C:5.2559, R:0.0100, T:1.7862(w:0.010)🚀)
Batch 850/913: Loss=7.0070 (C:5.0811, R:0.0100, T:1.9248(w:0.010)🚀)
Batch 875/913: Loss=6.1826 (C:5.6115, R:0.0100, T:0.5701(w:0.010)🎉)
Batch 900/913: Loss=6.6031 (C:5.3055, R:0.0100, T:1.2967(w:0.010)🚀)
🎉 MILESTONE: First topological learning detected at epoch 11!
   Initial topological loss: 4.3938
📈 New best topological loss: 4.3938

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 10.1990
  Contrastive: 5.8042
  Reconstruction: 0.0100
  Topological: 4.3938 (weight: 0.010)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 73.3823
  Contrastive: 1.9915
  Reconstruction: 0.0100
  Topological: 71.3898 (weight: 0.010)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 11/50 COMPLETE (201.2s)
Train Loss: 10.1990 (C:5.8042, R:0.0100, T:4.3938)
Val Loss:   73.3823 (C:1.9915, R:0.0100, T:71.3898)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 913 | Topological Weight: 0.0110
🌱 Early topological learning
============================================================
Batch   0/913: Loss=12.1281 (C:5.3680, R:0.0099, T:6.7591(w:0.011)🚀)
Batch  25/913: Loss=6.3501 (C:5.3190, R:0.0100, T:1.0300(w:0.011)🚀)
Batch  50/913: Loss=10.7918 (C:5.0889, R:0.0099, T:5.7019(w:0.011)🚀)
Batch  75/913: Loss=7.3328 (C:5.3643, R:0.0099, T:1.9675(w:0.011)🚀)
Batch 100/913: Loss=10.5705 (C:5.3511, R:0.0100, T:5.2183(w:0.011)🚀)
Batch 125/913: Loss=8.1062 (C:5.2250, R:0.0099, T:2.8802(w:0.011)🚀)
Batch 150/913: Loss=13.2468 (C:5.3274, R:0.0099, T:7.9185(w:0.011)🚀)
Batch 175/913: Loss=12.9124 (C:5.3869, R:0.0099, T:7.5245(w:0.011)🚀)
Batch 200/913: Loss=6.7055 (C:5.3650, R:0.0099, T:1.3395(w:0.011)🚀)
Batch 225/913: Loss=7.0334 (C:5.4360, R:0.0100, T:1.5963(w:0.011)🚀)
Batch 250/913: Loss=8.3982 (C:5.7382, R:0.0100, T:2.6590(w:0.011)🚀)
Batch 275/913: Loss=6.4777 (C:6.0386, R:0.0099, T:0.4380(w:0.011)🎉)
Batch 300/913: Loss=6.6860 (C:5.8329, R:0.0100, T:0.8521(w:0.011)🎉)
Batch 325/913: Loss=6.9053 (C:5.9013, R:0.0100, T:1.0030(w:0.011)🚀)
Batch 350/913: Loss=6.9927 (C:5.5508, R:0.0100, T:1.4409(w:0.011)🚀)
Batch 375/913: Loss=10.5088 (C:5.8453, R:0.0099, T:4.6625(w:0.011)🚀)
Batch 400/913: Loss=7.1576 (C:5.8924, R:0.0099, T:1.2642(w:0.011)🚀)
Batch 425/913: Loss=9.4421 (C:6.0917, R:0.0100, T:3.3493(w:0.011)🚀)
Batch 450/913: Loss=6.9230 (C:5.8168, R:0.0099, T:1.1052(w:0.011)🚀)
Batch 475/913: Loss=6.3520 (C:5.7762, R:0.0100, T:0.5748(w:0.011)🎉)
Batch 500/913: Loss=5.8715 (C:5.6999, R:0.0099, T:0.1706(w:0.011)🎉)
Batch 525/913: Loss=7.0691 (C:5.9149, R:0.0100, T:1.1532(w:0.011)🚀)
Batch 550/913: Loss=7.6203 (C:5.5003, R:0.0100, T:2.1190(w:0.011)🚀)
Batch 575/913: Loss=6.0686 (C:5.5629, R:0.0100, T:0.5048(w:0.011)🎉)
Batch 600/913: Loss=6.7352 (C:5.7220, R:0.0100, T:1.0122(w:0.011)🚀)
Batch 625/913: Loss=7.9399 (C:5.6098, R:0.0100, T:2.3291(w:0.011)🚀)
Batch 650/913: Loss=6.0612 (C:5.7582, R:0.0099, T:0.3019(w:0.011)🎉)
Batch 675/913: Loss=6.8560 (C:5.4882, R:0.0100, T:1.3668(w:0.011)🚀)
Batch 700/913: Loss=5.9246 (C:5.7262, R:0.0099, T:0.1973(w:0.011)🎉)
Batch 725/913: Loss=6.6492 (C:5.8350, R:0.0099, T:0.8132(w:0.011)🎉)
Batch 750/913: Loss=6.6490 (C:5.5688, R:0.0100, T:1.0792(w:0.011)🚀)
Batch 775/913: Loss=5.8139 (C:5.6475, R:0.0100, T:0.1654(w:0.011)🎉)
Batch 800/913: Loss=7.4948 (C:5.7177, R:0.0099, T:1.7761(w:0.011)🚀)
Batch 825/913: Loss=6.2067 (C:5.6073, R:0.0100, T:0.5984(w:0.011)🎉)
Batch 850/913: Loss=7.8747 (C:5.5580, R:0.0100, T:2.3157(w:0.011)🚀)
Batch 875/913: Loss=7.6451 (C:5.8643, R:0.0099, T:1.7798(w:0.011)🚀)
Batch 900/913: Loss=6.5054 (C:5.3691, R:0.0100, T:1.1353(w:0.011)🚀)
📈 New best topological loss: 2.1016

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 7.6948
  Contrastive: 5.5922
  Reconstruction: 0.0100
  Topological: 2.1016 (weight: 0.011)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 80.5211
  Contrastive: 1.9643
  Reconstruction: 0.0100
  Topological: 78.5557 (weight: 0.011)
  Batches with topology: 913/913 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (170.4s)
Train Loss: 7.6948 (C:5.5922, R:0.0100, T:2.1016)
Val Loss:   80.5211 (C:1.9643, R:0.0100, T:78.5557)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 913 | Topological Weight: 0.0120
🌱 Early topological learning
============================================================
Batch   0/913: Loss=6.1250 (C:5.4086, R:0.0100, T:0.7154(w:0.012)🎉)
Batch  25/913: Loss=9.2819 (C:5.5267, R:0.0099, T:3.7542(w:0.012)🚀)
Batch  50/913: Loss=7.9260 (C:5.7254, R:0.0099, T:2.1996(w:0.012)🚀)
Batch  75/913: Loss=8.7812 (C:5.4758, R:0.0100, T:3.3044(w:0.012)🚀)
Batch 100/913: Loss=7.3365 (C:5.5889, R:0.0100, T:1.7466(w:0.012)🚀)
Batch 125/913: Loss=8.1547 (C:5.3947, R:0.0100, T:2.7589(w:0.012)🚀)
Batch 150/913: Loss=6.2720 (C:5.5816, R:0.0099, T:0.6894(w:0.012)🎉)
Batch 175/913: Loss=6.9933 (C:5.4895, R:0.0099, T:1.5028(w:0.012)🚀)
Batch 200/913: Loss=7.2590 (C:5.6711, R:0.0099, T:1.5869(w:0.012)🚀)
Batch 225/913: Loss=5.8981 (C:5.6653, R:0.0099, T:0.2318(w:0.012)🎉)
Batch 250/913: Loss=6.4881 (C:5.5970, R:0.0100, T:0.8901(w:0.012)🎉)
Batch 275/913: Loss=5.8255 (C:5.6255, R:0.0099, T:0.1990(w:0.012)🎉)
Batch 300/913: Loss=5.8222 (C:5.6611, R:0.0100, T:0.1601(w:0.012)🎉)
Batch 325/913: Loss=7.2960 (C:5.6386, R:0.0100, T:1.6565(w:0.012)🚀)
Batch 350/913: Loss=6.0925 (C:5.5769, R:0.0100, T:0.5146(w:0.012)🎉)
Batch 375/913: Loss=7.0492 (C:5.3351, R:0.0100, T:1.7131(w:0.012)🚀)
Batch 400/913: Loss=5.9633 (C:5.6996, R:0.0100, T:0.2627(w:0.012)🎉)
Batch 425/913: Loss=7.8427 (C:5.5502, R:0.0100, T:2.2915(w:0.012)🚀)
Batch 450/913: Loss=6.1965 (C:5.5857, R:0.0100, T:0.6098(w:0.012)🎉)
Batch 475/913: Loss=7.0690 (C:5.6561, R:0.0100, T:1.4119(w:0.012)🚀)
Batch 500/913: Loss=8.6560 (C:5.6491, R:0.0100, T:3.0059(w:0.012)🚀)
Batch 525/913: Loss=7.8774 (C:5.5346, R:0.0099, T:2.3418(w:0.012)🚀)
Batch 550/913: Loss=8.9688 (C:5.6750, R:0.0100, T:3.2927(w:0.012)🚀)
Batch 575/913: Loss=7.6416 (C:5.9283, R:0.0099, T:1.7124(w:0.012)🚀)
Batch 600/913: Loss=5.9838 (C:5.5834, R:0.0100, T:0.3994(w:0.012)🎉)
Batch 625/913: Loss=9.3402 (C:5.2572, R:0.0099, T:4.0820(w:0.012)🚀)
Batch 650/913: Loss=6.6279 (C:5.4588, R:0.0099, T:1.1681(w:0.012)🚀)
Batch 675/913: Loss=7.1245 (C:5.5641, R:0.0100, T:1.5593(w:0.012)🚀)
Batch 700/913: Loss=9.0333 (C:5.5947, R:0.0099, T:3.4376(w:0.012)🚀)
Batch 725/913: Loss=5.8395 (C:5.3860, R:0.0100, T:0.4525(w:0.012)🎉)
Batch 750/913: Loss=7.5884 (C:5.4093, R:0.0099, T:2.1781(w:0.012)🚀)
Batch 775/913: Loss=9.1720 (C:5.3462, R:0.0099, T:3.8248(w:0.012)🚀)
Batch 800/913: Loss=6.9692 (C:5.3635, R:0.0099, T:1.6047(w:0.012)🚀)
Batch 825/913: Loss=5.5186 (C:5.5099, R:0.0100, T:0.0077(w:0.012)🎉)
Batch 850/913: Loss=7.3037 (C:5.4034, R:0.0100, T:1.8993(w:0.012)🚀)
Batch 875/913: Loss=6.0538 (C:5.4825, R:0.0100, T:0.5702(w:0.012)🎉)
Batch 900/913: Loss=10.6871 (C:5.6252, R:0.0100, T:5.0609(w:0.012)🚀)
📈 New best topological loss: 1.5754

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 7.1481
  Contrastive: 5.5717
  Reconstruction: 0.0100
  Topological: 1.5754 (weight: 0.012)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 87.7982
  Contrastive: 1.9947
  Reconstruction: 0.0100
  Topological: 85.8025 (weight: 0.012)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 13/50 COMPLETE (194.5s)
Train Loss: 7.1481 (C:5.5717, R:0.0100, T:1.5754)
Val Loss:   87.7982 (C:1.9947, R:0.0100, T:85.8025)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 913 | Topological Weight: 0.0130
🌱 Early topological learning
============================================================
Batch   0/913: Loss=7.0004 (C:5.3620, R:0.0100, T:1.6374(w:0.013)🚀)
Batch  25/913: Loss=9.0493 (C:5.5063, R:0.0100, T:3.5420(w:0.013)🚀)
Batch  50/913: Loss=6.0331 (C:5.6906, R:0.0100, T:0.3415(w:0.013)🎉)
Batch  75/913: Loss=7.7650 (C:5.6444, R:0.0100, T:2.1196(w:0.013)🚀)
Batch 100/913: Loss=6.9903 (C:5.6858, R:0.0100, T:1.3035(w:0.013)🚀)
Batch 125/913: Loss=6.3488 (C:5.6177, R:0.0099, T:0.7302(w:0.013)🎉)
Batch 150/913: Loss=8.6723 (C:5.7676, R:0.0100, T:2.9037(w:0.013)🚀)
Batch 175/913: Loss=10.8208 (C:5.6298, R:0.0099, T:5.1900(w:0.013)🚀)
Batch 200/913: Loss=6.2842 (C:5.6712, R:0.0100, T:0.6120(w:0.013)🎉)
Batch 225/913: Loss=9.8344 (C:5.3356, R:0.0100, T:4.4978(w:0.013)🚀)
Batch 250/913: Loss=9.0938 (C:5.4917, R:0.0100, T:3.6011(w:0.013)🚀)
Batch 275/913: Loss=8.2361 (C:5.7899, R:0.0100, T:2.4452(w:0.013)🚀)
Batch 300/913: Loss=6.1165 (C:5.5388, R:0.0099, T:0.5767(w:0.013)🎉)
Batch 325/913: Loss=6.4960 (C:5.7039, R:0.0099, T:0.7912(w:0.013)🎉)
Batch 350/913: Loss=6.2938 (C:5.4202, R:0.0100, T:0.8727(w:0.013)🎉)
Batch 375/913: Loss=10.3429 (C:5.4065, R:0.0100, T:4.9354(w:0.013)🚀)
Batch 400/913: Loss=6.0997 (C:5.4447, R:0.0099, T:0.6540(w:0.013)🎉)
Batch 425/913: Loss=6.5088 (C:5.3849, R:0.0099, T:1.1229(w:0.013)🚀)
Batch 450/913: Loss=6.4956 (C:5.6414, R:0.0099, T:0.8532(w:0.013)🎉)
Batch 475/913: Loss=6.4636 (C:5.5016, R:0.0100, T:0.9610(w:0.013)🎉)
Batch 500/913: Loss=7.8944 (C:5.4678, R:0.0099, T:2.4256(w:0.013)🚀)
Batch 525/913: Loss=7.1542 (C:5.5826, R:0.0100, T:1.5706(w:0.013)🚀)
Batch 550/913: Loss=6.0854 (C:5.6246, R:0.0099, T:0.4598(w:0.013)🎉)
Batch 575/913: Loss=6.2215 (C:5.5597, R:0.0099, T:0.6608(w:0.013)🎉)
Batch 600/913: Loss=7.7192 (C:5.2156, R:0.0099, T:2.5026(w:0.013)🚀)
Batch 625/913: Loss=6.2847 (C:5.6223, R:0.0099, T:0.6614(w:0.013)🎉)
Batch 650/913: Loss=7.8896 (C:5.7666, R:0.0100, T:2.1219(w:0.013)🚀)
Batch 675/913: Loss=7.0276 (C:5.4519, R:0.0100, T:1.5747(w:0.013)🚀)
Batch 700/913: Loss=8.3741 (C:5.6253, R:0.0100, T:2.7478(w:0.013)🚀)
Batch 725/913: Loss=7.8072 (C:5.5226, R:0.0100, T:2.2836(w:0.013)🚀)
Batch 750/913: Loss=6.9325 (C:5.5716, R:0.0100, T:1.3598(w:0.013)🚀)
Batch 775/913: Loss=6.6861 (C:5.5443, R:0.0099, T:1.1408(w:0.013)🚀)
Batch 800/913: Loss=8.0472 (C:5.3558, R:0.0099, T:2.6904(w:0.013)🚀)
Batch 825/913: Loss=8.0130 (C:5.4661, R:0.0099, T:2.5459(w:0.013)🚀)
Batch 850/913: Loss=13.5727 (C:5.2227, R:0.0099, T:8.3490(w:0.013)🚀)
Batch 875/913: Loss=6.0605 (C:5.3715, R:0.0100, T:0.6881(w:0.013)🎉)
Batch 900/913: Loss=6.1174 (C:5.3797, R:0.0100, T:0.7368(w:0.013)🎉)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 7.2102
  Contrastive: 5.5500
  Reconstruction: 0.0100
  Topological: 1.6591 (weight: 0.013)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 94.8190
  Contrastive: 1.9229
  Reconstruction: 0.0100
  Topological: 92.8951 (weight: 0.013)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 14/50 COMPLETE (193.4s)
Train Loss: 7.2102 (C:5.5500, R:0.0100, T:1.6591)
Val Loss:   94.8190 (C:1.9229, R:0.0100, T:92.8951)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 913 | Topological Weight: 0.0140
🌱 Early topological learning
============================================================
Batch   0/913: Loss=6.2598 (C:5.5336, R:0.0099, T:0.7251(w:0.014)🎉)
Batch  25/913: Loss=10.1178 (C:5.4063, R:0.0099, T:4.7105(w:0.014)🚀)
Batch  50/913: Loss=6.2299 (C:5.5741, R:0.0100, T:0.6549(w:0.014)🎉)
Batch  75/913: Loss=7.3211 (C:5.5357, R:0.0099, T:1.7844(w:0.014)🚀)
Batch 100/913: Loss=9.3166 (C:5.6520, R:0.0100, T:3.6636(w:0.014)🚀)
Batch 125/913: Loss=7.5386 (C:5.6518, R:0.0100, T:1.8858(w:0.014)🚀)
Batch 150/913: Loss=8.0564 (C:5.5660, R:0.0099, T:2.4894(w:0.014)🚀)
Batch 175/913: Loss=9.8177 (C:5.5380, R:0.0100, T:4.2788(w:0.014)🚀)
Batch 200/913: Loss=6.4280 (C:5.6538, R:0.0100, T:0.7732(w:0.014)🎉)
Batch 225/913: Loss=5.9901 (C:5.4133, R:0.0100, T:0.5759(w:0.014)🎉)
Batch 250/913: Loss=6.9012 (C:5.3943, R:0.0099, T:1.5059(w:0.014)🚀)
Batch 275/913: Loss=8.1045 (C:5.4279, R:0.0099, T:2.6756(w:0.014)🚀)
Batch 300/913: Loss=11.2392 (C:5.3385, R:0.0099, T:5.8998(w:0.014)🚀)
Batch 325/913: Loss=5.9790 (C:5.6813, R:0.0099, T:0.2967(w:0.014)🎉)
Batch 350/913: Loss=8.5038 (C:5.6836, R:0.0099, T:2.8192(w:0.014)🚀)
Batch 375/913: Loss=7.4348 (C:5.6460, R:0.0099, T:1.7878(w:0.014)🚀)
Batch 400/913: Loss=7.2465 (C:5.5995, R:0.0100, T:1.6460(w:0.014)🚀)
Batch 425/913: Loss=7.0569 (C:5.5552, R:0.0099, T:1.5008(w:0.014)🚀)
Batch 450/913: Loss=7.8015 (C:5.6252, R:0.0100, T:2.1753(w:0.014)🚀)
Batch 475/913: Loss=7.7926 (C:5.4527, R:0.0099, T:2.3389(w:0.014)🚀)
Batch 500/913: Loss=5.9898 (C:5.5757, R:0.0100, T:0.4131(w:0.014)🎉)
Batch 525/913: Loss=6.0473 (C:5.5597, R:0.0099, T:0.4866(w:0.014)🎉)
Batch 550/913: Loss=8.1998 (C:5.3974, R:0.0099, T:2.8014(w:0.014)🚀)
Batch 575/913: Loss=5.7303 (C:5.4363, R:0.0100, T:0.2930(w:0.014)🎉)
Batch 600/913: Loss=6.4269 (C:5.5871, R:0.0100, T:0.8389(w:0.014)🎉)
Batch 625/913: Loss=6.1500 (C:5.7594, R:0.0100, T:0.3896(w:0.014)🎉)
Batch 650/913: Loss=6.1128 (C:5.5406, R:0.0100, T:0.5713(w:0.014)🎉)
Batch 675/913: Loss=8.1744 (C:5.3934, R:0.0099, T:2.7800(w:0.014)🚀)
Batch 700/913: Loss=5.9072 (C:5.3902, R:0.0100, T:0.5160(w:0.014)🎉)
Batch 725/913: Loss=6.0285 (C:5.5324, R:0.0099, T:0.4950(w:0.014)🎉)
Batch 750/913: Loss=9.3237 (C:5.5093, R:0.0100, T:3.8135(w:0.014)🚀)
Batch 775/913: Loss=6.5299 (C:5.7682, R:0.0099, T:0.7607(w:0.014)🎉)
Batch 800/913: Loss=6.6763 (C:5.5437, R:0.0099, T:1.1316(w:0.014)🚀)
Batch 825/913: Loss=8.3649 (C:5.6024, R:0.0100, T:2.7616(w:0.014)🚀)
Batch 850/913: Loss=6.1212 (C:5.6280, R:0.0099, T:0.4922(w:0.014)🎉)
Batch 875/913: Loss=5.9199 (C:5.7053, R:0.0100, T:0.2136(w:0.014)🎉)
Batch 900/913: Loss=6.0614 (C:5.6977, R:0.0100, T:0.3627(w:0.014)🎉)
📈 New best topological loss: 1.5389

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 7.0736
  Contrastive: 5.5337
  Reconstruction: 0.0100
  Topological: 1.5389 (weight: 0.014)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 102.1195
  Contrastive: 2.0646
  Reconstruction: 0.0100
  Topological: 100.0539 (weight: 0.014)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 15/50 COMPLETE (188.8s)
Train Loss: 7.0736 (C:5.5337, R:0.0100, T:1.5389)
Val Loss:   102.1195 (C:2.0646, R:0.0100, T:100.0539)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 913 | Topological Weight: 0.0150
🌱 Early topological learning
============================================================
Batch   0/913: Loss=9.0512 (C:5.3742, R:0.0100, T:3.6760(w:0.015)🚀)
Batch  25/913: Loss=7.2401 (C:5.7025, R:0.0100, T:1.5366(w:0.015)🚀)
Batch  50/913: Loss=6.8795 (C:5.6240, R:0.0099, T:1.2545(w:0.015)🚀)
Batch  75/913: Loss=7.5539 (C:5.6723, R:0.0099, T:1.8806(w:0.015)🚀)
Batch 100/913: Loss=8.8631 (C:5.3588, R:0.0099, T:3.5033(w:0.015)🚀)
Batch 125/913: Loss=9.5753 (C:5.4878, R:0.0099, T:4.0866(w:0.015)🚀)
Batch 150/913: Loss=6.1260 (C:5.6517, R:0.0099, T:0.4733(w:0.015)🎉)
Batch 175/913: Loss=6.7732 (C:5.7147, R:0.0099, T:1.0576(w:0.015)🚀)
Batch 200/913: Loss=10.2694 (C:5.4659, R:0.0099, T:4.8026(w:0.015)🚀)
Batch 225/913: Loss=6.8790 (C:5.6170, R:0.0099, T:1.2610(w:0.015)🚀)
Batch 250/913: Loss=7.4270 (C:5.6896, R:0.0100, T:1.7364(w:0.015)🚀)
Batch 275/913: Loss=6.8778 (C:5.6324, R:0.0100, T:1.2444(w:0.015)🚀)
Batch 300/913: Loss=6.8585 (C:5.6365, R:0.0100, T:1.2211(w:0.015)🚀)
Batch 325/913: Loss=6.5593 (C:5.6370, R:0.0100, T:0.9213(w:0.015)🎉)
Batch 350/913: Loss=8.1068 (C:5.7732, R:0.0099, T:2.3327(w:0.015)🚀)
Batch 375/913: Loss=6.4551 (C:5.6390, R:0.0100, T:0.8151(w:0.015)🎉)
Batch 400/913: Loss=8.2105 (C:5.4172, R:0.0099, T:2.7923(w:0.015)🚀)
Batch 425/913: Loss=6.4292 (C:5.7443, R:0.0099, T:0.6839(w:0.015)🎉)
Batch 450/913: Loss=8.2202 (C:5.7318, R:0.0100, T:2.4874(w:0.015)🚀)
Batch 475/913: Loss=6.2700 (C:5.6102, R:0.0100, T:0.6589(w:0.015)🎉)
Batch 500/913: Loss=9.9265 (C:5.4058, R:0.0099, T:4.5197(w:0.015)🚀)
Batch 525/913: Loss=8.3436 (C:5.6522, R:0.0099, T:2.6904(w:0.015)🚀)
Batch 550/913: Loss=6.6993 (C:5.6657, R:0.0100, T:1.0326(w:0.015)🚀)
Batch 575/913: Loss=6.9860 (C:5.4948, R:0.0099, T:1.4903(w:0.015)🚀)
Batch 600/913: Loss=6.5185 (C:5.6458, R:0.0099, T:0.8717(w:0.015)🎉)
Batch 625/913: Loss=7.9949 (C:5.6717, R:0.0100, T:2.3222(w:0.015)🚀)
Batch 650/913: Loss=6.5247 (C:5.5133, R:0.0100, T:1.0104(w:0.015)🚀)
Batch 675/913: Loss=6.2965 (C:5.5455, R:0.0100, T:0.7500(w:0.015)🎉)
Batch 700/913: Loss=9.5445 (C:5.4196, R:0.0099, T:4.1239(w:0.015)🚀)
Batch 725/913: Loss=5.8114 (C:5.5273, R:0.0100, T:0.2832(w:0.015)🎉)
Batch 750/913: Loss=6.1670 (C:5.6873, R:0.0099, T:0.4787(w:0.015)🎉)
Batch 775/913: Loss=6.3963 (C:5.7147, R:0.0100, T:0.6807(w:0.015)🎉)
Batch 800/913: Loss=7.1546 (C:5.7448, R:0.0100, T:1.4088(w:0.015)🚀)
Batch 825/913: Loss=7.7285 (C:5.8474, R:0.0100, T:1.8801(w:0.015)🚀)
Batch 850/913: Loss=5.6818 (C:5.4557, R:0.0100, T:0.2250(w:0.015)🎉)
Batch 875/913: Loss=7.1847 (C:5.6908, R:0.0100, T:1.4929(w:0.015)🚀)
Batch 900/913: Loss=8.8281 (C:5.9885, R:0.0100, T:2.8386(w:0.015)🚀)
📈 New best topological loss: 1.5245

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 7.1604
  Contrastive: 5.6349
  Reconstruction: 0.0100
  Topological: 1.5245 (weight: 0.015)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 109.8526
  Contrastive: 2.7670
  Reconstruction: 0.0100
  Topological: 107.0846 (weight: 0.015)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 16/50 COMPLETE (169.2s)
Train Loss: 7.1604 (C:5.6349, R:0.0100, T:1.5245)
Val Loss:   109.8526 (C:2.7670, R:0.0100, T:107.0846)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 913 | Topological Weight: 0.0160
🌱 Early topological learning
============================================================
Batch   0/913: Loss=6.8715 (C:5.6689, R:0.0100, T:1.2016(w:0.016)🚀)
Batch  25/913: Loss=7.8497 (C:5.6370, R:0.0099, T:2.2117(w:0.016)🚀)
Batch  50/913: Loss=6.1795 (C:5.4484, R:0.0100, T:0.7301(w:0.016)🎉)
Batch  75/913: Loss=7.8322 (C:5.6679, R:0.0100, T:2.1633(w:0.016)🚀)
Batch 100/913: Loss=6.1778 (C:5.5522, R:0.0099, T:0.6246(w:0.016)🎉)
Batch 125/913: Loss=5.6321 (C:5.4406, R:0.0099, T:0.1905(w:0.016)🎉)
Batch 150/913: Loss=7.9638 (C:5.5740, R:0.0100, T:2.3887(w:0.016)🚀)
Batch 175/913: Loss=6.4221 (C:5.5560, R:0.0100, T:0.8651(w:0.016)🎉)
Batch 200/913: Loss=5.8815 (C:5.5373, R:0.0100, T:0.3433(w:0.016)🎉)
Batch 225/913: Loss=7.1834 (C:5.6377, R:0.0100, T:1.5447(w:0.016)🚀)
Batch 250/913: Loss=5.9478 (C:5.6032, R:0.0099, T:0.3436(w:0.016)🎉)
Batch 275/913: Loss=8.0487 (C:5.1504, R:0.0099, T:2.8974(w:0.016)🚀)
Batch 300/913: Loss=6.5869 (C:5.3962, R:0.0099, T:1.1897(w:0.016)🚀)
Batch 325/913: Loss=8.7516 (C:5.4567, R:0.0100, T:3.2939(w:0.016)🚀)
Batch 350/913: Loss=6.1471 (C:5.5162, R:0.0100, T:0.6299(w:0.016)🎉)
Batch 375/913: Loss=6.2683 (C:5.4945, R:0.0100, T:0.7728(w:0.016)🎉)
Batch 400/913: Loss=5.5975 (C:5.4628, R:0.0100, T:0.1337(w:0.016)🎉)
Batch 425/913: Loss=5.6753 (C:5.4614, R:0.0100, T:0.2129(w:0.016)🎉)
Batch 450/913: Loss=6.8410 (C:5.4566, R:0.0100, T:1.3834(w:0.016)🚀)
Batch 475/913: Loss=6.7056 (C:5.3571, R:0.0100, T:1.3475(w:0.016)🚀)
Batch 500/913: Loss=7.0878 (C:5.3874, R:0.0100, T:1.6994(w:0.016)🚀)
Batch 525/913: Loss=7.4514 (C:5.0712, R:0.0100, T:2.3793(w:0.016)🚀)
Batch 550/913: Loss=9.6138 (C:5.2645, R:0.0100, T:4.3484(w:0.016)🚀)
Batch 575/913: Loss=7.2076 (C:5.5860, R:0.0099, T:1.6207(w:0.016)🚀)
Batch 600/913: Loss=7.8184 (C:5.3459, R:0.0100, T:2.4715(w:0.016)🚀)
Batch 625/913: Loss=7.3605 (C:5.5131, R:0.0100, T:1.8464(w:0.016)🚀)
Batch 650/913: Loss=7.5932 (C:5.4502, R:0.0099, T:2.1420(w:0.016)🚀)
Batch 675/913: Loss=5.9872 (C:5.2798, R:0.0099, T:0.7064(w:0.016)🎉)
Batch 700/913: Loss=6.3408 (C:5.4427, R:0.0100, T:0.8971(w:0.016)🎉)
Batch 725/913: Loss=6.4069 (C:5.3348, R:0.0100, T:1.0712(w:0.016)🚀)
Batch 750/913: Loss=7.7656 (C:5.4532, R:0.0100, T:2.3114(w:0.016)🚀)
Batch 775/913: Loss=5.5855 (C:5.4388, R:0.0100, T:0.1457(w:0.016)🎉)
Batch 800/913: Loss=5.7616 (C:5.4388, R:0.0100, T:0.3218(w:0.016)🎉)
Batch 825/913: Loss=6.7268 (C:5.3217, R:0.0100, T:1.4041(w:0.016)🚀)
Batch 850/913: Loss=5.8599 (C:5.3357, R:0.0099, T:0.5232(w:0.016)🎉)
Batch 875/913: Loss=6.5685 (C:5.4447, R:0.0100, T:1.1229(w:0.016)🚀)
Batch 900/913: Loss=5.5716 (C:5.2382, R:0.0099, T:0.3325(w:0.016)🎉)
📈 New best topological loss: 1.4762

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 6.9034
  Contrastive: 5.4261
  Reconstruction: 0.0100
  Topological: 1.4762 (weight: 0.016)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 116.6818
  Contrastive: 2.5229
  Reconstruction: 0.0100
  Topological: 114.1579 (weight: 0.016)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 17/50 COMPLETE (151.4s)
Train Loss: 6.9034 (C:5.4261, R:0.0100, T:1.4762)
Val Loss:   116.6818 (C:2.5229, R:0.0100, T:114.1579)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 913 | Topological Weight: 0.0170
🌱 Early topological learning
============================================================
Batch   0/913: Loss=5.3089 (C:5.2061, R:0.0100, T:0.1018(w:0.017)🎉)
Batch  25/913: Loss=6.0979 (C:5.0527, R:0.0099, T:1.0442(w:0.017)🚀)
Batch  50/913: Loss=7.1859 (C:5.2422, R:0.0100, T:1.9427(w:0.017)🚀)
Batch  75/913: Loss=8.1475 (C:5.0982, R:0.0099, T:3.0483(w:0.017)🚀)
Batch 100/913: Loss=5.4874 (C:5.1516, R:0.0099, T:0.3348(w:0.017)🎉)
Batch 125/913: Loss=6.4598 (C:5.1772, R:0.0099, T:1.2817(w:0.017)🚀)
Batch 150/913: Loss=6.9457 (C:5.2464, R:0.0099, T:1.6983(w:0.017)🚀)
Batch 175/913: Loss=8.3482 (C:5.0084, R:0.0100, T:3.3388(w:0.017)🚀)
Batch 200/913: Loss=5.6479 (C:5.4069, R:0.0100, T:0.2400(w:0.017)🎉)
Batch 225/913: Loss=7.6183 (C:5.2757, R:0.0099, T:2.3415(w:0.017)🚀)
Batch 250/913: Loss=5.6551 (C:5.1650, R:0.0099, T:0.4891(w:0.017)🎉)
Batch 275/913: Loss=5.7248 (C:5.1146, R:0.0100, T:0.6092(w:0.017)🎉)
Batch 300/913: Loss=5.7007 (C:5.1259, R:0.0100, T:0.5739(w:0.017)🎉)
Batch 325/913: Loss=6.3611 (C:5.0075, R:0.0100, T:1.3526(w:0.017)🚀)
Batch 350/913: Loss=6.4992 (C:5.1254, R:0.0100, T:1.3728(w:0.017)🚀)
Batch 375/913: Loss=6.1642 (C:4.9700, R:0.0099, T:1.1932(w:0.017)🚀)
Batch 400/913: Loss=8.8868 (C:4.9796, R:0.0099, T:3.9062(w:0.017)🚀)
Batch 425/913: Loss=7.6804 (C:5.0455, R:0.0099, T:2.6339(w:0.017)🚀)
Batch 450/913: Loss=9.5459 (C:5.1522, R:0.0100, T:4.3927(w:0.017)🚀)
Batch 475/913: Loss=6.6252 (C:5.1583, R:0.0100, T:1.4659(w:0.017)🚀)
Batch 500/913: Loss=5.5024 (C:5.1234, R:0.0100, T:0.3779(w:0.017)🎉)
Batch 525/913: Loss=5.8951 (C:5.0861, R:0.0100, T:0.8079(w:0.017)🎉)
Batch 550/913: Loss=7.8277 (C:5.2621, R:0.0099, T:2.5646(w:0.017)🚀)
Batch 575/913: Loss=6.2028 (C:5.3358, R:0.0099, T:0.8660(w:0.017)🎉)
Batch 600/913: Loss=6.3644 (C:5.1764, R:0.0099, T:1.1870(w:0.017)🚀)
Batch 625/913: Loss=5.5934 (C:5.1672, R:0.0099, T:0.4252(w:0.017)🎉)
Batch 650/913: Loss=6.4786 (C:5.2947, R:0.0099, T:1.1828(w:0.017)🚀)
Batch 675/913: Loss=6.9165 (C:5.1974, R:0.0100, T:1.7180(w:0.017)🚀)
Batch 700/913: Loss=5.5552 (C:5.0794, R:0.0100, T:0.4748(w:0.017)🎉)
Batch 725/913: Loss=7.7461 (C:5.2544, R:0.0100, T:2.4907(w:0.017)🚀)
Batch 750/913: Loss=6.7017 (C:5.0971, R:0.0100, T:1.6036(w:0.017)🚀)
Batch 775/913: Loss=7.6938 (C:5.0558, R:0.0100, T:2.6370(w:0.017)🚀)
Batch 800/913: Loss=6.8892 (C:4.9651, R:0.0099, T:1.9232(w:0.017)🚀)
Batch 825/913: Loss=4.9804 (C:4.9666, R:0.0099, T:0.0128(w:0.017)🎉)
Batch 850/913: Loss=7.2640 (C:5.0774, R:0.0099, T:2.1856(w:0.017)🚀)
Batch 875/913: Loss=7.0676 (C:4.9552, R:0.0100, T:2.1114(w:0.017)🚀)
Batch 900/913: Loss=5.9545 (C:5.0071, R:0.0099, T:0.9464(w:0.017)🎉)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 6.6230
  Contrastive: 5.1355
  Reconstruction: 0.0100
  Topological: 1.4865 (weight: 0.017)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 123.9036
  Contrastive: 2.3664
  Reconstruction: 0.0100
  Topological: 121.5362 (weight: 0.017)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 18/50 COMPLETE (170.7s)
Train Loss: 6.6230 (C:5.1355, R:0.0100, T:1.4865)
Val Loss:   123.9036 (C:2.3664, R:0.0100, T:121.5362)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 913 | Topological Weight: 0.0180
🌱 Early topological learning
============================================================
Batch   0/913: Loss=6.1389 (C:4.9727, R:0.0099, T:1.1652(w:0.018)🚀)
Batch  25/913: Loss=8.4471 (C:4.8145, R:0.0099, T:3.6317(w:0.018)🚀)
Batch  50/913: Loss=6.6238 (C:5.1541, R:0.0100, T:1.4687(w:0.018)🚀)
Batch  75/913: Loss=6.7731 (C:5.2120, R:0.0099, T:1.5600(w:0.018)🚀)
Batch 100/913: Loss=5.3605 (C:5.1732, R:0.0100, T:0.1862(w:0.018)🎉)
Batch 125/913: Loss=5.6632 (C:5.0535, R:0.0099, T:0.6087(w:0.018)🎉)
Batch 150/913: Loss=5.8761 (C:5.1854, R:0.0100, T:0.6897(w:0.018)🎉)
Batch 175/913: Loss=7.3454 (C:5.1611, R:0.0101, T:2.1833(w:0.018)🚀)
Batch 200/913: Loss=8.7822 (C:4.9928, R:0.0099, T:3.7884(w:0.018)🚀)
Batch 225/913: Loss=6.0785 (C:5.0986, R:0.0099, T:0.9789(w:0.018)🎉)
Batch 250/913: Loss=5.2570 (C:5.0292, R:0.0100, T:0.2268(w:0.018)🎉)
Batch 275/913: Loss=6.6556 (C:5.1034, R:0.0099, T:1.5511(w:0.018)🚀)
Batch 300/913: Loss=7.1653 (C:5.0061, R:0.0100, T:2.1582(w:0.018)🚀)
Batch 325/913: Loss=5.1498 (C:4.9155, R:0.0100, T:0.2333(w:0.018)🎉)
Batch 350/913: Loss=9.2061 (C:4.9782, R:0.0099, T:4.2269(w:0.018)🚀)
Batch 375/913: Loss=6.7328 (C:4.8800, R:0.0100, T:1.8518(w:0.018)🚀)
Batch 400/913: Loss=7.6245 (C:4.8218, R:0.0100, T:2.8017(w:0.018)🚀)
Batch 425/913: Loss=6.3005 (C:5.0691, R:0.0099, T:1.2304(w:0.018)🚀)
Batch 450/913: Loss=7.9279 (C:5.0053, R:0.0100, T:2.9216(w:0.018)🚀)
Batch 475/913: Loss=6.0183 (C:5.0012, R:0.0099, T:1.0162(w:0.018)🚀)
Batch 500/913: Loss=8.7598 (C:5.1112, R:0.0099, T:3.6476(w:0.018)🚀)
Batch 525/913: Loss=7.0114 (C:4.8021, R:0.0100, T:2.2083(w:0.018)🚀)
Batch 550/913: Loss=8.5268 (C:4.9086, R:0.0099, T:3.6172(w:0.018)🚀)
Batch 575/913: Loss=6.0132 (C:4.8805, R:0.0099, T:1.1318(w:0.018)🚀)
Batch 600/913: Loss=6.6397 (C:4.9982, R:0.0099, T:1.6405(w:0.018)🚀)
Batch 625/913: Loss=5.5086 (C:4.8766, R:0.0100, T:0.6310(w:0.018)🎉)
Batch 650/913: Loss=5.6030 (C:4.8666, R:0.0100, T:0.7354(w:0.018)🎉)
Batch 675/913: Loss=7.9051 (C:4.8317, R:0.0100, T:3.0724(w:0.018)🚀)
Batch 700/913: Loss=5.3119 (C:4.9016, R:0.0100, T:0.4093(w:0.018)🎉)
Batch 725/913: Loss=5.7405 (C:4.9884, R:0.0099, T:0.7511(w:0.018)🎉)
Batch 750/913: Loss=5.9614 (C:4.8457, R:0.0099, T:1.1148(w:0.018)🚀)
Batch 775/913: Loss=6.1352 (C:4.8241, R:0.0099, T:1.3101(w:0.018)🚀)
Batch 800/913: Loss=5.9486 (C:4.6421, R:0.0099, T:1.3055(w:0.018)🚀)
Batch 825/913: Loss=10.3954 (C:4.8307, R:0.0100, T:5.5637(w:0.018)🚀)
Batch 850/913: Loss=5.9083 (C:4.7413, R:0.0099, T:1.1660(w:0.018)🚀)
Batch 875/913: Loss=5.7043 (C:5.0011, R:0.0099, T:0.7022(w:0.018)🎉)
Batch 900/913: Loss=9.8396 (C:4.9468, R:0.0100, T:4.8918(w:0.018)🚀)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 6.5404
  Contrastive: 4.9738
  Reconstruction: 0.0100
  Topological: 1.5656 (weight: 0.018)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 131.3340
  Contrastive: 2.5891
  Reconstruction: 0.0100
  Topological: 128.7439 (weight: 0.018)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 19/50 COMPLETE (164.1s)
Train Loss: 6.5404 (C:4.9738, R:0.0100, T:1.5656)
Val Loss:   131.3340 (C:2.5891, R:0.0100, T:128.7439)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 913 | Topological Weight: 0.0190
🌱 Early topological learning
============================================================
Batch   0/913: Loss=6.9981 (C:5.1005, R:0.0099, T:1.8966(w:0.019)🚀)
Batch  25/913: Loss=6.8865 (C:5.2553, R:0.0099, T:1.6302(w:0.019)🚀)
Batch  50/913: Loss=7.4823 (C:5.4385, R:0.0100, T:2.0428(w:0.019)🚀)
Batch  75/913: Loss=5.8159 (C:5.3059, R:0.0100, T:0.5090(w:0.019)🎉)
Batch 100/913: Loss=6.4453 (C:5.4592, R:0.0100, T:0.9851(w:0.019)🎉)
Batch 125/913: Loss=6.2721 (C:5.3816, R:0.0100, T:0.8895(w:0.019)🎉)
Batch 150/913: Loss=5.9462 (C:5.2109, R:0.0099, T:0.7343(w:0.019)🎉)
Batch 175/913: Loss=5.7378 (C:5.3585, R:0.0099, T:0.3783(w:0.019)🎉)
Batch 200/913: Loss=8.4399 (C:5.3335, R:0.0100, T:3.1053(w:0.019)🚀)
Batch 225/913: Loss=5.9733 (C:5.2604, R:0.0099, T:0.7120(w:0.019)🎉)
Batch 250/913: Loss=8.0822 (C:5.3459, R:0.0099, T:2.7353(w:0.019)🚀)
Batch 275/913: Loss=7.9662 (C:5.2379, R:0.0100, T:2.7273(w:0.019)🚀)
Batch 300/913: Loss=5.8272 (C:4.9251, R:0.0099, T:0.9011(w:0.019)🎉)
Batch 325/913: Loss=5.2411 (C:5.0484, R:0.0100, T:0.1917(w:0.019)🎉)
Batch 350/913: Loss=5.8055 (C:4.8878, R:0.0100, T:0.9167(w:0.019)🎉)
Batch 375/913: Loss=6.3530 (C:5.0276, R:0.0099, T:1.3245(w:0.019)🚀)
Batch 400/913: Loss=6.8764 (C:5.1767, R:0.0100, T:1.6986(w:0.019)🚀)
Batch 425/913: Loss=5.7196 (C:5.1043, R:0.0100, T:0.6143(w:0.019)🎉)
Batch 450/913: Loss=8.1951 (C:4.9502, R:0.0099, T:3.2439(w:0.019)🚀)
Batch 475/913: Loss=5.8388 (C:5.0724, R:0.0100, T:0.7653(w:0.019)🎉)
Batch 500/913: Loss=6.3016 (C:5.2128, R:0.0100, T:1.0878(w:0.019)🚀)
Batch 525/913: Loss=6.2683 (C:5.0963, R:0.0099, T:1.1710(w:0.019)🚀)
Batch 550/913: Loss=5.7871 (C:5.1545, R:0.0099, T:0.6317(w:0.019)🎉)
Batch 575/913: Loss=6.2614 (C:5.1732, R:0.0098, T:1.0872(w:0.019)🚀)
Batch 600/913: Loss=6.5794 (C:5.1333, R:0.0099, T:1.4452(w:0.019)🚀)
Batch 625/913: Loss=7.3164 (C:5.1005, R:0.0099, T:2.2150(w:0.019)🚀)
Batch 650/913: Loss=5.6243 (C:5.0354, R:0.0099, T:0.5879(w:0.019)🎉)
Batch 675/913: Loss=5.6847 (C:5.0966, R:0.0100, T:0.5871(w:0.019)🎉)
Batch 700/913: Loss=8.1207 (C:4.9240, R:0.0100, T:3.1956(w:0.019)🚀)
Batch 725/913: Loss=7.9783 (C:4.9416, R:0.0099, T:3.0357(w:0.019)🚀)
Batch 750/913: Loss=5.4048 (C:4.8416, R:0.0100, T:0.5622(w:0.019)🎉)
Batch 775/913: Loss=6.1707 (C:4.9181, R:0.0099, T:1.2516(w:0.019)🚀)
Batch 800/913: Loss=7.4188 (C:5.0834, R:0.0099, T:2.3344(w:0.019)🚀)
Batch 825/913: Loss=7.1597 (C:5.1368, R:0.0100, T:2.0220(w:0.019)🚀)
Batch 850/913: Loss=5.0971 (C:4.8931, R:0.0100, T:0.2029(w:0.019)🎉)
Batch 875/913: Loss=6.1497 (C:4.9562, R:0.0099, T:1.1925(w:0.019)🚀)
Batch 900/913: Loss=6.2533 (C:4.9850, R:0.0100, T:1.2674(w:0.019)🚀)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 6.8629
  Contrastive: 5.1112
  Reconstruction: 0.0100
  Topological: 1.7507 (weight: 0.019)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 138.3235
  Contrastive: 2.4639
  Reconstruction: 0.0100
  Topological: 135.8585 (weight: 0.019)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 20/50 COMPLETE (153.6s)
Train Loss: 6.8629 (C:5.1112, R:0.0100, T:1.7507)
Val Loss:   138.3235 (C:2.4639, R:0.0100, T:135.8585)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 913 | Topological Weight: 0.0200
🌱 Early topological learning
============================================================
Batch   0/913: Loss=5.9109 (C:5.0629, R:0.0099, T:0.8470(w:0.020)🎉)
Batch  25/913: Loss=6.0764 (C:4.9514, R:0.0100, T:1.1240(w:0.020)🚀)
Batch  50/913: Loss=5.7640 (C:4.8868, R:0.0099, T:0.8762(w:0.020)🎉)
Batch  75/913: Loss=7.8312 (C:4.9869, R:0.0100, T:2.8433(w:0.020)🚀)
Batch 100/913: Loss=5.8926 (C:4.9665, R:0.0099, T:0.9252(w:0.020)🎉)
Batch 125/913: Loss=5.3179 (C:4.9138, R:0.0100, T:0.4031(w:0.020)🎉)
Batch 150/913: Loss=5.5931 (C:4.8419, R:0.0099, T:0.7501(w:0.020)🎉)
Batch 175/913: Loss=5.8614 (C:4.9255, R:0.0099, T:0.9348(w:0.020)🎉)
Batch 200/913: Loss=4.7480 (C:4.7278, R:0.0100, T:0.0192(w:0.020)🎉)
Batch 225/913: Loss=7.0148 (C:4.7982, R:0.0099, T:2.2156(w:0.020)🚀)
Batch 250/913: Loss=6.5605 (C:5.0823, R:0.0100, T:1.4772(w:0.020)🚀)
Batch 275/913: Loss=6.8797 (C:5.0255, R:0.0100, T:1.8532(w:0.020)🚀)
Batch 300/913: Loss=6.2550 (C:5.0979, R:0.0100, T:1.1562(w:0.020)🚀)
Batch 325/913: Loss=5.0829 (C:4.9688, R:0.0100, T:0.1131(w:0.020)🎉)
Batch 350/913: Loss=5.9746 (C:4.9050, R:0.0099, T:1.0686(w:0.020)🚀)
Batch 375/913: Loss=5.1503 (C:5.0325, R:0.0099, T:0.1168(w:0.020)🎉)
Batch 400/913: Loss=5.9189 (C:4.8167, R:0.0099, T:1.1012(w:0.020)🚀)
Batch 425/913: Loss=10.9338 (C:4.9187, R:0.0099, T:6.0142(w:0.020)🚀)
Batch 450/913: Loss=7.4913 (C:5.0101, R:0.0099, T:2.4802(w:0.020)🚀)
Batch 475/913: Loss=5.8202 (C:5.0237, R:0.0100, T:0.7955(w:0.020)🎉)
Batch 500/913: Loss=6.5471 (C:5.0584, R:0.0100, T:1.4878(w:0.020)🚀)
Batch 525/913: Loss=5.4234 (C:4.9283, R:0.0100, T:0.4941(w:0.020)🎉)
Batch 550/913: Loss=5.8802 (C:4.9439, R:0.0099, T:0.9352(w:0.020)🎉)
Batch 575/913: Loss=10.5198 (C:4.8946, R:0.0100, T:5.6242(w:0.020)🚀)
Batch 600/913: Loss=5.5784 (C:4.9576, R:0.0100, T:0.6198(w:0.020)🎉)
Batch 625/913: Loss=7.3307 (C:5.0747, R:0.0100, T:2.2550(w:0.020)🚀)
Batch 650/913: Loss=5.2496 (C:4.8620, R:0.0100, T:0.3866(w:0.020)🎉)
Batch 675/913: Loss=6.0491 (C:4.9824, R:0.0099, T:1.0657(w:0.020)🚀)
Batch 700/913: Loss=8.1733 (C:4.8460, R:0.0100, T:3.3263(w:0.020)🚀)
Batch 725/913: Loss=5.8969 (C:4.8388, R:0.0100, T:1.0571(w:0.020)🚀)
Batch 750/913: Loss=5.1805 (C:4.8962, R:0.0100, T:0.2833(w:0.020)🎉)
Batch 775/913: Loss=6.8266 (C:4.8974, R:0.0099, T:1.9282(w:0.020)🚀)
Batch 800/913: Loss=5.3223 (C:4.8409, R:0.0099, T:0.4805(w:0.020)🎉)
Batch 825/913: Loss=5.6688 (C:4.8733, R:0.0099, T:0.7945(w:0.020)🎉)
Batch 850/913: Loss=5.7070 (C:4.9345, R:0.0100, T:0.7714(w:0.020)🎉)
Batch 875/913: Loss=6.6945 (C:4.9232, R:0.0100, T:1.7704(w:0.020)🚀)
Batch 900/913: Loss=5.2882 (C:4.8075, R:0.0100, T:0.4797(w:0.020)🎉)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 6.5018
  Contrastive: 4.9374
  Reconstruction: 0.0100
  Topological: 1.5634 (weight: 0.020)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 145.4518
  Contrastive: 2.4071
  Reconstruction: 0.0100
  Topological: 143.0438 (weight: 0.020)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 21/50 COMPLETE (161.9s)
Train Loss: 6.5018 (C:4.9374, R:0.0100, T:1.5634)
Val Loss:   145.4518 (C:2.4071, R:0.0100, T:143.0438)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 913 | Topological Weight: 0.0210
🌱 Early topological learning
============================================================
Batch   0/913: Loss=6.1573 (C:4.7560, R:0.0100, T:1.4003(w:0.021)🚀)
Batch  25/913: Loss=5.2020 (C:4.8573, R:0.0099, T:0.3437(w:0.021)🎉)
Batch  50/913: Loss=5.7046 (C:4.9072, R:0.0100, T:0.7964(w:0.021)🎉)
Batch  75/913: Loss=7.0677 (C:4.9159, R:0.0099, T:2.1507(w:0.021)🚀)
Batch 100/913: Loss=8.9062 (C:4.9785, R:0.0099, T:3.9267(w:0.021)🚀)
Batch 125/913: Loss=5.8326 (C:5.0531, R:0.0099, T:0.7785(w:0.021)🎉)
Batch 150/913: Loss=6.4438 (C:4.8376, R:0.0099, T:1.6052(w:0.021)🚀)
Batch 175/913: Loss=6.7001 (C:4.8862, R:0.0099, T:1.8129(w:0.021)🚀)
Batch 200/913: Loss=5.3469 (C:5.0214, R:0.0099, T:0.3245(w:0.021)🎉)
Batch 225/913: Loss=5.4310 (C:4.9110, R:0.0100, T:0.5189(w:0.021)🎉)
Batch 250/913: Loss=5.5260 (C:4.9018, R:0.0100, T:0.6232(w:0.021)🎉)
Batch 275/913: Loss=8.5289 (C:4.8029, R:0.0100, T:3.7250(w:0.021)🚀)
Batch 300/913: Loss=5.9925 (C:4.9423, R:0.0100, T:1.0491(w:0.021)🚀)
Batch 325/913: Loss=5.8742 (C:5.0162, R:0.0100, T:0.8570(w:0.021)🎉)
Batch 350/913: Loss=7.0244 (C:4.8210, R:0.0099, T:2.2024(w:0.021)🚀)
Batch 375/913: Loss=6.7299 (C:4.8496, R:0.0100, T:1.8793(w:0.021)🚀)
Batch 400/913: Loss=6.3697 (C:4.9058, R:0.0100, T:1.4630(w:0.021)🚀)
Batch 425/913: Loss=8.7232 (C:4.8665, R:0.0099, T:3.8557(w:0.021)🚀)
Batch 450/913: Loss=8.1038 (C:5.0563, R:0.0100, T:3.0464(w:0.021)🚀)
Batch 475/913: Loss=6.1067 (C:4.8486, R:0.0100, T:1.2571(w:0.021)🚀)
Batch 500/913: Loss=5.5541 (C:4.9936, R:0.0100, T:0.5594(w:0.021)🎉)
Batch 525/913: Loss=6.3533 (C:4.9521, R:0.0100, T:1.4002(w:0.021)🚀)
Batch 550/913: Loss=5.8178 (C:5.0456, R:0.0100, T:0.7712(w:0.021)🎉)
Batch 575/913: Loss=5.7949 (C:4.9522, R:0.0100, T:0.8417(w:0.021)🎉)
Batch 600/913: Loss=6.6403 (C:4.9090, R:0.0099, T:1.7304(w:0.021)🚀)
Batch 625/913: Loss=6.6974 (C:5.0552, R:0.0099, T:1.6413(w:0.021)🚀)
Batch 650/913: Loss=6.9796 (C:4.8996, R:0.0100, T:2.0790(w:0.021)🚀)
Batch 675/913: Loss=5.6218 (C:5.0738, R:0.0100, T:0.5470(w:0.021)🎉)
Batch 700/913: Loss=5.5823 (C:5.1301, R:0.0100, T:0.4512(w:0.021)🎉)
Batch 725/913: Loss=5.7835 (C:5.0460, R:0.0099, T:0.7365(w:0.021)🎉)
Batch 750/913: Loss=6.8057 (C:5.0349, R:0.0099, T:1.7698(w:0.021)🚀)
Batch 775/913: Loss=5.3439 (C:5.0649, R:0.0100, T:0.2780(w:0.021)🎉)
Batch 800/913: Loss=8.0498 (C:5.2029, R:0.0100, T:2.8459(w:0.021)🚀)
Batch 825/913: Loss=5.7566 (C:5.0309, R:0.0100, T:0.7247(w:0.021)🎉)
Batch 850/913: Loss=9.2198 (C:4.9489, R:0.0100, T:4.2699(w:0.021)🚀)
Batch 875/913: Loss=8.8134 (C:5.0772, R:0.0100, T:3.7352(w:0.021)🚀)
Batch 900/913: Loss=6.7767 (C:5.0111, R:0.0099, T:1.7646(w:0.021)🚀)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 6.5612
  Contrastive: 4.9703
  Reconstruction: 0.0100
  Topological: 1.5899 (weight: 0.021)
  Batches with topology: 913/913 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 152.5526
  Contrastive: 2.3121
  Reconstruction: 0.0100
  Topological: 150.2395 (weight: 0.021)
  Batches with topology: 913/913 (100.0%)

🎯 EPOCH 22/50 COMPLETE (160.8s)
Train Loss: 6.5612 (C:4.9703, R:0.0100, T:1.5899)
Val Loss:   152.5526 (C:2.3121, R:0.0100, T:150.2395)
🚀 Good topological learning progress
------------------------------------------------------------

🛑 Early stopping triggered after 22 epochs
Best model was at epoch 12 with Val Loss: 80.5211

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 11
Epochs with topology: 12/22
Max consecutive topology epochs: 12
Best topological loss: 1.4762
Final topological loss: 1.5899
✅ SUCCESS: Topological learning achieved!
👍 GOOD: Fairly consistent topological learning (>50%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_114819/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/916 batches
  Processed 51/916 batches
  Processed 101/916 batches
  Processed 151/916 batches
  Processed 201/916 batches
  Processed 251/916 batches
  Processed 301/916 batches
  Processed 351/916 batches
  Processed 401/916 batches
  Processed 451/916 batches
  Processed 501/916 batches
  Processed 551/916 batches
  Processed 601/916 batches
  Processed 651/916 batches
  Processed 701/916 batches
  Processed 751/916 batches
  Processed 801/916 batches
  Processed 851/916 batches
  Processed 901/916 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: -0.1374
  Adjusted Rand Score: 0.0145
  Clustering Accuracy: 0.3981
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/913 batches
  Processed 51/913 batches
  Processed 101/913 batches
  Processed 151/913 batches
  Processed 201/913 batches
  Processed 251/913 batches
  Processed 301/913 batches
  Processed 351/913 batches
  Processed 401/913 batches
  Processed 451/913 batches
  Processed 501/913 batches
  Processed 551/913 batches
  Processed 601/913 batches
  Processed 651/913 batches
  Processed 701/913 batches
  Processed 751/913 batches
  Processed 801/913 batches
  Processed 851/913 batches
  Processed 901/913 batches
Extracted representations: torch.Size([547800, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/913 batches
  Processed 51/913 batches
  Processed 101/913 batches
  Processed 151/913 batches
  Processed 201/913 batches
  Processed 251/913 batches
  Processed 301/913 batches
  Processed 351/913 batches
  Processed 401/913 batches
  Processed 451/913 batches
  Processed 501/913 batches
  Processed 551/913 batches
  Processed 601/913 batches
  Processed 651/913 batches
  Processed 701/913 batches
  Processed 751/913 batches
  Processed 801/913 batches
  Processed 851/913 batches
  Processed 901/913 batches
Extracted representations: torch.Size([547800, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.5352
  Per-class F1: [0.5543868992144855, 0.38389694041867956, 0.6618888358018793]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009954
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 0.356 ± 0.613
  Negative distances: 0.402 ± 0.642
  Separation ratio: 1.13x
  Gap: -2.422
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: -0.1374
  Clustering Accuracy: 0.3981
  Adjusted Rand Score: 0.0145

Classification Performance:
  Accuracy: 0.5352

Separation Quality:
  Separation Ratio: 1.13x
  Gap: -2.422
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009954
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_114819/results/evaluation_results_20250721_122951.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_114819/results/evaluation_results_20250721_122951.json

Key Results:
  Separation ratio: 1.13x
  Perfect separation: False
  Classification accuracy: 0.5352

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 22
  Epochs with topological learning: 12
  Current topological loss: 1.5899
  Current topological weight: 0.0210
  ⚠️  Topological loss is increasing (may need tuning)
✅ GOOD: Reasonable topological learning
Final topological loss: 1.5899
Epochs with topology: 12/22
⚠️  Poor clustering accuracy: 0.398

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_114819/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_114819

Analysis completed with exit code: 0
Time: Mon 21 Jul 12:29:54 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
