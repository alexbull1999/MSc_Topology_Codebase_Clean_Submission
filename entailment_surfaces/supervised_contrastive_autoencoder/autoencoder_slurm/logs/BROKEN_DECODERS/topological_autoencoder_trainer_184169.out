Starting Surface Distance Metric Analysis job...
Job ID: 184169
Node: gpuvm13
Time: Sun 20 Jul 10:09:45 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sun Jul 20 10:09:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_100957
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_100957/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,852,466
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,852,466
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 1.0
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=2.0011 (C:2.0000, R:0.0110, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.7149 (C:1.7139, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.5729 (C:1.5719, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.5666 (C:1.5656, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.5078 (C:1.5068, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.5461 (C:1.5451, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.4508 (C:1.4498, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.4614 (C:1.4604, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.5017 (C:1.5007, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.4556 (C:1.4546, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.4632 (C:1.4622, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.4361 (C:1.4352, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3998 (C:1.3989, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.4094 (C:1.4084, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.4076 (C:1.4066, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5008
  Contrastive: 1.4998
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3819
  Contrastive: 1.3809
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (20.3s)
Train Loss: 1.5008 (C:1.4998, R:0.0100, T:0.0000)
Val Loss:   1.3819 (C:1.3809, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.4148 (C:1.4138, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3954 (C:1.3944, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3840 (C:1.3830, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.4056 (C:1.4046, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3815 (C:1.3805, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.4060 (C:1.4050, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3711 (C:1.3701, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.4118 (C:1.4108, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3861 (C:1.3851, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3851 (C:1.3841, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.4052 (C:1.4042, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3702 (C:1.3692, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3844 (C:1.3834, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3819 (C:1.3809, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.4289 (C:1.4279, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.3892
  Contrastive: 1.3882
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3216
  Contrastive: 1.3206
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (21.0s)
Train Loss: 1.3892 (C:1.3882, R:0.0100, T:0.0000)
Val Loss:   1.3216 (C:1.3206, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.4210 (C:1.4200, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2948 (C:1.2938, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3901 (C:1.3892, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3704 (C:1.3694, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3665 (C:1.3655, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3587 (C:1.3578, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3539 (C:1.3529, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3111 (C:1.3101, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3549 (C:1.3539, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3960 (C:1.3950, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2966 (C:1.2956, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3530 (C:1.3520, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3487 (C:1.3477, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3443 (C:1.3433, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3328 (C:1.3318, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.3534
  Contrastive: 1.3524
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2792
  Contrastive: 1.2782
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (22.1s)
Train Loss: 1.3534 (C:1.3524, R:0.0100, T:0.0000)
Val Loss:   1.2792 (C:1.2782, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3147 (C:1.3137, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3060 (C:1.3050, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3480 (C:1.3470, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3222 (C:1.3212, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3681 (C:1.3671, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2958 (C:1.2949, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3609 (C:1.3599, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3547 (C:1.3537, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3076 (C:1.3066, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3875 (C:1.3865, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3278 (C:1.3268, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3303 (C:1.3293, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2995 (C:1.2985, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3386 (C:1.3376, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3572 (C:1.3562, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.3285
  Contrastive: 1.3275
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2458
  Contrastive: 1.2449
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (21.4s)
Train Loss: 1.3285 (C:1.3275, R:0.0100, T:0.0000)
Val Loss:   1.2458 (C:1.2449, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2761 (C:1.2751, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3696 (C:1.3686, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3090 (C:1.3080, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3341 (C:1.3331, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2934 (C:1.2924, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2518 (C:1.2508, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2961 (C:1.2951, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3049 (C:1.3039, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3146 (C:1.3136, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3267 (C:1.3257, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2704 (C:1.2694, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3428 (C:1.3418, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3183 (C:1.3174, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3176 (C:1.3166, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3382 (C:1.3372, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.3204
  Contrastive: 1.3194
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2620
  Contrastive: 1.2610
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 5/50 COMPLETE (21.5s)
Train Loss: 1.3204 (C:1.3194, R:0.0100, T:0.0000)
Val Loss:   1.2620 (C:1.2610, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3603 (C:1.3593, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3163 (C:1.3153, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3050 (C:1.3040, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3439 (C:1.3429, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3388 (C:1.3379, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3572 (C:1.3562, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3566 (C:1.3556, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3660 (C:1.3650, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3464 (C:1.3454, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3049 (C:1.3039, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3079 (C:1.3069, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3133 (C:1.3123, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2642 (C:1.2632, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3290 (C:1.3280, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2845 (C:1.2835, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.3067
  Contrastive: 1.3057
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2143
  Contrastive: 1.2133
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (21.4s)
Train Loss: 1.3067 (C:1.3057, R:0.0100, T:0.0000)
Val Loss:   1.2143 (C:1.2133, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3000 (C:1.2990, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2850 (C:1.2840, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3064 (C:1.3054, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3002 (C:1.2992, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2696 (C:1.2686, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2825 (C:1.2815, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2857 (C:1.2847, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2186 (C:1.2176, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2008 (C:1.1998, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3790 (C:1.3780, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3479 (C:1.3469, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2841 (C:1.2831, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2994 (C:1.2984, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2773 (C:1.2763, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3262 (C:1.3252, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.2893
  Contrastive: 1.2883
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2189
  Contrastive: 1.2179
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 7/50 COMPLETE (20.2s)
Train Loss: 1.2893 (C:1.2883, R:0.0100, T:0.0000)
Val Loss:   1.2189 (C:1.2179, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2237 (C:1.2227, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2943 (C:1.2933, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3191 (C:1.3181, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3065 (C:1.3055, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3167 (C:1.3158, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2929 (C:1.2919, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3420 (C:1.3410, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2591 (C:1.2581, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2479 (C:1.2470, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2444 (C:1.2434, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2548 (C:1.2538, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2846 (C:1.2837, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2582 (C:1.2572, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2952 (C:1.2942, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2855 (C:1.2845, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.2838
  Contrastive: 1.2828
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2285
  Contrastive: 1.2275
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 8/50 COMPLETE (21.3s)
Train Loss: 1.2838 (C:1.2828, R:0.0100, T:0.0000)
Val Loss:   1.2285 (C:1.2275, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2399 (C:1.2390, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2553 (C:1.2543, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2742 (C:1.2732, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2459 (C:1.2449, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2875 (C:1.2865, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2460 (C:1.2450, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2518 (C:1.2508, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2750 (C:1.2740, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2699 (C:1.2689, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2140 (C:1.2130, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3496 (C:1.3486, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2875 (C:1.2865, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3012 (C:1.3002, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2489 (C:1.2479, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2928 (C:1.2918, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.2686
  Contrastive: 1.2676
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1964
  Contrastive: 1.1954
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 9/50 COMPLETE (21.2s)
Train Loss: 1.2686 (C:1.2676, R:0.0100, T:0.0000)
Val Loss:   1.1964 (C:1.1954, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2662 (C:1.2652, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2152 (C:1.2143, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2822 (C:1.2812, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2625 (C:1.2615, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2391 (C:1.2381, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2868 (C:1.2858, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2749 (C:1.2739, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2487 (C:1.2477, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3179 (C:1.3169, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2824 (C:1.2814, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2696 (C:1.2686, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2378 (C:1.2368, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2764 (C:1.2754, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2700 (C:1.2690, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2988 (C:1.2978, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.2648
  Contrastive: 1.2638
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1960
  Contrastive: 1.1950
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 10/50 COMPLETE (21.1s)
Train Loss: 1.2648 (C:1.2638, R:0.0100, T:0.0000)
Val Loss:   1.1960 (C:1.1950, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=7158.9072 (C:1.2048, R:0.0100, T:7157.7012(w:1.000)⚠️)
Batch  25/365: Loss=2410.4153 (C:16.8317, R:0.0099, T:2393.5825(w:1.000)⚠️)
Batch  50/365: Loss=292.5153 (C:41.8936, R:0.0099, T:250.6207(w:1.000)⚠️)
Batch  75/365: Loss=474.4327 (C:37.2360, R:0.0099, T:437.1958(w:1.000)⚠️)
Batch 100/365: Loss=57.6625 (C:42.6840, R:0.0099, T:14.9774(w:1.000)⚠️)
Batch 125/365: Loss=173.9751 (C:39.3134, R:0.0099, T:134.6607(w:1.000)⚠️)
Batch 150/365: Loss=96.6892 (C:40.5905, R:0.0100, T:56.0977(w:1.000)⚠️)
Batch 175/365: Loss=189.7032 (C:46.5950, R:0.0099, T:143.1072(w:1.000)⚠️)
Batch 200/365: Loss=147.5652 (C:42.1892, R:0.0100, T:105.3750(w:1.000)⚠️)
Batch 225/365: Loss=338.0732 (C:34.0056, R:0.0100, T:304.0667(w:1.000)⚠️)
Batch 250/365: Loss=173.6262 (C:36.5108, R:0.0100, T:137.1144(w:1.000)⚠️)
Batch 275/365: Loss=431.3574 (C:41.8774, R:0.0099, T:389.4789(w:1.000)⚠️)
Batch 300/365: Loss=81.3581 (C:45.9020, R:0.0099, T:35.4551(w:1.000)⚠️)
Batch 325/365: Loss=457.2199 (C:44.3098, R:0.0100, T:412.9091(w:1.000)⚠️)
Batch 350/365: Loss=129.1652 (C:51.6929, R:0.0100, T:77.4713(w:1.000)⚠️)
🎉 MILESTONE: First topological learning detected at epoch 11!
   Initial topological loss: 644.0975
📈 New best topological loss: 644.0975

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 683.8687
  Contrastive: 39.7701
  Reconstruction: 0.0100
  Topological: 644.0975 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6523.0003
  Contrastive: 56.2910
  Reconstruction: 0.0100
  Topological: 6466.7083 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 11/50 COMPLETE (118.7s)
Train Loss: 683.8687 (C:39.7701, R:0.0100, T:644.0975)
Val Loss:   6523.0003 (C:56.2910, R:0.0100, T:6466.7083)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 1.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=378.1059 (C:61.3556, R:0.0099, T:316.7494(w:1.100)⚠️)
Batch  25/365: Loss=364.2833 (C:52.3058, R:0.0100, T:311.9766(w:1.100)⚠️)
Batch  50/365: Loss=199.7155 (C:44.0947, R:0.0100, T:155.6198(w:1.100)⚠️)
Batch  75/365: Loss=299.7373 (C:48.1910, R:0.0099, T:251.5453(w:1.100)⚠️)
Batch 100/365: Loss=104.8126 (C:38.3706, R:0.0099, T:66.4411(w:1.100)⚠️)
Batch 125/365: Loss=88.5488 (C:44.9170, R:0.0100, T:43.6309(w:1.100)⚠️)
Batch 150/365: Loss=141.9692 (C:38.5451, R:0.0099, T:103.4232(w:1.100)⚠️)
Batch 175/365: Loss=107.3456 (C:40.1509, R:0.0099, T:67.1937(w:1.100)⚠️)
Batch 200/365: Loss=124.7766 (C:44.6106, R:0.0100, T:80.1651(w:1.100)⚠️)
Batch 225/365: Loss=217.9891 (C:39.9293, R:0.0100, T:178.0588(w:1.100)⚠️)
Batch 250/365: Loss=232.2090 (C:36.6766, R:0.0099, T:195.5314(w:1.100)⚠️)
Batch 275/365: Loss=105.9575 (C:43.2953, R:0.0100, T:62.6612(w:1.100)⚠️)
Batch 300/365: Loss=115.0801 (C:35.1649, R:0.0100, T:79.9141(w:1.100)⚠️)
Batch 325/365: Loss=713.3411 (C:32.9945, R:0.0099, T:680.3456(w:1.100)⚠️)
Batch 350/365: Loss=660.9333 (C:34.7961, R:0.0099, T:626.1362(w:1.100)⚠️)
📈 New best topological loss: 213.4827

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 255.8049
  Contrastive: 42.3212
  Reconstruction: 0.0100
  Topological: 213.4827 (weight: 1.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6388.1064
  Contrastive: 19.9794
  Reconstruction: 0.0100
  Topological: 6368.1260 (weight: 1.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (123.6s)
Train Loss: 255.8049 (C:42.3212, R:0.0100, T:213.4827)
Val Loss:   6388.1064 (C:19.9794, R:0.0100, T:6368.1260)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 1.2000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=466.0895 (C:29.3344, R:0.0100, T:436.7541(w:1.200)⚠️)
Batch  25/365: Loss=382.4606 (C:22.0962, R:0.0099, T:360.3634(w:1.200)⚠️)
Batch  50/365: Loss=575.3503 (C:28.1662, R:0.0099, T:547.1831(w:1.200)⚠️)
Batch  75/365: Loss=116.6026 (C:32.1506, R:0.0100, T:84.4510(w:1.200)⚠️)
Batch 100/365: Loss=97.3939 (C:31.6000, R:0.0099, T:65.7930(w:1.200)⚠️)
Batch 125/365: Loss=349.1823 (C:28.0011, R:0.0099, T:321.1802(w:1.200)⚠️)
Batch 150/365: Loss=157.9049 (C:35.8156, R:0.0099, T:122.0883(w:1.200)⚠️)
Batch 175/365: Loss=212.4301 (C:32.6971, R:0.0100, T:179.7320(w:1.200)⚠️)
Batch 200/365: Loss=79.2783 (C:31.6380, R:0.0100, T:47.6393(w:1.200)⚠️)
Batch 225/365: Loss=190.7600 (C:35.5671, R:0.0099, T:155.1919(w:1.200)⚠️)
Batch 250/365: Loss=179.4819 (C:34.4955, R:0.0099, T:144.9854(w:1.200)⚠️)
Batch 275/365: Loss=85.9124 (C:36.1656, R:0.0099, T:49.7458(w:1.200)⚠️)
Batch 300/365: Loss=97.3735 (C:35.0869, R:0.0100, T:62.2856(w:1.200)⚠️)
Batch 325/365: Loss=160.1167 (C:36.1113, R:0.0099, T:124.0044(w:1.200)⚠️)
Batch 350/365: Loss=348.4305 (C:33.1652, R:0.0100, T:315.2643(w:1.200)⚠️)
📈 New best topological loss: 191.3427

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 223.5513
  Contrastive: 32.2076
  Reconstruction: 0.0100
  Topological: 191.3427 (weight: 1.200)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 3566.5064
  Contrastive: 22.1843
  Reconstruction: 0.0100
  Topological: 3544.3211 (weight: 1.200)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 13/50 COMPLETE (129.1s)
Train Loss: 223.5513 (C:32.2076, R:0.0100, T:191.3427)
Val Loss:   3566.5064 (C:22.1843, R:0.0100, T:3544.3211)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 1.3000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=114.2837 (C:33.8958, R:0.0100, T:80.3869(w:1.300)⚠️)
Batch  25/365: Loss=308.4088 (C:35.3541, R:0.0100, T:273.0537(w:1.300)⚠️)
Batch  50/365: Loss=85.5142 (C:41.5777, R:0.0099, T:43.9354(w:1.300)⚠️)
Batch  75/365: Loss=135.0675 (C:46.2130, R:0.0100, T:88.8536(w:1.300)⚠️)
Batch 100/365: Loss=326.7856 (C:45.0251, R:0.0100, T:281.7595(w:1.300)⚠️)
Batch 125/365: Loss=211.0818 (C:51.5056, R:0.0100, T:159.5752(w:1.300)⚠️)
Batch 150/365: Loss=146.8122 (C:51.2024, R:0.0100, T:95.6088(w:1.300)⚠️)
Batch 175/365: Loss=183.9672 (C:59.4595, R:0.0100, T:124.5066(w:1.300)⚠️)
Batch 200/365: Loss=65.6797 (C:53.3280, R:0.0100, T:12.3507(w:1.300)⚠️)
Batch 225/365: Loss=309.6569 (C:51.8867, R:0.0100, T:257.7692(w:1.300)⚠️)
Batch 250/365: Loss=180.6756 (C:54.8287, R:0.0100, T:125.8459(w:1.300)⚠️)
Batch 275/365: Loss=93.9825 (C:50.7725, R:0.0100, T:43.2090(w:1.300)⚠️)
Batch 300/365: Loss=946.4609 (C:63.7850, R:0.0100, T:882.6749(w:1.300)⚠️)
Batch 325/365: Loss=238.0786 (C:71.7114, R:0.0100, T:166.3663(w:1.300)⚠️)
Batch 350/365: Loss=377.9653 (C:76.9683, R:0.0099, T:300.9961(w:1.300)⚠️)
📈 New best topological loss: 188.5236

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 241.3838
  Contrastive: 52.8592
  Reconstruction: 0.0100
  Topological: 188.5236 (weight: 1.300)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 5974.0163
  Contrastive: 60.5544
  Reconstruction: 0.0100
  Topological: 5913.4608 (weight: 1.300)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 14/50 COMPLETE (121.3s)
Train Loss: 241.3838 (C:52.8592, R:0.0100, T:188.5236)
Val Loss:   5974.0163 (C:60.5544, R:0.0100, T:5913.4608)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 1.4000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=187.2237 (C:74.7495, R:0.0100, T:112.4733(w:1.400)⚠️)
Batch  25/365: Loss=254.1117 (C:68.5760, R:0.0100, T:185.5347(w:1.400)⚠️)
Batch  50/365: Loss=306.9485 (C:69.2773, R:0.0099, T:237.6702(w:1.400)⚠️)
Batch  75/365: Loss=177.6328 (C:70.7127, R:0.0100, T:106.9192(w:1.400)⚠️)
Batch 100/365: Loss=162.7533 (C:72.2813, R:0.0099, T:90.4711(w:1.400)⚠️)
Batch 125/365: Loss=360.3292 (C:74.0684, R:0.0100, T:286.2598(w:1.400)⚠️)
Batch 150/365: Loss=116.1699 (C:65.7836, R:0.0100, T:50.3853(w:1.400)⚠️)
Batch 175/365: Loss=280.6191 (C:56.9637, R:0.0099, T:223.6544(w:1.400)⚠️)
Batch 200/365: Loss=178.2760 (C:55.9338, R:0.0100, T:122.3413(w:1.400)⚠️)
Batch 225/365: Loss=261.7663 (C:59.6796, R:0.0100, T:202.0856(w:1.400)⚠️)
Batch 250/365: Loss=381.0230 (C:64.5546, R:0.0099, T:316.4674(w:1.400)⚠️)
Batch 275/365: Loss=82.3311 (C:58.1159, R:0.0100, T:24.2142(w:1.400)⚠️)
Batch 300/365: Loss=240.6811 (C:60.7456, R:0.0100, T:179.9346(w:1.400)⚠️)
Batch 325/365: Loss=289.5275 (C:61.5161, R:0.0099, T:228.0104(w:1.400)⚠️)
Batch 350/365: Loss=290.6199 (C:60.3037, R:0.0100, T:230.3152(w:1.400)⚠️)
📈 New best topological loss: 181.6719

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 246.2977
  Contrastive: 64.6248
  Reconstruction: 0.0100
  Topological: 181.6719 (weight: 1.400)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6871.9110
  Contrastive: 34.4349
  Reconstruction: 0.0100
  Topological: 6837.4752 (weight: 1.400)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 15/50 COMPLETE (123.4s)
Train Loss: 246.2977 (C:64.6248, R:0.0100, T:181.6719)
Val Loss:   6871.9110 (C:34.4349, R:0.0100, T:6837.4752)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 1.5000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=61.2719 (C:53.7086, R:0.0100, T:7.5623(w:1.500)🚀)
Batch  25/365: Loss=690.0471 (C:52.6773, R:0.0099, T:637.3688(w:1.500)⚠️)
Batch  50/365: Loss=68.3802 (C:58.3473, R:0.0099, T:10.0319(w:1.500)⚠️)
Batch  75/365: Loss=378.8594 (C:69.3305, R:0.0100, T:309.5280(w:1.500)⚠️)
Batch 100/365: Loss=550.1789 (C:62.6670, R:0.0100, T:487.5109(w:1.500)⚠️)
Batch 125/365: Loss=335.9207 (C:59.3434, R:0.0100, T:276.5763(w:1.500)⚠️)
Batch 150/365: Loss=304.5055 (C:54.7803, R:0.0100, T:249.7242(w:1.500)⚠️)
Batch 175/365: Loss=649.1852 (C:55.7261, R:0.0100, T:593.4581(w:1.500)⚠️)
Batch 200/365: Loss=87.6108 (C:50.7501, R:0.0100, T:36.8597(w:1.500)⚠️)
Batch 225/365: Loss=556.3104 (C:49.1984, R:0.0100, T:507.1110(w:1.500)⚠️)
Batch 250/365: Loss=268.1079 (C:50.8582, R:0.0099, T:217.2487(w:1.500)⚠️)
Batch 275/365: Loss=397.7051 (C:50.9891, R:0.0099, T:346.7151(w:1.500)⚠️)
Batch 300/365: Loss=156.1510 (C:44.7231, R:0.0100, T:111.4269(w:1.500)⚠️)
Batch 325/365: Loss=240.1930 (C:49.6696, R:0.0099, T:190.5224(w:1.500)⚠️)
Batch 350/365: Loss=196.5804 (C:51.7430, R:0.0100, T:144.8364(w:1.500)⚠️)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 242.2414
  Contrastive: 54.9258
  Reconstruction: 0.0100
  Topological: 187.3146 (weight: 1.500)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 8132.2707
  Contrastive: 35.7864
  Reconstruction: 0.0100
  Topological: 8096.4833 (weight: 1.500)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 16/50 COMPLETE (122.7s)
Train Loss: 242.2414 (C:54.9258, R:0.0100, T:187.3146)
Val Loss:   8132.2707 (C:35.7864, R:0.0100, T:8096.4833)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 1.6000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=71.0033 (C:52.1423, R:0.0100, T:18.8600(w:1.600)⚠️)
Batch  25/365: Loss=112.2534 (C:50.5195, R:0.0100, T:61.7329(w:1.600)⚠️)
Batch  50/365: Loss=199.3607 (C:54.2078, R:0.0099, T:145.1519(w:1.600)⚠️)
Batch  75/365: Loss=85.4364 (C:50.0677, R:0.0100, T:35.3677(w:1.600)⚠️)
Batch 100/365: Loss=295.6750 (C:44.8662, R:0.0099, T:250.8079(w:1.600)⚠️)
Batch 125/365: Loss=111.6160 (C:42.0990, R:0.0100, T:69.5160(w:1.600)⚠️)
Batch 150/365: Loss=176.9636 (C:44.8361, R:0.0100, T:132.1265(w:1.600)⚠️)
Batch 175/365: Loss=136.3166 (C:45.5894, R:0.0100, T:90.7262(w:1.600)⚠️)
Batch 200/365: Loss=67.3942 (C:44.5027, R:0.0100, T:22.8904(w:1.600)⚠️)
Batch 225/365: Loss=292.3721 (C:41.4251, R:0.0099, T:250.9460(w:1.600)⚠️)
Batch 250/365: Loss=228.2836 (C:37.5584, R:0.0100, T:190.7242(w:1.600)⚠️)
Batch 275/365: Loss=269.6776 (C:35.0228, R:0.0100, T:234.6538(w:1.600)⚠️)
Batch 300/365: Loss=214.3997 (C:32.7549, R:0.0099, T:181.6438(w:1.600)⚠️)
Batch 325/365: Loss=726.7839 (C:31.4112, R:0.0100, T:695.3718(w:1.600)⚠️)
Batch 350/365: Loss=64.3532 (C:38.1691, R:0.0099, T:26.1831(w:1.600)⚠️)
📈 New best topological loss: 155.6825

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 197.8553
  Contrastive: 42.1718
  Reconstruction: 0.0100
  Topological: 155.6825 (weight: 1.600)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 7442.1537
  Contrastive: 21.3456
  Reconstruction: 0.0100
  Topological: 7420.8071 (weight: 1.600)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 17/50 COMPLETE (127.0s)
Train Loss: 197.8553 (C:42.1718, R:0.0100, T:155.6825)
Val Loss:   7442.1537 (C:21.3456, R:0.0100, T:7420.8071)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 365 | Topological Weight: 1.7000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=201.9783 (C:38.6357, R:0.0099, T:163.3416(w:1.700)⚠️)
Batch  25/365: Loss=181.5695 (C:41.9279, R:0.0099, T:139.6407(w:1.700)⚠️)
Batch  50/365: Loss=261.6560 (C:48.1652, R:0.0099, T:213.4899(w:1.700)⚠️)
Batch  75/365: Loss=314.7823 (C:52.2599, R:0.0100, T:262.5214(w:1.700)⚠️)
Batch 100/365: Loss=180.6533 (C:48.0632, R:0.0100, T:132.5891(w:1.700)⚠️)
Batch 125/365: Loss=51.6140 (C:42.1388, R:0.0100, T:9.4743(w:1.700)🚀)
Batch 150/365: Loss=437.0919 (C:39.6290, R:0.0099, T:397.4619(w:1.700)⚠️)
Batch 175/365: Loss=534.2054 (C:40.9628, R:0.0099, T:493.2416(w:1.700)⚠️)
Batch 200/365: Loss=177.1200 (C:45.4012, R:0.0099, T:131.7178(w:1.700)⚠️)
Batch 225/365: Loss=251.1226 (C:43.8028, R:0.0100, T:207.3188(w:1.700)⚠️)
Batch 250/365: Loss=320.2761 (C:49.8262, R:0.0099, T:270.4489(w:1.700)⚠️)
Batch 275/365: Loss=1527.3153 (C:65.4555, R:0.0100, T:1461.8588(w:1.700)⚠️)
Batch 300/365: Loss=112.1002 (C:67.7023, R:0.0099, T:44.3969(w:1.700)⚠️)
Batch 325/365: Loss=1121.7495 (C:75.3718, R:0.0099, T:1046.3767(w:1.700)⚠️)
Batch 350/365: Loss=145.9103 (C:74.9366, R:0.0099, T:70.9727(w:1.700)⚠️)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 317.5115
  Contrastive: 51.9359
  Reconstruction: 0.0100
  Topological: 265.5745 (weight: 1.700)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 5298.7667
  Contrastive: 63.5586
  Reconstruction: 0.0100
  Topological: 5235.2072 (weight: 1.700)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 18/50 COMPLETE (124.2s)
Train Loss: 317.5115 (C:51.9359, R:0.0100, T:265.5745)
Val Loss:   5298.7667 (C:63.5586, R:0.0100, T:5235.2072)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 365 | Topological Weight: 1.8000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=310.9126 (C:84.4598, R:0.0099, T:226.4518(w:1.800)⚠️)
Batch  25/365: Loss=139.2619 (C:76.7996, R:0.0100, T:62.4614(w:1.800)⚠️)
Batch  50/365: Loss=152.4512 (C:77.6133, R:0.0100, T:74.8370(w:1.800)⚠️)
Batch  75/365: Loss=306.7528 (C:78.8108, R:0.0099, T:227.9410(w:1.800)⚠️)
Batch 100/365: Loss=209.8155 (C:74.6789, R:0.0100, T:135.1357(w:1.800)⚠️)
Batch 125/365: Loss=358.7588 (C:74.9931, R:0.0099, T:283.7646(w:1.800)⚠️)
Batch 150/365: Loss=189.2035 (C:75.6412, R:0.0100, T:113.5613(w:1.800)⚠️)
Batch 175/365: Loss=158.8707 (C:82.9642, R:0.0100, T:75.9056(w:1.800)⚠️)
Batch 200/365: Loss=770.5930 (C:68.8489, R:0.0100, T:701.7431(w:1.800)⚠️)
Batch 225/365: Loss=301.0390 (C:72.8558, R:0.0099, T:228.1822(w:1.800)⚠️)
Batch 250/365: Loss=95.7572 (C:71.7323, R:0.0100, T:24.0240(w:1.800)⚠️)
Batch 275/365: Loss=97.2400 (C:73.1821, R:0.0100, T:24.0569(w:1.800)⚠️)
Batch 300/365: Loss=284.2885 (C:65.0681, R:0.0099, T:219.2194(w:1.800)⚠️)
Batch 325/365: Loss=187.1847 (C:70.4990, R:0.0100, T:116.6846(w:1.800)⚠️)
Batch 350/365: Loss=91.4767 (C:57.4847, R:0.0100, T:33.9911(w:1.800)⚠️)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 294.5365
  Contrastive: 73.3042
  Reconstruction: 0.0100
  Topological: 221.2313 (weight: 1.800)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 5594.7433
  Contrastive: 38.8277
  Reconstruction: 0.0100
  Topological: 5555.9146 (weight: 1.800)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 19/50 COMPLETE (123.4s)
Train Loss: 294.5365 (C:73.3042, R:0.0100, T:221.2313)
Val Loss:   5594.7433 (C:38.8277, R:0.0100, T:5555.9146)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 365 | Topological Weight: 1.9000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=109.5735 (C:49.5319, R:0.0099, T:60.0405(w:1.900)⚠️)
Batch  25/365: Loss=255.0884 (C:55.7016, R:0.0099, T:199.3858(w:1.900)⚠️)
Batch  50/365: Loss=146.3666 (C:62.4028, R:0.0100, T:83.9628(w:1.900)⚠️)
Batch  75/365: Loss=136.0077 (C:74.0864, R:0.0100, T:61.9202(w:1.900)⚠️)
Batch 100/365: Loss=317.8955 (C:65.8916, R:0.0099, T:252.0029(w:1.900)⚠️)
Batch 125/365: Loss=357.8533 (C:61.9318, R:0.0099, T:295.9205(w:1.900)⚠️)
Batch 150/365: Loss=383.5134 (C:69.9331, R:0.0100, T:313.5794(w:1.900)⚠️)
Batch 175/365: Loss=282.5171 (C:72.8709, R:0.0100, T:209.6452(w:1.900)⚠️)
Batch 200/365: Loss=655.3059 (C:75.2940, R:0.0099, T:580.0109(w:1.900)⚠️)
Batch 225/365: Loss=83.9863 (C:77.6615, R:0.0100, T:6.3238(w:1.900)🚀)
Batch 250/365: Loss=681.0590 (C:77.5140, R:0.0099, T:603.5440(w:1.900)⚠️)
Batch 275/365: Loss=986.7792 (C:87.7329, R:0.0099, T:899.0453(w:1.900)⚠️)
Batch 300/365: Loss=535.9352 (C:80.4985, R:0.0100, T:455.4357(w:1.900)⚠️)
Batch 325/365: Loss=101.9884 (C:72.2067, R:0.0100, T:29.7807(w:1.900)⚠️)
Batch 350/365: Loss=513.5823 (C:77.2529, R:0.0100, T:436.3284(w:1.900)⚠️)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 344.7135
  Contrastive: 71.1909
  Reconstruction: 0.0100
  Topological: 273.5216 (weight: 1.900)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6250.5369
  Contrastive: 56.9125
  Reconstruction: 0.0100
  Topological: 6193.6234 (weight: 1.900)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 20/50 COMPLETE (123.3s)
Train Loss: 344.7135 (C:71.1909, R:0.0100, T:273.5216)
Val Loss:   6250.5369 (C:56.9125, R:0.0100, T:6193.6234)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 365 | Topological Weight: 2.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=269.5731 (C:72.3383, R:0.0099, T:197.2338(w:2.000)⚠️)
Batch  25/365: Loss=292.2012 (C:77.9439, R:0.0099, T:214.2563(w:2.000)⚠️)
Batch  50/365: Loss=384.8178 (C:79.4418, R:0.0100, T:305.3750(w:2.000)⚠️)
Batch  75/365: Loss=298.5252 (C:69.2005, R:0.0100, T:229.3237(w:2.000)⚠️)
Batch 100/365: Loss=272.1876 (C:72.5303, R:0.0100, T:199.6563(w:2.000)⚠️)
Batch 125/365: Loss=557.1310 (C:70.9013, R:0.0099, T:486.2286(w:2.000)⚠️)
Batch 150/365: Loss=154.7666 (C:67.1211, R:0.0099, T:87.6445(w:2.000)⚠️)
Batch 175/365: Loss=342.4598 (C:67.3635, R:0.0099, T:275.0953(w:2.000)⚠️)
Batch 200/365: Loss=169.8600 (C:77.9630, R:0.0100, T:91.8961(w:2.000)⚠️)
Batch 225/365: Loss=970.8918 (C:73.5850, R:0.0100, T:897.3058(w:2.000)⚠️)
Batch 250/365: Loss=178.3289 (C:81.9345, R:0.0099, T:96.3933(w:2.000)⚠️)
Batch 275/365: Loss=442.7601 (C:81.7609, R:0.0099, T:360.9982(w:2.000)⚠️)
Batch 300/365: Loss=535.9272 (C:78.2527, R:0.0099, T:457.6735(w:2.000)⚠️)
Batch 325/365: Loss=710.7275 (C:72.1745, R:0.0100, T:638.5520(w:2.000)⚠️)
Batch 350/365: Loss=74.0156 (C:55.1151, R:0.0099, T:18.8995(w:2.000)⚠️)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 357.7545
  Contrastive: 72.4361
  Reconstruction: 0.0100
  Topological: 285.3173 (weight: 2.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 8933.7761
  Contrastive: 33.9892
  Reconstruction: 0.0100
  Topological: 8899.7859 (weight: 2.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 21/50 COMPLETE (122.8s)
Train Loss: 357.7545 (C:72.4361, R:0.0100, T:285.3173)
Val Loss:   8933.7761 (C:33.9892, R:0.0100, T:8899.7859)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 365 | Topological Weight: 2.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=476.8059 (C:56.8934, R:0.0100, T:419.9115(w:2.100)⚠️)
Batch  25/365: Loss=281.3386 (C:46.4543, R:0.0100, T:234.8832(w:2.100)⚠️)
Batch  50/365: Loss=264.4105 (C:44.1154, R:0.0099, T:220.2941(w:2.100)⚠️)
Batch  75/365: Loss=101.7974 (C:40.9333, R:0.0099, T:60.8631(w:2.100)⚠️)
Batch 100/365: Loss=196.9293 (C:38.6860, R:0.0099, T:158.2423(w:2.100)⚠️)
Batch 125/365: Loss=259.6539 (C:44.1067, R:0.0099, T:215.5462(w:2.100)⚠️)
Batch 150/365: Loss=271.6927 (C:43.5332, R:0.0099, T:228.1586(w:2.100)⚠️)
Batch 175/365: Loss=603.4683 (C:43.2455, R:0.0099, T:560.2219(w:2.100)⚠️)
Batch 200/365: Loss=74.6496 (C:37.4889, R:0.0100, T:37.1597(w:2.100)⚠️)
Batch 225/365: Loss=162.6923 (C:36.7167, R:0.0100, T:125.9746(w:2.100)⚠️)
Batch 250/365: Loss=348.8393 (C:41.8933, R:0.0099, T:306.9451(w:2.100)⚠️)
Batch 275/365: Loss=161.2235 (C:39.1508, R:0.0099, T:122.0718(w:2.100)⚠️)
Batch 300/365: Loss=750.6637 (C:38.2279, R:0.0100, T:712.4349(w:2.100)⚠️)
Batch 325/365: Loss=370.2498 (C:31.5455, R:0.0100, T:338.7032(w:2.100)⚠️)
Batch 350/365: Loss=454.4157 (C:31.9586, R:0.0100, T:422.4561(w:2.100)⚠️)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 288.1861
  Contrastive: 40.3992
  Reconstruction: 0.0100
  Topological: 247.7859 (weight: 2.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 10617.9005
  Contrastive: 8.6063
  Reconstruction: 0.0100
  Topological: 10609.2932 (weight: 2.100)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 22/50 COMPLETE (127.9s)
Train Loss: 288.1861 (C:40.3992, R:0.0100, T:247.7859)
Val Loss:   10617.9005 (C:8.6063, R:0.0100, T:10609.2932)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 23 | Batches: 365 | Topological Weight: 2.2000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=116.9729 (C:29.7069, R:0.0099, T:87.2650(w:2.200)⚠️)
Batch  25/365: Loss=305.3182 (C:31.4946, R:0.0099, T:273.8226(w:2.200)⚠️)
Batch  50/365: Loss=445.3704 (C:33.5586, R:0.0100, T:411.8108(w:2.200)⚠️)
Batch  75/365: Loss=195.9717 (C:39.5556, R:0.0100, T:156.4151(w:2.200)⚠️)
Batch 100/365: Loss=76.0597 (C:41.4891, R:0.0100, T:34.5696(w:2.200)⚠️)
Batch 125/365: Loss=819.8585 (C:56.1671, R:0.0099, T:763.6903(w:2.200)⚠️)
Batch 150/365: Loss=460.9208 (C:77.3985, R:0.0099, T:383.5213(w:2.200)⚠️)
Batch 175/365: Loss=3940.6692 (C:88.8628, R:0.0100, T:3851.8054(w:2.200)⚠️)
Batch 200/365: Loss=265.0076 (C:84.1039, R:0.0099, T:180.9028(w:2.200)⚠️)
Batch 225/365: Loss=682.6257 (C:90.6203, R:0.0099, T:592.0044(w:2.200)⚠️)
Batch 250/365: Loss=1044.6306 (C:90.9369, R:0.0099, T:953.6927(w:2.200)⚠️)
Batch 275/365: Loss=2393.3052 (C:95.4487, R:0.0100, T:2297.8555(w:2.200)⚠️)
Batch 300/365: Loss=753.1075 (C:83.0579, R:0.0100, T:670.0486(w:2.200)⚠️)
Batch 325/365: Loss=996.4734 (C:79.7125, R:0.0100, T:916.7600(w:2.200)⚠️)
Batch 350/365: Loss=133.3620 (C:84.2379, R:0.0099, T:49.1231(w:2.200)⚠️)

📊 EPOCH 23 TRAINING SUMMARY:
  Total Loss: 466.3003
  Contrastive: 67.7365
  Reconstruction: 0.0100
  Topological: 398.5627 (weight: 2.200)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6114.8231
  Contrastive: 46.4381
  Reconstruction: 0.0100
  Topological: 6068.3840 (weight: 2.200)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 23/50 COMPLETE (123.8s)
Train Loss: 466.3003 (C:67.7365, R:0.0100, T:398.5627)
Val Loss:   6114.8231 (C:46.4381, R:0.0100, T:6068.3840)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

🛑 Early stopping triggered after 23 epochs
Best model was at epoch 13 with Val Loss: 3566.5064

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 11
Epochs with topology: 13/23
Max consecutive topology epochs: 13
Best topological loss: 155.6825
Final topological loss: 398.5627
✅ SUCCESS: Topological learning achieved!
👍 GOOD: Fairly consistent topological learning (>50%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_100957/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/367 batches
  Processed 51/367 batches
  Processed 101/367 batches
  Processed 151/367 batches
  Processed 201/367 batches
  Processed 251/367 batches
  Processed 301/367 batches
  Processed 351/367 batches
Extracted representations: torch.Size([549367, 50])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: -0.0258
  Adjusted Rand Score: 0.0017
  Clustering Accuracy: 0.3541
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 50])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 50])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.3892
  Per-class F1: [0.43503392812264385, 0.3589901477832512, 0.35881716552470244]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009953
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 22.248 ± 16.341
  Negative distances: 22.420 ± 16.450
  Separation ratio: 1.01x
  Gap: -137.966
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: -0.0258
  Clustering Accuracy: 0.3541
  Adjusted Rand Score: 0.0017

Classification Performance:
  Accuracy: 0.3892

Separation Quality:
  Separation Ratio: 1.01x
  Gap: -137.966
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009953
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_100957/results/evaluation_results_20250720_104212.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_100957/results/evaluation_results_20250720_104212.json

Key Results:
  Separation ratio: 1.01x
  Perfect separation: False
  Classification accuracy: 0.3892

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 23
  Epochs with topological learning: 13
  Current topological loss: 398.5627
  Current topological weight: 2.2000
  ⚠️  Topological loss is increasing (may need tuning)
✅ GOOD: Reasonable topological learning
Final topological loss: 398.5627
Epochs with topology: 13/23
⚠️  Poor clustering accuracy: 0.354

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_100957/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_100957

Analysis completed with exit code: 0
Time: Sun 20 Jul 10:42:13 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
