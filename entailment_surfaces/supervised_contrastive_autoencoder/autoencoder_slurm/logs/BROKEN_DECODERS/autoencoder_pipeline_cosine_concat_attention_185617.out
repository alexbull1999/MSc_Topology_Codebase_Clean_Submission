Starting Surface Distance Metric Analysis job...
Job ID: 185617
Node: gpuvm13
Time: Thu 24 Jul 10:38:36 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Thu Jul 24 10:38:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-24 10:38:46.142367
Using device: cuda

Configuration:
  Embedding type: cosine_concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 200
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'cosine_concat'
Output dimension will be: 1537
GlobalDataLoader initialized:
  Embedding type: cosine_concat
  Output dimension: 1537
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating cosine_concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated cosine_concat embeddings: torch.Size([549367, 1537])
Generating embeddings for validation...
Generating cosine_concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9842, 1537])
Generating embeddings for test...
Generating cosine_concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9824, 1537])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1537])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1537])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1537])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1537
Updated model input_dim to: 1537
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
AttentionAutoencoder initialized:
  Input dim: 1537
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Attention Heads: 5
  Total parameters: 5,883,890
Model created with 5,883,890 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created: Adam (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 5,883,890
Starting training...
========================================
Starting Global Dataset Training...
============================================================

üåç Updating global dataset at epoch 1
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.100 ¬± 0.016
    Neg distances: 0.101 ¬± 0.016
    Separation ratio: 1.00x
    Gap: -0.183
    ‚ùå Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=2.2458 (C:2.2458, R:0.0113)
Batch  25/537: Loss=2.0316 (C:2.0316, R:0.0111)
Batch  50/537: Loss=2.0051 (C:2.0051, R:0.0110)
Batch  75/537: Loss=2.0010 (C:2.0010, R:0.0109)
Batch 100/537: Loss=1.9883 (C:1.9883, R:0.0108)
Batch 125/537: Loss=1.9838 (C:1.9838, R:0.0108)
Batch 150/537: Loss=1.9618 (C:1.9618, R:0.0108)
Batch 175/537: Loss=1.9503 (C:1.9503, R:0.0107)
Batch 200/537: Loss=1.9440 (C:1.9440, R:0.0107)
Batch 225/537: Loss=1.9258 (C:1.9258, R:0.0107)
Batch 250/537: Loss=1.9246 (C:1.9246, R:0.0107)
Batch 275/537: Loss=1.9245 (C:1.9245, R:0.0107)
Batch 300/537: Loss=1.9109 (C:1.9109, R:0.0107)
Batch 325/537: Loss=1.8959 (C:1.8959, R:0.0107)
Batch 350/537: Loss=1.8956 (C:1.8956, R:0.0107)
Batch 375/537: Loss=1.9017 (C:1.9017, R:0.0107)
Batch 400/537: Loss=1.8999 (C:1.8999, R:0.0107)
Batch 425/537: Loss=1.9075 (C:1.9075, R:0.0107)
Batch 450/537: Loss=1.9087 (C:1.9087, R:0.0107)
Batch 475/537: Loss=1.8930 (C:1.8930, R:0.0107)
Batch 500/537: Loss=1.8921 (C:1.8921, R:0.0107)
Batch 525/537: Loss=1.9029 (C:1.9029, R:0.0108)

============================================================
Epoch 1/200 completed in 34.5s
Train: Loss=1.9416 (C:1.9416, R:0.0108) Ratio=1.48x
Val:   Loss=1.8696 (C:1.8696, R:0.0106) Ratio=2.45x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8696)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.8872 (C:1.8872, R:0.0108)
Batch  25/537: Loss=1.9011 (C:1.9011, R:0.0107)
Batch  50/537: Loss=1.8717 (C:1.8717, R:0.0107)
Batch  75/537: Loss=1.8901 (C:1.8901, R:0.0107)
Batch 100/537: Loss=1.8869 (C:1.8869, R:0.0107)
Batch 125/537: Loss=1.8961 (C:1.8961, R:0.0107)
Batch 150/537: Loss=1.8883 (C:1.8883, R:0.0107)
Batch 175/537: Loss=1.8882 (C:1.8882, R:0.0107)
Batch 200/537: Loss=1.8915 (C:1.8915, R:0.0107)
Batch 225/537: Loss=1.8971 (C:1.8971, R:0.0107)
Batch 250/537: Loss=1.9005 (C:1.9005, R:0.0107)
Batch 275/537: Loss=1.8691 (C:1.8691, R:0.0107)
Batch 300/537: Loss=1.8983 (C:1.8983, R:0.0107)
Batch 325/537: Loss=1.8687 (C:1.8687, R:0.0107)
Batch 350/537: Loss=1.8891 (C:1.8891, R:0.0107)
Batch 375/537: Loss=1.8873 (C:1.8873, R:0.0107)
Batch 400/537: Loss=1.9008 (C:1.9008, R:0.0107)
Batch 425/537: Loss=1.8941 (C:1.8941, R:0.0107)
Batch 450/537: Loss=1.8801 (C:1.8801, R:0.0107)
Batch 475/537: Loss=1.8788 (C:1.8788, R:0.0108)
Batch 500/537: Loss=1.8803 (C:1.8803, R:0.0107)
Batch 525/537: Loss=1.8900 (C:1.8900, R:0.0107)

============================================================
Epoch 2/200 completed in 27.6s
Train: Loss=1.8862 (C:1.8862, R:0.0107) Ratio=2.04x
Val:   Loss=1.8660 (C:1.8660, R:0.0106) Ratio=2.62x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8660)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8838 (C:1.8838, R:0.0107)
Batch  25/537: Loss=1.8647 (C:1.8647, R:0.0107)
Batch  50/537: Loss=1.8920 (C:1.8920, R:0.0107)
Batch  75/537: Loss=1.8600 (C:1.8600, R:0.0107)
Batch 100/537: Loss=1.8971 (C:1.8971, R:0.0107)
Batch 125/537: Loss=1.8835 (C:1.8835, R:0.0107)
Batch 150/537: Loss=1.9024 (C:1.9024, R:0.0107)
Batch 175/537: Loss=1.8714 (C:1.8714, R:0.0107)
Batch 200/537: Loss=1.8859 (C:1.8859, R:0.0107)
Batch 225/537: Loss=1.8865 (C:1.8865, R:0.0107)
Batch 250/537: Loss=1.8779 (C:1.8779, R:0.0107)
Batch 275/537: Loss=1.8720 (C:1.8720, R:0.0107)
Batch 300/537: Loss=1.8890 (C:1.8890, R:0.0107)
Batch 325/537: Loss=1.8807 (C:1.8807, R:0.0107)
Batch 350/537: Loss=1.8822 (C:1.8822, R:0.0107)
Batch 375/537: Loss=1.8677 (C:1.8677, R:0.0107)
Batch 400/537: Loss=1.8571 (C:1.8571, R:0.0107)
Batch 425/537: Loss=1.8724 (C:1.8724, R:0.0107)
Batch 450/537: Loss=1.8816 (C:1.8816, R:0.0107)
Batch 475/537: Loss=1.8640 (C:1.8640, R:0.0107)
Batch 500/537: Loss=1.8564 (C:1.8564, R:0.0107)
Batch 525/537: Loss=1.8978 (C:1.8978, R:0.0107)

============================================================
Epoch 3/200 completed in 25.7s
Train: Loss=1.8747 (C:1.8747, R:0.0107) Ratio=2.29x
Val:   Loss=1.8597 (C:1.8597, R:0.0106) Ratio=2.73x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8597)
============================================================

üåç Updating global dataset at epoch 4
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.499 ¬± 0.747
    Neg distances: 1.369 ¬± 0.896
    Separation ratio: 2.74x
    Gap: -2.319
    ‚úÖ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.2806 (C:1.2806, R:0.0107)
Batch  25/537: Loss=1.1960 (C:1.1960, R:0.0107)
Batch  50/537: Loss=1.2425 (C:1.2425, R:0.0107)
Batch  75/537: Loss=1.2646 (C:1.2646, R:0.0107)
Batch 100/537: Loss=1.2491 (C:1.2491, R:0.0107)
Batch 125/537: Loss=1.2496 (C:1.2496, R:0.0107)
Batch 150/537: Loss=1.2081 (C:1.2081, R:0.0107)
Batch 175/537: Loss=1.2423 (C:1.2423, R:0.0107)
Batch 200/537: Loss=1.2332 (C:1.2332, R:0.0107)
Batch 225/537: Loss=1.2438 (C:1.2438, R:0.0107)
Batch 250/537: Loss=1.2720 (C:1.2720, R:0.0107)
Batch 275/537: Loss=1.1959 (C:1.1959, R:0.0107)
Batch 300/537: Loss=1.2603 (C:1.2603, R:0.0107)
Batch 325/537: Loss=1.2140 (C:1.2140, R:0.0107)
Batch 350/537: Loss=1.2366 (C:1.2366, R:0.0107)
Batch 375/537: Loss=1.2109 (C:1.2109, R:0.0107)
Batch 400/537: Loss=1.2648 (C:1.2648, R:0.0107)
Batch 425/537: Loss=1.2257 (C:1.2257, R:0.0107)
Batch 450/537: Loss=1.1973 (C:1.1973, R:0.0107)
Batch 475/537: Loss=1.2324 (C:1.2324, R:0.0107)
Batch 500/537: Loss=1.2401 (C:1.2401, R:0.0107)
Batch 525/537: Loss=1.2490 (C:1.2490, R:0.0107)

============================================================
Epoch 4/200 completed in 32.9s
Train: Loss=1.2413 (C:1.2413, R:0.0107) Ratio=2.48x
Val:   Loss=1.2145 (C:1.2145, R:0.0106) Ratio=2.75x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2145)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.2546 (C:1.2546, R:0.0107)
Batch  25/537: Loss=1.1892 (C:1.1892, R:0.0107)
Batch  50/537: Loss=1.2347 (C:1.2347, R:0.0107)
Batch  75/537: Loss=1.2083 (C:1.2083, R:0.0107)
Batch 100/537: Loss=1.1765 (C:1.1765, R:0.0107)
Batch 125/537: Loss=1.2354 (C:1.2354, R:0.0107)
Batch 150/537: Loss=1.2677 (C:1.2677, R:0.0107)
Batch 175/537: Loss=1.2161 (C:1.2161, R:0.0107)
Batch 200/537: Loss=1.2186 (C:1.2186, R:0.0107)
Batch 225/537: Loss=1.2448 (C:1.2448, R:0.0107)
Batch 250/537: Loss=1.2390 (C:1.2390, R:0.0107)
Batch 275/537: Loss=1.2343 (C:1.2343, R:0.0107)
Batch 300/537: Loss=1.2690 (C:1.2690, R:0.0107)
Batch 325/537: Loss=1.2142 (C:1.2142, R:0.0107)
Batch 350/537: Loss=1.1908 (C:1.1908, R:0.0107)
Batch 375/537: Loss=1.2073 (C:1.2073, R:0.0107)
Batch 400/537: Loss=1.2317 (C:1.2317, R:0.0107)
Batch 425/537: Loss=1.2555 (C:1.2555, R:0.0107)
Batch 450/537: Loss=1.1980 (C:1.1980, R:0.0107)
Batch 475/537: Loss=1.2055 (C:1.2055, R:0.0107)
Batch 500/537: Loss=1.1833 (C:1.1833, R:0.0107)
Batch 525/537: Loss=1.1824 (C:1.1824, R:0.0107)

============================================================
Epoch 5/200 completed in 25.5s
Train: Loss=1.2211 (C:1.2211, R:0.0107) Ratio=2.65x
Val:   Loss=1.2119 (C:1.2119, R:0.0106) Ratio=2.89x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2119)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.2106 (C:1.2106, R:0.0107)
Batch  25/537: Loss=1.2071 (C:1.2071, R:0.0107)
Batch  50/537: Loss=1.1956 (C:1.1956, R:0.0107)
Batch  75/537: Loss=1.2130 (C:1.2130, R:0.0107)
Batch 100/537: Loss=1.2235 (C:1.2235, R:0.0107)
Batch 125/537: Loss=1.1820 (C:1.1820, R:0.0107)
Batch 150/537: Loss=1.2668 (C:1.2668, R:0.0107)
Batch 175/537: Loss=1.2747 (C:1.2747, R:0.0107)
Batch 200/537: Loss=1.1843 (C:1.1843, R:0.0107)
Batch 225/537: Loss=1.2131 (C:1.2131, R:0.0107)
Batch 250/537: Loss=1.1904 (C:1.1904, R:0.0107)
Batch 275/537: Loss=1.2171 (C:1.2171, R:0.0107)
Batch 300/537: Loss=1.1740 (C:1.1740, R:0.0106)
Batch 325/537: Loss=1.2018 (C:1.2018, R:0.0107)
Batch 350/537: Loss=1.2226 (C:1.2226, R:0.0107)
Batch 375/537: Loss=1.2245 (C:1.2245, R:0.0107)
Batch 400/537: Loss=1.2203 (C:1.2203, R:0.0107)
Batch 425/537: Loss=1.1976 (C:1.1976, R:0.0107)
Batch 450/537: Loss=1.2083 (C:1.2083, R:0.0107)
Batch 475/537: Loss=1.1920 (C:1.1920, R:0.0108)
Batch 500/537: Loss=1.1852 (C:1.1852, R:0.0107)
Batch 525/537: Loss=1.1979 (C:1.1979, R:0.0107)

============================================================
Epoch 6/200 completed in 26.4s
Train: Loss=1.2074 (C:1.2074, R:0.0107) Ratio=2.78x
Val:   Loss=1.1864 (C:1.1864, R:0.0106) Ratio=2.93x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1864)
============================================================

üåç Updating global dataset at epoch 7
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.448 ¬± 0.734
    Neg distances: 1.429 ¬± 0.897
    Separation ratio: 3.19x
    Gap: -2.331
    ‚úÖ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.1076 (C:1.1076, R:0.0107)
Batch  25/537: Loss=1.1420 (C:1.1420, R:0.0107)
Batch  50/537: Loss=1.1357 (C:1.1357, R:0.0107)
Batch  75/537: Loss=1.1361 (C:1.1361, R:0.0107)
Batch 100/537: Loss=1.1459 (C:1.1459, R:0.0107)
Batch 125/537: Loss=1.1227 (C:1.1227, R:0.0107)
Batch 150/537: Loss=1.1306 (C:1.1306, R:0.0107)
Batch 175/537: Loss=1.1368 (C:1.1368, R:0.0107)
Batch 200/537: Loss=1.1659 (C:1.1659, R:0.0107)
Batch 225/537: Loss=1.1184 (C:1.1184, R:0.0107)
Batch 250/537: Loss=1.1240 (C:1.1240, R:0.0107)
Batch 275/537: Loss=1.1279 (C:1.1279, R:0.0107)
Batch 300/537: Loss=1.1402 (C:1.1402, R:0.0107)
Batch 325/537: Loss=1.1181 (C:1.1181, R:0.0107)
Batch 350/537: Loss=1.0973 (C:1.0973, R:0.0107)
Batch 375/537: Loss=1.1723 (C:1.1723, R:0.0107)
Batch 400/537: Loss=1.1366 (C:1.1366, R:0.0107)
Batch 425/537: Loss=1.1409 (C:1.1409, R:0.0107)
Batch 450/537: Loss=1.1322 (C:1.1322, R:0.0107)
Batch 475/537: Loss=1.1316 (C:1.1316, R:0.0107)
Batch 500/537: Loss=1.1263 (C:1.1263, R:0.0107)
Batch 525/537: Loss=1.1412 (C:1.1412, R:0.0107)

============================================================
Epoch 7/200 completed in 33.0s
Train: Loss=1.1381 (C:1.1381, R:0.0107) Ratio=2.90x
Val:   Loss=1.1332 (C:1.1332, R:0.0106) Ratio=2.96x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1332)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.1469 (C:1.1469, R:0.0107)
Batch  25/537: Loss=1.1240 (C:1.1240, R:0.0107)
Batch  50/537: Loss=1.1616 (C:1.1616, R:0.0107)
Batch  75/537: Loss=1.1073 (C:1.1073, R:0.0107)
Batch 100/537: Loss=1.1249 (C:1.1249, R:0.0107)
Batch 125/537: Loss=1.1543 (C:1.1543, R:0.0107)
Batch 150/537: Loss=1.1009 (C:1.1009, R:0.0108)
Batch 175/537: Loss=1.0738 (C:1.0738, R:0.0107)
Batch 200/537: Loss=1.1267 (C:1.1267, R:0.0107)
Batch 225/537: Loss=1.1036 (C:1.1036, R:0.0107)
Batch 250/537: Loss=1.1005 (C:1.1005, R:0.0107)
Batch 275/537: Loss=1.1048 (C:1.1048, R:0.0108)
Batch 300/537: Loss=1.1232 (C:1.1232, R:0.0108)
Batch 325/537: Loss=1.1146 (C:1.1146, R:0.0107)
Batch 350/537: Loss=1.0970 (C:1.0970, R:0.0107)
Batch 375/537: Loss=1.1424 (C:1.1424, R:0.0107)
Batch 400/537: Loss=1.1556 (C:1.1556, R:0.0107)
Batch 425/537: Loss=1.1366 (C:1.1366, R:0.0107)
Batch 450/537: Loss=1.1398 (C:1.1398, R:0.0107)
Batch 475/537: Loss=1.1617 (C:1.1617, R:0.0107)
Batch 500/537: Loss=1.1035 (C:1.1035, R:0.0107)
Batch 525/537: Loss=1.1332 (C:1.1332, R:0.0107)

============================================================
Epoch 8/200 completed in 25.9s
Train: Loss=1.1286 (C:1.1286, R:0.0107) Ratio=3.06x
Val:   Loss=1.1235 (C:1.1235, R:0.0106) Ratio=3.08x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1235)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.1099 (C:1.1099, R:0.0107)
Batch  25/537: Loss=1.1188 (C:1.1188, R:0.0107)
Batch  50/537: Loss=1.1450 (C:1.1450, R:0.0107)
Batch  75/537: Loss=1.0587 (C:1.0587, R:0.0107)
Batch 100/537: Loss=1.1642 (C:1.1642, R:0.0108)
Batch 125/537: Loss=1.1012 (C:1.1012, R:0.0107)
Batch 150/537: Loss=1.1743 (C:1.1743, R:0.0108)
Batch 175/537: Loss=1.1079 (C:1.1079, R:0.0107)
Batch 200/537: Loss=1.1332 (C:1.1332, R:0.0107)
Batch 225/537: Loss=1.1439 (C:1.1439, R:0.0107)
Batch 250/537: Loss=1.1072 (C:1.1072, R:0.0107)
Batch 275/537: Loss=1.1190 (C:1.1190, R:0.0107)
Batch 300/537: Loss=1.1527 (C:1.1527, R:0.0107)
Batch 325/537: Loss=1.1108 (C:1.1108, R:0.0107)
Batch 350/537: Loss=1.1357 (C:1.1357, R:0.0108)
Batch 375/537: Loss=1.1253 (C:1.1253, R:0.0107)
Batch 400/537: Loss=1.1032 (C:1.1032, R:0.0107)
Batch 425/537: Loss=1.1310 (C:1.1310, R:0.0107)
Batch 450/537: Loss=1.0914 (C:1.0914, R:0.0107)
Batch 475/537: Loss=1.1309 (C:1.1309, R:0.0107)
Batch 500/537: Loss=1.1266 (C:1.1266, R:0.0107)
Batch 525/537: Loss=1.0881 (C:1.0881, R:0.0107)

============================================================
Epoch 9/200 completed in 26.4s
Train: Loss=1.1237 (C:1.1237, R:0.0107) Ratio=3.14x
Val:   Loss=1.1288 (C:1.1288, R:0.0106) Ratio=3.04x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 10
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.419 ¬± 0.716
    Neg distances: 1.439 ¬± 0.886
    Separation ratio: 3.44x
    Gap: -2.315
    ‚úÖ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.1175 (C:1.1175, R:0.0107)
Batch  25/537: Loss=1.0911 (C:1.0911, R:0.0107)
Batch  50/537: Loss=1.1140 (C:1.1140, R:0.0107)
Batch  75/537: Loss=1.0853 (C:1.0853, R:0.0107)
Batch 100/537: Loss=1.0650 (C:1.0650, R:0.0107)
Batch 125/537: Loss=1.0879 (C:1.0879, R:0.0107)
Batch 150/537: Loss=1.0929 (C:1.0929, R:0.0108)
Batch 175/537: Loss=1.0843 (C:1.0843, R:0.0107)
Batch 200/537: Loss=1.0867 (C:1.0867, R:0.0107)
Batch 225/537: Loss=1.0844 (C:1.0844, R:0.0107)
Batch 250/537: Loss=1.0896 (C:1.0896, R:0.0107)
Batch 275/537: Loss=1.0855 (C:1.0855, R:0.0107)
Batch 300/537: Loss=1.1042 (C:1.1042, R:0.0107)
Batch 325/537: Loss=1.1004 (C:1.1004, R:0.0107)
Batch 350/537: Loss=1.1067 (C:1.1067, R:0.0107)
Batch 375/537: Loss=1.1404 (C:1.1404, R:0.0107)
Batch 400/537: Loss=1.1275 (C:1.1275, R:0.0107)
Batch 425/537: Loss=1.0771 (C:1.0771, R:0.0107)
Batch 450/537: Loss=1.1218 (C:1.1218, R:0.0107)
Batch 475/537: Loss=1.1084 (C:1.1084, R:0.0107)
Batch 500/537: Loss=1.0598 (C:1.0598, R:0.0108)
Batch 525/537: Loss=1.0928 (C:1.0928, R:0.0107)

============================================================
Epoch 10/200 completed in 34.1s
Train: Loss=1.0928 (C:1.0928, R:0.0107) Ratio=3.13x
Val:   Loss=1.0969 (C:1.0969, R:0.0106) Ratio=3.09x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0969)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.0820 (C:1.0820, R:0.0107)
Batch  25/537: Loss=1.0601 (C:1.0601, R:0.0107)
Batch  50/537: Loss=1.0423 (C:1.0423, R:0.0107)
Batch  75/537: Loss=1.1232 (C:1.1232, R:0.0107)
Batch 100/537: Loss=1.0812 (C:1.0812, R:0.0107)
Batch 125/537: Loss=1.0952 (C:1.0952, R:0.0107)
Batch 150/537: Loss=1.1264 (C:1.1264, R:0.0107)
Batch 175/537: Loss=1.1126 (C:1.1126, R:0.0107)
Batch 200/537: Loss=1.1409 (C:1.1409, R:0.0107)
Batch 225/537: Loss=1.0811 (C:1.0811, R:0.0107)
Batch 250/537: Loss=1.0555 (C:1.0555, R:0.0107)
Batch 275/537: Loss=1.1190 (C:1.1190, R:0.0107)
Batch 300/537: Loss=1.0537 (C:1.0537, R:0.0107)
Batch 325/537: Loss=1.0663 (C:1.0663, R:0.0107)
Batch 350/537: Loss=1.0808 (C:1.0808, R:0.0107)
Batch 375/537: Loss=1.0998 (C:1.0998, R:0.0107)
Batch 400/537: Loss=1.0975 (C:1.0975, R:0.0107)
Batch 425/537: Loss=1.0902 (C:1.0902, R:0.0107)
Batch 450/537: Loss=1.0980 (C:1.0980, R:0.0107)
Batch 475/537: Loss=1.1049 (C:1.1049, R:0.0107)
Batch 500/537: Loss=1.0971 (C:1.0971, R:0.0107)
Batch 525/537: Loss=1.1181 (C:1.1181, R:0.0107)

============================================================
Epoch 11/200 completed in 26.4s
Train: Loss=1.0909 (C:1.0909, R:0.0107) Ratio=3.17x
Val:   Loss=1.1023 (C:1.1023, R:0.0106) Ratio=3.09x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.0531 (C:1.0531, R:0.0108)
Batch  25/537: Loss=1.0605 (C:1.0605, R:0.0107)
Batch  50/537: Loss=1.1291 (C:1.1291, R:0.0107)
Batch  75/537: Loss=1.0522 (C:1.0522, R:0.0107)
Batch 100/537: Loss=1.0963 (C:1.0963, R:0.0107)
Batch 125/537: Loss=1.0473 (C:1.0473, R:0.0107)
Batch 150/537: Loss=1.0812 (C:1.0812, R:0.0107)
Batch 175/537: Loss=1.0802 (C:1.0802, R:0.0107)
Batch 200/537: Loss=1.0854 (C:1.0854, R:0.0107)
Batch 225/537: Loss=1.0995 (C:1.0995, R:0.0107)
Batch 250/537: Loss=1.1000 (C:1.1000, R:0.0107)
Batch 275/537: Loss=1.0393 (C:1.0393, R:0.0107)
Batch 300/537: Loss=1.1049 (C:1.1049, R:0.0107)
Batch 325/537: Loss=1.1002 (C:1.1002, R:0.0107)
Batch 350/537: Loss=1.0638 (C:1.0638, R:0.0107)
Batch 375/537: Loss=1.1094 (C:1.1094, R:0.0107)
Batch 400/537: Loss=1.0702 (C:1.0702, R:0.0107)
Batch 425/537: Loss=1.0739 (C:1.0739, R:0.0107)
Batch 450/537: Loss=1.1317 (C:1.1317, R:0.0107)
Batch 475/537: Loss=1.0905 (C:1.0905, R:0.0108)
Batch 500/537: Loss=1.0660 (C:1.0660, R:0.0107)
Batch 525/537: Loss=1.1050 (C:1.1050, R:0.0107)

============================================================
Epoch 12/200 completed in 26.4s
Train: Loss=1.0876 (C:1.0876, R:0.0107) Ratio=3.34x
Val:   Loss=1.1077 (C:1.1077, R:0.0106) Ratio=3.09x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 13
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.395 ¬± 0.706
    Neg distances: 1.469 ¬± 0.885
    Separation ratio: 3.72x
    Gap: -2.320
    ‚úÖ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=1.0556 (C:1.0556, R:0.0107)
Batch  25/537: Loss=1.0474 (C:1.0474, R:0.0107)
Batch  50/537: Loss=1.0728 (C:1.0728, R:0.0107)
Batch  75/537: Loss=1.0388 (C:1.0388, R:0.0108)
Batch 100/537: Loss=1.0613 (C:1.0613, R:0.0107)
Batch 125/537: Loss=1.0583 (C:1.0583, R:0.0107)
Batch 150/537: Loss=1.0140 (C:1.0140, R:0.0107)
Batch 175/537: Loss=1.0485 (C:1.0485, R:0.0107)
Batch 200/537: Loss=1.0937 (C:1.0937, R:0.0107)
Batch 225/537: Loss=1.0816 (C:1.0816, R:0.0107)
Batch 250/537: Loss=1.0749 (C:1.0749, R:0.0107)
Batch 275/537: Loss=1.0103 (C:1.0103, R:0.0107)
Batch 300/537: Loss=1.0449 (C:1.0449, R:0.0107)
Batch 325/537: Loss=1.0760 (C:1.0760, R:0.0107)
Batch 350/537: Loss=1.0315 (C:1.0315, R:0.0107)
Batch 375/537: Loss=1.0475 (C:1.0475, R:0.0107)
Batch 400/537: Loss=1.0794 (C:1.0794, R:0.0107)
Batch 425/537: Loss=1.0318 (C:1.0318, R:0.0107)
Batch 450/537: Loss=1.0645 (C:1.0645, R:0.0107)
Batch 475/537: Loss=1.0345 (C:1.0345, R:0.0107)
Batch 500/537: Loss=1.0519 (C:1.0519, R:0.0107)
Batch 525/537: Loss=1.0390 (C:1.0390, R:0.0107)

============================================================
Epoch 13/200 completed in 34.4s
Train: Loss=1.0533 (C:1.0533, R:0.0107) Ratio=3.28x
Val:   Loss=1.0702 (C:1.0702, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0702)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=1.0395 (C:1.0395, R:0.0107)
Batch  25/537: Loss=1.0723 (C:1.0723, R:0.0107)
Batch  50/537: Loss=1.0667 (C:1.0667, R:0.0107)
Batch  75/537: Loss=1.0670 (C:1.0670, R:0.0107)
Batch 100/537: Loss=1.0066 (C:1.0066, R:0.0107)
Batch 125/537: Loss=1.0560 (C:1.0560, R:0.0107)
Batch 150/537: Loss=1.0235 (C:1.0235, R:0.0107)
Batch 175/537: Loss=1.0413 (C:1.0413, R:0.0107)
Batch 200/537: Loss=1.0388 (C:1.0388, R:0.0107)
Batch 225/537: Loss=0.9955 (C:0.9955, R:0.0107)
Batch 250/537: Loss=1.0348 (C:1.0348, R:0.0107)
Batch 275/537: Loss=1.0900 (C:1.0900, R:0.0107)
Batch 300/537: Loss=1.0430 (C:1.0430, R:0.0107)
Batch 325/537: Loss=1.0759 (C:1.0759, R:0.0107)
Batch 350/537: Loss=1.0422 (C:1.0422, R:0.0107)
Batch 375/537: Loss=1.0258 (C:1.0258, R:0.0107)
Batch 400/537: Loss=1.0447 (C:1.0447, R:0.0107)
Batch 425/537: Loss=1.0744 (C:1.0744, R:0.0107)
Batch 450/537: Loss=1.0791 (C:1.0791, R:0.0107)
Batch 475/537: Loss=1.0255 (C:1.0255, R:0.0107)
Batch 500/537: Loss=1.0419 (C:1.0419, R:0.0107)
Batch 525/537: Loss=1.0515 (C:1.0515, R:0.0107)

============================================================
Epoch 14/200 completed in 26.7s
Train: Loss=1.0479 (C:1.0479, R:0.0107) Ratio=3.36x
Val:   Loss=1.0664 (C:1.0664, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0664)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=1.0197 (C:1.0197, R:0.0108)
Batch  25/537: Loss=1.0278 (C:1.0278, R:0.0107)
Batch  50/537: Loss=1.0911 (C:1.0911, R:0.0107)
Batch  75/537: Loss=1.0805 (C:1.0805, R:0.0107)
Batch 100/537: Loss=1.0546 (C:1.0546, R:0.0107)
Batch 125/537: Loss=1.0359 (C:1.0359, R:0.0107)
Batch 150/537: Loss=1.0318 (C:1.0318, R:0.0107)
Batch 175/537: Loss=1.0245 (C:1.0245, R:0.0108)
Batch 200/537: Loss=1.0429 (C:1.0429, R:0.0107)
Batch 225/537: Loss=1.0585 (C:1.0585, R:0.0107)
Batch 250/537: Loss=1.0389 (C:1.0389, R:0.0107)
Batch 275/537: Loss=1.0466 (C:1.0466, R:0.0107)
Batch 300/537: Loss=1.0241 (C:1.0241, R:0.0107)
Batch 325/537: Loss=1.0527 (C:1.0527, R:0.0107)
Batch 350/537: Loss=1.0819 (C:1.0819, R:0.0107)
Batch 375/537: Loss=1.0816 (C:1.0816, R:0.0107)
Batch 400/537: Loss=1.0489 (C:1.0489, R:0.0107)
Batch 425/537: Loss=1.0838 (C:1.0838, R:0.0107)
Batch 450/537: Loss=1.0499 (C:1.0499, R:0.0107)
Batch 475/537: Loss=1.0405 (C:1.0405, R:0.0107)
Batch 500/537: Loss=1.0374 (C:1.0374, R:0.0107)
Batch 525/537: Loss=1.0309 (C:1.0309, R:0.0107)

============================================================
Epoch 15/200 completed in 26.4s
Train: Loss=1.0446 (C:1.0446, R:0.0107) Ratio=3.43x
Val:   Loss=1.0712 (C:1.0712, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 16
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.412 ¬± 0.735
    Neg distances: 1.463 ¬± 0.893
    Separation ratio: 3.55x
    Gap: -2.318
    ‚úÖ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=1.0558 (C:1.0558, R:0.0107)
Batch  25/537: Loss=1.0696 (C:1.0696, R:0.0107)
Batch  50/537: Loss=1.0633 (C:1.0633, R:0.0107)
Batch  75/537: Loss=1.0543 (C:1.0543, R:0.0107)
Batch 100/537: Loss=1.0582 (C:1.0582, R:0.0107)
Batch 125/537: Loss=1.0314 (C:1.0314, R:0.0107)
Batch 150/537: Loss=1.0572 (C:1.0572, R:0.0107)
Batch 175/537: Loss=1.0314 (C:1.0314, R:0.0107)
Batch 200/537: Loss=1.0907 (C:1.0907, R:0.0107)
Batch 225/537: Loss=1.0565 (C:1.0565, R:0.0107)
Batch 250/537: Loss=1.0406 (C:1.0406, R:0.0107)
Batch 275/537: Loss=1.1238 (C:1.1238, R:0.0108)
Batch 300/537: Loss=1.0518 (C:1.0518, R:0.0107)
Batch 325/537: Loss=1.1007 (C:1.1007, R:0.0107)
Batch 350/537: Loss=1.0264 (C:1.0264, R:0.0107)
Batch 375/537: Loss=1.1007 (C:1.1007, R:0.0107)
Batch 400/537: Loss=1.0934 (C:1.0934, R:0.0107)
Batch 425/537: Loss=1.0580 (C:1.0580, R:0.0107)
Batch 450/537: Loss=1.0348 (C:1.0348, R:0.0107)
Batch 475/537: Loss=1.0563 (C:1.0563, R:0.0107)
Batch 500/537: Loss=1.0140 (C:1.0140, R:0.0108)
Batch 525/537: Loss=1.0860 (C:1.0860, R:0.0107)

============================================================
Epoch 16/200 completed in 34.5s
Train: Loss=1.0573 (C:1.0573, R:0.0107) Ratio=3.50x
Val:   Loss=1.0798 (C:1.0798, R:0.0106) Ratio=3.20x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=1.0655 (C:1.0655, R:0.0107)
Batch  25/537: Loss=1.0713 (C:1.0713, R:0.0107)
Batch  50/537: Loss=1.0676 (C:1.0676, R:0.0107)
Batch  75/537: Loss=1.0319 (C:1.0319, R:0.0107)
Batch 100/537: Loss=1.0577 (C:1.0577, R:0.0108)
Batch 125/537: Loss=1.0524 (C:1.0524, R:0.0107)
Batch 150/537: Loss=1.0611 (C:1.0611, R:0.0107)
Batch 175/537: Loss=1.0807 (C:1.0807, R:0.0107)
Batch 200/537: Loss=1.0732 (C:1.0732, R:0.0107)
Batch 225/537: Loss=1.0620 (C:1.0620, R:0.0106)
Batch 250/537: Loss=1.0680 (C:1.0680, R:0.0107)
Batch 275/537: Loss=1.0632 (C:1.0632, R:0.0107)
Batch 300/537: Loss=1.0831 (C:1.0831, R:0.0107)
Batch 325/537: Loss=1.0193 (C:1.0193, R:0.0108)
Batch 350/537: Loss=1.0249 (C:1.0249, R:0.0107)
Batch 375/537: Loss=1.0257 (C:1.0257, R:0.0107)
Batch 400/537: Loss=1.0438 (C:1.0438, R:0.0107)
Batch 425/537: Loss=1.0663 (C:1.0663, R:0.0107)
Batch 450/537: Loss=1.0462 (C:1.0462, R:0.0107)
Batch 475/537: Loss=1.0342 (C:1.0342, R:0.0107)
Batch 500/537: Loss=1.0586 (C:1.0586, R:0.0107)
Batch 525/537: Loss=1.0132 (C:1.0132, R:0.0107)

============================================================
Epoch 17/200 completed in 26.8s
Train: Loss=1.0532 (C:1.0532, R:0.0107) Ratio=3.48x
Val:   Loss=1.0864 (C:1.0864, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.000
No improvement for 3 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=1.0028 (C:1.0028, R:0.0107)
Batch  25/537: Loss=1.0392 (C:1.0392, R:0.0107)
Batch  50/537: Loss=1.1052 (C:1.1052, R:0.0107)
Batch  75/537: Loss=1.0269 (C:1.0269, R:0.0108)
Batch 100/537: Loss=1.0549 (C:1.0549, R:0.0107)
Batch 125/537: Loss=1.0394 (C:1.0394, R:0.0107)
Batch 150/537: Loss=1.0425 (C:1.0425, R:0.0107)
Batch 175/537: Loss=1.0599 (C:1.0599, R:0.0107)
Batch 200/537: Loss=1.0084 (C:1.0084, R:0.0107)
Batch 225/537: Loss=1.0426 (C:1.0426, R:0.0107)
Batch 250/537: Loss=1.0459 (C:1.0459, R:0.0107)
Batch 275/537: Loss=1.0699 (C:1.0699, R:0.0107)
Batch 300/537: Loss=1.0497 (C:1.0497, R:0.0107)
Batch 325/537: Loss=0.9927 (C:0.9927, R:0.0107)
Batch 350/537: Loss=1.0560 (C:1.0560, R:0.0107)
Batch 375/537: Loss=1.0440 (C:1.0440, R:0.0107)
Batch 400/537: Loss=1.0192 (C:1.0192, R:0.0107)
Batch 425/537: Loss=1.0155 (C:1.0155, R:0.0107)
Batch 450/537: Loss=1.0290 (C:1.0290, R:0.0107)
Batch 475/537: Loss=1.0710 (C:1.0710, R:0.0107)
Batch 500/537: Loss=1.0977 (C:1.0977, R:0.0107)
Batch 525/537: Loss=1.0697 (C:1.0697, R:0.0108)

============================================================
Epoch 18/200 completed in 26.2s
Train: Loss=1.0525 (C:1.0525, R:0.0107) Ratio=3.68x
Val:   Loss=1.0832 (C:1.0832, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 19
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.381 ¬± 0.716
    Neg distances: 1.488 ¬± 0.884
    Separation ratio: 3.90x
    Gap: -2.319
    ‚úÖ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.9963 (C:0.9963, R:0.0107)
Batch  25/537: Loss=0.9766 (C:0.9766, R:0.0107)
Batch  50/537: Loss=1.0600 (C:1.0600, R:0.0107)
Batch  75/537: Loss=1.0258 (C:1.0258, R:0.0107)
Batch 100/537: Loss=1.0198 (C:1.0198, R:0.0107)
Batch 125/537: Loss=1.0455 (C:1.0455, R:0.0107)
Batch 150/537: Loss=1.0311 (C:1.0311, R:0.0107)
Batch 175/537: Loss=0.9938 (C:0.9938, R:0.0107)
Batch 200/537: Loss=0.9985 (C:0.9985, R:0.0107)
Batch 225/537: Loss=1.0535 (C:1.0535, R:0.0107)
Batch 250/537: Loss=0.9898 (C:0.9898, R:0.0107)
Batch 275/537: Loss=1.0131 (C:1.0131, R:0.0107)
Batch 300/537: Loss=1.0264 (C:1.0264, R:0.0107)
Batch 325/537: Loss=1.0117 (C:1.0117, R:0.0107)
Batch 350/537: Loss=1.0322 (C:1.0322, R:0.0107)
Batch 375/537: Loss=1.0351 (C:1.0351, R:0.0107)
Batch 400/537: Loss=0.9837 (C:0.9837, R:0.0107)
Batch 425/537: Loss=1.0404 (C:1.0404, R:0.0107)
Batch 450/537: Loss=1.0332 (C:1.0332, R:0.0107)
Batch 475/537: Loss=1.0177 (C:1.0177, R:0.0107)
Batch 500/537: Loss=1.0452 (C:1.0452, R:0.0107)
Batch 525/537: Loss=1.0516 (C:1.0516, R:0.0107)

============================================================
Epoch 19/200 completed in 35.1s
Train: Loss=1.0189 (C:1.0189, R:0.0107) Ratio=3.61x
Val:   Loss=1.0585 (C:1.0585, R:0.0106) Ratio=3.19x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0585)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=1.0151 (C:1.0151, R:0.0107)
Batch  25/537: Loss=1.0309 (C:1.0309, R:0.0107)
Batch  50/537: Loss=1.0300 (C:1.0300, R:0.0107)
Batch  75/537: Loss=0.9963 (C:0.9963, R:0.0107)
Batch 100/537: Loss=0.9982 (C:0.9982, R:0.0107)
Batch 125/537: Loss=0.9991 (C:0.9991, R:0.0107)
Batch 150/537: Loss=1.0022 (C:1.0022, R:0.0107)
Batch 175/537: Loss=1.0025 (C:1.0025, R:0.0107)
Batch 200/537: Loss=0.9971 (C:0.9971, R:0.0107)
Batch 225/537: Loss=1.0390 (C:1.0390, R:0.0107)
Batch 250/537: Loss=0.9858 (C:0.9858, R:0.0107)
Batch 275/537: Loss=1.0304 (C:1.0304, R:0.0107)
Batch 300/537: Loss=1.0247 (C:1.0247, R:0.0107)
Batch 325/537: Loss=1.0393 (C:1.0393, R:0.0107)
Batch 350/537: Loss=0.9774 (C:0.9774, R:0.0107)
Batch 375/537: Loss=0.9794 (C:0.9794, R:0.0107)
Batch 400/537: Loss=1.0204 (C:1.0204, R:0.0107)
Batch 425/537: Loss=1.0328 (C:1.0328, R:0.0107)
Batch 450/537: Loss=1.0171 (C:1.0171, R:0.0108)
Batch 475/537: Loss=1.0045 (C:1.0045, R:0.0107)
Batch 500/537: Loss=1.0120 (C:1.0120, R:0.0107)
Batch 525/537: Loss=1.0147 (C:1.0147, R:0.0107)

============================================================
Epoch 20/200 completed in 26.3s
Train: Loss=1.0147 (C:1.0147, R:0.0107) Ratio=3.67x
Val:   Loss=1.0447 (C:1.0447, R:0.0106) Ratio=3.19x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0447)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.9839 (C:0.9839, R:0.0107)
Batch  25/537: Loss=1.0347 (C:1.0347, R:0.0107)
Batch  50/537: Loss=1.0597 (C:1.0597, R:0.0107)
Batch  75/537: Loss=1.0357 (C:1.0357, R:0.0107)
Batch 100/537: Loss=1.0194 (C:1.0194, R:0.0107)
Batch 125/537: Loss=1.0169 (C:1.0169, R:0.0107)
Batch 150/537: Loss=1.0453 (C:1.0453, R:0.0107)
Batch 175/537: Loss=1.0096 (C:1.0096, R:0.0107)
Batch 200/537: Loss=1.0255 (C:1.0255, R:0.0107)
Batch 225/537: Loss=1.0458 (C:1.0458, R:0.0107)
Batch 250/537: Loss=1.0324 (C:1.0324, R:0.0107)
Batch 275/537: Loss=1.0462 (C:1.0462, R:0.0107)
Batch 300/537: Loss=1.0100 (C:1.0100, R:0.0107)
Batch 325/537: Loss=1.0111 (C:1.0111, R:0.0106)
Batch 350/537: Loss=0.9672 (C:0.9672, R:0.0107)
Batch 375/537: Loss=0.9786 (C:0.9786, R:0.0107)
Batch 400/537: Loss=1.0258 (C:1.0258, R:0.0107)
Batch 425/537: Loss=0.9980 (C:0.9980, R:0.0107)
Batch 450/537: Loss=1.0204 (C:1.0204, R:0.0107)
Batch 475/537: Loss=1.0053 (C:1.0053, R:0.0107)
Batch 500/537: Loss=0.9836 (C:0.9836, R:0.0107)
Batch 525/537: Loss=1.0365 (C:1.0365, R:0.0107)

============================================================
Epoch 21/200 completed in 26.0s
Train: Loss=1.0155 (C:1.0155, R:0.0107) Ratio=3.57x
Val:   Loss=1.0411 (C:1.0411, R:0.0106) Ratio=3.29x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0411)
============================================================

üåç Updating global dataset at epoch 22
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.383 ¬± 0.724
    Neg distances: 1.500 ¬± 0.883
    Separation ratio: 3.92x
    Gap: -2.327
    ‚úÖ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.9747 (C:0.9747, R:0.0107)
Batch  25/537: Loss=0.9928 (C:0.9928, R:0.0107)
Batch  50/537: Loss=1.0082 (C:1.0082, R:0.0107)
Batch  75/537: Loss=1.0485 (C:1.0485, R:0.0107)
Batch 100/537: Loss=0.9670 (C:0.9670, R:0.0107)
Batch 125/537: Loss=1.0233 (C:1.0233, R:0.0107)
Batch 150/537: Loss=0.9985 (C:0.9985, R:0.0107)
Batch 175/537: Loss=0.9983 (C:0.9983, R:0.0107)
Batch 200/537: Loss=0.9953 (C:0.9953, R:0.0107)
Batch 225/537: Loss=0.9831 (C:0.9831, R:0.0107)
Batch 250/537: Loss=0.9566 (C:0.9566, R:0.0108)
Batch 275/537: Loss=1.0226 (C:1.0226, R:0.0107)
Batch 300/537: Loss=0.9932 (C:0.9932, R:0.0107)
Batch 325/537: Loss=1.0178 (C:1.0178, R:0.0107)
Batch 350/537: Loss=1.0283 (C:1.0283, R:0.0107)
Batch 375/537: Loss=1.0310 (C:1.0310, R:0.0107)
Batch 400/537: Loss=0.9971 (C:0.9971, R:0.0107)
Batch 425/537: Loss=0.9923 (C:0.9923, R:0.0107)
Batch 450/537: Loss=0.9829 (C:0.9829, R:0.0107)
Batch 475/537: Loss=0.9709 (C:0.9709, R:0.0107)
Batch 500/537: Loss=1.0517 (C:1.0517, R:0.0107)
Batch 525/537: Loss=0.9966 (C:0.9966, R:0.0107)

============================================================
Epoch 22/200 completed in 35.2s
Train: Loss=1.0056 (C:1.0056, R:0.0107) Ratio=3.75x
Val:   Loss=1.0441 (C:1.0441, R:0.0106) Ratio=3.24x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=1.0065 (C:1.0065, R:0.0107)
Batch  25/537: Loss=1.0353 (C:1.0353, R:0.0107)
Batch  50/537: Loss=0.9933 (C:0.9933, R:0.0107)
Batch  75/537: Loss=0.9802 (C:0.9802, R:0.0107)
Batch 100/537: Loss=1.0281 (C:1.0281, R:0.0107)
Batch 125/537: Loss=1.0172 (C:1.0172, R:0.0107)
Batch 150/537: Loss=1.0090 (C:1.0090, R:0.0107)
Batch 175/537: Loss=0.9755 (C:0.9755, R:0.0107)
Batch 200/537: Loss=1.0340 (C:1.0340, R:0.0107)
Batch 225/537: Loss=1.0389 (C:1.0389, R:0.0107)
Batch 250/537: Loss=1.0104 (C:1.0104, R:0.0107)
Batch 275/537: Loss=0.9980 (C:0.9980, R:0.0107)
Batch 300/537: Loss=0.9750 (C:0.9750, R:0.0107)
Batch 325/537: Loss=0.9815 (C:0.9815, R:0.0107)
Batch 350/537: Loss=0.9961 (C:0.9961, R:0.0107)
Batch 375/537: Loss=0.9830 (C:0.9830, R:0.0107)
Batch 400/537: Loss=1.0178 (C:1.0178, R:0.0107)
Batch 425/537: Loss=0.9972 (C:0.9972, R:0.0107)
Batch 450/537: Loss=1.0251 (C:1.0251, R:0.0107)
Batch 475/537: Loss=1.0384 (C:1.0384, R:0.0107)
Batch 500/537: Loss=0.9869 (C:0.9869, R:0.0107)
Batch 525/537: Loss=0.9779 (C:0.9779, R:0.0107)

============================================================
Epoch 23/200 completed in 26.0s
Train: Loss=1.0040 (C:1.0040, R:0.0107) Ratio=3.66x
Val:   Loss=1.0414 (C:1.0414, R:0.0106) Ratio=3.20x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.9925 (C:0.9925, R:0.0107)
Batch  25/537: Loss=0.9913 (C:0.9913, R:0.0107)
Batch  50/537: Loss=1.0076 (C:1.0076, R:0.0107)
Batch  75/537: Loss=0.9868 (C:0.9868, R:0.0107)
Batch 100/537: Loss=1.0427 (C:1.0427, R:0.0107)
Batch 125/537: Loss=1.0231 (C:1.0231, R:0.0107)
Batch 150/537: Loss=1.0212 (C:1.0212, R:0.0107)
Batch 175/537: Loss=1.0019 (C:1.0019, R:0.0107)
Batch 200/537: Loss=1.0166 (C:1.0166, R:0.0107)
Batch 225/537: Loss=1.0159 (C:1.0159, R:0.0107)
Batch 250/537: Loss=1.0061 (C:1.0061, R:0.0107)
Batch 275/537: Loss=1.0160 (C:1.0160, R:0.0107)
Batch 300/537: Loss=1.0142 (C:1.0142, R:0.0107)
Batch 325/537: Loss=1.0247 (C:1.0247, R:0.0107)
Batch 350/537: Loss=0.9770 (C:0.9770, R:0.0107)
Batch 375/537: Loss=1.0016 (C:1.0016, R:0.0107)
Batch 400/537: Loss=0.9949 (C:0.9949, R:0.0107)
Batch 425/537: Loss=0.9882 (C:0.9882, R:0.0107)
Batch 450/537: Loss=1.0574 (C:1.0574, R:0.0108)
Batch 475/537: Loss=1.0358 (C:1.0358, R:0.0107)
Batch 500/537: Loss=0.9837 (C:0.9837, R:0.0108)
Batch 525/537: Loss=0.9952 (C:0.9952, R:0.0107)

============================================================
Epoch 24/200 completed in 26.5s
Train: Loss=1.0024 (C:1.0024, R:0.0107) Ratio=3.70x
Val:   Loss=1.0403 (C:1.0403, R:0.0106) Ratio=3.19x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0403)
============================================================

üåç Updating global dataset at epoch 25
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.356 ¬± 0.697
    Neg distances: 1.506 ¬± 0.881
    Separation ratio: 4.23x
    Gap: -2.328
    ‚úÖ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=1.0048 (C:1.0048, R:0.0107)
Batch  25/537: Loss=0.9781 (C:0.9781, R:0.0107)
Batch  50/537: Loss=0.9615 (C:0.9615, R:0.0107)
Batch  75/537: Loss=0.9616 (C:0.9616, R:0.0107)
Batch 100/537: Loss=0.9850 (C:0.9850, R:0.0107)
Batch 125/537: Loss=0.9810 (C:0.9810, R:0.0107)
Batch 150/537: Loss=0.9912 (C:0.9912, R:0.0107)
Batch 175/537: Loss=0.9978 (C:0.9978, R:0.0107)
Batch 200/537: Loss=0.9421 (C:0.9421, R:0.0107)
Batch 225/537: Loss=0.9817 (C:0.9817, R:0.0107)
Batch 250/537: Loss=0.9595 (C:0.9595, R:0.0108)
Batch 275/537: Loss=1.0129 (C:1.0129, R:0.0107)
Batch 300/537: Loss=1.0059 (C:1.0059, R:0.0107)
Batch 325/537: Loss=1.0060 (C:1.0060, R:0.0107)
Batch 350/537: Loss=0.9722 (C:0.9722, R:0.0107)
Batch 375/537: Loss=1.0039 (C:1.0039, R:0.0107)
Batch 400/537: Loss=0.9892 (C:0.9892, R:0.0107)
Batch 425/537: Loss=1.0079 (C:1.0079, R:0.0107)
Batch 450/537: Loss=0.9745 (C:0.9745, R:0.0107)
Batch 475/537: Loss=0.9399 (C:0.9399, R:0.0107)
Batch 500/537: Loss=0.9546 (C:0.9546, R:0.0107)
Batch 525/537: Loss=0.9848 (C:0.9848, R:0.0107)

============================================================
Epoch 25/200 completed in 35.3s
Train: Loss=0.9792 (C:0.9792, R:0.0107) Ratio=3.81x
Val:   Loss=1.0257 (C:1.0257, R:0.0106) Ratio=3.18x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0257)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.9742 (C:0.9742, R:0.0107)
Batch  25/537: Loss=1.0027 (C:1.0027, R:0.0107)
Batch  50/537: Loss=0.9643 (C:0.9643, R:0.0107)
Batch  75/537: Loss=0.9659 (C:0.9659, R:0.0107)
Batch 100/537: Loss=0.9673 (C:0.9673, R:0.0107)
Batch 125/537: Loss=0.9952 (C:0.9952, R:0.0107)
Batch 150/537: Loss=0.9962 (C:0.9962, R:0.0107)
Batch 175/537: Loss=0.9761 (C:0.9761, R:0.0107)
Batch 200/537: Loss=0.9461 (C:0.9461, R:0.0107)
Batch 225/537: Loss=0.9414 (C:0.9414, R:0.0107)
Batch 250/537: Loss=0.9680 (C:0.9680, R:0.0107)
Batch 275/537: Loss=0.9752 (C:0.9752, R:0.0107)
Batch 300/537: Loss=0.9725 (C:0.9725, R:0.0107)
Batch 325/537: Loss=0.9902 (C:0.9902, R:0.0107)
Batch 350/537: Loss=0.9685 (C:0.9685, R:0.0107)
Batch 375/537: Loss=0.9814 (C:0.9814, R:0.0107)
Batch 400/537: Loss=0.9769 (C:0.9769, R:0.0107)
Batch 425/537: Loss=0.9659 (C:0.9659, R:0.0107)
Batch 450/537: Loss=0.9757 (C:0.9757, R:0.0107)
Batch 475/537: Loss=0.9857 (C:0.9857, R:0.0107)
Batch 500/537: Loss=0.9712 (C:0.9712, R:0.0107)
Batch 525/537: Loss=0.9491 (C:0.9491, R:0.0108)

============================================================
Epoch 26/200 completed in 26.2s
Train: Loss=0.9779 (C:0.9779, R:0.0107) Ratio=3.83x
Val:   Loss=1.0125 (C:1.0125, R:0.0106) Ratio=3.19x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0125)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.9315 (C:0.9315, R:0.0107)
Batch  25/537: Loss=1.0174 (C:1.0174, R:0.0107)
Batch  50/537: Loss=0.9160 (C:0.9160, R:0.0108)
Batch  75/537: Loss=1.0155 (C:1.0155, R:0.0107)
Batch 100/537: Loss=0.9909 (C:0.9909, R:0.0107)
Batch 125/537: Loss=0.9957 (C:0.9957, R:0.0107)
Batch 150/537: Loss=0.9781 (C:0.9781, R:0.0108)
Batch 175/537: Loss=1.0098 (C:1.0098, R:0.0107)
Batch 200/537: Loss=0.9969 (C:0.9969, R:0.0107)
Batch 225/537: Loss=1.0143 (C:1.0143, R:0.0107)
Batch 250/537: Loss=0.9603 (C:0.9603, R:0.0107)
Batch 275/537: Loss=1.0019 (C:1.0019, R:0.0107)
Batch 300/537: Loss=0.9663 (C:0.9663, R:0.0107)
Batch 325/537: Loss=1.0212 (C:1.0212, R:0.0107)
Batch 350/537: Loss=0.9365 (C:0.9365, R:0.0107)
Batch 375/537: Loss=0.9709 (C:0.9709, R:0.0107)
Batch 400/537: Loss=0.9695 (C:0.9695, R:0.0107)
Batch 425/537: Loss=1.0064 (C:1.0064, R:0.0107)
Batch 450/537: Loss=0.9610 (C:0.9610, R:0.0107)
Batch 475/537: Loss=0.9785 (C:0.9785, R:0.0107)
Batch 500/537: Loss=1.0089 (C:1.0089, R:0.0107)
Batch 525/537: Loss=0.9635 (C:0.9635, R:0.0107)

============================================================
Epoch 27/200 completed in 26.8s
Train: Loss=0.9752 (C:0.9752, R:0.0107) Ratio=3.73x
Val:   Loss=1.0176 (C:1.0176, R:0.0106) Ratio=3.28x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 28
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.376 ¬± 0.720
    Neg distances: 1.508 ¬± 0.886
    Separation ratio: 4.01x
    Gap: -2.334
    ‚úÖ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.9890 (C:0.9890, R:0.0107)
Batch  25/537: Loss=0.9971 (C:0.9971, R:0.0107)
Batch  50/537: Loss=1.0117 (C:1.0117, R:0.0107)
Batch  75/537: Loss=0.9810 (C:0.9810, R:0.0107)
Batch 100/537: Loss=0.9423 (C:0.9423, R:0.0107)
Batch 125/537: Loss=0.9824 (C:0.9824, R:0.0107)
Batch 150/537: Loss=0.9809 (C:0.9809, R:0.0107)
Batch 175/537: Loss=0.9841 (C:0.9841, R:0.0107)
Batch 200/537: Loss=0.9839 (C:0.9839, R:0.0107)
Batch 225/537: Loss=0.9760 (C:0.9760, R:0.0106)
Batch 250/537: Loss=0.9907 (C:0.9907, R:0.0107)
Batch 275/537: Loss=1.0091 (C:1.0091, R:0.0107)
Batch 300/537: Loss=1.0162 (C:1.0162, R:0.0107)
Batch 325/537: Loss=0.9792 (C:0.9792, R:0.0107)
Batch 350/537: Loss=0.9937 (C:0.9937, R:0.0107)
Batch 375/537: Loss=0.9828 (C:0.9828, R:0.0107)
Batch 400/537: Loss=0.9774 (C:0.9774, R:0.0107)
Batch 425/537: Loss=0.9822 (C:0.9822, R:0.0107)
Batch 450/537: Loss=1.0087 (C:1.0087, R:0.0107)
Batch 475/537: Loss=0.9850 (C:0.9850, R:0.0107)
Batch 500/537: Loss=0.9577 (C:0.9577, R:0.0107)
Batch 525/537: Loss=1.0046 (C:1.0046, R:0.0107)

============================================================
Epoch 28/200 completed in 34.7s
Train: Loss=0.9867 (C:0.9867, R:0.0107) Ratio=3.83x
Val:   Loss=1.0268 (C:1.0268, R:0.0106) Ratio=3.25x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.9665 (C:0.9665, R:0.0107)
Batch  25/537: Loss=0.9681 (C:0.9681, R:0.0107)
Batch  50/537: Loss=0.9577 (C:0.9577, R:0.0107)
Batch  75/537: Loss=0.9581 (C:0.9581, R:0.0107)
Batch 100/537: Loss=0.9998 (C:0.9998, R:0.0107)
Batch 125/537: Loss=0.9583 (C:0.9583, R:0.0107)
Batch 150/537: Loss=0.9833 (C:0.9833, R:0.0107)
Batch 175/537: Loss=0.9783 (C:0.9783, R:0.0107)
Batch 200/537: Loss=0.9823 (C:0.9823, R:0.0107)
Batch 225/537: Loss=0.9714 (C:0.9714, R:0.0108)
Batch 250/537: Loss=0.9636 (C:0.9636, R:0.0107)
Batch 275/537: Loss=1.0147 (C:1.0147, R:0.0107)
Batch 300/537: Loss=0.9530 (C:0.9530, R:0.0107)
Batch 325/537: Loss=0.9855 (C:0.9855, R:0.0107)
Batch 350/537: Loss=0.9492 (C:0.9492, R:0.0107)
Batch 375/537: Loss=1.0381 (C:1.0381, R:0.0107)
Batch 400/537: Loss=0.9894 (C:0.9894, R:0.0107)
Batch 425/537: Loss=0.9592 (C:0.9592, R:0.0107)
Batch 450/537: Loss=1.0370 (C:1.0370, R:0.0107)
Batch 475/537: Loss=1.0071 (C:1.0071, R:0.0107)
Batch 500/537: Loss=0.9860 (C:0.9860, R:0.0107)
Batch 525/537: Loss=1.0129 (C:1.0129, R:0.0107)

============================================================
Epoch 29/200 completed in 26.2s
Train: Loss=0.9835 (C:0.9835, R:0.0107) Ratio=3.81x
Val:   Loss=1.0340 (C:1.0340, R:0.0106) Ratio=3.20x
Reconstruction weight: 0.000
No improvement for 3 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.9696 (C:0.9696, R:0.0107)
Batch  25/537: Loss=0.9517 (C:0.9517, R:0.0107)
Batch  50/537: Loss=0.9895 (C:0.9895, R:0.0107)
Batch  75/537: Loss=1.0108 (C:1.0108, R:0.0107)
Batch 100/537: Loss=0.9693 (C:0.9693, R:0.0107)
Batch 125/537: Loss=0.9730 (C:0.9730, R:0.0107)
Batch 150/537: Loss=0.9649 (C:0.9649, R:0.0107)
Batch 175/537: Loss=0.9537 (C:0.9537, R:0.0107)
Batch 200/537: Loss=0.9605 (C:0.9605, R:0.0107)
Batch 225/537: Loss=0.9968 (C:0.9968, R:0.0107)
Batch 250/537: Loss=0.9736 (C:0.9736, R:0.0107)
Batch 275/537: Loss=0.9692 (C:0.9692, R:0.0107)
Batch 300/537: Loss=0.9744 (C:0.9744, R:0.0107)
Batch 325/537: Loss=1.0160 (C:1.0160, R:0.0107)
Batch 350/537: Loss=1.0056 (C:1.0056, R:0.0107)
Batch 375/537: Loss=0.9930 (C:0.9930, R:0.0107)
Batch 400/537: Loss=0.9997 (C:0.9997, R:0.0106)
Batch 425/537: Loss=0.9345 (C:0.9345, R:0.0107)
Batch 450/537: Loss=1.0291 (C:1.0291, R:0.0107)
Batch 475/537: Loss=1.0303 (C:1.0303, R:0.0107)
Batch 500/537: Loss=0.9972 (C:0.9972, R:0.0108)
Batch 525/537: Loss=0.9943 (C:0.9943, R:0.0107)

============================================================
Epoch 30/200 completed in 26.8s
Train: Loss=0.9831 (C:0.9831, R:0.0107) Ratio=3.91x
Val:   Loss=1.0355 (C:1.0355, R:0.0106) Ratio=3.21x
Reconstruction weight: 0.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 31
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.375 ¬± 0.720
    Neg distances: 1.505 ¬± 0.884
    Separation ratio: 4.01x
    Gap: -2.333
    ‚úÖ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.9493 (C:0.9493, R:0.0107)
Batch  25/537: Loss=0.9679 (C:0.9679, R:0.0107)
Batch  50/537: Loss=0.9635 (C:0.9635, R:0.0107)
Batch  75/537: Loss=1.0128 (C:1.0128, R:0.0107)
Batch 100/537: Loss=0.9938 (C:0.9938, R:0.0107)
Batch 125/537: Loss=0.9712 (C:0.9712, R:0.0108)
Batch 150/537: Loss=1.0007 (C:1.0007, R:0.0108)
Batch 175/537: Loss=0.9821 (C:0.9821, R:0.0107)
Batch 200/537: Loss=0.9522 (C:0.9522, R:0.0107)
Batch 225/537: Loss=0.9416 (C:0.9416, R:0.0107)
Batch 250/537: Loss=0.9785 (C:0.9785, R:0.0107)
Batch 275/537: Loss=0.9973 (C:0.9973, R:0.0107)
Batch 300/537: Loss=0.9678 (C:0.9678, R:0.0107)
Batch 325/537: Loss=0.9731 (C:0.9731, R:0.0107)
Batch 350/537: Loss=0.9586 (C:0.9586, R:0.0107)
Batch 375/537: Loss=0.9644 (C:0.9644, R:0.0107)
Batch 400/537: Loss=1.0082 (C:1.0082, R:0.0107)
Batch 425/537: Loss=0.9724 (C:0.9724, R:0.0107)
Batch 450/537: Loss=0.9877 (C:0.9877, R:0.0107)
Batch 475/537: Loss=0.9425 (C:0.9425, R:0.0108)
Batch 500/537: Loss=0.9997 (C:0.9997, R:0.0107)
Batch 525/537: Loss=0.9600 (C:0.9600, R:0.0107)

============================================================
Epoch 31/200 completed in 34.3s
Train: Loss=0.9838 (C:0.9838, R:0.0107) Ratio=4.02x
Val:   Loss=1.0372 (C:1.0372, R:0.0106) Ratio=3.18x
Reconstruction weight: 0.015
No improvement for 5 epochs
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.9867 (C:0.9867, R:0.0107)
Batch  25/537: Loss=1.0029 (C:1.0029, R:0.0107)
Batch  50/537: Loss=0.9893 (C:0.9893, R:0.0107)
Batch  75/537: Loss=0.9880 (C:0.9880, R:0.0107)
Batch 100/537: Loss=1.0112 (C:1.0112, R:0.0107)
Batch 125/537: Loss=0.9853 (C:0.9853, R:0.0107)
Batch 150/537: Loss=0.9680 (C:0.9680, R:0.0107)
Batch 175/537: Loss=1.0064 (C:1.0064, R:0.0107)
Batch 200/537: Loss=0.9550 (C:0.9550, R:0.0107)
Batch 225/537: Loss=0.9524 (C:0.9524, R:0.0107)
Batch 250/537: Loss=0.9920 (C:0.9920, R:0.0107)
Batch 275/537: Loss=0.9619 (C:0.9619, R:0.0107)
Batch 300/537: Loss=0.9816 (C:0.9816, R:0.0107)
Batch 325/537: Loss=0.9460 (C:0.9460, R:0.0107)
Batch 350/537: Loss=0.9595 (C:0.9595, R:0.0107)
Batch 375/537: Loss=0.9819 (C:0.9819, R:0.0107)
Batch 400/537: Loss=0.9543 (C:0.9543, R:0.0107)
Batch 425/537: Loss=0.9741 (C:0.9741, R:0.0107)
Batch 450/537: Loss=0.9652 (C:0.9652, R:0.0107)
Batch 475/537: Loss=0.9160 (C:0.9160, R:0.0107)
Batch 500/537: Loss=1.0250 (C:1.0250, R:0.0107)
Batch 525/537: Loss=0.9811 (C:0.9811, R:0.0107)

============================================================
Epoch 32/200 completed in 26.2s
Train: Loss=0.9809 (C:0.9809, R:0.0107) Ratio=4.03x
Val:   Loss=1.0281 (C:1.0281, R:0.0106) Ratio=3.24x
Reconstruction weight: 0.030
No improvement for 6 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.9720 (C:0.9720, R:0.0107)
Batch  25/537: Loss=0.9499 (C:0.9499, R:0.0107)
Batch  50/537: Loss=0.9729 (C:0.9729, R:0.0107)
Batch  75/537: Loss=0.9932 (C:0.9932, R:0.0107)
Batch 100/537: Loss=0.9814 (C:0.9814, R:0.0107)
Batch 125/537: Loss=1.0019 (C:1.0019, R:0.0107)
Batch 150/537: Loss=1.0352 (C:1.0352, R:0.0107)
Batch 175/537: Loss=1.0248 (C:1.0248, R:0.0107)
Batch 200/537: Loss=1.0192 (C:1.0192, R:0.0107)
Batch 225/537: Loss=0.9339 (C:0.9339, R:0.0107)
Batch 250/537: Loss=0.9497 (C:0.9497, R:0.0107)
Batch 275/537: Loss=1.0183 (C:1.0183, R:0.0107)
Batch 300/537: Loss=0.9845 (C:0.9845, R:0.0107)
Batch 325/537: Loss=0.9797 (C:0.9797, R:0.0107)
Batch 350/537: Loss=1.0101 (C:1.0101, R:0.0107)
Batch 375/537: Loss=0.9648 (C:0.9648, R:0.0108)
Batch 400/537: Loss=0.9836 (C:0.9836, R:0.0107)
Batch 425/537: Loss=1.0054 (C:1.0054, R:0.0107)
Batch 450/537: Loss=0.9780 (C:0.9780, R:0.0107)
Batch 475/537: Loss=1.0012 (C:1.0012, R:0.0107)
Batch 500/537: Loss=0.9757 (C:0.9757, R:0.0107)
Batch 525/537: Loss=0.9823 (C:0.9823, R:0.0107)

============================================================
Epoch 33/200 completed in 26.9s
Train: Loss=0.9792 (C:0.9792, R:0.0107) Ratio=3.99x
Val:   Loss=1.0115 (C:1.0115, R:0.0106) Ratio=3.26x
Reconstruction weight: 0.045
‚úÖ New best model saved (Val Loss: 1.0115)
============================================================

üåç Updating global dataset at epoch 34
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.324 ¬± 0.683
    Neg distances: 1.556 ¬± 0.866
    Separation ratio: 4.80x
    Gap: -2.336
    ‚úÖ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.8977 (C:0.8977, R:0.0107)
Batch  25/537: Loss=0.8763 (C:0.8763, R:0.0108)
Batch  50/537: Loss=0.9328 (C:0.9328, R:0.0107)
Batch  75/537: Loss=0.8604 (C:0.8604, R:0.0107)
Batch 100/537: Loss=0.9009 (C:0.9009, R:0.0107)
Batch 125/537: Loss=0.8775 (C:0.8775, R:0.0107)
Batch 150/537: Loss=0.9098 (C:0.9098, R:0.0107)
Batch 175/537: Loss=0.9362 (C:0.9362, R:0.0107)
Batch 200/537: Loss=0.8744 (C:0.8744, R:0.0107)
Batch 225/537: Loss=0.8778 (C:0.8778, R:0.0107)
Batch 250/537: Loss=0.9556 (C:0.9556, R:0.0107)
Batch 275/537: Loss=0.9314 (C:0.9314, R:0.0107)
Batch 300/537: Loss=0.9160 (C:0.9160, R:0.0108)
Batch 325/537: Loss=0.9042 (C:0.9042, R:0.0107)
Batch 350/537: Loss=0.8980 (C:0.8980, R:0.0108)
Batch 375/537: Loss=0.9264 (C:0.9264, R:0.0107)
Batch 400/537: Loss=0.8975 (C:0.8975, R:0.0107)
Batch 425/537: Loss=0.9400 (C:0.9400, R:0.0107)
Batch 450/537: Loss=0.9292 (C:0.9292, R:0.0107)
Batch 475/537: Loss=0.9570 (C:0.9570, R:0.0108)
Batch 500/537: Loss=0.9056 (C:0.9056, R:0.0107)
Batch 525/537: Loss=0.9459 (C:0.9459, R:0.0107)

============================================================
Epoch 34/200 completed in 35.0s
Train: Loss=0.9217 (C:0.9217, R:0.0107) Ratio=4.07x
Val:   Loss=0.9634 (C:0.9634, R:0.0106) Ratio=3.28x
Reconstruction weight: 0.060
‚úÖ New best model saved (Val Loss: 0.9634)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.9009 (C:0.9009, R:0.0107)
Batch  25/537: Loss=0.9222 (C:0.9222, R:0.0107)
Batch  50/537: Loss=0.9176 (C:0.9176, R:0.0107)
Batch  75/537: Loss=0.8990 (C:0.8990, R:0.0107)
Batch 100/537: Loss=0.9141 (C:0.9141, R:0.0107)
Batch 125/537: Loss=0.9204 (C:0.9204, R:0.0107)
Batch 150/537: Loss=0.9086 (C:0.9086, R:0.0107)
Batch 175/537: Loss=0.9127 (C:0.9127, R:0.0107)
Batch 200/537: Loss=0.9275 (C:0.9275, R:0.0107)
Batch 225/537: Loss=0.9275 (C:0.9275, R:0.0107)
Batch 250/537: Loss=0.9464 (C:0.9464, R:0.0107)
Batch 275/537: Loss=0.9566 (C:0.9566, R:0.0107)
Batch 300/537: Loss=0.9092 (C:0.9092, R:0.0107)
Batch 325/537: Loss=0.8886 (C:0.8886, R:0.0107)
Batch 350/537: Loss=0.9401 (C:0.9401, R:0.0107)
Batch 375/537: Loss=0.9412 (C:0.9412, R:0.0107)
Batch 400/537: Loss=0.9567 (C:0.9567, R:0.0107)
Batch 425/537: Loss=0.9065 (C:0.9065, R:0.0108)
Batch 450/537: Loss=0.9492 (C:0.9492, R:0.0107)
Batch 475/537: Loss=0.9669 (C:0.9669, R:0.0107)
Batch 500/537: Loss=0.9182 (C:0.9182, R:0.0107)
Batch 525/537: Loss=0.9058 (C:0.9058, R:0.0107)

============================================================
Epoch 35/200 completed in 26.0s
Train: Loss=0.9195 (C:0.9195, R:0.0107) Ratio=4.06x
Val:   Loss=0.9708 (C:0.9708, R:0.0106) Ratio=3.24x
Reconstruction weight: 0.075
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.9191 (C:0.9191, R:0.0107)
Batch  25/537: Loss=0.9310 (C:0.9310, R:0.0107)
Batch  50/537: Loss=0.9344 (C:0.9344, R:0.0107)
Batch  75/537: Loss=0.9168 (C:0.9168, R:0.0107)
Batch 100/537: Loss=0.9211 (C:0.9211, R:0.0107)
Batch 125/537: Loss=0.8934 (C:0.8934, R:0.0107)
Batch 150/537: Loss=0.9477 (C:0.9477, R:0.0107)
Batch 175/537: Loss=0.9396 (C:0.9396, R:0.0107)
Batch 200/537: Loss=0.9106 (C:0.9106, R:0.0107)
Batch 225/537: Loss=0.9376 (C:0.9376, R:0.0107)
Batch 250/537: Loss=0.9279 (C:0.9279, R:0.0107)
Batch 275/537: Loss=0.9507 (C:0.9507, R:0.0107)
Batch 300/537: Loss=0.8984 (C:0.8984, R:0.0107)
Batch 325/537: Loss=0.8999 (C:0.8999, R:0.0107)
Batch 350/537: Loss=0.9182 (C:0.9182, R:0.0107)
Batch 375/537: Loss=0.9151 (C:0.9151, R:0.0107)
Batch 400/537: Loss=0.9410 (C:0.9410, R:0.0107)
Batch 425/537: Loss=0.9099 (C:0.9099, R:0.0107)
Batch 450/537: Loss=0.9429 (C:0.9429, R:0.0107)
Batch 475/537: Loss=0.9283 (C:0.9283, R:0.0107)
Batch 500/537: Loss=0.9245 (C:0.9245, R:0.0107)
Batch 525/537: Loss=0.9647 (C:0.9647, R:0.0107)

============================================================
Epoch 36/200 completed in 26.2s
Train: Loss=0.9183 (C:0.9183, R:0.0107) Ratio=4.04x
Val:   Loss=0.9701 (C:0.9701, R:0.0106) Ratio=3.19x
Reconstruction weight: 0.090
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 37
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.334 ¬± 0.690
    Neg distances: 1.545 ¬± 0.870
    Separation ratio: 4.63x
    Gap: -2.334
    ‚úÖ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.9651 (C:0.9651, R:0.0107)
Batch  25/537: Loss=0.9233 (C:0.9233, R:0.0107)
Batch  50/537: Loss=0.9044 (C:0.9044, R:0.0107)
Batch  75/537: Loss=0.9563 (C:0.9563, R:0.0107)
Batch 100/537: Loss=0.9179 (C:0.9179, R:0.0107)
Batch 125/537: Loss=0.9204 (C:0.9204, R:0.0107)
Batch 150/537: Loss=0.8925 (C:0.8925, R:0.0107)
Batch 175/537: Loss=0.9142 (C:0.9142, R:0.0107)
Batch 200/537: Loss=0.9165 (C:0.9165, R:0.0107)
Batch 225/537: Loss=0.9541 (C:0.9541, R:0.0107)
Batch 250/537: Loss=0.9314 (C:0.9314, R:0.0107)
Batch 275/537: Loss=0.9214 (C:0.9214, R:0.0107)
Batch 300/537: Loss=0.9034 (C:0.9034, R:0.0107)
Batch 325/537: Loss=0.9042 (C:0.9042, R:0.0107)
Batch 350/537: Loss=0.9223 (C:0.9223, R:0.0107)
Batch 375/537: Loss=0.9133 (C:0.9133, R:0.0107)
Batch 400/537: Loss=0.9026 (C:0.9026, R:0.0107)
Batch 425/537: Loss=0.9497 (C:0.9497, R:0.0107)
Batch 450/537: Loss=0.9157 (C:0.9157, R:0.0107)
Batch 475/537: Loss=0.9179 (C:0.9179, R:0.0107)
Batch 500/537: Loss=0.9144 (C:0.9144, R:0.0108)
Batch 525/537: Loss=0.9128 (C:0.9128, R:0.0107)

============================================================
Epoch 37/200 completed in 35.5s
Train: Loss=0.9297 (C:0.9297, R:0.0107) Ratio=4.17x
Val:   Loss=0.9882 (C:0.9882, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.105
No improvement for 3 epochs
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.9402 (C:0.9402, R:0.0107)
Batch  25/537: Loss=0.8894 (C:0.8894, R:0.0107)
Batch  50/537: Loss=0.9154 (C:0.9154, R:0.0107)
Batch  75/537: Loss=0.8960 (C:0.8960, R:0.0107)
Batch 100/537: Loss=0.9626 (C:0.9626, R:0.0107)
Batch 125/537: Loss=0.9249 (C:0.9249, R:0.0108)
Batch 150/537: Loss=0.9369 (C:0.9369, R:0.0107)
Batch 175/537: Loss=0.9109 (C:0.9109, R:0.0107)
Batch 200/537: Loss=0.9320 (C:0.9320, R:0.0107)
Batch 225/537: Loss=0.9633 (C:0.9633, R:0.0107)
Batch 250/537: Loss=0.9664 (C:0.9664, R:0.0107)
Batch 275/537: Loss=0.8687 (C:0.8687, R:0.0107)
Batch 300/537: Loss=0.9305 (C:0.9305, R:0.0107)
Batch 325/537: Loss=0.9153 (C:0.9153, R:0.0107)
Batch 350/537: Loss=0.9217 (C:0.9217, R:0.0107)
Batch 375/537: Loss=0.9184 (C:0.9184, R:0.0107)
Batch 400/537: Loss=0.9408 (C:0.9408, R:0.0107)
Batch 425/537: Loss=0.9498 (C:0.9498, R:0.0107)
Batch 450/537: Loss=0.9704 (C:0.9704, R:0.0107)
Batch 475/537: Loss=0.9146 (C:0.9146, R:0.0107)
Batch 500/537: Loss=0.9485 (C:0.9485, R:0.0107)
Batch 525/537: Loss=0.9638 (C:0.9638, R:0.0107)

============================================================
Epoch 38/200 completed in 26.2s
Train: Loss=0.9284 (C:0.9284, R:0.0107) Ratio=4.06x
Val:   Loss=0.9814 (C:0.9814, R:0.0106) Ratio=3.24x
Reconstruction weight: 0.120
No improvement for 4 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.9226 (C:0.9226, R:0.0107)
Batch  25/537: Loss=0.9176 (C:0.9176, R:0.0107)
Batch  50/537: Loss=0.8997 (C:0.8997, R:0.0107)
Batch  75/537: Loss=0.9265 (C:0.9265, R:0.0107)
Batch 100/537: Loss=0.8726 (C:0.8726, R:0.0107)
Batch 125/537: Loss=0.8810 (C:0.8810, R:0.0107)
Batch 150/537: Loss=0.9415 (C:0.9415, R:0.0107)
Batch 175/537: Loss=0.9287 (C:0.9287, R:0.0107)
Batch 200/537: Loss=0.8774 (C:0.8774, R:0.0107)
Batch 225/537: Loss=0.9119 (C:0.9119, R:0.0107)
Batch 250/537: Loss=0.9352 (C:0.9352, R:0.0107)
Batch 275/537: Loss=0.9342 (C:0.9342, R:0.0107)
Batch 300/537: Loss=0.9370 (C:0.9370, R:0.0107)
Batch 325/537: Loss=0.9167 (C:0.9167, R:0.0107)
Batch 350/537: Loss=0.8898 (C:0.8898, R:0.0107)
Batch 375/537: Loss=0.9316 (C:0.9316, R:0.0108)
Batch 400/537: Loss=0.9501 (C:0.9501, R:0.0107)
Batch 425/537: Loss=0.8950 (C:0.8950, R:0.0107)
Batch 450/537: Loss=0.9366 (C:0.9366, R:0.0107)
Batch 475/537: Loss=0.9119 (C:0.9119, R:0.0107)
Batch 500/537: Loss=0.9160 (C:0.9160, R:0.0107)
Batch 525/537: Loss=0.8674 (C:0.8674, R:0.0107)

============================================================
Epoch 39/200 completed in 26.5s
Train: Loss=0.9250 (C:0.9250, R:0.0107) Ratio=4.24x
Val:   Loss=0.9884 (C:0.9884, R:0.0106) Ratio=3.29x
Reconstruction weight: 0.135
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 40
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.320 ¬± 0.676
    Neg distances: 1.554 ¬± 0.862
    Separation ratio: 4.86x
    Gap: -2.336
    ‚úÖ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.9283 (C:0.9283, R:0.0107)
Batch  25/537: Loss=0.9039 (C:0.9039, R:0.0107)
Batch  50/537: Loss=0.8843 (C:0.8843, R:0.0107)
Batch  75/537: Loss=0.9247 (C:0.9247, R:0.0107)
Batch 100/537: Loss=0.9016 (C:0.9016, R:0.0107)
Batch 125/537: Loss=0.9355 (C:0.9355, R:0.0107)
Batch 150/537: Loss=0.8999 (C:0.8999, R:0.0107)
Batch 175/537: Loss=0.8910 (C:0.8910, R:0.0107)
Batch 200/537: Loss=0.8982 (C:0.8982, R:0.0107)
Batch 225/537: Loss=0.9241 (C:0.9241, R:0.0107)
Batch 250/537: Loss=0.9375 (C:0.9375, R:0.0107)
Batch 275/537: Loss=0.9403 (C:0.9403, R:0.0107)
Batch 300/537: Loss=0.9082 (C:0.9082, R:0.0107)
Batch 325/537: Loss=0.9046 (C:0.9046, R:0.0107)
Batch 350/537: Loss=0.9647 (C:0.9647, R:0.0107)
Batch 375/537: Loss=0.9357 (C:0.9357, R:0.0107)
Batch 400/537: Loss=0.9166 (C:0.9166, R:0.0107)
Batch 425/537: Loss=0.9471 (C:0.9471, R:0.0107)
Batch 450/537: Loss=0.8628 (C:0.8628, R:0.0107)
Batch 475/537: Loss=0.9072 (C:0.9072, R:0.0108)
Batch 500/537: Loss=0.9050 (C:0.9050, R:0.0107)
Batch 525/537: Loss=0.9006 (C:0.9006, R:0.0107)

============================================================
Epoch 40/200 completed in 35.3s
Train: Loss=0.9095 (C:0.9095, R:0.0107) Ratio=4.16x
Val:   Loss=0.9717 (C:0.9717, R:0.0106) Ratio=3.30x
Reconstruction weight: 0.150
No improvement for 6 epochs
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.9333 (C:0.9333, R:0.0107)
Batch  25/537: Loss=0.9643 (C:0.9643, R:0.0107)
Batch  50/537: Loss=0.8726 (C:0.8726, R:0.0107)
Batch  75/537: Loss=0.9122 (C:0.9122, R:0.0107)
Batch 100/537: Loss=0.9093 (C:0.9093, R:0.0107)
Batch 125/537: Loss=0.9156 (C:0.9156, R:0.0107)
Batch 150/537: Loss=0.8757 (C:0.8757, R:0.0107)
Batch 175/537: Loss=0.9098 (C:0.9098, R:0.0107)
Batch 200/537: Loss=0.8883 (C:0.8883, R:0.0107)
Batch 225/537: Loss=0.9063 (C:0.9063, R:0.0107)
Batch 250/537: Loss=0.9504 (C:0.9504, R:0.0107)
Batch 275/537: Loss=0.8920 (C:0.8920, R:0.0107)
Batch 300/537: Loss=0.9042 (C:0.9042, R:0.0107)
Batch 325/537: Loss=0.8813 (C:0.8813, R:0.0107)
Batch 350/537: Loss=0.9099 (C:0.9099, R:0.0107)
Batch 375/537: Loss=0.9179 (C:0.9179, R:0.0107)
Batch 400/537: Loss=0.9549 (C:0.9549, R:0.0107)
Batch 425/537: Loss=0.9011 (C:0.9011, R:0.0107)
Batch 450/537: Loss=0.8745 (C:0.8745, R:0.0107)
Batch 475/537: Loss=0.9299 (C:0.9299, R:0.0107)
Batch 500/537: Loss=0.9247 (C:0.9247, R:0.0107)
Batch 525/537: Loss=0.9298 (C:0.9298, R:0.0107)

============================================================
Epoch 41/200 completed in 25.9s
Train: Loss=0.9080 (C:0.9080, R:0.0107) Ratio=4.14x
Val:   Loss=0.9573 (C:0.9573, R:0.0106) Ratio=3.33x
Reconstruction weight: 0.165
‚úÖ New best model saved (Val Loss: 0.9573)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.8852 (C:0.8852, R:0.0107)
Batch  25/537: Loss=0.9062 (C:0.9062, R:0.0107)
Batch  50/537: Loss=0.8955 (C:0.8955, R:0.0107)
Batch  75/537: Loss=0.9288 (C:0.9288, R:0.0107)
Batch 100/537: Loss=0.9510 (C:0.9510, R:0.0107)
Batch 125/537: Loss=0.8668 (C:0.8668, R:0.0107)
Batch 150/537: Loss=0.9045 (C:0.9045, R:0.0107)
Batch 175/537: Loss=0.8729 (C:0.8729, R:0.0107)
Batch 200/537: Loss=0.9000 (C:0.9000, R:0.0107)
Batch 225/537: Loss=0.9212 (C:0.9212, R:0.0107)
Batch 250/537: Loss=0.8817 (C:0.8817, R:0.0107)
Batch 275/537: Loss=0.8938 (C:0.8938, R:0.0107)
Batch 300/537: Loss=0.8872 (C:0.8872, R:0.0107)
Batch 325/537: Loss=0.9274 (C:0.9274, R:0.0107)
Batch 350/537: Loss=0.9460 (C:0.9460, R:0.0107)
Batch 375/537: Loss=0.9250 (C:0.9250, R:0.0107)
Batch 400/537: Loss=0.9263 (C:0.9263, R:0.0107)
Batch 425/537: Loss=0.8995 (C:0.8995, R:0.0107)
Batch 450/537: Loss=0.8988 (C:0.8988, R:0.0107)
Batch 475/537: Loss=0.9026 (C:0.9026, R:0.0107)
Batch 500/537: Loss=0.9164 (C:0.9164, R:0.0107)
Batch 525/537: Loss=0.9279 (C:0.9279, R:0.0107)

============================================================
Epoch 42/200 completed in 26.5s
Train: Loss=0.9127 (C:0.9127, R:0.0107) Ratio=4.20x
Val:   Loss=0.9782 (C:0.9782, R:0.0106) Ratio=3.21x
Reconstruction weight: 0.180
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 43
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.320 ¬± 0.682
    Neg distances: 1.562 ¬± 0.866
    Separation ratio: 4.88x
    Gap: -2.336
    ‚úÖ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.9077 (C:0.9077, R:0.0107)
Batch  25/537: Loss=0.8757 (C:0.8757, R:0.0107)
Batch  50/537: Loss=0.9416 (C:0.9416, R:0.0107)
Batch  75/537: Loss=0.8814 (C:0.8814, R:0.0107)
Batch 100/537: Loss=0.9104 (C:0.9104, R:0.0107)
Batch 125/537: Loss=0.9166 (C:0.9166, R:0.0107)
Batch 150/537: Loss=0.9194 (C:0.9194, R:0.0107)
Batch 175/537: Loss=0.9345 (C:0.9345, R:0.0107)
Batch 200/537: Loss=0.8990 (C:0.8990, R:0.0107)
Batch 225/537: Loss=0.9151 (C:0.9151, R:0.0107)
Batch 250/537: Loss=0.8552 (C:0.8552, R:0.0107)
Batch 275/537: Loss=0.8937 (C:0.8937, R:0.0107)
Batch 300/537: Loss=0.9095 (C:0.9095, R:0.0107)
Batch 325/537: Loss=0.9071 (C:0.9071, R:0.0107)
Batch 350/537: Loss=0.9173 (C:0.9173, R:0.0107)
Batch 375/537: Loss=0.9089 (C:0.9089, R:0.0107)
Batch 400/537: Loss=0.9137 (C:0.9137, R:0.0107)
Batch 425/537: Loss=0.8932 (C:0.8932, R:0.0107)
Batch 450/537: Loss=0.9277 (C:0.9277, R:0.0107)
Batch 475/537: Loss=0.8828 (C:0.8828, R:0.0107)
Batch 500/537: Loss=0.9098 (C:0.9098, R:0.0108)
Batch 525/537: Loss=0.8849 (C:0.8849, R:0.0107)

============================================================
Epoch 43/200 completed in 34.8s
Train: Loss=0.9057 (C:0.9057, R:0.0107) Ratio=4.23x
Val:   Loss=0.9561 (C:0.9561, R:0.0106) Ratio=3.36x
Reconstruction weight: 0.195
‚úÖ New best model saved (Val Loss: 0.9561)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.9376 (C:0.9376, R:0.0107)
Batch  25/537: Loss=0.9177 (C:0.9177, R:0.0107)
Batch  50/537: Loss=0.8639 (C:0.8639, R:0.0107)
Batch  75/537: Loss=0.9213 (C:0.9213, R:0.0107)
Batch 100/537: Loss=0.9090 (C:0.9090, R:0.0107)
Batch 125/537: Loss=0.9362 (C:0.9362, R:0.0107)
Batch 150/537: Loss=0.8906 (C:0.8906, R:0.0107)
Batch 175/537: Loss=0.9229 (C:0.9229, R:0.0107)
Batch 200/537: Loss=0.8995 (C:0.8995, R:0.0107)
Batch 225/537: Loss=0.8961 (C:0.8961, R:0.0107)
Batch 250/537: Loss=0.9124 (C:0.9124, R:0.0107)
Batch 275/537: Loss=0.8809 (C:0.8809, R:0.0107)
Batch 300/537: Loss=0.8992 (C:0.8992, R:0.0107)
Batch 325/537: Loss=0.9325 (C:0.9325, R:0.0107)
Batch 350/537: Loss=0.9232 (C:0.9232, R:0.0107)
Batch 375/537: Loss=0.8670 (C:0.8670, R:0.0108)
Batch 400/537: Loss=0.9207 (C:0.9207, R:0.0107)
Batch 425/537: Loss=0.8821 (C:0.8821, R:0.0107)
Batch 450/537: Loss=0.9125 (C:0.9125, R:0.0107)
Batch 475/537: Loss=0.9092 (C:0.9092, R:0.0107)
Batch 500/537: Loss=0.9259 (C:0.9259, R:0.0107)
Batch 525/537: Loss=0.9145 (C:0.9145, R:0.0107)

============================================================
Epoch 44/200 completed in 26.3s
Train: Loss=0.9050 (C:0.9050, R:0.0107) Ratio=4.20x
Val:   Loss=0.9649 (C:0.9649, R:0.0106) Ratio=3.33x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.9234 (C:0.9234, R:0.0107)
Batch  25/537: Loss=0.9125 (C:0.9125, R:0.0107)
Batch  50/537: Loss=0.9168 (C:0.9168, R:0.0107)
Batch  75/537: Loss=0.9373 (C:0.9373, R:0.0107)
Batch 100/537: Loss=0.9022 (C:0.9022, R:0.0107)
Batch 125/537: Loss=0.8648 (C:0.8648, R:0.0107)
Batch 150/537: Loss=0.9282 (C:0.9282, R:0.0107)
Batch 175/537: Loss=0.8753 (C:0.8753, R:0.0107)
Batch 200/537: Loss=0.9187 (C:0.9187, R:0.0107)
Batch 225/537: Loss=0.9419 (C:0.9419, R:0.0107)
Batch 250/537: Loss=0.8874 (C:0.8874, R:0.0108)
Batch 275/537: Loss=0.8953 (C:0.8953, R:0.0107)
Batch 300/537: Loss=0.8676 (C:0.8676, R:0.0107)
Batch 325/537: Loss=0.9024 (C:0.9024, R:0.0107)
Batch 350/537: Loss=0.9210 (C:0.9210, R:0.0107)
Batch 375/537: Loss=0.9041 (C:0.9041, R:0.0107)
Batch 400/537: Loss=0.9021 (C:0.9021, R:0.0107)
Batch 425/537: Loss=0.9126 (C:0.9126, R:0.0107)
Batch 450/537: Loss=0.8741 (C:0.8741, R:0.0107)
Batch 475/537: Loss=0.9141 (C:0.9141, R:0.0107)
Batch 500/537: Loss=0.8926 (C:0.8926, R:0.0107)
Batch 525/537: Loss=0.9216 (C:0.9216, R:0.0107)

============================================================
Epoch 45/200 completed in 27.0s
Train: Loss=0.9067 (C:0.9067, R:0.0107) Ratio=4.22x
Val:   Loss=0.9556 (C:0.9556, R:0.0106) Ratio=3.37x
Reconstruction weight: 0.225
‚úÖ New best model saved (Val Loss: 0.9556)
============================================================

üåç Updating global dataset at epoch 46
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.348 ¬± 0.705
    Neg distances: 1.548 ¬± 0.868
    Separation ratio: 4.45x
    Gap: -2.337
    ‚úÖ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.8965 (C:0.8965, R:0.0107)
Batch  25/537: Loss=0.9191 (C:0.9191, R:0.0107)
Batch  50/537: Loss=0.9271 (C:0.9271, R:0.0107)
Batch  75/537: Loss=0.9297 (C:0.9297, R:0.0107)
Batch 100/537: Loss=0.9337 (C:0.9337, R:0.0107)
Batch 125/537: Loss=0.9135 (C:0.9135, R:0.0107)
Batch 150/537: Loss=0.9230 (C:0.9230, R:0.0107)
Batch 175/537: Loss=0.9461 (C:0.9461, R:0.0107)
Batch 200/537: Loss=0.9270 (C:0.9270, R:0.0107)
Batch 225/537: Loss=0.9385 (C:0.9385, R:0.0107)
Batch 250/537: Loss=0.8878 (C:0.8878, R:0.0107)
Batch 275/537: Loss=0.9094 (C:0.9094, R:0.0107)
Batch 300/537: Loss=0.9261 (C:0.9261, R:0.0107)
Batch 325/537: Loss=0.8792 (C:0.8792, R:0.0107)
Batch 350/537: Loss=0.9094 (C:0.9094, R:0.0108)
Batch 375/537: Loss=0.9165 (C:0.9165, R:0.0107)
Batch 400/537: Loss=0.9282 (C:0.9282, R:0.0107)
Batch 425/537: Loss=0.9219 (C:0.9219, R:0.0107)
Batch 450/537: Loss=0.9416 (C:0.9416, R:0.0107)
Batch 475/537: Loss=0.9647 (C:0.9647, R:0.0107)
Batch 500/537: Loss=0.9680 (C:0.9680, R:0.0107)
Batch 525/537: Loss=0.9352 (C:0.9352, R:0.0107)

============================================================
Epoch 46/200 completed in 34.8s
Train: Loss=0.9253 (C:0.9253, R:0.0107) Ratio=4.39x
Val:   Loss=0.9880 (C:0.9880, R:0.0106) Ratio=3.26x
Reconstruction weight: 0.240
No improvement for 1 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.8994 (C:0.8994, R:0.0107)
Batch  25/537: Loss=0.9003 (C:0.9003, R:0.0107)
Batch  50/537: Loss=0.8948 (C:0.8948, R:0.0107)
Batch  75/537: Loss=0.9302 (C:0.9302, R:0.0107)
Batch 100/537: Loss=0.8815 (C:0.8815, R:0.0107)
Batch 125/537: Loss=0.9455 (C:0.9455, R:0.0107)
Batch 150/537: Loss=0.9117 (C:0.9117, R:0.0107)
Batch 175/537: Loss=0.9257 (C:0.9257, R:0.0107)
Batch 200/537: Loss=0.9151 (C:0.9151, R:0.0107)
Batch 225/537: Loss=0.9575 (C:0.9575, R:0.0107)
Batch 250/537: Loss=0.8918 (C:0.8918, R:0.0107)
Batch 275/537: Loss=0.9182 (C:0.9182, R:0.0107)
Batch 300/537: Loss=0.9578 (C:0.9578, R:0.0107)
Batch 325/537: Loss=0.9275 (C:0.9275, R:0.0107)
Batch 350/537: Loss=0.9400 (C:0.9400, R:0.0107)
Batch 375/537: Loss=0.9093 (C:0.9093, R:0.0107)
Batch 400/537: Loss=0.8996 (C:0.8996, R:0.0107)
Batch 425/537: Loss=0.9203 (C:0.9203, R:0.0107)
Batch 450/537: Loss=0.9372 (C:0.9372, R:0.0107)
Batch 475/537: Loss=0.9207 (C:0.9207, R:0.0107)
Batch 500/537: Loss=0.8750 (C:0.8750, R:0.0108)
Batch 525/537: Loss=0.9265 (C:0.9265, R:0.0107)

============================================================
Epoch 47/200 completed in 26.9s
Train: Loss=0.9226 (C:0.9226, R:0.0107) Ratio=4.46x
Val:   Loss=0.9864 (C:0.9864, R:0.0106) Ratio=3.22x
Reconstruction weight: 0.255
No improvement for 2 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.9115 (C:0.9115, R:0.0107)
Batch  25/537: Loss=0.9199 (C:0.9199, R:0.0107)
Batch  50/537: Loss=0.9393 (C:0.9393, R:0.0107)
Batch  75/537: Loss=0.8972 (C:0.8972, R:0.0107)
Batch 100/537: Loss=0.9469 (C:0.9469, R:0.0107)
Batch 125/537: Loss=0.8974 (C:0.8974, R:0.0107)
Batch 150/537: Loss=0.9054 (C:0.9054, R:0.0107)
Batch 175/537: Loss=0.9356 (C:0.9356, R:0.0107)
Batch 200/537: Loss=0.9389 (C:0.9389, R:0.0107)
Batch 225/537: Loss=0.8927 (C:0.8927, R:0.0107)
Batch 250/537: Loss=0.9107 (C:0.9107, R:0.0108)
Batch 275/537: Loss=0.9085 (C:0.9085, R:0.0107)
Batch 300/537: Loss=0.9041 (C:0.9041, R:0.0107)
Batch 325/537: Loss=0.8989 (C:0.8989, R:0.0107)
Batch 350/537: Loss=0.8964 (C:0.8964, R:0.0107)
Batch 375/537: Loss=0.8912 (C:0.8912, R:0.0107)
Batch 400/537: Loss=0.9496 (C:0.9496, R:0.0107)
Batch 425/537: Loss=0.9105 (C:0.9105, R:0.0107)
Batch 450/537: Loss=0.9018 (C:0.9018, R:0.0107)
Batch 475/537: Loss=0.9487 (C:0.9487, R:0.0107)
Batch 500/537: Loss=0.9463 (C:0.9463, R:0.0107)
Batch 525/537: Loss=0.9365 (C:0.9365, R:0.0107)

============================================================
Epoch 48/200 completed in 26.6s
Train: Loss=0.9219 (C:0.9219, R:0.0107) Ratio=4.39x
Val:   Loss=0.9814 (C:0.9814, R:0.0106) Ratio=3.29x
Reconstruction weight: 0.270
No improvement for 3 epochs
============================================================

üåç Updating global dataset at epoch 49
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.307 ¬± 0.661
    Neg distances: 1.555 ¬± 0.852
    Separation ratio: 5.06x
    Gap: -2.325
    ‚úÖ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.9559 (C:0.9559, R:0.0107)
Batch  25/537: Loss=0.8757 (C:0.8757, R:0.0108)
Batch  50/537: Loss=0.8971 (C:0.8971, R:0.0107)
Batch  75/537: Loss=0.8978 (C:0.8978, R:0.0107)
Batch 100/537: Loss=0.8791 (C:0.8791, R:0.0107)
Batch 125/537: Loss=0.8995 (C:0.8995, R:0.0107)
Batch 150/537: Loss=0.8967 (C:0.8967, R:0.0107)
Batch 175/537: Loss=0.9030 (C:0.9030, R:0.0107)
Batch 200/537: Loss=0.9043 (C:0.9043, R:0.0107)
Batch 225/537: Loss=0.8692 (C:0.8692, R:0.0107)
Batch 250/537: Loss=0.8966 (C:0.8966, R:0.0107)
Batch 275/537: Loss=0.8984 (C:0.8984, R:0.0107)
Batch 300/537: Loss=0.8848 (C:0.8848, R:0.0107)
Batch 325/537: Loss=0.9547 (C:0.9547, R:0.0107)
Batch 350/537: Loss=0.8667 (C:0.8667, R:0.0107)
Batch 375/537: Loss=0.9169 (C:0.9169, R:0.0107)
Batch 400/537: Loss=0.8784 (C:0.8784, R:0.0107)
Batch 425/537: Loss=0.9043 (C:0.9043, R:0.0108)
Batch 450/537: Loss=0.9072 (C:0.9072, R:0.0107)
Batch 475/537: Loss=0.9132 (C:0.9132, R:0.0107)
Batch 500/537: Loss=0.9219 (C:0.9219, R:0.0107)
Batch 525/537: Loss=0.9271 (C:0.9271, R:0.0107)

============================================================
Epoch 49/200 completed in 34.1s
Train: Loss=0.8926 (C:0.8926, R:0.0107) Ratio=4.17x
Val:   Loss=0.9579 (C:0.9579, R:0.0106) Ratio=3.27x
Reconstruction weight: 0.285
No improvement for 4 epochs
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.8542 (C:0.8542, R:0.0107)
Batch  25/537: Loss=0.9001 (C:0.9001, R:0.0107)
Batch  50/537: Loss=0.8534 (C:0.8534, R:0.0107)
Batch  75/537: Loss=0.8736 (C:0.8736, R:0.0107)
Batch 100/537: Loss=0.9067 (C:0.9067, R:0.0107)
Batch 125/537: Loss=0.8862 (C:0.8862, R:0.0108)
Batch 150/537: Loss=0.8839 (C:0.8839, R:0.0107)
Batch 175/537: Loss=0.8842 (C:0.8842, R:0.0107)
Batch 200/537: Loss=0.8695 (C:0.8695, R:0.0107)
Batch 225/537: Loss=0.9004 (C:0.9004, R:0.0107)
Batch 250/537: Loss=0.8914 (C:0.8914, R:0.0107)
Batch 275/537: Loss=0.9051 (C:0.9051, R:0.0107)
Batch 300/537: Loss=0.8958 (C:0.8958, R:0.0107)
Batch 325/537: Loss=0.8682 (C:0.8682, R:0.0107)
Batch 350/537: Loss=0.8859 (C:0.8859, R:0.0107)
Batch 375/537: Loss=0.8903 (C:0.8903, R:0.0107)
Batch 400/537: Loss=0.9179 (C:0.9179, R:0.0107)
Batch 425/537: Loss=0.8954 (C:0.8954, R:0.0107)
Batch 450/537: Loss=0.8941 (C:0.8941, R:0.0107)
Batch 475/537: Loss=0.8994 (C:0.8994, R:0.0108)
Batch 500/537: Loss=0.8880 (C:0.8880, R:0.0108)
Batch 525/537: Loss=0.8785 (C:0.8785, R:0.0107)

============================================================
Epoch 50/200 completed in 26.7s
Train: Loss=0.8932 (C:0.8932, R:0.0107) Ratio=4.40x
Val:   Loss=0.9558 (C:0.9558, R:0.0106) Ratio=3.30x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.9470 (C:0.9470, R:0.0107)
Batch  25/537: Loss=0.8817 (C:0.8817, R:0.0107)
Batch  50/537: Loss=0.8898 (C:0.8898, R:0.0107)
Batch  75/537: Loss=0.8621 (C:0.8621, R:0.0107)
Batch 100/537: Loss=0.8421 (C:0.8421, R:0.0108)
Batch 125/537: Loss=0.8880 (C:0.8880, R:0.0107)
Batch 150/537: Loss=0.9096 (C:0.9096, R:0.0107)
Batch 175/537: Loss=0.9039 (C:0.9039, R:0.0107)
Batch 200/537: Loss=0.8879 (C:0.8879, R:0.0107)
Batch 225/537: Loss=0.8749 (C:0.8749, R:0.0107)
Batch 250/537: Loss=0.8842 (C:0.8842, R:0.0107)
Batch 275/537: Loss=0.8940 (C:0.8940, R:0.0107)
Batch 300/537: Loss=0.8760 (C:0.8760, R:0.0107)
Batch 325/537: Loss=0.8785 (C:0.8785, R:0.0107)
Batch 350/537: Loss=0.8708 (C:0.8708, R:0.0107)
Batch 375/537: Loss=0.9072 (C:0.9072, R:0.0107)
Batch 400/537: Loss=0.8777 (C:0.8777, R:0.0108)
Batch 425/537: Loss=0.8711 (C:0.8711, R:0.0107)
Batch 450/537: Loss=0.8955 (C:0.8955, R:0.0107)
Batch 475/537: Loss=0.8475 (C:0.8475, R:0.0107)
Batch 500/537: Loss=0.9154 (C:0.9154, R:0.0107)
Batch 525/537: Loss=0.8419 (C:0.8419, R:0.0107)

============================================================
Epoch 51/200 completed in 26.0s
Train: Loss=0.8916 (C:0.8916, R:0.0107) Ratio=4.54x
Val:   Loss=0.9518 (C:0.9518, R:0.0106) Ratio=3.31x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.9518)
============================================================

üåç Updating global dataset at epoch 52
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.297 ¬± 0.655
    Neg distances: 1.558 ¬± 0.842
    Separation ratio: 5.25x
    Gap: -2.328
    ‚úÖ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.8651 (C:0.8651, R:0.0107)
Batch  25/537: Loss=0.8627 (C:0.8627, R:0.0107)
Batch  50/537: Loss=0.8550 (C:0.8550, R:0.0107)
Batch  75/537: Loss=0.8566 (C:0.8566, R:0.0107)
Batch 100/537: Loss=0.8879 (C:0.8879, R:0.0107)
Batch 125/537: Loss=0.8991 (C:0.8991, R:0.0107)
Batch 150/537: Loss=0.9255 (C:0.9255, R:0.0107)
Batch 175/537: Loss=0.8537 (C:0.8537, R:0.0107)
Batch 200/537: Loss=0.9016 (C:0.9016, R:0.0107)
Batch 225/537: Loss=0.8974 (C:0.8974, R:0.0107)
Batch 250/537: Loss=0.8852 (C:0.8852, R:0.0107)
Batch 275/537: Loss=0.8746 (C:0.8746, R:0.0107)
Batch 300/537: Loss=0.8745 (C:0.8745, R:0.0108)
Batch 325/537: Loss=0.8738 (C:0.8738, R:0.0107)
Batch 350/537: Loss=0.9213 (C:0.9213, R:0.0107)
Batch 375/537: Loss=0.8931 (C:0.8931, R:0.0107)
Batch 400/537: Loss=0.8947 (C:0.8947, R:0.0107)
Batch 425/537: Loss=0.8710 (C:0.8710, R:0.0107)
Batch 450/537: Loss=0.8804 (C:0.8804, R:0.0107)
Batch 475/537: Loss=0.8978 (C:0.8978, R:0.0107)
Batch 500/537: Loss=0.8848 (C:0.8848, R:0.0107)
Batch 525/537: Loss=0.8972 (C:0.8972, R:0.0107)

============================================================
Epoch 52/200 completed in 33.3s
Train: Loss=0.8807 (C:0.8807, R:0.0107) Ratio=4.34x
Val:   Loss=0.9494 (C:0.9494, R:0.0106) Ratio=3.22x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.9494)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.8817 (C:0.8817, R:0.0107)
Batch  25/537: Loss=0.8626 (C:0.8626, R:0.0107)
Batch  50/537: Loss=0.8838 (C:0.8838, R:0.0107)
Batch  75/537: Loss=0.8901 (C:0.8901, R:0.0107)
Batch 100/537: Loss=0.8815 (C:0.8815, R:0.0107)
Batch 125/537: Loss=0.8641 (C:0.8641, R:0.0107)
Batch 150/537: Loss=0.8835 (C:0.8835, R:0.0107)
Batch 175/537: Loss=0.8696 (C:0.8696, R:0.0107)
Batch 200/537: Loss=0.8989 (C:0.8989, R:0.0107)
Batch 225/537: Loss=0.8847 (C:0.8847, R:0.0107)
Batch 250/537: Loss=0.8516 (C:0.8516, R:0.0107)
Batch 275/537: Loss=0.8488 (C:0.8488, R:0.0107)
Batch 300/537: Loss=0.8597 (C:0.8597, R:0.0107)
Batch 325/537: Loss=0.8607 (C:0.8607, R:0.0107)
Batch 350/537: Loss=0.8598 (C:0.8598, R:0.0107)
Batch 375/537: Loss=0.9039 (C:0.9039, R:0.0107)
Batch 400/537: Loss=0.9137 (C:0.9137, R:0.0107)
Batch 425/537: Loss=0.8502 (C:0.8502, R:0.0107)
Batch 450/537: Loss=0.8810 (C:0.8810, R:0.0107)
Batch 475/537: Loss=0.9235 (C:0.9235, R:0.0107)
Batch 500/537: Loss=0.8849 (C:0.8849, R:0.0107)
Batch 525/537: Loss=0.9205 (C:0.9205, R:0.0107)

============================================================
Epoch 53/200 completed in 26.1s
Train: Loss=0.8785 (C:0.8785, R:0.0107) Ratio=4.46x
Val:   Loss=0.9329 (C:0.9329, R:0.0106) Ratio=3.27x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.9329)
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.8933 (C:0.8933, R:0.0107)
Batch  25/537: Loss=0.8777 (C:0.8777, R:0.0107)
Batch  50/537: Loss=0.9114 (C:0.9114, R:0.0107)
Batch  75/537: Loss=0.8334 (C:0.8334, R:0.0108)
Batch 100/537: Loss=0.8636 (C:0.8636, R:0.0107)
Batch 125/537: Loss=0.8746 (C:0.8746, R:0.0107)
Batch 150/537: Loss=0.9230 (C:0.9230, R:0.0107)
Batch 175/537: Loss=0.8763 (C:0.8763, R:0.0107)
Batch 200/537: Loss=0.8751 (C:0.8751, R:0.0107)
Batch 225/537: Loss=0.9280 (C:0.9280, R:0.0107)
Batch 250/537: Loss=0.9022 (C:0.9022, R:0.0107)
Batch 275/537: Loss=0.8847 (C:0.8847, R:0.0107)
Batch 300/537: Loss=0.8625 (C:0.8625, R:0.0107)
Batch 325/537: Loss=0.8555 (C:0.8555, R:0.0107)
Batch 350/537: Loss=0.8693 (C:0.8693, R:0.0107)
Batch 375/537: Loss=0.8812 (C:0.8812, R:0.0107)
Batch 400/537: Loss=0.8830 (C:0.8830, R:0.0107)
Batch 425/537: Loss=0.8733 (C:0.8733, R:0.0107)
Batch 450/537: Loss=0.8547 (C:0.8547, R:0.0107)
Batch 475/537: Loss=0.9103 (C:0.9103, R:0.0107)
Batch 500/537: Loss=0.8845 (C:0.8845, R:0.0107)
Batch 525/537: Loss=0.8635 (C:0.8635, R:0.0107)

============================================================
Epoch 54/200 completed in 26.6s
Train: Loss=0.8764 (C:0.8764, R:0.0107) Ratio=4.52x
Val:   Loss=0.9437 (C:0.9437, R:0.0106) Ratio=3.31x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 55
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.302 ¬± 0.664
    Neg distances: 1.576 ¬± 0.847
    Separation ratio: 5.22x
    Gap: -2.329
    ‚úÖ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.8495 (C:0.8495, R:0.0107)
Batch  25/537: Loss=0.8665 (C:0.8665, R:0.0107)
Batch  50/537: Loss=0.8799 (C:0.8799, R:0.0107)
Batch  75/537: Loss=0.8675 (C:0.8675, R:0.0107)
Batch 100/537: Loss=0.8650 (C:0.8650, R:0.0107)
Batch 125/537: Loss=0.8634 (C:0.8634, R:0.0107)
Batch 150/537: Loss=0.8471 (C:0.8471, R:0.0107)
Batch 175/537: Loss=0.8822 (C:0.8822, R:0.0107)
Batch 200/537: Loss=0.8881 (C:0.8881, R:0.0107)
Batch 225/537: Loss=0.8825 (C:0.8825, R:0.0107)
Batch 250/537: Loss=0.8627 (C:0.8627, R:0.0107)
Batch 275/537: Loss=0.8672 (C:0.8672, R:0.0107)
Batch 300/537: Loss=0.9001 (C:0.9001, R:0.0108)
Batch 325/537: Loss=0.8943 (C:0.8943, R:0.0107)
Batch 350/537: Loss=0.8468 (C:0.8468, R:0.0107)
Batch 375/537: Loss=0.8482 (C:0.8482, R:0.0107)
Batch 400/537: Loss=0.8711 (C:0.8711, R:0.0107)
Batch 425/537: Loss=0.8647 (C:0.8647, R:0.0107)
Batch 450/537: Loss=0.8658 (C:0.8658, R:0.0107)
Batch 475/537: Loss=0.8739 (C:0.8739, R:0.0107)
Batch 500/537: Loss=0.8680 (C:0.8680, R:0.0107)
Batch 525/537: Loss=0.8925 (C:0.8925, R:0.0107)

============================================================
Epoch 55/200 completed in 34.0s
Train: Loss=0.8734 (C:0.8734, R:0.0107) Ratio=4.38x
Val:   Loss=0.9421 (C:0.9421, R:0.0106) Ratio=3.25x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.8676 (C:0.8676, R:0.0107)
Batch  25/537: Loss=0.9033 (C:0.9033, R:0.0107)
Batch  50/537: Loss=0.9030 (C:0.9030, R:0.0107)
Batch  75/537: Loss=0.8570 (C:0.8570, R:0.0107)
Batch 100/537: Loss=0.8950 (C:0.8950, R:0.0107)
Batch 125/537: Loss=0.9016 (C:0.9016, R:0.0107)
Batch 150/537: Loss=0.8441 (C:0.8441, R:0.0107)
Batch 175/537: Loss=0.8598 (C:0.8598, R:0.0107)
Batch 200/537: Loss=0.8912 (C:0.8912, R:0.0107)
Batch 225/537: Loss=0.8743 (C:0.8743, R:0.0107)
Batch 250/537: Loss=0.8477 (C:0.8477, R:0.0107)
Batch 275/537: Loss=0.8903 (C:0.8903, R:0.0107)
Batch 300/537: Loss=0.8615 (C:0.8615, R:0.0107)
Batch 325/537: Loss=0.9028 (C:0.9028, R:0.0108)
Batch 350/537: Loss=0.8905 (C:0.8905, R:0.0107)
Batch 375/537: Loss=0.8930 (C:0.8930, R:0.0107)
Batch 400/537: Loss=0.8767 (C:0.8767, R:0.0107)
Batch 425/537: Loss=0.8838 (C:0.8838, R:0.0107)
Batch 450/537: Loss=0.8560 (C:0.8560, R:0.0107)
Batch 475/537: Loss=0.8864 (C:0.8864, R:0.0107)
Batch 500/537: Loss=0.9119 (C:0.9119, R:0.0107)
Batch 525/537: Loss=0.8947 (C:0.8947, R:0.0107)

============================================================
Epoch 56/200 completed in 26.6s
Train: Loss=0.8758 (C:0.8758, R:0.0107) Ratio=4.31x
Val:   Loss=0.9523 (C:0.9523, R:0.0106) Ratio=3.29x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.8764 (C:0.8764, R:0.0107)
Batch  25/537: Loss=0.8796 (C:0.8796, R:0.0107)
Batch  50/537: Loss=0.8621 (C:0.8621, R:0.0107)
Batch  75/537: Loss=0.8798 (C:0.8798, R:0.0107)
Batch 100/537: Loss=0.8496 (C:0.8496, R:0.0107)
Batch 125/537: Loss=0.8440 (C:0.8440, R:0.0107)
Batch 150/537: Loss=0.8950 (C:0.8950, R:0.0107)
Batch 175/537: Loss=0.8575 (C:0.8575, R:0.0107)
Batch 200/537: Loss=0.9042 (C:0.9042, R:0.0107)
Batch 225/537: Loss=0.8830 (C:0.8830, R:0.0107)
Batch 250/537: Loss=0.9001 (C:0.9001, R:0.0108)
Batch 275/537: Loss=0.8792 (C:0.8792, R:0.0107)
Batch 300/537: Loss=0.8855 (C:0.8855, R:0.0107)
Batch 325/537: Loss=0.8577 (C:0.8577, R:0.0107)
Batch 350/537: Loss=0.8359 (C:0.8359, R:0.0107)
Batch 375/537: Loss=0.8930 (C:0.8930, R:0.0107)
Batch 400/537: Loss=0.8344 (C:0.8344, R:0.0107)
Batch 425/537: Loss=0.8578 (C:0.8578, R:0.0107)
Batch 450/537: Loss=0.8339 (C:0.8339, R:0.0107)
Batch 475/537: Loss=0.8404 (C:0.8404, R:0.0107)
Batch 500/537: Loss=0.8795 (C:0.8795, R:0.0107)
Batch 525/537: Loss=0.8577 (C:0.8577, R:0.0107)

============================================================
Epoch 57/200 completed in 26.3s
Train: Loss=0.8712 (C:0.8712, R:0.0107) Ratio=4.47x
Val:   Loss=0.9380 (C:0.9380, R:0.0106) Ratio=3.37x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 58
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.294 ¬± 0.650
    Neg distances: 1.562 ¬± 0.840
    Separation ratio: 5.31x
    Gap: -2.335
    ‚úÖ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.8711 (C:0.8711, R:0.0107)
Batch  25/537: Loss=0.8485 (C:0.8485, R:0.0107)
Batch  50/537: Loss=0.8679 (C:0.8679, R:0.0107)
Batch  75/537: Loss=0.8303 (C:0.8303, R:0.0107)
Batch 100/537: Loss=0.8641 (C:0.8641, R:0.0107)
Batch 125/537: Loss=0.8429 (C:0.8429, R:0.0107)
Batch 150/537: Loss=0.8627 (C:0.8627, R:0.0107)
Batch 175/537: Loss=0.8653 (C:0.8653, R:0.0107)
Batch 200/537: Loss=0.8491 (C:0.8491, R:0.0107)
Batch 225/537: Loss=0.8606 (C:0.8606, R:0.0107)
Batch 250/537: Loss=0.8802 (C:0.8802, R:0.0106)
Batch 275/537: Loss=0.8670 (C:0.8670, R:0.0107)
Batch 300/537: Loss=0.9195 (C:0.9195, R:0.0107)
Batch 325/537: Loss=0.8413 (C:0.8413, R:0.0107)
Batch 350/537: Loss=0.8671 (C:0.8671, R:0.0107)
Batch 375/537: Loss=0.8745 (C:0.8745, R:0.0107)
Batch 400/537: Loss=0.8660 (C:0.8660, R:0.0107)
Batch 425/537: Loss=0.8501 (C:0.8501, R:0.0107)
Batch 450/537: Loss=0.8601 (C:0.8601, R:0.0107)
Batch 475/537: Loss=0.8748 (C:0.8748, R:0.0107)
Batch 500/537: Loss=0.8546 (C:0.8546, R:0.0107)
Batch 525/537: Loss=0.8751 (C:0.8751, R:0.0107)

============================================================
Epoch 58/200 completed in 34.1s
Train: Loss=0.8718 (C:0.8718, R:0.0107) Ratio=4.72x
Val:   Loss=0.9445 (C:0.9445, R:0.0106) Ratio=3.22x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.8885 (C:0.8885, R:0.0107)
Batch  25/537: Loss=0.8420 (C:0.8420, R:0.0107)
Batch  50/537: Loss=0.8790 (C:0.8790, R:0.0107)
Batch  75/537: Loss=0.8418 (C:0.8418, R:0.0107)
Batch 100/537: Loss=0.8901 (C:0.8901, R:0.0107)
Batch 125/537: Loss=0.8609 (C:0.8609, R:0.0107)
Batch 150/537: Loss=0.9031 (C:0.9031, R:0.0108)
Batch 175/537: Loss=0.9042 (C:0.9042, R:0.0107)
Batch 200/537: Loss=0.8807 (C:0.8807, R:0.0107)
Batch 225/537: Loss=0.8761 (C:0.8761, R:0.0107)
Batch 250/537: Loss=0.8379 (C:0.8379, R:0.0107)
Batch 275/537: Loss=0.8709 (C:0.8709, R:0.0107)
Batch 300/537: Loss=0.8434 (C:0.8434, R:0.0107)
Batch 325/537: Loss=0.8331 (C:0.8331, R:0.0107)
Batch 350/537: Loss=0.8517 (C:0.8517, R:0.0107)
Batch 375/537: Loss=0.8469 (C:0.8469, R:0.0107)
Batch 400/537: Loss=0.8579 (C:0.8579, R:0.0107)
Batch 425/537: Loss=0.8853 (C:0.8853, R:0.0107)
Batch 450/537: Loss=0.8973 (C:0.8973, R:0.0107)
Batch 475/537: Loss=0.8594 (C:0.8594, R:0.0107)
Batch 500/537: Loss=0.9012 (C:0.9012, R:0.0107)
Batch 525/537: Loss=0.8978 (C:0.8978, R:0.0107)

============================================================
Epoch 59/200 completed in 26.5s
Train: Loss=0.8725 (C:0.8725, R:0.0107) Ratio=4.66x
Val:   Loss=0.9349 (C:0.9349, R:0.0106) Ratio=3.25x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.8755 (C:0.8755, R:0.0107)
Batch  25/537: Loss=0.8884 (C:0.8884, R:0.0107)
Batch  50/537: Loss=0.8747 (C:0.8747, R:0.0107)
Batch  75/537: Loss=0.8919 (C:0.8919, R:0.0107)
Batch 100/537: Loss=0.8517 (C:0.8517, R:0.0107)
Batch 125/537: Loss=0.8354 (C:0.8354, R:0.0107)
Batch 150/537: Loss=0.8346 (C:0.8346, R:0.0107)
Batch 175/537: Loss=0.8438 (C:0.8438, R:0.0107)
Batch 200/537: Loss=0.9038 (C:0.9038, R:0.0107)
Batch 225/537: Loss=0.8545 (C:0.8545, R:0.0107)
Batch 250/537: Loss=0.9117 (C:0.9117, R:0.0107)
Batch 275/537: Loss=0.8962 (C:0.8962, R:0.0107)
Batch 300/537: Loss=0.8703 (C:0.8703, R:0.0107)
Batch 325/537: Loss=0.9181 (C:0.9181, R:0.0107)
Batch 350/537: Loss=0.8532 (C:0.8532, R:0.0107)
Batch 375/537: Loss=0.8536 (C:0.8536, R:0.0107)
Batch 400/537: Loss=0.8824 (C:0.8824, R:0.0107)
Batch 425/537: Loss=0.8257 (C:0.8257, R:0.0107)
Batch 450/537: Loss=0.8834 (C:0.8834, R:0.0107)
Batch 475/537: Loss=0.8521 (C:0.8521, R:0.0107)
Batch 500/537: Loss=0.8562 (C:0.8562, R:0.0107)
Batch 525/537: Loss=0.8822 (C:0.8822, R:0.0107)

============================================================
Epoch 60/200 completed in 26.0s
Train: Loss=0.8715 (C:0.8715, R:0.0107) Ratio=4.60x
Val:   Loss=0.9540 (C:0.9540, R:0.0106) Ratio=3.24x
Reconstruction weight: 0.300
No improvement for 7 epochs
Checkpoint saved at epoch 60
============================================================

üåç Updating global dataset at epoch 61
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.292 ¬± 0.643
    Neg distances: 1.569 ¬± 0.840
    Separation ratio: 5.37x
    Gap: -2.336
    ‚úÖ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.8651 (C:0.8651, R:0.0107)
Batch  25/537: Loss=0.8881 (C:0.8881, R:0.0107)
Batch  50/537: Loss=0.8518 (C:0.8518, R:0.0107)
Batch  75/537: Loss=0.8615 (C:0.8615, R:0.0107)
Batch 100/537: Loss=0.8357 (C:0.8357, R:0.0107)
Batch 125/537: Loss=0.8776 (C:0.8776, R:0.0107)
Batch 150/537: Loss=0.9011 (C:0.9011, R:0.0107)
Batch 175/537: Loss=0.8696 (C:0.8696, R:0.0107)
Batch 200/537: Loss=0.8455 (C:0.8455, R:0.0107)
Batch 225/537: Loss=0.8303 (C:0.8303, R:0.0107)
Batch 250/537: Loss=0.8918 (C:0.8918, R:0.0108)
Batch 275/537: Loss=0.8478 (C:0.8478, R:0.0107)
Batch 300/537: Loss=0.8513 (C:0.8513, R:0.0107)
Batch 325/537: Loss=0.8405 (C:0.8405, R:0.0107)
Batch 350/537: Loss=0.8416 (C:0.8416, R:0.0107)
Batch 375/537: Loss=0.8773 (C:0.8773, R:0.0107)
Batch 400/537: Loss=0.8949 (C:0.8949, R:0.0107)
Batch 425/537: Loss=0.8484 (C:0.8484, R:0.0107)
Batch 450/537: Loss=0.8324 (C:0.8324, R:0.0107)
Batch 475/537: Loss=0.8515 (C:0.8515, R:0.0107)
Batch 500/537: Loss=0.8903 (C:0.8903, R:0.0108)
Batch 525/537: Loss=0.8719 (C:0.8719, R:0.0107)

============================================================
Epoch 61/200 completed in 34.4s
Train: Loss=0.8661 (C:0.8661, R:0.0107) Ratio=4.55x
Val:   Loss=0.9313 (C:0.9313, R:0.0106) Ratio=3.33x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.9313)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.8406 (C:0.8406, R:0.0107)
Batch  25/537: Loss=0.8926 (C:0.8926, R:0.0107)
Batch  50/537: Loss=0.8861 (C:0.8861, R:0.0107)
Batch  75/537: Loss=0.8642 (C:0.8642, R:0.0107)
Batch 100/537: Loss=0.8721 (C:0.8721, R:0.0108)
Batch 125/537: Loss=0.9030 (C:0.9030, R:0.0107)
Batch 150/537: Loss=0.8709 (C:0.8709, R:0.0107)
Batch 175/537: Loss=0.8621 (C:0.8621, R:0.0107)
Batch 200/537: Loss=0.8574 (C:0.8574, R:0.0107)
Batch 225/537: Loss=0.8520 (C:0.8520, R:0.0107)
Batch 250/537: Loss=0.8499 (C:0.8499, R:0.0107)
Batch 275/537: Loss=0.8717 (C:0.8717, R:0.0107)
Batch 300/537: Loss=0.8348 (C:0.8348, R:0.0107)
Batch 325/537: Loss=0.8583 (C:0.8583, R:0.0107)
Batch 350/537: Loss=0.8477 (C:0.8477, R:0.0107)
Batch 375/537: Loss=0.8127 (C:0.8127, R:0.0107)
Batch 400/537: Loss=0.8500 (C:0.8500, R:0.0107)
Batch 425/537: Loss=0.8378 (C:0.8378, R:0.0107)
Batch 450/537: Loss=0.9066 (C:0.9066, R:0.0107)
Batch 475/537: Loss=0.9326 (C:0.9326, R:0.0107)
Batch 500/537: Loss=0.9034 (C:0.9034, R:0.0107)
Batch 525/537: Loss=0.8790 (C:0.8790, R:0.0107)

============================================================
Epoch 62/200 completed in 26.3s
Train: Loss=0.8627 (C:0.8627, R:0.0107) Ratio=4.64x
Val:   Loss=0.9495 (C:0.9495, R:0.0106) Ratio=3.26x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.8699 (C:0.8699, R:0.0107)
Batch  25/537: Loss=0.8266 (C:0.8266, R:0.0107)
Batch  50/537: Loss=0.8722 (C:0.8722, R:0.0107)
Batch  75/537: Loss=0.8759 (C:0.8759, R:0.0108)
Batch 100/537: Loss=0.8415 (C:0.8415, R:0.0107)
Batch 125/537: Loss=0.8720 (C:0.8720, R:0.0107)
Batch 150/537: Loss=0.8421 (C:0.8421, R:0.0107)
Batch 175/537: Loss=0.8541 (C:0.8541, R:0.0107)
Batch 200/537: Loss=0.8934 (C:0.8934, R:0.0107)
Batch 225/537: Loss=0.8675 (C:0.8675, R:0.0107)
Batch 250/537: Loss=0.8516 (C:0.8516, R:0.0107)
Batch 275/537: Loss=0.9198 (C:0.9198, R:0.0107)
Batch 300/537: Loss=0.8870 (C:0.8870, R:0.0107)
Batch 325/537: Loss=0.8211 (C:0.8211, R:0.0107)
Batch 350/537: Loss=0.8624 (C:0.8624, R:0.0107)
Batch 375/537: Loss=0.8891 (C:0.8891, R:0.0107)
Batch 400/537: Loss=0.8180 (C:0.8180, R:0.0107)
Batch 425/537: Loss=0.8536 (C:0.8536, R:0.0107)
Batch 450/537: Loss=0.8796 (C:0.8796, R:0.0107)
Batch 475/537: Loss=0.8298 (C:0.8298, R:0.0107)
Batch 500/537: Loss=0.8414 (C:0.8414, R:0.0107)
Batch 525/537: Loss=0.8788 (C:0.8788, R:0.0107)

============================================================
Epoch 63/200 completed in 26.1s
Train: Loss=0.8613 (C:0.8613, R:0.0107) Ratio=4.70x
Val:   Loss=0.9333 (C:0.9333, R:0.0106) Ratio=3.26x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 64
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.295 ¬± 0.645
    Neg distances: 1.557 ¬± 0.840
    Separation ratio: 5.27x
    Gap: -2.337
    ‚úÖ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.8993 (C:0.8993, R:0.0108)
Batch  25/537: Loss=0.8314 (C:0.8314, R:0.0107)
Batch  50/537: Loss=0.8692 (C:0.8692, R:0.0107)
Batch  75/537: Loss=0.8470 (C:0.8470, R:0.0107)
Batch 100/537: Loss=0.8589 (C:0.8589, R:0.0107)
Batch 125/537: Loss=0.8456 (C:0.8456, R:0.0107)
Batch 150/537: Loss=0.8395 (C:0.8395, R:0.0107)
Batch 175/537: Loss=0.8695 (C:0.8695, R:0.0107)
Batch 200/537: Loss=0.8882 (C:0.8882, R:0.0107)
Batch 225/537: Loss=0.8494 (C:0.8494, R:0.0107)
Batch 250/537: Loss=0.8914 (C:0.8914, R:0.0107)
Batch 275/537: Loss=0.8941 (C:0.8941, R:0.0107)
Batch 300/537: Loss=0.8127 (C:0.8127, R:0.0107)
Batch 325/537: Loss=0.8823 (C:0.8823, R:0.0107)
Batch 350/537: Loss=0.8257 (C:0.8257, R:0.0107)
Batch 375/537: Loss=0.8461 (C:0.8461, R:0.0107)
Batch 400/537: Loss=0.8600 (C:0.8600, R:0.0107)
Batch 425/537: Loss=0.8725 (C:0.8725, R:0.0107)
Batch 450/537: Loss=0.8614 (C:0.8614, R:0.0107)
Batch 475/537: Loss=0.8628 (C:0.8628, R:0.0107)
Batch 500/537: Loss=0.8738 (C:0.8738, R:0.0107)
Batch 525/537: Loss=0.8769 (C:0.8769, R:0.0107)

============================================================
Epoch 64/200 completed in 34.9s
Train: Loss=0.8686 (C:0.8686, R:0.0107) Ratio=4.77x
Val:   Loss=0.9364 (C:0.9364, R:0.0106) Ratio=3.37x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.9123 (C:0.9123, R:0.0107)
Batch  25/537: Loss=0.8657 (C:0.8657, R:0.0107)
Batch  50/537: Loss=0.8677 (C:0.8677, R:0.0107)
Batch  75/537: Loss=0.9024 (C:0.9024, R:0.0106)
Batch 100/537: Loss=0.9061 (C:0.9061, R:0.0107)
Batch 125/537: Loss=0.8355 (C:0.8355, R:0.0107)
Batch 150/537: Loss=0.8647 (C:0.8647, R:0.0107)
Batch 175/537: Loss=0.8610 (C:0.8610, R:0.0107)
Batch 200/537: Loss=0.8647 (C:0.8647, R:0.0107)
Batch 225/537: Loss=0.8286 (C:0.8286, R:0.0107)
Batch 250/537: Loss=0.8185 (C:0.8185, R:0.0107)
Batch 275/537: Loss=0.8871 (C:0.8871, R:0.0107)
Batch 300/537: Loss=0.8509 (C:0.8509, R:0.0107)
Batch 325/537: Loss=0.8643 (C:0.8643, R:0.0107)
Batch 350/537: Loss=0.9077 (C:0.9077, R:0.0107)
Batch 375/537: Loss=0.8512 (C:0.8512, R:0.0108)
Batch 400/537: Loss=0.8227 (C:0.8227, R:0.0108)
Batch 425/537: Loss=0.8504 (C:0.8504, R:0.0107)
Batch 450/537: Loss=0.8550 (C:0.8550, R:0.0107)
Batch 475/537: Loss=0.8847 (C:0.8847, R:0.0107)
Batch 500/537: Loss=0.8774 (C:0.8774, R:0.0107)
Batch 525/537: Loss=0.8370 (C:0.8370, R:0.0107)

============================================================
Epoch 65/200 completed in 26.0s
Train: Loss=0.8691 (C:0.8691, R:0.0107) Ratio=4.72x
Val:   Loss=0.9417 (C:0.9417, R:0.0106) Ratio=3.28x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.8358 (C:0.8358, R:0.0107)
Batch  25/537: Loss=0.8878 (C:0.8878, R:0.0107)
Batch  50/537: Loss=0.8997 (C:0.8997, R:0.0107)
Batch  75/537: Loss=0.8288 (C:0.8288, R:0.0107)
Batch 100/537: Loss=0.8954 (C:0.8954, R:0.0107)
Batch 125/537: Loss=0.8772 (C:0.8772, R:0.0107)
Batch 150/537: Loss=0.8453 (C:0.8453, R:0.0107)
Batch 175/537: Loss=0.8188 (C:0.8188, R:0.0107)
Batch 200/537: Loss=0.8757 (C:0.8757, R:0.0107)
Batch 225/537: Loss=0.8793 (C:0.8793, R:0.0107)
Batch 250/537: Loss=0.8852 (C:0.8852, R:0.0107)
Batch 275/537: Loss=0.8443 (C:0.8443, R:0.0107)
Batch 300/537: Loss=0.8911 (C:0.8911, R:0.0107)
Batch 325/537: Loss=0.8751 (C:0.8751, R:0.0107)
Batch 350/537: Loss=0.8619 (C:0.8619, R:0.0107)
Batch 375/537: Loss=0.8848 (C:0.8848, R:0.0107)
Batch 400/537: Loss=0.8425 (C:0.8425, R:0.0108)
Batch 425/537: Loss=0.8148 (C:0.8148, R:0.0107)
Batch 450/537: Loss=0.8695 (C:0.8695, R:0.0107)
Batch 475/537: Loss=0.8452 (C:0.8452, R:0.0107)
Batch 500/537: Loss=0.8551 (C:0.8551, R:0.0107)
Batch 525/537: Loss=0.8707 (C:0.8707, R:0.0107)

============================================================
Epoch 66/200 completed in 26.0s
Train: Loss=0.8681 (C:0.8681, R:0.0107) Ratio=4.73x
Val:   Loss=0.9334 (C:0.9334, R:0.0106) Ratio=3.32x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 67
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.302 ¬± 0.662
    Neg distances: 1.567 ¬± 0.834
    Separation ratio: 5.18x
    Gap: -2.336
    ‚úÖ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.8413 (C:0.8413, R:0.0107)
Batch  25/537: Loss=0.8316 (C:0.8316, R:0.0107)
Batch  50/537: Loss=0.8505 (C:0.8505, R:0.0107)
Batch  75/537: Loss=0.8377 (C:0.8377, R:0.0107)
Batch 100/537: Loss=0.8842 (C:0.8842, R:0.0107)
Batch 125/537: Loss=0.8645 (C:0.8645, R:0.0107)
Batch 150/537: Loss=0.8292 (C:0.8292, R:0.0107)
Batch 175/537: Loss=0.8177 (C:0.8177, R:0.0107)
Batch 200/537: Loss=0.8685 (C:0.8685, R:0.0108)
Batch 225/537: Loss=0.8433 (C:0.8433, R:0.0107)
Batch 250/537: Loss=0.8800 (C:0.8800, R:0.0107)
Batch 275/537: Loss=0.8672 (C:0.8672, R:0.0107)
Batch 300/537: Loss=0.8357 (C:0.8357, R:0.0107)
Batch 325/537: Loss=0.8361 (C:0.8361, R:0.0107)
Batch 350/537: Loss=0.8813 (C:0.8813, R:0.0107)
Batch 375/537: Loss=0.8403 (C:0.8403, R:0.0107)
Batch 400/537: Loss=0.8516 (C:0.8516, R:0.0107)
Batch 425/537: Loss=0.8962 (C:0.8962, R:0.0107)
Batch 450/537: Loss=0.8825 (C:0.8825, R:0.0107)
Batch 475/537: Loss=0.8670 (C:0.8670, R:0.0107)
Batch 500/537: Loss=0.8638 (C:0.8638, R:0.0107)
Batch 525/537: Loss=0.8682 (C:0.8682, R:0.0107)

============================================================
Epoch 67/200 completed in 34.6s
Train: Loss=0.8625 (C:0.8625, R:0.0107) Ratio=4.89x
Val:   Loss=0.9523 (C:0.9523, R:0.0106) Ratio=3.25x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.8758 (C:0.8758, R:0.0107)
Batch  25/537: Loss=0.8976 (C:0.8976, R:0.0107)
Batch  50/537: Loss=0.8558 (C:0.8558, R:0.0107)
Batch  75/537: Loss=0.8318 (C:0.8318, R:0.0107)
Batch 100/537: Loss=0.8553 (C:0.8553, R:0.0107)
Batch 125/537: Loss=0.8457 (C:0.8457, R:0.0107)
Batch 150/537: Loss=0.8679 (C:0.8679, R:0.0107)
Batch 175/537: Loss=0.8684 (C:0.8684, R:0.0107)
Batch 200/537: Loss=0.8501 (C:0.8501, R:0.0107)
Batch 225/537: Loss=0.8328 (C:0.8328, R:0.0107)
Batch 250/537: Loss=0.8939 (C:0.8939, R:0.0107)
Batch 275/537: Loss=0.8556 (C:0.8556, R:0.0107)
Batch 300/537: Loss=0.8531 (C:0.8531, R:0.0107)
Batch 325/537: Loss=0.9058 (C:0.9058, R:0.0107)
Batch 350/537: Loss=0.8752 (C:0.8752, R:0.0107)
Batch 375/537: Loss=0.8454 (C:0.8454, R:0.0107)
Batch 400/537: Loss=0.9038 (C:0.9038, R:0.0107)
Batch 425/537: Loss=0.8705 (C:0.8705, R:0.0107)
Batch 450/537: Loss=0.8654 (C:0.8654, R:0.0107)
Batch 475/537: Loss=0.8715 (C:0.8715, R:0.0107)
Batch 500/537: Loss=0.8848 (C:0.8848, R:0.0107)
Batch 525/537: Loss=0.8646 (C:0.8646, R:0.0107)

============================================================
Epoch 68/200 completed in 25.9s
Train: Loss=0.8646 (C:0.8646, R:0.0107) Ratio=4.61x
Val:   Loss=0.9435 (C:0.9435, R:0.0106) Ratio=3.30x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.8392 (C:0.8392, R:0.0107)
Batch  25/537: Loss=0.8715 (C:0.8715, R:0.0107)
Batch  50/537: Loss=0.8411 (C:0.8411, R:0.0107)
Batch  75/537: Loss=0.8831 (C:0.8831, R:0.0107)
Batch 100/537: Loss=0.8521 (C:0.8521, R:0.0107)
Batch 125/537: Loss=0.8467 (C:0.8467, R:0.0107)
Batch 150/537: Loss=0.8408 (C:0.8408, R:0.0107)
Batch 175/537: Loss=0.8514 (C:0.8514, R:0.0107)
Batch 200/537: Loss=0.8735 (C:0.8735, R:0.0107)
Batch 225/537: Loss=0.8177 (C:0.8177, R:0.0107)
Batch 250/537: Loss=0.8735 (C:0.8735, R:0.0107)
Batch 275/537: Loss=0.8677 (C:0.8677, R:0.0107)
Batch 300/537: Loss=0.8653 (C:0.8653, R:0.0107)
Batch 325/537: Loss=0.8415 (C:0.8415, R:0.0107)
Batch 350/537: Loss=0.8720 (C:0.8720, R:0.0107)
Batch 375/537: Loss=0.8611 (C:0.8611, R:0.0107)
Batch 400/537: Loss=0.8298 (C:0.8298, R:0.0107)
Batch 425/537: Loss=0.8810 (C:0.8810, R:0.0107)
Batch 450/537: Loss=0.8696 (C:0.8696, R:0.0107)
Batch 475/537: Loss=0.8971 (C:0.8971, R:0.0107)
Batch 500/537: Loss=0.9165 (C:0.9165, R:0.0107)
Batch 525/537: Loss=0.8889 (C:0.8889, R:0.0107)

============================================================
Epoch 69/200 completed in 25.8s
Train: Loss=0.8634 (C:0.8634, R:0.0107) Ratio=4.60x
Val:   Loss=0.9374 (C:0.9374, R:0.0106) Ratio=3.31x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 69 epochs
Best model was at epoch 61 with Val Loss: 0.9313

Global Dataset Training Completed!
Best epoch: 61
Best validation loss: 0.9313
Final separation ratios: Train=4.60x, Val=3.31x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4490
  Adjusted Rand Score: 0.5517
  Clustering Accuracy: 0.8236
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8260
  Per-class F1: [0.8537398761314912, 0.7668721879686718, 0.8558231606730452]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010623
Evaluating separation quality...
Separation Results:
  Positive distances: 0.460 ¬± 0.781
  Negative distances: 1.490 ¬± 0.880
  Separation ratio: 3.24x
  Gap: -2.337
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4490
  Clustering Accuracy: 0.8236
  Adjusted Rand Score: 0.5517

Classification Performance:
  Accuracy: 0.8260

Separation Quality:
  Separation Ratio: 3.24x
  Gap: -2.337
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010623
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846/results/evaluation_results_20250724_111244.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846/results/evaluation_results_20250724_111244.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_with_attention_20250724_103846/final_results.json

Key Results:
  Separation ratio: 3.24x
  Perfect separation: False
  Classification accuracy: 0.8260

Analysis completed with exit code: 0
Time: Thu 24 Jul 11:12:45 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
