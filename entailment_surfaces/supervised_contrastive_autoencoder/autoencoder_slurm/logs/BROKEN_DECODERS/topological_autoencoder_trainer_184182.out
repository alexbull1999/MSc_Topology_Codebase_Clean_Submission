Starting Surface Distance Metric Analysis job...
Job ID: 184182
Node: gpuvm13
Time: Sun 20 Jul 12:26:26 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sun Jul 20 12:26:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_122636
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_122636/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,858,891
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,858,891
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.01
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=2.0011 (C:2.0000, R:0.0110, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.7302 (C:1.7292, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.5420 (C:1.5410, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.5402 (C:1.5392, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.5089 (C:1.5079, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.4910 (C:1.4900, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.4855 (C:1.4845, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.4708 (C:1.4698, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.4469 (C:1.4459, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.4437 (C:1.4427, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.4073 (C:1.4063, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.4123 (C:1.4113, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3863 (C:1.3853, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.4667 (C:1.4657, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3988 (C:1.3978, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5091
  Contrastive: 1.5081
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3394
  Contrastive: 1.3384
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (20.2s)
Train Loss: 1.5091 (C:1.5081, R:0.0100, T:0.0000)
Val Loss:   1.3394 (C:1.3384, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3620 (C:1.3610, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.4042 (C:1.4032, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.4248 (C:1.4238, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3633 (C:1.3623, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3824 (C:1.3814, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.4042 (C:1.4032, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3896 (C:1.3886, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.4462 (C:1.4452, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3962 (C:1.3952, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.4250 (C:1.4240, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3843 (C:1.3833, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3603 (C:1.3593, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.4060 (C:1.4050, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3747 (C:1.3737, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3782 (C:1.3772, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.3867
  Contrastive: 1.3857
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2934
  Contrastive: 1.2924
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (22.0s)
Train Loss: 1.3867 (C:1.3857, R:0.0100, T:0.0000)
Val Loss:   1.2934 (C:1.2924, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2980 (C:1.2970, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3970 (C:1.3960, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2956 (C:1.2946, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2920 (C:1.2910, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3406 (C:1.3396, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3552 (C:1.3542, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3373 (C:1.3363, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.4201 (C:1.4191, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3709 (C:1.3699, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3198 (C:1.3188, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3936 (C:1.3926, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3295 (C:1.3285, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2921 (C:1.2911, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.4099 (C:1.4089, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3983 (C:1.3973, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.3599
  Contrastive: 1.3589
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3200
  Contrastive: 1.3190
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 3/50 COMPLETE (20.9s)
Train Loss: 1.3599 (C:1.3589, R:0.0100, T:0.0000)
Val Loss:   1.3200 (C:1.3190, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3376 (C:1.3366, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3660 (C:1.3651, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3698 (C:1.3688, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3301 (C:1.3291, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3437 (C:1.3427, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3644 (C:1.3635, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3218 (C:1.3208, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3223 (C:1.3214, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3365 (C:1.3355, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3646 (C:1.3636, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3915 (C:1.3905, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3432 (C:1.3422, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3134 (C:1.3125, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2981 (C:1.2971, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3219 (C:1.3209, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.3409
  Contrastive: 1.3399
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2739
  Contrastive: 1.2729
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (21.0s)
Train Loss: 1.3409 (C:1.3399, R:0.0100, T:0.0000)
Val Loss:   1.2739 (C:1.2729, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2634 (C:1.2624, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3290 (C:1.3280, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3339 (C:1.3329, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3321 (C:1.3311, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3774 (C:1.3764, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2710 (C:1.2700, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3345 (C:1.3335, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3440 (C:1.3430, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3090 (C:1.3080, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3468 (C:1.3458, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2507 (C:1.2497, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3095 (C:1.3085, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3200 (C:1.3190, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3049 (C:1.3039, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2741 (C:1.2731, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.3251
  Contrastive: 1.3241
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2561
  Contrastive: 1.2551
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (20.6s)
Train Loss: 1.3251 (C:1.3241, R:0.0100, T:0.0000)
Val Loss:   1.2561 (C:1.2551, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3362 (C:1.3352, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3147 (C:1.3137, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3043 (C:1.3033, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2535 (C:1.2525, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3491 (C:1.3481, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3085 (C:1.3075, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3392 (C:1.3382, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2488 (C:1.2478, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3552 (C:1.3542, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3738 (C:1.3728, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2959 (C:1.2949, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3446 (C:1.3436, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2729 (C:1.2719, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3852 (C:1.3842, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3543 (C:1.3533, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.3132
  Contrastive: 1.3122
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2521
  Contrastive: 1.2511
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (21.3s)
Train Loss: 1.3132 (C:1.3122, R:0.0100, T:0.0000)
Val Loss:   1.2521 (C:1.2511, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2184 (C:1.2174, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3507 (C:1.3497, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3275 (C:1.3265, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2511 (C:1.2501, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2756 (C:1.2746, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2831 (C:1.2822, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3123 (C:1.3113, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2647 (C:1.2637, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3535 (C:1.3525, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3150 (C:1.3140, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2918 (C:1.2908, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3327 (C:1.3317, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2745 (C:1.2735, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2743 (C:1.2733, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3673 (C:1.3663, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.3037
  Contrastive: 1.3028
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2382
  Contrastive: 1.2372
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 7/50 COMPLETE (21.1s)
Train Loss: 1.3037 (C:1.3028, R:0.0100, T:0.0000)
Val Loss:   1.2382 (C:1.2372, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3185 (C:1.3175, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2543 (C:1.2533, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3254 (C:1.3244, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2914 (C:1.2904, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2805 (C:1.2795, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3005 (C:1.2995, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3178 (C:1.3168, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2765 (C:1.2755, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2357 (C:1.2347, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2881 (C:1.2871, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3006 (C:1.2996, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3537 (C:1.3527, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2844 (C:1.2834, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3530 (C:1.3520, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2707 (C:1.2697, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.2911
  Contrastive: 1.2901
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2108
  Contrastive: 1.2098
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 8/50 COMPLETE (21.8s)
Train Loss: 1.2911 (C:1.2901, R:0.0100, T:0.0000)
Val Loss:   1.2108 (C:1.2098, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2786 (C:1.2776, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3649 (C:1.3639, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2501 (C:1.2491, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2915 (C:1.2905, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2653 (C:1.2643, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3012 (C:1.3002, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2773 (C:1.2763, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2735 (C:1.2725, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2859 (C:1.2849, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2520 (C:1.2510, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3046 (C:1.3036, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3212 (C:1.3202, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3316 (C:1.3306, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2681 (C:1.2671, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2507 (C:1.2497, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.2853
  Contrastive: 1.2843
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2221
  Contrastive: 1.2211
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 9/50 COMPLETE (20.9s)
Train Loss: 1.2853 (C:1.2843, R:0.0100, T:0.0000)
Val Loss:   1.2221 (C:1.2211, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2453 (C:1.2443, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2549 (C:1.2539, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2081 (C:1.2071, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2575 (C:1.2565, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2254 (C:1.2244, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2649 (C:1.2639, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2845 (C:1.2835, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2665 (C:1.2655, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3313 (C:1.3303, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2658 (C:1.2648, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2487 (C:1.2477, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3120 (C:1.3110, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3199 (C:1.3189, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2796 (C:1.2786, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3451 (C:1.3441, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.2703
  Contrastive: 1.2693
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1894
  Contrastive: 1.1885
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 10/50 COMPLETE (21.7s)
Train Loss: 1.2703 (C:1.2693, R:0.0100, T:0.0000)
Val Loss:   1.1894 (C:1.1885, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 0.0100
🌱 Early topological learning
============================================================
Batch   0/365: Loss=72.7834 (C:1.2111, R:0.0100, T:71.5713(w:0.010)⚠️)
Batch  25/365: Loss=32.3097 (C:21.4142, R:0.0100, T:10.8946(w:0.010)⚠️)
Batch  50/365: Loss=18.8248 (C:9.5321, R:0.0100, T:9.2917(w:0.010)🚀)
Batch  75/365: Loss=15.8407 (C:11.9798, R:0.0099, T:3.8599(w:0.010)🚀)
Batch 100/365: Loss=11.6714 (C:9.4018, R:0.0100, T:2.2687(w:0.010)🚀)
Batch 125/365: Loss=9.5156 (C:8.7121, R:0.0100, T:0.8025(w:0.010)🎉)
Batch 150/365: Loss=8.4837 (C:8.0252, R:0.0100, T:0.4575(w:0.010)🎉)
Batch 175/365: Loss=8.9382 (C:8.4196, R:0.0100, T:0.5176(w:0.010)🎉)
Batch 200/365: Loss=7.1415 (C:6.7924, R:0.0100, T:0.3480(w:0.010)🎉)
Batch 225/365: Loss=7.2281 (C:6.8539, R:0.0100, T:0.3732(w:0.010)🎉)
Batch 250/365: Loss=7.0224 (C:6.5693, R:0.0100, T:0.4521(w:0.010)🎉)
Batch 275/365: Loss=7.6412 (C:6.6609, R:0.0100, T:0.9793(w:0.010)🎉)
Batch 300/365: Loss=7.7346 (C:7.0746, R:0.0099, T:0.6589(w:0.010)🎉)
Batch 325/365: Loss=6.7546 (C:6.1999, R:0.0099, T:0.5537(w:0.010)🎉)
Batch 350/365: Loss=7.4268 (C:6.1708, R:0.0099, T:1.2550(w:0.010)🚀)
🎉 MILESTONE: First topological learning detected at epoch 11!
   Initial topological loss: 5.8438
📈 New best topological loss: 5.8438

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 14.0080
  Contrastive: 8.1632
  Reconstruction: 0.0100
  Topological: 5.8438 (weight: 0.010)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 46.0355
  Contrastive: 3.2505
  Reconstruction: 0.0100
  Topological: 42.7840 (weight: 0.010)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 11/50 COMPLETE (153.0s)
Train Loss: 14.0080 (C:8.1632, R:0.0100, T:5.8438)
Val Loss:   46.0355 (C:3.2505, R:0.0100, T:42.7840)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 0.0110
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.4924 (C:6.1870, R:0.0100, T:0.3044(w:0.011)🎉)
Batch  25/365: Loss=6.7290 (C:6.2296, R:0.0099, T:0.4984(w:0.011)🎉)
Batch  50/365: Loss=6.3511 (C:6.1005, R:0.0100, T:0.2496(w:0.011)🎉)
Batch  75/365: Loss=6.5609 (C:5.9228, R:0.0099, T:0.6371(w:0.011)🎉)
Batch 100/365: Loss=6.4395 (C:6.1977, R:0.0100, T:0.2408(w:0.011)🎉)
Batch 125/365: Loss=6.0551 (C:5.7595, R:0.0100, T:0.2946(w:0.011)🎉)
Batch 150/365: Loss=6.2062 (C:5.8894, R:0.0100, T:0.3158(w:0.011)🎉)
Batch 175/365: Loss=6.0373 (C:5.9286, R:0.0099, T:0.1078(w:0.011)🎉)
Batch 200/365: Loss=6.0714 (C:5.9116, R:0.0100, T:0.1588(w:0.011)🎉)
Batch 225/365: Loss=6.2186 (C:5.6969, R:0.0100, T:0.5206(w:0.011)🎉)
Batch 250/365: Loss=7.1917 (C:6.5109, R:0.0099, T:0.6798(w:0.011)🎉)
Batch 275/365: Loss=6.4450 (C:5.7994, R:0.0099, T:0.6446(w:0.011)🎉)
Batch 300/365: Loss=6.8795 (C:6.8333, R:0.0100, T:0.0452(w:0.011)🎉)
Batch 325/365: Loss=6.8915 (C:6.3801, R:0.0100, T:0.5103(w:0.011)🎉)
Batch 350/365: Loss=6.5346 (C:6.3801, R:0.0099, T:0.1536(w:0.011)🎉)
📈 New best topological loss: 0.5084

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 6.6252
  Contrastive: 6.1158
  Reconstruction: 0.0100
  Topological: 0.5084 (weight: 0.011)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 38.5698
  Contrastive: 3.8152
  Reconstruction: 0.0100
  Topological: 34.7536 (weight: 0.011)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (162.9s)
Train Loss: 6.6252 (C:6.1158, R:0.0100, T:0.5084)
Val Loss:   38.5698 (C:3.8152, R:0.0100, T:34.7536)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 0.0120
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.4941 (C:6.3592, R:0.0099, T:0.1340(w:0.012)🎉)
Batch  25/365: Loss=6.5044 (C:6.1009, R:0.0100, T:0.4025(w:0.012)🎉)
Batch  50/365: Loss=6.4349 (C:5.8822, R:0.0100, T:0.5517(w:0.012)🎉)
Batch  75/365: Loss=5.8236 (C:5.8135, R:0.0099, T:0.0091(w:0.012)🎉)
Batch 100/365: Loss=6.3761 (C:5.7256, R:0.0099, T:0.6496(w:0.012)🎉)
Batch 125/365: Loss=6.4159 (C:5.8813, R:0.0100, T:0.5337(w:0.012)🎉)
Batch 150/365: Loss=6.4607 (C:6.1866, R:0.0099, T:0.2731(w:0.012)🎉)
Batch 175/365: Loss=6.1902 (C:5.9870, R:0.0099, T:0.2022(w:0.012)🎉)
Batch 200/365: Loss=6.3811 (C:5.9866, R:0.0099, T:0.3935(w:0.012)🎉)
Batch 225/365: Loss=6.9808 (C:5.7518, R:0.0100, T:1.2280(w:0.012)🚀)
Batch 250/365: Loss=6.7525 (C:5.6902, R:0.0099, T:1.0613(w:0.012)🚀)
Batch 275/365: Loss=6.3615 (C:5.8526, R:0.0100, T:0.5079(w:0.012)🎉)
Batch 300/365: Loss=5.8680 (C:5.7111, R:0.0099, T:0.1559(w:0.012)🎉)
Batch 325/365: Loss=5.9543 (C:5.6045, R:0.0099, T:0.3488(w:0.012)🎉)
Batch 350/365: Loss=6.1527 (C:5.9437, R:0.0100, T:0.2080(w:0.012)🎉)
📈 New best topological loss: 0.4818

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 6.3790
  Contrastive: 5.8962
  Reconstruction: 0.0100
  Topological: 0.4818 (weight: 0.012)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 42.4181
  Contrastive: 3.1485
  Reconstruction: 0.0100
  Topological: 39.2687 (weight: 0.012)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 13/50 COMPLETE (165.6s)
Train Loss: 6.3790 (C:5.8962, R:0.0100, T:0.4818)
Val Loss:   42.4181 (C:3.1485, R:0.0100, T:39.2687)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 0.0130
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.2577 (C:5.8354, R:0.0100, T:0.4213(w:0.013)🎉)
Batch  25/365: Loss=5.9280 (C:5.7782, R:0.0100, T:0.1487(w:0.013)🎉)
Batch  50/365: Loss=6.6810 (C:6.0626, R:0.0099, T:0.6173(w:0.013)🎉)
Batch  75/365: Loss=6.4449 (C:6.2642, R:0.0100, T:0.1797(w:0.013)🎉)
Batch 100/365: Loss=6.1422 (C:5.8135, R:0.0100, T:0.3277(w:0.013)🎉)
Batch 125/365: Loss=7.3662 (C:6.0351, R:0.0100, T:1.3301(w:0.013)🚀)
Batch 150/365: Loss=6.1020 (C:5.8935, R:0.0099, T:0.2075(w:0.013)🎉)
Batch 175/365: Loss=6.1706 (C:6.0635, R:0.0100, T:0.1061(w:0.013)🎉)
Batch 200/365: Loss=6.6807 (C:5.8116, R:0.0100, T:0.8682(w:0.013)🎉)
Batch 225/365: Loss=5.9124 (C:5.8022, R:0.0099, T:0.1092(w:0.013)🎉)
Batch 250/365: Loss=6.4181 (C:5.4340, R:0.0100, T:0.9831(w:0.013)🎉)
Batch 275/365: Loss=6.1319 (C:6.0728, R:0.0100, T:0.0582(w:0.013)🎉)
Batch 300/365: Loss=6.3632 (C:5.9435, R:0.0099, T:0.4187(w:0.013)🎉)
Batch 325/365: Loss=6.0610 (C:5.8372, R:0.0099, T:0.2229(w:0.013)🎉)
Batch 350/365: Loss=5.7328 (C:5.5148, R:0.0100, T:0.2170(w:0.013)🎉)
📈 New best topological loss: 0.4275

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 6.2668
  Contrastive: 5.8383
  Reconstruction: 0.0100
  Topological: 0.4275 (weight: 0.013)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 53.9687
  Contrastive: 2.8577
  Reconstruction: 0.0100
  Topological: 51.1099 (weight: 0.013)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 14/50 COMPLETE (164.5s)
Train Loss: 6.2668 (C:5.8383, R:0.0100, T:0.4275)
Val Loss:   53.9687 (C:2.8577, R:0.0100, T:51.1099)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 0.0140
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.1696 (C:6.0034, R:0.0100, T:0.1652(w:0.014)🎉)
Batch  25/365: Loss=6.6293 (C:5.8184, R:0.0100, T:0.8099(w:0.014)🎉)
Batch  50/365: Loss=7.2742 (C:6.1087, R:0.0100, T:1.1645(w:0.014)🚀)
Batch  75/365: Loss=6.7423 (C:5.3856, R:0.0100, T:1.3557(w:0.014)🚀)
Batch 100/365: Loss=6.0073 (C:5.8614, R:0.0099, T:0.1449(w:0.014)🎉)
Batch 125/365: Loss=6.6383 (C:6.4376, R:0.0100, T:0.1997(w:0.014)🎉)
Batch 150/365: Loss=6.2912 (C:5.9707, R:0.0100, T:0.3195(w:0.014)🎉)
Batch 175/365: Loss=5.8803 (C:5.7425, R:0.0100, T:0.1368(w:0.014)🎉)
Batch 200/365: Loss=5.8910 (C:5.7180, R:0.0100, T:0.1720(w:0.014)🎉)
Batch 225/365: Loss=6.9358 (C:5.4235, R:0.0099, T:1.5113(w:0.014)🚀)
Batch 250/365: Loss=7.2559 (C:5.7319, R:0.0099, T:1.5230(w:0.014)🚀)
Batch 275/365: Loss=6.4901 (C:6.0125, R:0.0100, T:0.4767(w:0.014)🎉)
Batch 300/365: Loss=5.9067 (C:5.8751, R:0.0100, T:0.0307(w:0.014)🎉)
Batch 325/365: Loss=6.3281 (C:5.6525, R:0.0099, T:0.6746(w:0.014)🎉)
Batch 350/365: Loss=6.1007 (C:5.9964, R:0.0099, T:0.1033(w:0.014)🎉)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 6.2855
  Contrastive: 5.8184
  Reconstruction: 0.0100
  Topological: 0.4661 (weight: 0.014)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 62.9572
  Contrastive: 3.0191
  Reconstruction: 0.0100
  Topological: 59.9372 (weight: 0.014)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 15/50 COMPLETE (164.2s)
Train Loss: 6.2855 (C:5.8184, R:0.0100, T:0.4661)
Val Loss:   62.9572 (C:3.0191, R:0.0100, T:59.9372)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 0.0150
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.0771 (C:5.7993, R:0.0100, T:0.2768(w:0.015)🎉)
Batch  25/365: Loss=7.2702 (C:6.4021, R:0.0100, T:0.8670(w:0.015)🎉)
Batch  50/365: Loss=6.6753 (C:5.2664, R:0.0100, T:1.4079(w:0.015)🚀)
Batch  75/365: Loss=6.1901 (C:5.8719, R:0.0100, T:0.3171(w:0.015)🎉)
Batch 100/365: Loss=6.6343 (C:5.7378, R:0.0100, T:0.8955(w:0.015)🎉)
Batch 125/365: Loss=6.1040 (C:5.9879, R:0.0099, T:0.1152(w:0.015)🎉)
Batch 150/365: Loss=6.9260 (C:5.8538, R:0.0100, T:1.0712(w:0.015)🚀)
Batch 175/365: Loss=7.3010 (C:6.4861, R:0.0100, T:0.8139(w:0.015)🎉)
Batch 200/365: Loss=6.5440 (C:5.7818, R:0.0100, T:0.7612(w:0.015)🎉)
Batch 225/365: Loss=6.3598 (C:5.8839, R:0.0099, T:0.4750(w:0.015)🎉)
Batch 250/365: Loss=6.0013 (C:5.8517, R:0.0099, T:0.1486(w:0.015)🎉)
Batch 275/365: Loss=5.9490 (C:5.7031, R:0.0100, T:0.2449(w:0.015)🎉)
Batch 300/365: Loss=6.2826 (C:5.8815, R:0.0099, T:0.4001(w:0.015)🎉)
Batch 325/365: Loss=7.5105 (C:5.6573, R:0.0100, T:1.8522(w:0.015)🚀)
Batch 350/365: Loss=6.6032 (C:6.0926, R:0.0099, T:0.5096(w:0.015)🎉)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 6.5529
  Contrastive: 5.9729
  Reconstruction: 0.0100
  Topological: 0.5790 (weight: 0.015)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 58.4305
  Contrastive: 3.5835
  Reconstruction: 0.0100
  Topological: 54.8460 (weight: 0.015)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 16/50 COMPLETE (166.8s)
Train Loss: 6.5529 (C:5.9729, R:0.0100, T:0.5790)
Val Loss:   58.4305 (C:3.5835, R:0.0100, T:54.8460)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 0.0160
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.7951 (C:6.6524, R:0.0100, T:0.1417(w:0.016)🎉)
Batch  25/365: Loss=6.5134 (C:6.0980, R:0.0100, T:0.4144(w:0.016)🎉)
Batch  50/365: Loss=6.2368 (C:5.9830, R:0.0099, T:0.2528(w:0.016)🎉)
Batch  75/365: Loss=6.4309 (C:6.1944, R:0.0099, T:0.2355(w:0.016)🎉)
Batch 100/365: Loss=6.3790 (C:6.0229, R:0.0099, T:0.3551(w:0.016)🎉)
Batch 125/365: Loss=6.2544 (C:6.1744, R:0.0100, T:0.0790(w:0.016)🎉)
Batch 150/365: Loss=6.2371 (C:5.8677, R:0.0099, T:0.3684(w:0.016)🎉)
Batch 175/365: Loss=7.1825 (C:5.8822, R:0.0100, T:1.2993(w:0.016)🚀)
Batch 200/365: Loss=6.4231 (C:6.1395, R:0.0100, T:0.2826(w:0.016)🎉)
Batch 225/365: Loss=6.3534 (C:5.9488, R:0.0100, T:0.4037(w:0.016)🎉)
Batch 250/365: Loss=6.4579 (C:6.3099, R:0.0099, T:0.1469(w:0.016)🎉)
Batch 275/365: Loss=6.4406 (C:5.7914, R:0.0099, T:0.6483(w:0.016)🎉)
Batch 300/365: Loss=6.7173 (C:6.1095, R:0.0100, T:0.6068(w:0.016)🎉)
Batch 325/365: Loss=7.0959 (C:6.5250, R:0.0100, T:0.5699(w:0.016)🎉)
Batch 350/365: Loss=7.3688 (C:5.9932, R:0.0100, T:1.3747(w:0.016)🚀)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 6.5168
  Contrastive: 6.0349
  Reconstruction: 0.0100
  Topological: 0.4809 (weight: 0.016)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 63.7467
  Contrastive: 2.9847
  Reconstruction: 0.0100
  Topological: 60.7610 (weight: 0.016)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 17/50 COMPLETE (164.6s)
Train Loss: 6.5168 (C:6.0349, R:0.0100, T:0.4809)
Val Loss:   63.7467 (C:2.9847, R:0.0100, T:60.7610)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 365 | Topological Weight: 0.0170
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.6998 (C:5.8987, R:0.0100, T:0.8001(w:0.017)🎉)
Batch  25/365: Loss=6.0962 (C:5.9885, R:0.0099, T:0.1068(w:0.017)🎉)
Batch  50/365: Loss=6.3341 (C:6.2085, R:0.0100, T:0.1246(w:0.017)🎉)
Batch  75/365: Loss=6.3118 (C:5.9140, R:0.0100, T:0.3968(w:0.017)🎉)
Batch 100/365: Loss=6.0649 (C:5.9499, R:0.0100, T:0.1140(w:0.017)🎉)
Batch 125/365: Loss=5.7849 (C:5.7412, R:0.0099, T:0.0427(w:0.017)🎉)
Batch 150/365: Loss=6.5518 (C:5.9235, R:0.0100, T:0.6274(w:0.017)🎉)
Batch 175/365: Loss=6.6223 (C:6.3800, R:0.0100, T:0.2413(w:0.017)🎉)
Batch 200/365: Loss=7.0989 (C:6.1701, R:0.0099, T:0.9278(w:0.017)🎉)
Batch 225/365: Loss=7.1323 (C:6.3007, R:0.0099, T:0.8306(w:0.017)🎉)
Batch 250/365: Loss=6.0269 (C:5.8038, R:0.0100, T:0.2221(w:0.017)🎉)
Batch 275/365: Loss=7.7105 (C:6.0504, R:0.0099, T:1.6590(w:0.017)🚀)
Batch 300/365: Loss=6.6113 (C:6.4015, R:0.0100, T:0.2088(w:0.017)🎉)
Batch 325/365: Loss=6.0588 (C:5.8670, R:0.0100, T:0.1908(w:0.017)🎉)
Batch 350/365: Loss=6.6129 (C:6.3273, R:0.0099, T:0.2846(w:0.017)🎉)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 6.6099
  Contrastive: 6.0600
  Reconstruction: 0.0100
  Topological: 0.5489 (weight: 0.017)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 67.5889
  Contrastive: 3.2535
  Reconstruction: 0.0100
  Topological: 64.3344 (weight: 0.017)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 18/50 COMPLETE (164.7s)
Train Loss: 6.6099 (C:6.0600, R:0.0100, T:0.5489)
Val Loss:   67.5889 (C:3.2535, R:0.0100, T:64.3344)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 365 | Topological Weight: 0.0180
🌱 Early topological learning
============================================================
Batch   0/365: Loss=7.6431 (C:6.1957, R:0.0100, T:1.4464(w:0.018)🚀)
Batch  25/365: Loss=6.3356 (C:5.9419, R:0.0100, T:0.3927(w:0.018)🎉)
Batch  50/365: Loss=6.3544 (C:5.7739, R:0.0100, T:0.5795(w:0.018)🎉)
Batch  75/365: Loss=6.9044 (C:6.5449, R:0.0100, T:0.3585(w:0.018)🎉)
Batch 100/365: Loss=6.7589 (C:6.5331, R:0.0100, T:0.2249(w:0.018)🎉)
Batch 125/365: Loss=6.5261 (C:6.3997, R:0.0099, T:0.1255(w:0.018)🎉)
Batch 150/365: Loss=6.9102 (C:6.4740, R:0.0099, T:0.4352(w:0.018)🎉)
Batch 175/365: Loss=6.3448 (C:6.2221, R:0.0100, T:0.1217(w:0.018)🎉)
Batch 200/365: Loss=7.4941 (C:6.4392, R:0.0100, T:1.0539(w:0.018)🚀)
Batch 225/365: Loss=7.0162 (C:6.4149, R:0.0099, T:0.6003(w:0.018)🎉)
Batch 250/365: Loss=6.0542 (C:6.0054, R:0.0100, T:0.0479(w:0.018)🎉)
Batch 275/365: Loss=8.9266 (C:6.6037, R:0.0100, T:2.3218(w:0.018)🚀)
Batch 300/365: Loss=6.7185 (C:6.6123, R:0.0099, T:0.1052(w:0.018)🎉)
Batch 325/365: Loss=7.6347 (C:6.6536, R:0.0099, T:0.9801(w:0.018)🎉)
Batch 350/365: Loss=7.3043 (C:6.3760, R:0.0100, T:0.9273(w:0.018)🎉)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 6.9954
  Contrastive: 6.3327
  Reconstruction: 0.0100
  Topological: 0.6617 (weight: 0.018)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 65.6548
  Contrastive: 3.6471
  Reconstruction: 0.0100
  Topological: 62.0067 (weight: 0.018)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 19/50 COMPLETE (161.2s)
Train Loss: 6.9954 (C:6.3327, R:0.0100, T:0.6617)
Val Loss:   65.6548 (C:3.6471, R:0.0100, T:62.0067)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 365 | Topological Weight: 0.0190
🌱 Early topological learning
============================================================
Batch   0/365: Loss=7.0848 (C:6.2237, R:0.0100, T:0.8602(w:0.019)🎉)
Batch  25/365: Loss=7.8210 (C:6.4255, R:0.0100, T:1.3945(w:0.019)🚀)
Batch  50/365: Loss=7.0977 (C:6.0854, R:0.0100, T:1.0114(w:0.019)🚀)
Batch  75/365: Loss=6.2271 (C:6.0448, R:0.0099, T:0.1814(w:0.019)🎉)
Batch 100/365: Loss=6.6484 (C:6.0375, R:0.0099, T:0.6099(w:0.019)🎉)
Batch 125/365: Loss=6.7376 (C:6.1350, R:0.0099, T:0.6016(w:0.019)🎉)
Batch 150/365: Loss=6.6717 (C:6.1438, R:0.0099, T:0.5269(w:0.019)🎉)
Batch 175/365: Loss=6.2424 (C:6.2105, R:0.0100, T:0.0309(w:0.019)🎉)
Batch 200/365: Loss=7.1820 (C:6.1613, R:0.0100, T:1.0197(w:0.019)🚀)
Batch 225/365: Loss=7.6082 (C:6.1233, R:0.0099, T:1.4839(w:0.019)🚀)
Batch 250/365: Loss=6.3565 (C:6.1606, R:0.0099, T:0.1949(w:0.019)🎉)
Batch 275/365: Loss=6.0786 (C:5.8185, R:0.0100, T:0.2591(w:0.019)🎉)
Batch 300/365: Loss=7.1445 (C:6.6032, R:0.0099, T:0.5402(w:0.019)🎉)
Batch 325/365: Loss=7.1796 (C:6.2174, R:0.0099, T:0.9612(w:0.019)🎉)
Batch 350/365: Loss=6.4029 (C:6.0746, R:0.0100, T:0.3273(w:0.019)🎉)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 6.7758
  Contrastive: 6.1754
  Reconstruction: 0.0100
  Topological: 0.5994 (weight: 0.019)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 79.3300
  Contrastive: 3.1863
  Reconstruction: 0.0100
  Topological: 76.1427 (weight: 0.019)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 20/50 COMPLETE (161.9s)
Train Loss: 6.7758 (C:6.1754, R:0.0100, T:0.5994)
Val Loss:   79.3300 (C:3.1863, R:0.0100, T:76.1427)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 365 | Topological Weight: 0.0200
🌱 Early topological learning
============================================================
Batch   0/365: Loss=6.5168 (C:6.3896, R:0.0100, T:0.1262(w:0.020)🎉)
Batch  25/365: Loss=6.2568 (C:5.9368, R:0.0099, T:0.3191(w:0.020)🎉)
Batch  50/365: Loss=6.8493 (C:5.8947, R:0.0099, T:0.9535(w:0.020)🎉)
Batch  75/365: Loss=6.3609 (C:6.0001, R:0.0100, T:0.3598(w:0.020)🎉)
Batch 100/365: Loss=6.2232 (C:6.0466, R:0.0099, T:0.1756(w:0.020)🎉)
Batch 125/365: Loss=6.5176 (C:6.0361, R:0.0100, T:0.4805(w:0.020)🎉)
Batch 150/365: Loss=6.5023 (C:5.9531, R:0.0100, T:0.5482(w:0.020)🎉)
Batch 175/365: Loss=7.2833 (C:6.3196, R:0.0099, T:0.9626(w:0.020)🎉)
Batch 200/365: Loss=6.5704 (C:6.4999, R:0.0099, T:0.0696(w:0.020)🎉)
Batch 225/365: Loss=9.8900 (C:5.7738, R:0.0099, T:4.1152(w:0.020)🚀)
Batch 250/365: Loss=7.3804 (C:6.8502, R:0.0099, T:0.5293(w:0.020)🎉)
Batch 275/365: Loss=6.7420 (C:6.5125, R:0.0100, T:0.2285(w:0.020)🎉)
Batch 300/365: Loss=7.0533 (C:6.2287, R:0.0100, T:0.8236(w:0.020)🎉)
Batch 325/365: Loss=6.6200 (C:5.8779, R:0.0100, T:0.7411(w:0.020)🎉)
Batch 350/365: Loss=6.3692 (C:6.1411, R:0.0100, T:0.2270(w:0.020)🎉)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 6.8551
  Contrastive: 6.2235
  Reconstruction: 0.0100
  Topological: 0.6306 (weight: 0.020)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 69.1274
  Contrastive: 3.5187
  Reconstruction: 0.0100
  Topological: 65.6077 (weight: 0.020)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 21/50 COMPLETE (163.0s)
Train Loss: 6.8551 (C:6.2235, R:0.0100, T:0.6306)
Val Loss:   69.1274 (C:3.5187, R:0.0100, T:65.6077)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 365 | Topological Weight: 0.0210
🌱 Early topological learning
============================================================
Batch   0/365: Loss=7.8193 (C:6.1895, R:0.0099, T:1.6287(w:0.021)🚀)
Batch  25/365: Loss=7.2599 (C:6.2671, R:0.0100, T:0.9918(w:0.021)🎉)
Batch  50/365: Loss=6.5936 (C:6.2984, R:0.0099, T:0.2942(w:0.021)🎉)
Batch  75/365: Loss=6.8293 (C:6.0761, R:0.0099, T:0.7522(w:0.021)🎉)
Batch 100/365: Loss=6.9581 (C:6.5252, R:0.0099, T:0.4319(w:0.021)🎉)
Batch 125/365: Loss=7.8986 (C:6.2580, R:0.0100, T:1.6396(w:0.021)🚀)
Batch 150/365: Loss=7.3883 (C:6.6053, R:0.0100, T:0.7820(w:0.021)🎉)
Batch 175/365: Loss=7.0188 (C:6.5583, R:0.0099, T:0.4595(w:0.021)🎉)
Batch 200/365: Loss=8.0299 (C:6.5320, R:0.0099, T:1.4969(w:0.021)🚀)
Batch 225/365: Loss=6.4200 (C:6.1145, R:0.0100, T:0.3045(w:0.021)🎉)
Batch 250/365: Loss=7.0126 (C:6.1304, R:0.0100, T:0.8812(w:0.021)🎉)
Batch 275/365: Loss=6.3690 (C:5.7165, R:0.0100, T:0.6515(w:0.021)🎉)
Batch 300/365: Loss=6.7277 (C:6.5185, R:0.0099, T:0.2082(w:0.021)🎉)
Batch 325/365: Loss=6.7774 (C:6.1933, R:0.0099, T:0.5831(w:0.021)🎉)
Batch 350/365: Loss=7.0364 (C:6.2721, R:0.0100, T:0.7633(w:0.021)🎉)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 7.0443
  Contrastive: 6.3171
  Reconstruction: 0.0100
  Topological: 0.7262 (weight: 0.021)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 88.4560
  Contrastive: 3.0024
  Reconstruction: 0.0100
  Topological: 85.4526 (weight: 0.021)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 22/50 COMPLETE (163.1s)
Train Loss: 7.0443 (C:6.3171, R:0.0100, T:0.7262)
Val Loss:   88.4560 (C:3.0024, R:0.0100, T:85.4526)
🎉 Topological complexity detected!
------------------------------------------------------------

🛑 Early stopping triggered after 22 epochs
Best model was at epoch 12 with Val Loss: 38.5698

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 11
Epochs with topology: 12/22
Max consecutive topology epochs: 12
Best topological loss: 0.4275
Final topological loss: 0.7262
✅ SUCCESS: Topological learning achieved!
👍 GOOD: Fairly consistent topological learning (>50%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_122636/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/367 batches
  Processed 51/367 batches
  Processed 101/367 batches
  Processed 151/367 batches
  Processed 201/367 batches
  Processed 251/367 batches
  Processed 301/367 batches
  Processed 351/367 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: -0.0277
  Adjusted Rand Score: 0.0264
  Clustering Accuracy: 0.4067
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.4569
  Per-class F1: [0.48565916398713826, 0.3761919409412488, 0.5095229774593745]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009953
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 3.741 ± 1.677
  Negative distances: 3.896 ± 1.764
  Separation ratio: 1.04x
  Gap: -15.774
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: -0.0277
  Clustering Accuracy: 0.4067
  Adjusted Rand Score: 0.0264

Classification Performance:
  Accuracy: 0.4569

Separation Quality:
  Separation Ratio: 1.04x
  Gap: -15.774
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009953
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_122636/results/evaluation_results_20250720_130430.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_122636/results/evaluation_results_20250720_130430.json

Key Results:
  Separation ratio: 1.04x
  Perfect separation: False
  Classification accuracy: 0.4569

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 22
  Epochs with topological learning: 12
  Current topological loss: 0.7262
  Current topological weight: 0.0210
  ⚠️  Topological loss is increasing (may need tuning)
✅ GOOD: Reasonable topological learning
Final topological loss: 0.7262
Epochs with topology: 12/22
⚠️  Poor clustering accuracy: 0.407

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_122636/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250720_122636

Analysis completed with exit code: 0
Time: Sun 20 Jul 13:04:32 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
