Starting Surface Distance Metric Analysis job...
Job ID: 184464
Node: gpuvm17
Time: Mon 21 Jul 14:13:44 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Mon Jul 21 14:13:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   29C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_141357
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_141357/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 128
  Effective batch size: 384
  Number of batches: 1427
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 128
  Effective batch size: 384
  Number of batches: 1427
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 384
  Balanced sampling: True
  Train batches: 1427
  Val batches: 1427
  Test batches: 1431
Data loading completed!
  Train: 549367 samples, 1427 batches
  Val: 549367 samples, 1427 batches
  Test: 549367 samples, 1431 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,858,891
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
MoorTopologicalLoss Initialized: Using 0-dimensional persistence pairings (MST edges).
No prototypes being used for topological loss - whole dataset instead.
TopologicalTrainer initialized on device: cuda
Model parameters: 5,858,891
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.3
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=16.7090 (C:1.9999, R:0.0111, T:49.0264(w:0.300)⚠️)
Batch  25/1427: Loss=6.5914 (C:3.4869, R:0.0100, T:10.3450(w:0.300)⚠️)
Batch  50/1427: Loss=5.9027 (C:4.3792, R:0.0100, T:5.0751(w:0.300)🚀)
Batch  75/1427: Loss=5.7968 (C:4.3709, R:0.0100, T:4.7499(w:0.300)🚀)
Batch 100/1427: Loss=5.7379 (C:4.3608, R:0.0099, T:4.5871(w:0.300)🚀)
Batch 125/1427: Loss=5.6803 (C:4.3790, R:0.0099, T:4.3345(w:0.300)🚀)
Batch 150/1427: Loss=5.6928 (C:4.3863, R:0.0100, T:4.3516(w:0.300)🚀)
Batch 175/1427: Loss=5.6934 (C:4.5159, R:0.0099, T:3.9219(w:0.300)🚀)
Batch 200/1427: Loss=5.6172 (C:4.3583, R:0.0099, T:4.1932(w:0.300)🚀)
Batch 225/1427: Loss=5.6330 (C:4.5004, R:0.0100, T:3.7721(w:0.300)🚀)
Batch 250/1427: Loss=5.5978 (C:4.4441, R:0.0099, T:3.8424(w:0.300)🚀)
Batch 275/1427: Loss=5.6561 (C:4.2350, R:0.0101, T:4.7335(w:0.300)🚀)
Batch 300/1427: Loss=5.6490 (C:4.3275, R:0.0100, T:4.4014(w:0.300)🚀)
Batch 325/1427: Loss=5.6301 (C:4.3645, R:0.0100, T:4.2153(w:0.300)🚀)
Batch 350/1427: Loss=5.6386 (C:4.4612, R:0.0100, T:3.9213(w:0.300)🚀)
Batch 375/1427: Loss=5.5645 (C:4.3928, R:0.0099, T:3.9023(w:0.300)🚀)
Batch 400/1427: Loss=5.6140 (C:4.3256, R:0.0100, T:4.2914(w:0.300)🚀)
Batch 425/1427: Loss=5.5904 (C:4.4436, R:0.0100, T:3.8193(w:0.300)🚀)
Batch 450/1427: Loss=5.6213 (C:4.3412, R:0.0099, T:4.2637(w:0.300)🚀)
Batch 475/1427: Loss=5.6054 (C:4.3523, R:0.0099, T:4.1737(w:0.300)🚀)
Batch 500/1427: Loss=5.5573 (C:4.3808, R:0.0100, T:3.9184(w:0.300)🚀)
Batch 525/1427: Loss=5.6088 (C:4.4086, R:0.0100, T:3.9973(w:0.300)🚀)
Batch 550/1427: Loss=5.6396 (C:4.2086, R:0.0100, T:4.7667(w:0.300)🚀)
Batch 575/1427: Loss=5.5735 (C:4.3949, R:0.0099, T:3.9255(w:0.300)🚀)
Batch 600/1427: Loss=5.5894 (C:4.4892, R:0.0099, T:3.6640(w:0.300)🚀)
Batch 625/1427: Loss=5.6451 (C:4.4190, R:0.0100, T:4.0836(w:0.300)🚀)
Batch 650/1427: Loss=5.5966 (C:4.2408, R:0.0099, T:4.5159(w:0.300)🚀)
Batch 675/1427: Loss=5.5261 (C:4.4090, R:0.0100, T:3.7203(w:0.300)🚀)
Batch 700/1427: Loss=5.6104 (C:4.4124, R:0.0099, T:3.9900(w:0.300)🚀)
Batch 725/1427: Loss=5.5696 (C:4.3833, R:0.0100, T:3.9512(w:0.300)🚀)
Batch 750/1427: Loss=5.5628 (C:4.2897, R:0.0100, T:4.2403(w:0.300)🚀)
Batch 775/1427: Loss=5.5508 (C:4.4718, R:0.0100, T:3.5933(w:0.300)🚀)
Batch 800/1427: Loss=5.5727 (C:4.3360, R:0.0100, T:4.1187(w:0.300)🚀)
Batch 825/1427: Loss=5.5432 (C:4.3143, R:0.0100, T:4.0929(w:0.300)🚀)
Batch 850/1427: Loss=5.5863 (C:4.3295, R:0.0100, T:4.1863(w:0.300)🚀)
Batch 875/1427: Loss=5.5824 (C:4.3929, R:0.0100, T:3.9614(w:0.300)🚀)
Batch 900/1427: Loss=5.5265 (C:4.4120, R:0.0099, T:3.7117(w:0.300)🚀)
Batch 925/1427: Loss=5.5499 (C:4.2278, R:0.0099, T:4.4036(w:0.300)🚀)
Batch 950/1427: Loss=5.5455 (C:4.2472, R:0.0099, T:4.3242(w:0.300)🚀)
Batch 975/1427: Loss=5.5238 (C:4.3777, R:0.0099, T:3.8170(w:0.300)🚀)
Batch 1000/1427: Loss=5.5142 (C:4.3852, R:0.0100, T:3.7597(w:0.300)🚀)
Batch 1025/1427: Loss=5.5161 (C:4.2834, R:0.0099, T:4.1059(w:0.300)🚀)
Batch 1050/1427: Loss=5.5631 (C:4.4129, R:0.0099, T:3.8306(w:0.300)🚀)
Batch 1075/1427: Loss=5.5885 (C:4.4358, R:0.0100, T:3.8391(w:0.300)🚀)
Batch 1100/1427: Loss=5.5020 (C:4.4755, R:0.0099, T:3.4182(w:0.300)🚀)
Batch 1125/1427: Loss=5.5493 (C:4.5341, R:0.0099, T:3.3808(w:0.300)🚀)
Batch 1150/1427: Loss=5.5642 (C:4.4660, R:0.0100, T:3.6574(w:0.300)🚀)
Batch 1175/1427: Loss=5.5259 (C:4.3790, R:0.0099, T:3.8197(w:0.300)🚀)
Batch 1200/1427: Loss=5.4946 (C:4.3589, R:0.0099, T:3.7825(w:0.300)🚀)
Batch 1225/1427: Loss=5.5436 (C:4.3645, R:0.0099, T:3.9270(w:0.300)🚀)
Batch 1250/1427: Loss=5.5122 (C:4.4058, R:0.0100, T:3.6847(w:0.300)🚀)
Batch 1275/1427: Loss=5.4784 (C:4.3563, R:0.0099, T:3.7372(w:0.300)🚀)
Batch 1300/1427: Loss=5.5400 (C:4.3299, R:0.0100, T:4.0304(w:0.300)🚀)
Batch 1325/1427: Loss=5.5592 (C:4.3688, R:0.0100, T:3.9646(w:0.300)🚀)
Batch 1350/1427: Loss=5.5383 (C:4.3083, R:0.0100, T:4.0969(w:0.300)🚀)
Batch 1375/1427: Loss=5.5103 (C:4.3396, R:0.0100, T:3.8988(w:0.300)🚀)
Batch 1400/1427: Loss=5.5338 (C:4.3544, R:0.0100, T:3.9279(w:0.300)🚀)
Batch 1425/1427: Loss=5.5129 (C:4.3363, R:0.0099, T:3.9188(w:0.300)🚀)
🎉 MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 4.2922
📈 New best topological loss: 4.2922

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 5.6520
  Contrastive: 4.3633
  Reconstruction: 0.0100
  Topological: 4.2922 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.3915
  Contrastive: 1.9930
  Reconstruction: 0.0100
  Topological: 44.6583 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (69.2s)
Train Loss: 5.6520 (C:4.3633, R:0.0100, T:4.2922)
Val Loss:   15.3915 (C:1.9930, R:0.0100, T:44.6583)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4960 (C:4.3350, R:0.0099, T:3.8666(w:0.300)🚀)
Batch  25/1427: Loss=5.5354 (C:4.3255, R:0.0099, T:4.0299(w:0.300)🚀)
Batch  50/1427: Loss=5.5222 (C:4.4729, R:0.0099, T:3.4943(w:0.300)🚀)
Batch  75/1427: Loss=5.5466 (C:4.4454, R:0.0100, T:3.6672(w:0.300)🚀)
Batch 100/1427: Loss=5.5408 (C:4.2836, R:0.0099, T:4.1875(w:0.300)🚀)
Batch 125/1427: Loss=5.4892 (C:4.2842, R:0.0099, T:4.0131(w:0.300)🚀)
Batch 150/1427: Loss=5.5332 (C:4.3212, R:0.0100, T:4.0368(w:0.300)🚀)
Batch 175/1427: Loss=5.5436 (C:4.3323, R:0.0100, T:4.0342(w:0.300)🚀)
Batch 200/1427: Loss=5.5110 (C:4.4287, R:0.0100, T:3.6042(w:0.300)🚀)
Batch 225/1427: Loss=5.5059 (C:4.4444, R:0.0100, T:3.5352(w:0.300)🚀)
Batch 250/1427: Loss=5.5120 (C:4.4057, R:0.0099, T:3.6843(w:0.300)🚀)
Batch 275/1427: Loss=5.5390 (C:4.3971, R:0.0100, T:3.8029(w:0.300)🚀)
Batch 300/1427: Loss=5.5664 (C:4.3057, R:0.0100, T:4.1989(w:0.300)🚀)
Batch 325/1427: Loss=5.5071 (C:4.3124, R:0.0099, T:3.9787(w:0.300)🚀)
Batch 350/1427: Loss=5.5046 (C:4.3822, R:0.0100, T:3.7378(w:0.300)🚀)
Batch 375/1427: Loss=5.5168 (C:4.3614, R:0.0100, T:3.8479(w:0.300)🚀)
Batch 400/1427: Loss=5.5366 (C:4.3895, R:0.0099, T:3.8204(w:0.300)🚀)
Batch 425/1427: Loss=5.5623 (C:4.3688, R:0.0100, T:3.9752(w:0.300)🚀)
Batch 450/1427: Loss=5.4948 (C:4.3890, R:0.0099, T:3.6828(w:0.300)🚀)
Batch 475/1427: Loss=5.5256 (C:4.4367, R:0.0099, T:3.6265(w:0.300)🚀)
Batch 500/1427: Loss=5.5433 (C:4.2639, R:0.0099, T:4.2615(w:0.300)🚀)
Batch 525/1427: Loss=5.5292 (C:4.4542, R:0.0099, T:3.5800(w:0.300)🚀)
Batch 550/1427: Loss=5.5027 (C:4.3782, R:0.0099, T:3.7450(w:0.300)🚀)
Batch 575/1427: Loss=5.5048 (C:4.3777, R:0.0099, T:3.7539(w:0.300)🚀)
Batch 600/1427: Loss=5.4845 (C:4.3715, R:0.0100, T:3.7068(w:0.300)🚀)
Batch 625/1427: Loss=5.5032 (C:4.3101, R:0.0100, T:3.9735(w:0.300)🚀)
Batch 650/1427: Loss=5.5087 (C:4.3461, R:0.0100, T:3.8720(w:0.300)🚀)
Batch 675/1427: Loss=5.5654 (C:4.3893, R:0.0100, T:3.9170(w:0.300)🚀)
Batch 700/1427: Loss=5.5132 (C:4.3323, R:0.0099, T:3.9332(w:0.300)🚀)
Batch 725/1427: Loss=5.5211 (C:4.4272, R:0.0100, T:3.6430(w:0.300)🚀)
Batch 750/1427: Loss=5.4949 (C:4.3114, R:0.0100, T:3.9417(w:0.300)🚀)
Batch 775/1427: Loss=5.4952 (C:4.4244, R:0.0100, T:3.5661(w:0.300)🚀)
Batch 800/1427: Loss=5.5148 (C:4.3694, R:0.0099, T:3.8145(w:0.300)🚀)
Batch 825/1427: Loss=5.5316 (C:4.4216, R:0.0099, T:3.6968(w:0.300)🚀)
Batch 850/1427: Loss=5.4950 (C:4.3143, R:0.0099, T:3.9322(w:0.300)🚀)
Batch 875/1427: Loss=5.5500 (C:4.3859, R:0.0100, T:3.8772(w:0.300)🚀)
Batch 900/1427: Loss=5.4954 (C:4.3598, R:0.0099, T:3.7820(w:0.300)🚀)
Batch 925/1427: Loss=5.5384 (C:4.4095, R:0.0099, T:3.7600(w:0.300)🚀)
Batch 950/1427: Loss=5.4911 (C:4.4085, R:0.0100, T:3.6052(w:0.300)🚀)
Batch 975/1427: Loss=5.5066 (C:4.3834, R:0.0100, T:3.7408(w:0.300)🚀)
Batch 1000/1427: Loss=5.5130 (C:4.3405, R:0.0099, T:3.9052(w:0.300)🚀)
Batch 1025/1427: Loss=5.5210 (C:4.3739, R:0.0100, T:3.8205(w:0.300)🚀)
Batch 1050/1427: Loss=5.4812 (C:4.2977, R:0.0099, T:3.9417(w:0.300)🚀)
Batch 1075/1427: Loss=5.4813 (C:4.3332, R:0.0099, T:3.8238(w:0.300)🚀)
Batch 1100/1427: Loss=5.5275 (C:4.4017, R:0.0100, T:3.7492(w:0.300)🚀)
Batch 1125/1427: Loss=5.5183 (C:4.3268, R:0.0100, T:3.9686(w:0.300)🚀)
Batch 1150/1427: Loss=5.5463 (C:4.3806, R:0.0100, T:3.8822(w:0.300)🚀)
Batch 1175/1427: Loss=5.5104 (C:4.3947, R:0.0099, T:3.7157(w:0.300)🚀)
Batch 1200/1427: Loss=5.4984 (C:4.4022, R:0.0099, T:3.6504(w:0.300)🚀)
Batch 1225/1427: Loss=5.4952 (C:4.3103, R:0.0100, T:3.9465(w:0.300)🚀)
Batch 1250/1427: Loss=5.4725 (C:4.3697, R:0.0099, T:3.6726(w:0.300)🚀)
Batch 1275/1427: Loss=5.5616 (C:4.4017, R:0.0100, T:3.8628(w:0.300)🚀)
Batch 1300/1427: Loss=5.5134 (C:4.3777, R:0.0100, T:3.7823(w:0.300)🚀)
Batch 1325/1427: Loss=5.5310 (C:4.3720, R:0.0100, T:3.8599(w:0.300)🚀)
Batch 1350/1427: Loss=5.5467 (C:4.4027, R:0.0099, T:3.8101(w:0.300)🚀)
Batch 1375/1427: Loss=5.5205 (C:4.4082, R:0.0100, T:3.7041(w:0.300)🚀)
Batch 1400/1427: Loss=5.5303 (C:4.3607, R:0.0100, T:3.8956(w:0.300)🚀)
Batch 1425/1427: Loss=5.5180 (C:4.3190, R:0.0100, T:3.9933(w:0.300)🚀)
📈 New best topological loss: 3.7994

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 5.5155
  Contrastive: 4.3747
  Reconstruction: 0.0100
  Topological: 3.7994 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.3216
  Contrastive: 1.9943
  Reconstruction: 0.0100
  Topological: 44.4213 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (76.4s)
Train Loss: 5.5155 (C:4.3747, R:0.0100, T:3.7994)
Val Loss:   15.3216 (C:1.9943, R:0.0100, T:44.4213)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.5057 (C:4.4168, R:0.0100, T:3.6261(w:0.300)🚀)
Batch  25/1427: Loss=5.5164 (C:4.3907, R:0.0100, T:3.7489(w:0.300)🚀)
Batch  50/1427: Loss=5.5295 (C:4.3143, R:0.0100, T:4.0474(w:0.300)🚀)
Batch  75/1427: Loss=5.5286 (C:4.3578, R:0.0100, T:3.8992(w:0.300)🚀)
Batch 100/1427: Loss=5.5171 (C:4.3326, R:0.0100, T:3.9450(w:0.300)🚀)
Batch 125/1427: Loss=5.4690 (C:4.3948, R:0.0099, T:3.5774(w:0.300)🚀)
Batch 150/1427: Loss=5.4822 (C:4.3430, R:0.0100, T:3.7941(w:0.300)🚀)
Batch 175/1427: Loss=5.5168 (C:4.3664, R:0.0099, T:3.8314(w:0.300)🚀)
Batch 200/1427: Loss=5.5402 (C:4.3858, R:0.0100, T:3.8444(w:0.300)🚀)
Batch 225/1427: Loss=5.5015 (C:4.3278, R:0.0100, T:3.9093(w:0.300)🚀)
Batch 250/1427: Loss=5.4930 (C:4.3675, R:0.0099, T:3.7484(w:0.300)🚀)
Batch 275/1427: Loss=5.4921 (C:4.3066, R:0.0099, T:3.9484(w:0.300)🚀)
Batch 300/1427: Loss=5.4795 (C:4.3082, R:0.0099, T:3.9009(w:0.300)🚀)
Batch 325/1427: Loss=5.5358 (C:4.3975, R:0.0100, T:3.7909(w:0.300)🚀)
Batch 350/1427: Loss=5.5089 (C:4.3768, R:0.0100, T:3.7702(w:0.300)🚀)
Batch 375/1427: Loss=5.4475 (C:4.4012, R:0.0099, T:3.4845(w:0.300)🚀)
Batch 400/1427: Loss=5.5120 (C:4.3944, R:0.0100, T:3.7221(w:0.300)🚀)
Batch 425/1427: Loss=5.5129 (C:4.3925, R:0.0100, T:3.7313(w:0.300)🚀)
Batch 450/1427: Loss=5.4683 (C:4.3491, R:0.0099, T:3.7275(w:0.300)🚀)
Batch 475/1427: Loss=5.4967 (C:4.4241, R:0.0100, T:3.5721(w:0.300)🚀)
Batch 500/1427: Loss=5.4725 (C:4.4811, R:0.0099, T:3.3014(w:0.300)🚀)
Batch 525/1427: Loss=5.4992 (C:4.3608, R:0.0099, T:3.7911(w:0.300)🚀)
Batch 550/1427: Loss=5.4051 (C:4.3828, R:0.0099, T:3.4043(w:0.300)🚀)
Batch 575/1427: Loss=5.4742 (C:4.3702, R:0.0100, T:3.6765(w:0.300)🚀)
Batch 600/1427: Loss=5.4812 (C:4.3248, R:0.0100, T:3.8514(w:0.300)🚀)
Batch 625/1427: Loss=5.5252 (C:4.3441, R:0.0100, T:3.9336(w:0.300)🚀)
Batch 650/1427: Loss=5.4568 (C:4.4201, R:0.0098, T:3.4526(w:0.300)🚀)
Batch 675/1427: Loss=5.4652 (C:4.3697, R:0.0100, T:3.6483(w:0.300)🚀)
Batch 700/1427: Loss=5.5389 (C:4.3145, R:0.0100, T:4.0780(w:0.300)🚀)
Batch 725/1427: Loss=5.4943 (C:4.3013, R:0.0099, T:3.9735(w:0.300)🚀)
Batch 750/1427: Loss=5.5114 (C:4.3544, R:0.0100, T:3.8532(w:0.300)🚀)
Batch 775/1427: Loss=5.4787 (C:4.3269, R:0.0099, T:3.8359(w:0.300)🚀)
Batch 800/1427: Loss=5.4886 (C:4.3163, R:0.0100, T:3.9044(w:0.300)🚀)
Batch 825/1427: Loss=5.5247 (C:4.4163, R:0.0100, T:3.6915(w:0.300)🚀)
Batch 850/1427: Loss=5.5250 (C:4.3885, R:0.0100, T:3.7852(w:0.300)🚀)
Batch 875/1427: Loss=5.4580 (C:4.3722, R:0.0099, T:3.6162(w:0.300)🚀)
Batch 900/1427: Loss=5.5221 (C:4.4244, R:0.0100, T:3.6558(w:0.300)🚀)
Batch 925/1427: Loss=5.4981 (C:4.4322, R:0.0099, T:3.5496(w:0.300)🚀)
Batch 950/1427: Loss=5.4663 (C:4.4227, R:0.0099, T:3.4754(w:0.300)🚀)
Batch 975/1427: Loss=5.4785 (C:4.3666, R:0.0100, T:3.7030(w:0.300)🚀)
Batch 1000/1427: Loss=5.4937 (C:4.3634, R:0.0099, T:3.7643(w:0.300)🚀)
Batch 1025/1427: Loss=5.4984 (C:4.3448, R:0.0100, T:3.8420(w:0.300)🚀)
Batch 1050/1427: Loss=5.4967 (C:4.3914, R:0.0100, T:3.6808(w:0.300)🚀)
Batch 1075/1427: Loss=5.4482 (C:4.3241, R:0.0100, T:3.7438(w:0.300)🚀)
Batch 1100/1427: Loss=5.4633 (C:4.3452, R:0.0099, T:3.7236(w:0.300)🚀)
Batch 1125/1427: Loss=5.5090 (C:4.3537, R:0.0100, T:3.8478(w:0.300)🚀)
Batch 1150/1427: Loss=5.4687 (C:4.3621, R:0.0099, T:3.6854(w:0.300)🚀)
Batch 1175/1427: Loss=5.4685 (C:4.2999, R:0.0100, T:3.8921(w:0.300)🚀)
Batch 1200/1427: Loss=5.4833 (C:4.3139, R:0.0100, T:3.8949(w:0.300)🚀)
Batch 1225/1427: Loss=5.4814 (C:4.3494, R:0.0099, T:3.7697(w:0.300)🚀)
Batch 1250/1427: Loss=5.4736 (C:4.4433, R:0.0099, T:3.4312(w:0.300)🚀)
Batch 1275/1427: Loss=5.4864 (C:4.4224, R:0.0100, T:3.5432(w:0.300)🚀)
Batch 1300/1427: Loss=5.5136 (C:4.3728, R:0.0100, T:3.7991(w:0.300)🚀)
Batch 1325/1427: Loss=5.5020 (C:4.3978, R:0.0100, T:3.6771(w:0.300)🚀)
Batch 1350/1427: Loss=5.4579 (C:4.3633, R:0.0099, T:3.6454(w:0.300)🚀)
Batch 1375/1427: Loss=5.4767 (C:4.3543, R:0.0100, T:3.7379(w:0.300)🚀)
Batch 1400/1427: Loss=5.5202 (C:4.4081, R:0.0100, T:3.7039(w:0.300)🚀)
Batch 1425/1427: Loss=5.4649 (C:4.3937, R:0.0100, T:3.5674(w:0.300)🚀)
📈 New best topological loss: 3.7439

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 5.4984
  Contrastive: 4.3742
  Reconstruction: 0.0100
  Topological: 3.7439 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.7284
  Contrastive: 1.9934
  Reconstruction: 0.0100
  Topological: 42.4465 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (77.6s)
Train Loss: 5.4984 (C:4.3742, R:0.0100, T:3.7439)
Val Loss:   14.7284 (C:1.9934, R:0.0100, T:42.4465)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.5106 (C:4.3252, R:0.0100, T:3.9477(w:0.300)🚀)
Batch  25/1427: Loss=5.5029 (C:4.3212, R:0.0100, T:3.9357(w:0.300)🚀)
Batch  50/1427: Loss=5.5074 (C:4.4262, R:0.0100, T:3.6008(w:0.300)🚀)
Batch  75/1427: Loss=5.5098 (C:4.3929, R:0.0100, T:3.7196(w:0.300)🚀)
Batch 100/1427: Loss=5.4820 (C:4.4029, R:0.0100, T:3.5938(w:0.300)🚀)
Batch 125/1427: Loss=5.4537 (C:4.4215, R:0.0099, T:3.4373(w:0.300)🚀)
Batch 150/1427: Loss=5.5094 (C:4.3933, R:0.0099, T:3.7173(w:0.300)🚀)
Batch 175/1427: Loss=5.5030 (C:4.3881, R:0.0100, T:3.7129(w:0.300)🚀)
Batch 200/1427: Loss=5.4646 (C:4.3210, R:0.0099, T:3.8086(w:0.300)🚀)
Batch 225/1427: Loss=5.4737 (C:4.3702, R:0.0100, T:3.6749(w:0.300)🚀)
Batch 250/1427: Loss=5.4526 (C:4.4619, R:0.0100, T:3.2987(w:0.300)🚀)
Batch 275/1427: Loss=5.5071 (C:4.3548, R:0.0100, T:3.8376(w:0.300)🚀)
Batch 300/1427: Loss=5.4654 (C:4.3153, R:0.0099, T:3.8301(w:0.300)🚀)
Batch 325/1427: Loss=5.4704 (C:4.3701, R:0.0099, T:3.6644(w:0.300)🚀)
Batch 350/1427: Loss=5.4993 (C:4.3359, R:0.0100, T:3.8746(w:0.300)🚀)
Batch 375/1427: Loss=5.5095 (C:4.3871, R:0.0099, T:3.7378(w:0.300)🚀)
Batch 400/1427: Loss=5.5007 (C:4.3672, R:0.0100, T:3.7749(w:0.300)🚀)
Batch 425/1427: Loss=5.4876 (C:4.3041, R:0.0100, T:3.9419(w:0.300)🚀)
Batch 450/1427: Loss=5.4331 (C:4.2912, R:0.0099, T:3.8032(w:0.300)🚀)
Batch 475/1427: Loss=5.4578 (C:4.3759, R:0.0099, T:3.6031(w:0.300)🚀)
Batch 500/1427: Loss=5.5540 (C:4.3467, R:0.0100, T:4.0209(w:0.300)🚀)
Batch 525/1427: Loss=5.5227 (C:4.3931, R:0.0100, T:3.7621(w:0.300)🚀)
Batch 550/1427: Loss=5.4906 (C:4.4329, R:0.0099, T:3.5221(w:0.300)🚀)
Batch 575/1427: Loss=5.4195 (C:4.3445, R:0.0099, T:3.5801(w:0.300)🚀)
Batch 600/1427: Loss=5.4879 (C:4.3804, R:0.0100, T:3.6885(w:0.300)🚀)
Batch 625/1427: Loss=5.4728 (C:4.3641, R:0.0100, T:3.6924(w:0.300)🚀)
Batch 650/1427: Loss=5.4924 (C:4.3694, R:0.0099, T:3.7400(w:0.300)🚀)
Batch 675/1427: Loss=5.4993 (C:4.3508, R:0.0100, T:3.8248(w:0.300)🚀)
Batch 700/1427: Loss=5.5093 (C:4.3694, R:0.0099, T:3.7960(w:0.300)🚀)
Batch 725/1427: Loss=5.4771 (C:4.3626, R:0.0100, T:3.7117(w:0.300)🚀)
Batch 750/1427: Loss=5.4896 (C:4.3587, R:0.0099, T:3.7666(w:0.300)🚀)
Batch 775/1427: Loss=5.5076 (C:4.3783, R:0.0100, T:3.7612(w:0.300)🚀)
Batch 800/1427: Loss=5.4715 (C:4.3516, R:0.0100, T:3.7297(w:0.300)🚀)
Batch 825/1427: Loss=5.4812 (C:4.3910, R:0.0100, T:3.6304(w:0.300)🚀)
Batch 850/1427: Loss=5.4948 (C:4.4230, R:0.0100, T:3.5694(w:0.300)🚀)
Batch 875/1427: Loss=5.5361 (C:4.3563, R:0.0100, T:3.9295(w:0.300)🚀)
Batch 900/1427: Loss=5.4568 (C:4.3238, R:0.0099, T:3.7733(w:0.300)🚀)
Batch 925/1427: Loss=5.5155 (C:4.3325, R:0.0100, T:3.9398(w:0.300)🚀)
Batch 950/1427: Loss=5.5308 (C:4.3225, R:0.0100, T:4.0246(w:0.300)🚀)
Batch 975/1427: Loss=5.4986 (C:4.3051, R:0.0100, T:3.9749(w:0.300)🚀)
Batch 1000/1427: Loss=5.5045 (C:4.3460, R:0.0099, T:3.8586(w:0.300)🚀)
Batch 1025/1427: Loss=5.4625 (C:4.3096, R:0.0099, T:3.8399(w:0.300)🚀)
Batch 1050/1427: Loss=5.5026 (C:4.3686, R:0.0100, T:3.7767(w:0.300)🚀)
Batch 1075/1427: Loss=5.5063 (C:4.3920, R:0.0100, T:3.7110(w:0.300)🚀)
Batch 1100/1427: Loss=5.5371 (C:4.3757, R:0.0101, T:3.8682(w:0.300)🚀)
Batch 1125/1427: Loss=5.4785 (C:4.2843, R:0.0100, T:3.9774(w:0.300)🚀)
Batch 1150/1427: Loss=5.4569 (C:4.3988, R:0.0100, T:3.5236(w:0.300)🚀)
Batch 1175/1427: Loss=5.4928 (C:4.4274, R:0.0099, T:3.5480(w:0.300)🚀)
Batch 1200/1427: Loss=5.4765 (C:4.3501, R:0.0100, T:3.7514(w:0.300)🚀)
Batch 1225/1427: Loss=5.5012 (C:4.3910, R:0.0100, T:3.6973(w:0.300)🚀)
Batch 1250/1427: Loss=5.4906 (C:4.3580, R:0.0100, T:3.7720(w:0.300)🚀)
Batch 1275/1427: Loss=5.4870 (C:4.3893, R:0.0100, T:3.6555(w:0.300)🚀)
Batch 1300/1427: Loss=5.4615 (C:4.4047, R:0.0099, T:3.5195(w:0.300)🚀)
Batch 1325/1427: Loss=5.5297 (C:4.4386, R:0.0100, T:3.6338(w:0.300)🚀)
Batch 1350/1427: Loss=5.5211 (C:4.3756, R:0.0100, T:3.8150(w:0.300)🚀)
Batch 1375/1427: Loss=5.4526 (C:4.3936, R:0.0100, T:3.5269(w:0.300)🚀)
Batch 1400/1427: Loss=5.5005 (C:4.3676, R:0.0099, T:3.7729(w:0.300)🚀)
Batch 1425/1427: Loss=5.5043 (C:4.3413, R:0.0101, T:3.8730(w:0.300)🚀)
📈 New best topological loss: 3.6972

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 5.4848
  Contrastive: 4.3746
  Reconstruction: 0.0100
  Topological: 3.6972 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.2883
  Contrastive: 1.9925
  Reconstruction: 0.0100
  Topological: 40.9827 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (75.3s)
Train Loss: 5.4848 (C:4.3746, R:0.0100, T:3.6972)
Val Loss:   14.2883 (C:1.9925, R:0.0100, T:40.9827)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4637 (C:4.3805, R:0.0100, T:3.6073(w:0.300)🚀)
Batch  25/1427: Loss=5.4811 (C:4.3841, R:0.0099, T:3.6534(w:0.300)🚀)
Batch  50/1427: Loss=5.4690 (C:4.3898, R:0.0099, T:3.5940(w:0.300)🚀)
Batch  75/1427: Loss=5.5061 (C:4.3649, R:0.0100, T:3.8006(w:0.300)🚀)
Batch 100/1427: Loss=5.4847 (C:4.4114, R:0.0100, T:3.5744(w:0.300)🚀)
Batch 125/1427: Loss=5.4550 (C:4.3108, R:0.0099, T:3.8107(w:0.300)🚀)
Batch 150/1427: Loss=5.4580 (C:4.4069, R:0.0100, T:3.5005(w:0.300)🚀)
Batch 175/1427: Loss=5.4415 (C:4.3472, R:0.0100, T:3.6444(w:0.300)🚀)
Batch 200/1427: Loss=5.4649 (C:4.3598, R:0.0100, T:3.6806(w:0.300)🚀)
Batch 225/1427: Loss=5.4544 (C:4.3897, R:0.0099, T:3.5455(w:0.300)🚀)
Batch 250/1427: Loss=5.4817 (C:4.3232, R:0.0099, T:3.8585(w:0.300)🚀)
Batch 275/1427: Loss=5.4602 (C:4.3513, R:0.0099, T:3.6931(w:0.300)🚀)
Batch 300/1427: Loss=5.4430 (C:4.3682, R:0.0099, T:3.5796(w:0.300)🚀)
Batch 325/1427: Loss=5.4752 (C:4.3697, R:0.0100, T:3.6817(w:0.300)🚀)
Batch 350/1427: Loss=5.4589 (C:4.3240, R:0.0100, T:3.7797(w:0.300)🚀)
Batch 375/1427: Loss=5.4397 (C:4.4131, R:0.0099, T:3.4187(w:0.300)🚀)
Batch 400/1427: Loss=5.4813 (C:4.3893, R:0.0099, T:3.6368(w:0.300)🚀)
Batch 425/1427: Loss=5.4688 (C:4.3824, R:0.0100, T:3.6178(w:0.300)🚀)
Batch 450/1427: Loss=5.4748 (C:4.3325, R:0.0100, T:3.8046(w:0.300)🚀)
Batch 475/1427: Loss=5.4624 (C:4.3567, R:0.0099, T:3.6826(w:0.300)🚀)
Batch 500/1427: Loss=5.4759 (C:4.3664, R:0.0099, T:3.6950(w:0.300)🚀)
Batch 525/1427: Loss=5.4984 (C:4.3960, R:0.0100, T:3.6714(w:0.300)🚀)
Batch 550/1427: Loss=5.4342 (C:4.3815, R:0.0099, T:3.5057(w:0.300)🚀)
Batch 575/1427: Loss=5.4776 (C:4.3188, R:0.0100, T:3.8594(w:0.300)🚀)
Batch 600/1427: Loss=5.4807 (C:4.3916, R:0.0100, T:3.6269(w:0.300)🚀)
Batch 625/1427: Loss=5.4568 (C:4.3867, R:0.0099, T:3.5638(w:0.300)🚀)
Batch 650/1427: Loss=5.4553 (C:4.3697, R:0.0099, T:3.6156(w:0.300)🚀)
Batch 675/1427: Loss=5.4580 (C:4.3763, R:0.0099, T:3.6026(w:0.300)🚀)
Batch 700/1427: Loss=5.4577 (C:4.4002, R:0.0099, T:3.5218(w:0.300)🚀)
Batch 725/1427: Loss=5.4813 (C:4.3967, R:0.0100, T:3.6120(w:0.300)🚀)
Batch 750/1427: Loss=5.4774 (C:4.3835, R:0.0099, T:3.6430(w:0.300)🚀)
Batch 775/1427: Loss=5.4511 (C:4.3923, R:0.0099, T:3.5261(w:0.300)🚀)
Batch 800/1427: Loss=5.4963 (C:4.3850, R:0.0100, T:3.7012(w:0.300)🚀)
Batch 825/1427: Loss=5.4824 (C:4.3325, R:0.0099, T:3.8297(w:0.300)🚀)
Batch 850/1427: Loss=5.4856 (C:4.3519, R:0.0099, T:3.7755(w:0.300)🚀)
Batch 875/1427: Loss=5.4706 (C:4.4079, R:0.0100, T:3.5388(w:0.300)🚀)
Batch 900/1427: Loss=5.4515 (C:4.3798, R:0.0099, T:3.5691(w:0.300)🚀)
Batch 925/1427: Loss=5.4501 (C:4.3855, R:0.0100, T:3.5456(w:0.300)🚀)
Batch 950/1427: Loss=5.4497 (C:4.3620, R:0.0100, T:3.6222(w:0.300)🚀)
Batch 975/1427: Loss=5.4722 (C:4.3709, R:0.0099, T:3.6676(w:0.300)🚀)
Batch 1000/1427: Loss=5.5134 (C:4.3974, R:0.0100, T:3.7168(w:0.300)🚀)
Batch 1025/1427: Loss=5.5085 (C:4.3584, R:0.0100, T:3.8304(w:0.300)🚀)
Batch 1050/1427: Loss=5.4255 (C:4.3378, R:0.0099, T:3.6225(w:0.300)🚀)
Batch 1075/1427: Loss=5.4631 (C:4.3188, R:0.0100, T:3.8111(w:0.300)🚀)
Batch 1100/1427: Loss=5.4894 (C:4.3238, R:0.0100, T:3.8820(w:0.300)🚀)
Batch 1125/1427: Loss=5.4530 (C:4.3834, R:0.0099, T:3.5618(w:0.300)🚀)
Batch 1150/1427: Loss=5.4603 (C:4.3831, R:0.0100, T:3.5874(w:0.300)🚀)
Batch 1175/1427: Loss=5.5167 (C:4.3637, R:0.0100, T:3.8398(w:0.300)🚀)
Batch 1200/1427: Loss=5.4544 (C:4.3458, R:0.0099, T:3.6918(w:0.300)🚀)
Batch 1225/1427: Loss=5.4589 (C:4.3474, R:0.0100, T:3.7017(w:0.300)🚀)
Batch 1250/1427: Loss=5.5187 (C:4.3403, R:0.0100, T:3.9249(w:0.300)🚀)
Batch 1275/1427: Loss=5.4495 (C:4.3558, R:0.0099, T:3.6425(w:0.300)🚀)
Batch 1300/1427: Loss=5.5015 (C:4.3745, R:0.0099, T:3.7534(w:0.300)🚀)
Batch 1325/1427: Loss=5.4667 (C:4.3493, R:0.0099, T:3.7214(w:0.300)🚀)
Batch 1350/1427: Loss=5.4572 (C:4.3233, R:0.0100, T:3.7762(w:0.300)🚀)
Batch 1375/1427: Loss=5.4721 (C:4.3725, R:0.0099, T:3.6621(w:0.300)🚀)
Batch 1400/1427: Loss=5.4745 (C:4.3192, R:0.0099, T:3.8476(w:0.300)🚀)
Batch 1425/1427: Loss=5.4750 (C:4.3551, R:0.0100, T:3.7298(w:0.300)🚀)
📈 New best topological loss: 3.6677

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 5.4750
  Contrastive: 4.3737
  Reconstruction: 0.0100
  Topological: 3.6677 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.1074
  Contrastive: 1.9905
  Reconstruction: 0.0100
  Topological: 40.3864 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (67.2s)
Train Loss: 5.4750 (C:4.3737, R:0.0100, T:3.6677)
Val Loss:   14.1074 (C:1.9905, R:0.0100, T:40.3864)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4821 (C:4.3508, R:0.0100, T:3.7679(w:0.300)🚀)
Batch  25/1427: Loss=5.4959 (C:4.3797, R:0.0100, T:3.7175(w:0.300)🚀)
Batch  50/1427: Loss=5.4718 (C:4.3814, R:0.0100, T:3.6311(w:0.300)🚀)
Batch  75/1427: Loss=5.4889 (C:4.3549, R:0.0100, T:3.7767(w:0.300)🚀)
Batch 100/1427: Loss=5.4855 (C:4.4062, R:0.0099, T:3.5945(w:0.300)🚀)
Batch 125/1427: Loss=5.4778 (C:4.4091, R:0.0099, T:3.5590(w:0.300)🚀)
Batch 150/1427: Loss=5.5169 (C:4.3800, R:0.0100, T:3.7864(w:0.300)🚀)
Batch 175/1427: Loss=5.4843 (C:4.3785, R:0.0100, T:3.6827(w:0.300)🚀)
Batch 200/1427: Loss=5.4382 (C:4.3499, R:0.0099, T:3.6245(w:0.300)🚀)
Batch 225/1427: Loss=5.4604 (C:4.3503, R:0.0099, T:3.6970(w:0.300)🚀)
Batch 250/1427: Loss=5.4675 (C:4.3442, R:0.0100, T:3.7412(w:0.300)🚀)
Batch 275/1427: Loss=5.4383 (C:4.3443, R:0.0100, T:3.6435(w:0.300)🚀)
Batch 300/1427: Loss=5.4774 (C:4.3598, R:0.0100, T:3.7221(w:0.300)🚀)
Batch 325/1427: Loss=5.4642 (C:4.3382, R:0.0099, T:3.7502(w:0.300)🚀)
Batch 350/1427: Loss=5.4503 (C:4.3948, R:0.0099, T:3.5148(w:0.300)🚀)
Batch 375/1427: Loss=5.4849 (C:4.4330, R:0.0100, T:3.5030(w:0.300)🚀)
Batch 400/1427: Loss=5.4782 (C:4.4158, R:0.0099, T:3.5379(w:0.300)🚀)
Batch 425/1427: Loss=5.5047 (C:4.4148, R:0.0100, T:3.6298(w:0.300)🚀)
Batch 450/1427: Loss=5.4616 (C:4.3446, R:0.0099, T:3.7201(w:0.300)🚀)
Batch 475/1427: Loss=5.4593 (C:4.3376, R:0.0100, T:3.7358(w:0.300)🚀)
Batch 500/1427: Loss=5.5111 (C:4.3162, R:0.0100, T:3.9795(w:0.300)🚀)
Batch 525/1427: Loss=5.4181 (C:4.3584, R:0.0099, T:3.5288(w:0.300)🚀)
Batch 550/1427: Loss=5.4796 (C:4.3659, R:0.0100, T:3.7090(w:0.300)🚀)
Batch 575/1427: Loss=5.4718 (C:4.4007, R:0.0099, T:3.5670(w:0.300)🚀)
Batch 600/1427: Loss=5.4629 (C:4.3374, R:0.0100, T:3.7481(w:0.300)🚀)
Batch 625/1427: Loss=5.5002 (C:4.4009, R:0.0100, T:3.6610(w:0.300)🚀)
Batch 650/1427: Loss=5.4713 (C:4.4047, R:0.0100, T:3.5520(w:0.300)🚀)
Batch 675/1427: Loss=5.5135 (C:4.4423, R:0.0100, T:3.5673(w:0.300)🚀)
Batch 700/1427: Loss=5.4820 (C:4.3845, R:0.0099, T:3.6548(w:0.300)🚀)
Batch 725/1427: Loss=5.4699 (C:4.3694, R:0.0100, T:3.6650(w:0.300)🚀)
Batch 750/1427: Loss=5.4743 (C:4.3804, R:0.0099, T:3.6430(w:0.300)🚀)
Batch 775/1427: Loss=5.4900 (C:4.3490, R:0.0099, T:3.8002(w:0.300)🚀)
Batch 800/1427: Loss=5.4559 (C:4.3836, R:0.0099, T:3.5710(w:0.300)🚀)
Batch 825/1427: Loss=5.4940 (C:4.4269, R:0.0100, T:3.5535(w:0.300)🚀)
Batch 850/1427: Loss=5.4967 (C:4.3636, R:0.0100, T:3.7736(w:0.300)🚀)
Batch 875/1427: Loss=5.4662 (C:4.3440, R:0.0100, T:3.7373(w:0.300)🚀)
Batch 900/1427: Loss=5.4809 (C:4.3303, R:0.0099, T:3.8318(w:0.300)🚀)
Batch 925/1427: Loss=5.4616 (C:4.3851, R:0.0099, T:3.5850(w:0.300)🚀)
Batch 950/1427: Loss=5.4753 (C:4.3714, R:0.0099, T:3.6766(w:0.300)🚀)
Batch 975/1427: Loss=5.4088 (C:4.3750, R:0.0099, T:3.4429(w:0.300)🚀)
Batch 1000/1427: Loss=5.4565 (C:4.4198, R:0.0099, T:3.4523(w:0.300)🚀)
Batch 1025/1427: Loss=5.4843 (C:4.3618, R:0.0100, T:3.7383(w:0.300)🚀)
Batch 1050/1427: Loss=5.4841 (C:4.3413, R:0.0099, T:3.8060(w:0.300)🚀)
Batch 1075/1427: Loss=5.4507 (C:4.3630, R:0.0100, T:3.6222(w:0.300)🚀)
Batch 1100/1427: Loss=5.5027 (C:4.3988, R:0.0100, T:3.6765(w:0.300)🚀)
Batch 1125/1427: Loss=5.4526 (C:4.3736, R:0.0098, T:3.5933(w:0.300)🚀)
Batch 1150/1427: Loss=5.4597 (C:4.3890, R:0.0099, T:3.5656(w:0.300)🚀)
Batch 1175/1427: Loss=5.4559 (C:4.3381, R:0.0099, T:3.7228(w:0.300)🚀)
Batch 1200/1427: Loss=5.4682 (C:4.3461, R:0.0100, T:3.7371(w:0.300)🚀)
Batch 1225/1427: Loss=5.3827 (C:4.3684, R:0.0099, T:3.3779(w:0.300)🚀)
Batch 1250/1427: Loss=5.4581 (C:4.3472, R:0.0099, T:3.6996(w:0.300)🚀)
Batch 1275/1427: Loss=5.4417 (C:4.3357, R:0.0099, T:3.6833(w:0.300)🚀)
Batch 1300/1427: Loss=5.4351 (C:4.3562, R:0.0099, T:3.5932(w:0.300)🚀)
Batch 1325/1427: Loss=5.5237 (C:4.3659, R:0.0100, T:3.8562(w:0.300)🚀)
Batch 1350/1427: Loss=5.4685 (C:4.3420, R:0.0100, T:3.7517(w:0.300)🚀)
Batch 1375/1427: Loss=5.4465 (C:4.3738, R:0.0099, T:3.5722(w:0.300)🚀)
Batch 1400/1427: Loss=5.5044 (C:4.4073, R:0.0100, T:3.6535(w:0.300)🚀)
Batch 1425/1427: Loss=5.4665 (C:4.3729, R:0.0099, T:3.6422(w:0.300)🚀)
📈 New best topological loss: 3.6576

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 5.4708
  Contrastive: 4.3726
  Reconstruction: 0.0100
  Topological: 3.6576 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.3511
  Contrastive: 1.9902
  Reconstruction: 0.0100
  Topological: 41.1997 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 6/50 COMPLETE (67.2s)
Train Loss: 5.4708 (C:4.3726, R:0.0100, T:3.6576)
Val Loss:   14.3511 (C:1.9902, R:0.0100, T:41.1997)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4788 (C:4.3570, R:0.0100, T:3.7360(w:0.300)🚀)
Batch  25/1427: Loss=5.4479 (C:4.3256, R:0.0100, T:3.7378(w:0.300)🚀)
Batch  50/1427: Loss=5.4373 (C:4.3731, R:0.0100, T:3.5441(w:0.300)🚀)
Batch  75/1427: Loss=5.4726 (C:4.3736, R:0.0099, T:3.6600(w:0.300)🚀)
Batch 100/1427: Loss=5.4426 (C:4.3495, R:0.0099, T:3.6402(w:0.300)🚀)
Batch 125/1427: Loss=5.4642 (C:4.3497, R:0.0099, T:3.7116(w:0.300)🚀)
Batch 150/1427: Loss=5.5003 (C:4.3290, R:0.0101, T:3.9008(w:0.300)🚀)
Batch 175/1427: Loss=5.4583 (C:4.3519, R:0.0100, T:3.6847(w:0.300)🚀)
Batch 200/1427: Loss=5.4751 (C:4.3717, R:0.0099, T:3.6748(w:0.300)🚀)
Batch 225/1427: Loss=5.4904 (C:4.3487, R:0.0100, T:3.8025(w:0.300)🚀)
Batch 250/1427: Loss=5.4842 (C:4.3815, R:0.0099, T:3.6724(w:0.300)🚀)
Batch 275/1427: Loss=5.4790 (C:4.4134, R:0.0100, T:3.5487(w:0.300)🚀)
Batch 300/1427: Loss=5.4799 (C:4.4446, R:0.0100, T:3.4475(w:0.300)🚀)
Batch 325/1427: Loss=5.4489 (C:4.4086, R:0.0099, T:3.4641(w:0.300)🚀)
Batch 350/1427: Loss=5.4741 (C:4.3352, R:0.0100, T:3.7933(w:0.300)🚀)
Batch 375/1427: Loss=5.4851 (C:4.3655, R:0.0100, T:3.7286(w:0.300)🚀)
Batch 400/1427: Loss=5.4624 (C:4.4000, R:0.0100, T:3.5381(w:0.300)🚀)
Batch 425/1427: Loss=5.4624 (C:4.3811, R:0.0099, T:3.6010(w:0.300)🚀)
Batch 450/1427: Loss=5.4723 (C:4.4080, R:0.0100, T:3.5444(w:0.300)🚀)
Batch 475/1427: Loss=5.4245 (C:4.3853, R:0.0099, T:3.4606(w:0.300)🚀)
Batch 500/1427: Loss=5.4320 (C:4.4093, R:0.0100, T:3.4055(w:0.300)🚀)
Batch 525/1427: Loss=5.4788 (C:4.3987, R:0.0100, T:3.5970(w:0.300)🚀)
Batch 550/1427: Loss=5.4746 (C:4.3888, R:0.0099, T:3.6162(w:0.300)🚀)
Batch 575/1427: Loss=5.4615 (C:4.3735, R:0.0100, T:3.6233(w:0.300)🚀)
Batch 600/1427: Loss=5.4899 (C:4.3341, R:0.0100, T:3.8494(w:0.300)🚀)
Batch 625/1427: Loss=5.4719 (C:4.4196, R:0.0100, T:3.5044(w:0.300)🚀)
Batch 650/1427: Loss=5.4457 (C:4.3751, R:0.0100, T:3.5653(w:0.300)🚀)
Batch 675/1427: Loss=5.4857 (C:4.3996, R:0.0100, T:3.6169(w:0.300)🚀)
Batch 700/1427: Loss=5.4702 (C:4.3801, R:0.0100, T:3.6303(w:0.300)🚀)
Batch 725/1427: Loss=5.4469 (C:4.3773, R:0.0099, T:3.5620(w:0.300)🚀)
Batch 750/1427: Loss=5.4252 (C:4.4039, R:0.0099, T:3.4011(w:0.300)🚀)
Batch 775/1427: Loss=5.4095 (C:4.3530, R:0.0099, T:3.5186(w:0.300)🚀)
Batch 800/1427: Loss=5.4479 (C:4.3686, R:0.0099, T:3.5944(w:0.300)🚀)
Batch 825/1427: Loss=5.4725 (C:4.3698, R:0.0099, T:3.6723(w:0.300)🚀)
Batch 850/1427: Loss=5.4477 (C:4.3876, R:0.0099, T:3.5301(w:0.300)🚀)
Batch 875/1427: Loss=5.4087 (C:4.3816, R:0.0099, T:3.4202(w:0.300)🚀)
Batch 900/1427: Loss=5.4753 (C:4.3776, R:0.0100, T:3.6554(w:0.300)🚀)
Batch 925/1427: Loss=5.4582 (C:4.3819, R:0.0099, T:3.5842(w:0.300)🚀)
Batch 950/1427: Loss=5.4812 (C:4.3770, R:0.0099, T:3.6775(w:0.300)🚀)
Batch 975/1427: Loss=5.4865 (C:4.3922, R:0.0100, T:3.6442(w:0.300)🚀)
Batch 1000/1427: Loss=5.4786 (C:4.3855, R:0.0100, T:3.6402(w:0.300)🚀)
Batch 1025/1427: Loss=5.4875 (C:4.3534, R:0.0099, T:3.7772(w:0.300)🚀)
Batch 1050/1427: Loss=5.4712 (C:4.3795, R:0.0100, T:3.6357(w:0.300)🚀)
Batch 1075/1427: Loss=5.4936 (C:4.3915, R:0.0100, T:3.6705(w:0.300)🚀)
Batch 1100/1427: Loss=5.4569 (C:4.3653, R:0.0099, T:3.6354(w:0.300)🚀)
Batch 1125/1427: Loss=5.4479 (C:4.3714, R:0.0100, T:3.5852(w:0.300)🚀)
Batch 1150/1427: Loss=5.4840 (C:4.3949, R:0.0100, T:3.6270(w:0.300)🚀)
Batch 1175/1427: Loss=5.4509 (C:4.3880, R:0.0099, T:3.5394(w:0.300)🚀)
Batch 1200/1427: Loss=5.4770 (C:4.3431, R:0.0100, T:3.7764(w:0.300)🚀)
Batch 1225/1427: Loss=5.4300 (C:4.4099, R:0.0099, T:3.3970(w:0.300)🚀)
Batch 1250/1427: Loss=5.4738 (C:4.3837, R:0.0100, T:3.6304(w:0.300)🚀)
Batch 1275/1427: Loss=5.4656 (C:4.3692, R:0.0100, T:3.6513(w:0.300)🚀)
Batch 1300/1427: Loss=5.4503 (C:4.3654, R:0.0099, T:3.6132(w:0.300)🚀)
Batch 1325/1427: Loss=5.4925 (C:4.3733, R:0.0100, T:3.7274(w:0.300)🚀)
Batch 1350/1427: Loss=5.5059 (C:4.3776, R:0.0101, T:3.7575(w:0.300)🚀)
Batch 1375/1427: Loss=5.4655 (C:4.3453, R:0.0100, T:3.7307(w:0.300)🚀)
Batch 1400/1427: Loss=5.4715 (C:4.3256, R:0.0100, T:3.8162(w:0.300)🚀)
Batch 1425/1427: Loss=5.4813 (C:4.3588, R:0.0099, T:3.7382(w:0.300)🚀)
📈 New best topological loss: 3.6462

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 5.4681
  Contrastive: 4.3732
  Reconstruction: 0.0100
  Topological: 3.6462 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.5946
  Contrastive: 1.9934
  Reconstruction: 0.0100
  Topological: 42.0006 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 7/50 COMPLETE (69.4s)
Train Loss: 5.4681 (C:4.3732, R:0.0100, T:3.6462)
Val Loss:   14.5946 (C:1.9934, R:0.0100, T:42.0006)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4648 (C:4.3947, R:0.0101, T:3.5636(w:0.300)🚀)
Batch  25/1427: Loss=5.5008 (C:4.3983, R:0.0100, T:3.6716(w:0.300)🚀)
Batch  50/1427: Loss=5.4836 (C:4.3769, R:0.0100, T:3.6856(w:0.300)🚀)
Batch  75/1427: Loss=5.4985 (C:4.4006, R:0.0100, T:3.6565(w:0.300)🚀)
Batch 100/1427: Loss=5.4482 (C:4.3692, R:0.0100, T:3.5934(w:0.300)🚀)
Batch 125/1427: Loss=5.4730 (C:4.3461, R:0.0100, T:3.7531(w:0.300)🚀)
Batch 150/1427: Loss=5.4472 (C:4.3445, R:0.0099, T:3.6722(w:0.300)🚀)
Batch 175/1427: Loss=5.4655 (C:4.3692, R:0.0100, T:3.6508(w:0.300)🚀)
Batch 200/1427: Loss=5.4335 (C:4.3725, R:0.0099, T:3.5334(w:0.300)🚀)
Batch 225/1427: Loss=5.4995 (C:4.3803, R:0.0100, T:3.7271(w:0.300)🚀)
Batch 250/1427: Loss=5.4975 (C:4.3598, R:0.0100, T:3.7891(w:0.300)🚀)
Batch 275/1427: Loss=5.4674 (C:4.3605, R:0.0099, T:3.6863(w:0.300)🚀)
Batch 300/1427: Loss=5.4872 (C:4.3906, R:0.0100, T:3.6520(w:0.300)🚀)
Batch 325/1427: Loss=5.4481 (C:4.3639, R:0.0099, T:3.6107(w:0.300)🚀)
Batch 350/1427: Loss=5.4597 (C:4.3369, R:0.0100, T:3.7391(w:0.300)🚀)
Batch 375/1427: Loss=5.4676 (C:4.3629, R:0.0099, T:3.6788(w:0.300)🚀)
Batch 400/1427: Loss=5.4580 (C:4.4052, R:0.0099, T:3.5062(w:0.300)🚀)
Batch 425/1427: Loss=5.4632 (C:4.3451, R:0.0100, T:3.7237(w:0.300)🚀)
Batch 450/1427: Loss=5.4558 (C:4.4014, R:0.0099, T:3.5115(w:0.300)🚀)
Batch 475/1427: Loss=5.4162 (C:4.4021, R:0.0099, T:3.3771(w:0.300)🚀)
Batch 500/1427: Loss=5.4627 (C:4.3765, R:0.0100, T:3.6174(w:0.300)🚀)
Batch 525/1427: Loss=5.4424 (C:4.3495, R:0.0100, T:3.6395(w:0.300)🚀)
Batch 550/1427: Loss=5.4767 (C:4.3698, R:0.0100, T:3.6861(w:0.300)🚀)
Batch 575/1427: Loss=5.4658 (C:4.3619, R:0.0100, T:3.6764(w:0.300)🚀)
Batch 600/1427: Loss=5.4663 (C:4.3821, R:0.0100, T:3.6108(w:0.300)🚀)
Batch 625/1427: Loss=5.4607 (C:4.3784, R:0.0099, T:3.6043(w:0.300)🚀)
Batch 650/1427: Loss=5.4497 (C:4.3743, R:0.0099, T:3.5813(w:0.300)🚀)
Batch 675/1427: Loss=5.4649 (C:4.3495, R:0.0099, T:3.7144(w:0.300)🚀)
Batch 700/1427: Loss=5.4799 (C:4.3797, R:0.0099, T:3.6639(w:0.300)🚀)
Batch 725/1427: Loss=5.4634 (C:4.3195, R:0.0100, T:3.8097(w:0.300)🚀)
Batch 750/1427: Loss=5.4365 (C:4.3559, R:0.0100, T:3.5985(w:0.300)🚀)
Batch 775/1427: Loss=5.4319 (C:4.3931, R:0.0100, T:3.4594(w:0.300)🚀)
Batch 800/1427: Loss=5.4723 (C:4.3806, R:0.0100, T:3.6357(w:0.300)🚀)
Batch 825/1427: Loss=5.4754 (C:4.3747, R:0.0099, T:3.6659(w:0.300)🚀)
Batch 850/1427: Loss=5.3934 (C:4.4015, R:0.0099, T:3.3030(w:0.300)🚀)
Batch 875/1427: Loss=5.4814 (C:4.3757, R:0.0100, T:3.6822(w:0.300)🚀)
Batch 900/1427: Loss=5.4796 (C:4.3549, R:0.0100, T:3.7458(w:0.300)🚀)
Batch 925/1427: Loss=5.5104 (C:4.3250, R:0.0100, T:3.9481(w:0.300)🚀)
Batch 950/1427: Loss=5.4804 (C:4.3531, R:0.0100, T:3.7543(w:0.300)🚀)
Batch 975/1427: Loss=5.4311 (C:4.3569, R:0.0099, T:3.5771(w:0.300)🚀)
Batch 1000/1427: Loss=5.4827 (C:4.3564, R:0.0100, T:3.7512(w:0.300)🚀)
Batch 1025/1427: Loss=5.4884 (C:4.4026, R:0.0100, T:3.6161(w:0.300)🚀)
Batch 1050/1427: Loss=5.4612 (C:4.3797, R:0.0100, T:3.6017(w:0.300)🚀)
Batch 1075/1427: Loss=5.4862 (C:4.3896, R:0.0100, T:3.6522(w:0.300)🚀)
Batch 1100/1427: Loss=5.4726 (C:4.3340, R:0.0100, T:3.7918(w:0.300)🚀)
Batch 1125/1427: Loss=5.4473 (C:4.3953, R:0.0099, T:3.5034(w:0.300)🚀)
Batch 1150/1427: Loss=5.4708 (C:4.3912, R:0.0100, T:3.5952(w:0.300)🚀)
Batch 1175/1427: Loss=5.4875 (C:4.3848, R:0.0100, T:3.6726(w:0.300)🚀)
Batch 1200/1427: Loss=5.4521 (C:4.3957, R:0.0100, T:3.5178(w:0.300)🚀)
Batch 1225/1427: Loss=5.4791 (C:4.3901, R:0.0100, T:3.6265(w:0.300)🚀)
Batch 1250/1427: Loss=5.4661 (C:4.3692, R:0.0100, T:3.6528(w:0.300)🚀)
Batch 1275/1427: Loss=5.4640 (C:4.3330, R:0.0100, T:3.7666(w:0.300)🚀)
Batch 1300/1427: Loss=5.4459 (C:4.3484, R:0.0099, T:3.6550(w:0.300)🚀)
Batch 1325/1427: Loss=5.4639 (C:4.3899, R:0.0100, T:3.5767(w:0.300)🚀)
Batch 1350/1427: Loss=5.4869 (C:4.4010, R:0.0100, T:3.6164(w:0.300)🚀)
Batch 1375/1427: Loss=5.4465 (C:4.3659, R:0.0099, T:3.5985(w:0.300)🚀)
Batch 1400/1427: Loss=5.4702 (C:4.3726, R:0.0100, T:3.6555(w:0.300)🚀)
Batch 1425/1427: Loss=5.4581 (C:4.3577, R:0.0100, T:3.6649(w:0.300)🚀)
📈 New best topological loss: 3.6411

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 5.4659
  Contrastive: 4.3726
  Reconstruction: 0.0100
  Topological: 3.6411 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.5267
  Contrastive: 1.9901
  Reconstruction: 0.0100
  Topological: 41.7851 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 8/50 COMPLETE (67.7s)
Train Loss: 5.4659 (C:4.3726, R:0.0100, T:3.6411)
Val Loss:   14.5267 (C:1.9901, R:0.0100, T:41.7851)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.5044 (C:4.3922, R:0.0100, T:3.7040(w:0.300)🚀)
Batch  25/1427: Loss=5.4540 (C:4.3660, R:0.0099, T:3.6235(w:0.300)🚀)
Batch  50/1427: Loss=5.4884 (C:4.3936, R:0.0100, T:3.6459(w:0.300)🚀)
Batch  75/1427: Loss=5.4373 (C:4.3666, R:0.0099, T:3.5657(w:0.300)🚀)
Batch 100/1427: Loss=5.4827 (C:4.3648, R:0.0100, T:3.7231(w:0.300)🚀)
Batch 125/1427: Loss=5.4613 (C:4.3633, R:0.0099, T:3.6567(w:0.300)🚀)
Batch 150/1427: Loss=5.4315 (C:4.3529, R:0.0100, T:3.5919(w:0.300)🚀)
Batch 175/1427: Loss=5.4540 (C:4.3916, R:0.0100, T:3.5382(w:0.300)🚀)
Batch 200/1427: Loss=5.4686 (C:4.3724, R:0.0100, T:3.6505(w:0.300)🚀)
Batch 225/1427: Loss=5.4647 (C:4.3036, R:0.0099, T:3.8669(w:0.300)🚀)
Batch 250/1427: Loss=5.4859 (C:4.3734, R:0.0100, T:3.7049(w:0.300)🚀)
Batch 275/1427: Loss=5.4525 (C:4.3635, R:0.0100, T:3.6266(w:0.300)🚀)
Batch 300/1427: Loss=5.4937 (C:4.3303, R:0.0100, T:3.8746(w:0.300)🚀)
Batch 325/1427: Loss=5.4754 (C:4.3690, R:0.0099, T:3.6847(w:0.300)🚀)
Batch 350/1427: Loss=5.5253 (C:4.3813, R:0.0100, T:3.8102(w:0.300)🚀)
Batch 375/1427: Loss=5.4932 (C:4.3787, R:0.0100, T:3.7118(w:0.300)🚀)
Batch 400/1427: Loss=5.4691 (C:4.3718, R:0.0100, T:3.6545(w:0.300)🚀)
Batch 425/1427: Loss=5.4372 (C:4.3834, R:0.0099, T:3.5095(w:0.300)🚀)
Batch 450/1427: Loss=5.4689 (C:4.3210, R:0.0100, T:3.8231(w:0.300)🚀)
Batch 475/1427: Loss=5.4506 (C:4.3372, R:0.0099, T:3.7079(w:0.300)🚀)
Batch 500/1427: Loss=5.4753 (C:4.3559, R:0.0099, T:3.7279(w:0.300)🚀)
Batch 525/1427: Loss=5.4428 (C:4.3977, R:0.0100, T:3.4803(w:0.300)🚀)
Batch 550/1427: Loss=5.4668 (C:4.3415, R:0.0099, T:3.7479(w:0.300)🚀)
Batch 575/1427: Loss=5.4566 (C:4.3591, R:0.0099, T:3.6552(w:0.300)🚀)
Batch 600/1427: Loss=5.4472 (C:4.3902, R:0.0099, T:3.5201(w:0.300)🚀)
Batch 625/1427: Loss=5.4228 (C:4.3780, R:0.0099, T:3.4796(w:0.300)🚀)
Batch 650/1427: Loss=5.4245 (C:4.4352, R:0.0099, T:3.2944(w:0.300)🚀)
Batch 675/1427: Loss=5.4409 (C:4.3617, R:0.0100, T:3.5940(w:0.300)🚀)
Batch 700/1427: Loss=5.4546 (C:4.4255, R:0.0099, T:3.4268(w:0.300)🚀)
Batch 725/1427: Loss=5.4682 (C:4.3824, R:0.0099, T:3.6161(w:0.300)🚀)
Batch 750/1427: Loss=5.4889 (C:4.3541, R:0.0100, T:3.7795(w:0.300)🚀)
Batch 775/1427: Loss=5.4637 (C:4.3622, R:0.0100, T:3.6682(w:0.300)🚀)
Batch 800/1427: Loss=5.4637 (C:4.3720, R:0.0100, T:3.6355(w:0.300)🚀)
Batch 825/1427: Loss=5.4895 (C:4.3937, R:0.0100, T:3.6493(w:0.300)🚀)
Batch 850/1427: Loss=5.4880 (C:4.3806, R:0.0100, T:3.6881(w:0.300)🚀)
Batch 875/1427: Loss=5.4642 (C:4.3428, R:0.0100, T:3.7345(w:0.300)🚀)
Batch 900/1427: Loss=5.4543 (C:4.3826, R:0.0099, T:3.5691(w:0.300)🚀)
Batch 925/1427: Loss=5.4602 (C:4.3531, R:0.0099, T:3.6870(w:0.300)🚀)
Batch 950/1427: Loss=5.3960 (C:4.3913, R:0.0099, T:3.3457(w:0.300)🚀)
Batch 975/1427: Loss=5.4641 (C:4.3690, R:0.0099, T:3.6472(w:0.300)🚀)
Batch 1000/1427: Loss=5.4605 (C:4.3826, R:0.0099, T:3.5896(w:0.300)🚀)
Batch 1025/1427: Loss=5.4378 (C:4.3816, R:0.0099, T:3.5172(w:0.300)🚀)
Batch 1050/1427: Loss=5.4146 (C:4.3648, R:0.0099, T:3.4958(w:0.300)🚀)
Batch 1075/1427: Loss=5.4603 (C:4.4253, R:0.0099, T:3.4467(w:0.300)🚀)
Batch 1100/1427: Loss=5.4623 (C:4.4023, R:0.0099, T:3.5299(w:0.300)🚀)
Batch 1125/1427: Loss=5.4755 (C:4.4200, R:0.0099, T:3.5151(w:0.300)🚀)
Batch 1150/1427: Loss=5.4429 (C:4.3993, R:0.0099, T:3.4755(w:0.300)🚀)
Batch 1175/1427: Loss=5.4353 (C:4.3533, R:0.0099, T:3.6033(w:0.300)🚀)
Batch 1200/1427: Loss=5.4466 (C:4.3888, R:0.0099, T:3.5227(w:0.300)🚀)
Batch 1225/1427: Loss=5.4162 (C:4.3580, R:0.0099, T:3.5242(w:0.300)🚀)
Batch 1250/1427: Loss=5.4688 (C:4.3378, R:0.0100, T:3.7667(w:0.300)🚀)
Batch 1275/1427: Loss=5.4448 (C:4.4004, R:0.0099, T:3.4780(w:0.300)🚀)
Batch 1300/1427: Loss=5.4620 (C:4.4177, R:0.0099, T:3.4777(w:0.300)🚀)
Batch 1325/1427: Loss=5.4685 (C:4.3577, R:0.0100, T:3.6992(w:0.300)🚀)
Batch 1350/1427: Loss=5.4720 (C:4.3455, R:0.0100, T:3.7517(w:0.300)🚀)
Batch 1375/1427: Loss=5.4484 (C:4.3583, R:0.0100, T:3.6304(w:0.300)🚀)
Batch 1400/1427: Loss=5.4744 (C:4.3565, R:0.0100, T:3.7231(w:0.300)🚀)
Batch 1425/1427: Loss=5.4568 (C:4.3369, R:0.0100, T:3.7296(w:0.300)🚀)
📈 New best topological loss: 3.6351

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 5.4639
  Contrastive: 4.3724
  Reconstruction: 0.0100
  Topological: 3.6351 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.7157
  Contrastive: 1.9858
  Reconstruction: 0.0100
  Topological: 42.4296 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 9/50 COMPLETE (71.3s)
Train Loss: 5.4639 (C:4.3724, R:0.0100, T:3.6351)
Val Loss:   14.7157 (C:1.9858, R:0.0100, T:42.4296)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4118 (C:4.3693, R:0.0099, T:3.4718(w:0.300)🚀)
Batch  25/1427: Loss=5.4328 (C:4.3661, R:0.0099, T:3.5525(w:0.300)🚀)
Batch  50/1427: Loss=5.4281 (C:4.3665, R:0.0099, T:3.5351(w:0.300)🚀)
Batch  75/1427: Loss=5.4748 (C:4.3354, R:0.0100, T:3.7944(w:0.300)🚀)
Batch 100/1427: Loss=5.4866 (C:4.3347, R:0.0099, T:3.8361(w:0.300)🚀)
Batch 125/1427: Loss=5.4610 (C:4.3914, R:0.0100, T:3.5620(w:0.300)🚀)
Batch 150/1427: Loss=5.4672 (C:4.3792, R:0.0099, T:3.6232(w:0.300)🚀)
Batch 175/1427: Loss=5.4242 (C:4.3557, R:0.0099, T:3.5583(w:0.300)🚀)
Batch 200/1427: Loss=5.4521 (C:4.3616, R:0.0099, T:3.6316(w:0.300)🚀)
Batch 225/1427: Loss=5.5080 (C:4.3985, R:0.0099, T:3.6951(w:0.300)🚀)
Batch 250/1427: Loss=5.4455 (C:4.3791, R:0.0099, T:3.5512(w:0.300)🚀)
Batch 275/1427: Loss=5.4843 (C:4.3474, R:0.0100, T:3.7863(w:0.300)🚀)
Batch 300/1427: Loss=5.4528 (C:4.3741, R:0.0100, T:3.5924(w:0.300)🚀)
Batch 325/1427: Loss=5.4993 (C:4.4018, R:0.0100, T:3.6551(w:0.300)🚀)
Batch 350/1427: Loss=5.4909 (C:4.3873, R:0.0100, T:3.6754(w:0.300)🚀)
Batch 375/1427: Loss=5.4408 (C:4.3565, R:0.0099, T:3.6110(w:0.300)🚀)
Batch 400/1427: Loss=5.4525 (C:4.3250, R:0.0100, T:3.7552(w:0.300)🚀)
Batch 425/1427: Loss=5.4476 (C:4.3460, R:0.0099, T:3.6686(w:0.300)🚀)
Batch 450/1427: Loss=5.5092 (C:4.3601, R:0.0100, T:3.8271(w:0.300)🚀)
Batch 475/1427: Loss=5.4232 (C:4.3405, R:0.0099, T:3.6055(w:0.300)🚀)
Batch 500/1427: Loss=5.4772 (C:4.3638, R:0.0100, T:3.7080(w:0.300)🚀)
Batch 525/1427: Loss=5.4398 (C:4.3837, R:0.0100, T:3.5169(w:0.300)🚀)
Batch 550/1427: Loss=5.5024 (C:4.3404, R:0.0100, T:3.8697(w:0.300)🚀)
Batch 575/1427: Loss=5.4904 (C:4.3465, R:0.0100, T:3.8094(w:0.300)🚀)
Batch 600/1427: Loss=5.4670 (C:4.3539, R:0.0100, T:3.7069(w:0.300)🚀)
Batch 625/1427: Loss=5.4618 (C:4.3964, R:0.0100, T:3.5483(w:0.300)🚀)
Batch 650/1427: Loss=5.4457 (C:4.4048, R:0.0100, T:3.4663(w:0.300)🚀)
Batch 675/1427: Loss=5.4492 (C:4.3655, R:0.0099, T:3.6090(w:0.300)🚀)
Batch 700/1427: Loss=5.4102 (C:4.4102, R:0.0099, T:3.3297(w:0.300)🚀)
Batch 725/1427: Loss=5.4606 (C:4.4170, R:0.0100, T:3.4753(w:0.300)🚀)
Batch 750/1427: Loss=5.4245 (C:4.3565, R:0.0100, T:3.5566(w:0.300)🚀)
Batch 775/1427: Loss=5.4883 (C:4.3519, R:0.0100, T:3.7848(w:0.300)🚀)
Batch 800/1427: Loss=5.4636 (C:4.3960, R:0.0100, T:3.5554(w:0.300)🚀)
Batch 825/1427: Loss=5.4853 (C:4.3719, R:0.0100, T:3.7079(w:0.300)🚀)
Batch 850/1427: Loss=5.4223 (C:4.3615, R:0.0099, T:3.5327(w:0.300)🚀)
Batch 875/1427: Loss=5.4857 (C:4.3553, R:0.0099, T:3.7646(w:0.300)🚀)
Batch 900/1427: Loss=5.4548 (C:4.3514, R:0.0099, T:3.6746(w:0.300)🚀)
Batch 925/1427: Loss=5.4260 (C:4.2992, R:0.0099, T:3.7526(w:0.300)🚀)
Batch 950/1427: Loss=5.4922 (C:4.3720, R:0.0100, T:3.7306(w:0.300)🚀)
Batch 975/1427: Loss=5.4632 (C:4.3818, R:0.0099, T:3.6015(w:0.300)🚀)
Batch 1000/1427: Loss=5.4633 (C:4.4022, R:0.0100, T:3.5335(w:0.300)🚀)
Batch 1025/1427: Loss=5.4290 (C:4.4059, R:0.0100, T:3.4072(w:0.300)🚀)
Batch 1050/1427: Loss=5.5002 (C:4.3524, R:0.0100, T:3.8224(w:0.300)🚀)
Batch 1075/1427: Loss=5.4627 (C:4.3692, R:0.0099, T:3.6417(w:0.300)🚀)
Batch 1100/1427: Loss=5.4388 (C:4.4026, R:0.0099, T:3.4505(w:0.300)🚀)
Batch 1125/1427: Loss=5.4291 (C:4.3810, R:0.0100, T:3.4902(w:0.300)🚀)
Batch 1150/1427: Loss=5.5042 (C:4.4039, R:0.0100, T:3.6642(w:0.300)🚀)
Batch 1175/1427: Loss=5.4451 (C:4.3592, R:0.0099, T:3.6164(w:0.300)🚀)
Batch 1200/1427: Loss=5.4858 (C:4.3449, R:0.0100, T:3.7997(w:0.300)🚀)
Batch 1225/1427: Loss=5.4548 (C:4.3505, R:0.0100, T:3.6778(w:0.300)🚀)
Batch 1250/1427: Loss=5.4880 (C:4.3751, R:0.0099, T:3.7061(w:0.300)🚀)
Batch 1275/1427: Loss=5.4308 (C:4.3864, R:0.0099, T:3.4783(w:0.300)🚀)
Batch 1300/1427: Loss=5.4252 (C:4.3766, R:0.0099, T:3.4918(w:0.300)🚀)
Batch 1325/1427: Loss=5.4708 (C:4.3873, R:0.0099, T:3.6084(w:0.300)🚀)
Batch 1350/1427: Loss=5.4741 (C:4.4000, R:0.0099, T:3.5773(w:0.300)🚀)
Batch 1375/1427: Loss=5.4567 (C:4.3552, R:0.0100, T:3.6683(w:0.300)🚀)
Batch 1400/1427: Loss=5.4742 (C:4.3945, R:0.0100, T:3.5957(w:0.300)🚀)
Batch 1425/1427: Loss=5.5210 (C:4.3289, R:0.0100, T:3.9703(w:0.300)🚀)
📈 New best topological loss: 3.6347

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 5.4634
  Contrastive: 4.3720
  Reconstruction: 0.0100
  Topological: 3.6347 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.0255
  Contrastive: 1.9907
  Reconstruction: 0.0100
  Topological: 43.4460 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 10/50 COMPLETE (68.7s)
Train Loss: 5.4634 (C:4.3720, R:0.0100, T:3.6347)
Val Loss:   15.0255 (C:1.9907, R:0.0100, T:43.4460)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4959 (C:4.3340, R:0.0100, T:3.8698(w:0.300)🚀)
Batch  25/1427: Loss=5.4561 (C:4.3304, R:0.0099, T:3.7490(w:0.300)🚀)
Batch  50/1427: Loss=5.4749 (C:4.3433, R:0.0100, T:3.7686(w:0.300)🚀)
Batch  75/1427: Loss=5.4454 (C:4.3455, R:0.0099, T:3.6629(w:0.300)🚀)
Batch 100/1427: Loss=5.4702 (C:4.4150, R:0.0100, T:3.5139(w:0.300)🚀)
Batch 125/1427: Loss=5.4409 (C:4.4186, R:0.0100, T:3.4043(w:0.300)🚀)
Batch 150/1427: Loss=5.4538 (C:4.4021, R:0.0099, T:3.5024(w:0.300)🚀)
Batch 175/1427: Loss=5.4094 (C:4.3775, R:0.0099, T:3.4362(w:0.300)🚀)
Batch 200/1427: Loss=5.4697 (C:4.3523, R:0.0099, T:3.7212(w:0.300)🚀)
Batch 225/1427: Loss=5.4606 (C:4.3366, R:0.0100, T:3.7433(w:0.300)🚀)
Batch 250/1427: Loss=5.4507 (C:4.3706, R:0.0100, T:3.5970(w:0.300)🚀)
Batch 275/1427: Loss=5.4357 (C:4.3877, R:0.0099, T:3.4900(w:0.300)🚀)
Batch 300/1427: Loss=5.4828 (C:4.3603, R:0.0099, T:3.7384(w:0.300)🚀)
Batch 325/1427: Loss=5.4605 (C:4.3732, R:0.0100, T:3.6209(w:0.300)🚀)
Batch 350/1427: Loss=5.4863 (C:4.3578, R:0.0099, T:3.7583(w:0.300)🚀)
Batch 375/1427: Loss=5.4533 (C:4.3576, R:0.0100, T:3.6491(w:0.300)🚀)
Batch 400/1427: Loss=5.3869 (C:4.3322, R:0.0098, T:3.5124(w:0.300)🚀)
Batch 425/1427: Loss=5.4637 (C:4.4075, R:0.0100, T:3.5173(w:0.300)🚀)
Batch 450/1427: Loss=5.4741 (C:4.3709, R:0.0100, T:3.6741(w:0.300)🚀)
Batch 475/1427: Loss=5.4670 (C:4.3616, R:0.0100, T:3.6811(w:0.300)🚀)
Batch 500/1427: Loss=5.4500 (C:4.3669, R:0.0099, T:3.6072(w:0.300)🚀)
Batch 525/1427: Loss=5.4227 (C:4.3892, R:0.0099, T:3.4416(w:0.300)🚀)
Batch 550/1427: Loss=5.4491 (C:4.3865, R:0.0099, T:3.5388(w:0.300)🚀)
Batch 575/1427: Loss=5.4849 (C:4.4009, R:0.0100, T:3.6102(w:0.300)🚀)
Batch 600/1427: Loss=5.4253 (C:4.3572, R:0.0099, T:3.5571(w:0.300)🚀)
Batch 625/1427: Loss=5.4964 (C:4.3868, R:0.0100, T:3.6951(w:0.300)🚀)
Batch 650/1427: Loss=5.3912 (C:4.3583, R:0.0099, T:3.4398(w:0.300)🚀)
Batch 675/1427: Loss=5.4360 (C:4.3891, R:0.0100, T:3.4864(w:0.300)🚀)
Batch 700/1427: Loss=5.4509 (C:4.3556, R:0.0099, T:3.6476(w:0.300)🚀)
Batch 725/1427: Loss=5.4468 (C:4.3640, R:0.0099, T:3.6061(w:0.300)🚀)
Batch 750/1427: Loss=5.4669 (C:4.3377, R:0.0100, T:3.7607(w:0.300)🚀)
Batch 775/1427: Loss=5.4390 (C:4.3623, R:0.0099, T:3.5855(w:0.300)🚀)
Batch 800/1427: Loss=5.4922 (C:4.3792, R:0.0100, T:3.7068(w:0.300)🚀)
Batch 825/1427: Loss=5.4536 (C:4.3689, R:0.0099, T:3.6122(w:0.300)🚀)
Batch 850/1427: Loss=5.4956 (C:4.3346, R:0.0100, T:3.8666(w:0.300)🚀)
Batch 875/1427: Loss=5.4894 (C:4.3793, R:0.0100, T:3.6972(w:0.300)🚀)
Batch 900/1427: Loss=5.4629 (C:4.3516, R:0.0099, T:3.7013(w:0.300)🚀)
Batch 925/1427: Loss=5.4522 (C:4.3699, R:0.0100, T:3.6045(w:0.300)🚀)
Batch 950/1427: Loss=5.4476 (C:4.3872, R:0.0099, T:3.5311(w:0.300)🚀)
Batch 975/1427: Loss=5.4751 (C:4.3914, R:0.0100, T:3.6088(w:0.300)🚀)
Batch 1000/1427: Loss=5.4625 (C:4.3669, R:0.0100, T:3.6487(w:0.300)🚀)
Batch 1025/1427: Loss=5.4773 (C:4.3843, R:0.0100, T:3.6398(w:0.300)🚀)
Batch 1050/1427: Loss=5.4683 (C:4.3970, R:0.0100, T:3.5679(w:0.300)🚀)
Batch 1075/1427: Loss=5.4712 (C:4.4158, R:0.0099, T:3.5145(w:0.300)🚀)
Batch 1100/1427: Loss=5.4796 (C:4.3800, R:0.0099, T:3.6620(w:0.300)🚀)
Batch 1125/1427: Loss=5.4839 (C:4.3861, R:0.0100, T:3.6561(w:0.300)🚀)
Batch 1150/1427: Loss=5.4645 (C:4.4271, R:0.0100, T:3.4547(w:0.300)🚀)
Batch 1175/1427: Loss=5.4356 (C:4.3845, R:0.0100, T:3.5004(w:0.300)🚀)
Batch 1200/1427: Loss=5.4740 (C:4.3842, R:0.0099, T:3.6291(w:0.300)🚀)
Batch 1225/1427: Loss=5.4451 (C:4.3922, R:0.0099, T:3.5063(w:0.300)🚀)
Batch 1250/1427: Loss=5.4577 (C:4.3778, R:0.0100, T:3.5964(w:0.300)🚀)
Batch 1275/1427: Loss=5.4952 (C:4.3697, R:0.0100, T:3.7482(w:0.300)🚀)
Batch 1300/1427: Loss=5.4406 (C:4.3128, R:0.0100, T:3.7558(w:0.300)🚀)
Batch 1325/1427: Loss=5.5236 (C:4.3497, R:0.0100, T:3.9096(w:0.300)🚀)
Batch 1350/1427: Loss=5.4545 (C:4.3711, R:0.0099, T:3.6080(w:0.300)🚀)
Batch 1375/1427: Loss=5.4486 (C:4.3964, R:0.0100, T:3.5040(w:0.300)🚀)
Batch 1400/1427: Loss=5.4552 (C:4.3961, R:0.0099, T:3.5268(w:0.300)🚀)
Batch 1425/1427: Loss=5.4706 (C:4.3764, R:0.0100, T:3.6440(w:0.300)🚀)
📈 New best topological loss: 3.6310

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 5.4621
  Contrastive: 4.3718
  Reconstruction: 0.0100
  Topological: 3.6310 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.9390
  Contrastive: 1.9900
  Reconstruction: 0.0100
  Topological: 43.1601 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 11/50 COMPLETE (69.4s)
Train Loss: 5.4621 (C:4.3718, R:0.0100, T:3.6310)
Val Loss:   14.9390 (C:1.9900, R:0.0100, T:43.1601)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4829 (C:4.3620, R:0.0099, T:3.7328(w:0.300)🚀)
Batch  25/1427: Loss=5.4657 (C:4.3528, R:0.0100, T:3.7063(w:0.300)🚀)
Batch  50/1427: Loss=5.4620 (C:4.3642, R:0.0100, T:3.6557(w:0.300)🚀)
Batch  75/1427: Loss=5.3939 (C:4.3328, R:0.0099, T:3.5336(w:0.300)🚀)
Batch 100/1427: Loss=5.4847 (C:4.3700, R:0.0099, T:3.7124(w:0.300)🚀)
Batch 125/1427: Loss=5.4448 (C:4.3593, R:0.0100, T:3.6151(w:0.300)🚀)
Batch 150/1427: Loss=5.4502 (C:4.3812, R:0.0099, T:3.5597(w:0.300)🚀)
Batch 175/1427: Loss=5.4387 (C:4.3679, R:0.0099, T:3.5658(w:0.300)🚀)
Batch 200/1427: Loss=5.4830 (C:4.3778, R:0.0100, T:3.6808(w:0.300)🚀)
Batch 225/1427: Loss=5.4613 (C:4.3382, R:0.0099, T:3.7405(w:0.300)🚀)
Batch 250/1427: Loss=5.4161 (C:4.3308, R:0.0099, T:3.6141(w:0.300)🚀)
Batch 275/1427: Loss=5.4402 (C:4.3926, R:0.0099, T:3.4888(w:0.300)🚀)
Batch 300/1427: Loss=5.4207 (C:4.4003, R:0.0100, T:3.3979(w:0.300)🚀)
Batch 325/1427: Loss=5.4858 (C:4.3639, R:0.0100, T:3.7365(w:0.300)🚀)
Batch 350/1427: Loss=5.4511 (C:4.3538, R:0.0099, T:3.6546(w:0.300)🚀)
Batch 375/1427: Loss=5.4467 (C:4.3831, R:0.0099, T:3.5421(w:0.300)🚀)
Batch 400/1427: Loss=5.4691 (C:4.4008, R:0.0100, T:3.5579(w:0.300)🚀)
Batch 425/1427: Loss=5.4682 (C:4.3494, R:0.0100, T:3.7259(w:0.300)🚀)
Batch 450/1427: Loss=5.4768 (C:4.3636, R:0.0100, T:3.7073(w:0.300)🚀)
Batch 475/1427: Loss=5.4735 (C:4.3382, R:0.0100, T:3.7807(w:0.300)🚀)
Batch 500/1427: Loss=5.4448 (C:4.3762, R:0.0100, T:3.5585(w:0.300)🚀)
Batch 525/1427: Loss=5.4727 (C:4.4130, R:0.0100, T:3.5291(w:0.300)🚀)
Batch 550/1427: Loss=5.4788 (C:4.3887, R:0.0099, T:3.6304(w:0.300)🚀)
Batch 575/1427: Loss=5.4543 (C:4.4019, R:0.0099, T:3.5046(w:0.300)🚀)
Batch 600/1427: Loss=5.4737 (C:4.3732, R:0.0099, T:3.6648(w:0.300)🚀)
Batch 625/1427: Loss=5.4646 (C:4.3980, R:0.0099, T:3.5519(w:0.300)🚀)
Batch 650/1427: Loss=5.4742 (C:4.3973, R:0.0100, T:3.5862(w:0.300)🚀)
Batch 675/1427: Loss=5.4661 (C:4.3612, R:0.0100, T:3.6799(w:0.300)🚀)
Batch 700/1427: Loss=5.4738 (C:4.3198, R:0.0100, T:3.8434(w:0.300)🚀)
Batch 725/1427: Loss=5.4598 (C:4.3339, R:0.0100, T:3.7497(w:0.300)🚀)
Batch 750/1427: Loss=5.4475 (C:4.3682, R:0.0100, T:3.5945(w:0.300)🚀)
Batch 775/1427: Loss=5.4474 (C:4.3663, R:0.0100, T:3.6003(w:0.300)🚀)
Batch 800/1427: Loss=5.4473 (C:4.3508, R:0.0099, T:3.6516(w:0.300)🚀)
Batch 825/1427: Loss=5.4952 (C:4.3797, R:0.0100, T:3.7151(w:0.300)🚀)
Batch 850/1427: Loss=5.4954 (C:4.3570, R:0.0100, T:3.7913(w:0.300)🚀)
Batch 875/1427: Loss=5.4252 (C:4.4107, R:0.0100, T:3.3784(w:0.300)🚀)
Batch 900/1427: Loss=5.4396 (C:4.4111, R:0.0100, T:3.4249(w:0.300)🚀)
Batch 925/1427: Loss=5.4935 (C:4.3854, R:0.0100, T:3.6901(w:0.300)🚀)
Batch 950/1427: Loss=5.4530 (C:4.3526, R:0.0100, T:3.6646(w:0.300)🚀)
Batch 975/1427: Loss=5.5035 (C:4.3913, R:0.0100, T:3.7039(w:0.300)🚀)
Batch 1000/1427: Loss=5.4567 (C:4.3684, R:0.0100, T:3.6243(w:0.300)🚀)
Batch 1025/1427: Loss=5.4844 (C:4.3889, R:0.0100, T:3.6483(w:0.300)🚀)
Batch 1050/1427: Loss=5.4598 (C:4.3429, R:0.0099, T:3.7197(w:0.300)🚀)
Batch 1075/1427: Loss=5.4334 (C:4.3727, R:0.0099, T:3.5323(w:0.300)🚀)
Batch 1100/1427: Loss=5.4874 (C:4.4025, R:0.0100, T:3.6131(w:0.300)🚀)
Batch 1125/1427: Loss=5.4560 (C:4.3638, R:0.0100, T:3.6374(w:0.300)🚀)
Batch 1150/1427: Loss=5.5064 (C:4.3250, R:0.0101, T:3.9344(w:0.300)🚀)
Batch 1175/1427: Loss=5.4680 (C:4.3837, R:0.0100, T:3.6110(w:0.300)🚀)
Batch 1200/1427: Loss=5.4211 (C:4.4286, R:0.0098, T:3.3050(w:0.300)🚀)
Batch 1225/1427: Loss=5.4586 (C:4.3491, R:0.0099, T:3.6948(w:0.300)🚀)
Batch 1250/1427: Loss=5.4357 (C:4.3273, R:0.0100, T:3.6912(w:0.300)🚀)
Batch 1275/1427: Loss=5.4632 (C:4.3403, R:0.0100, T:3.7397(w:0.300)🚀)
Batch 1300/1427: Loss=5.4841 (C:4.3768, R:0.0100, T:3.6878(w:0.300)🚀)
Batch 1325/1427: Loss=5.4130 (C:4.3396, R:0.0099, T:3.5747(w:0.300)🚀)
Batch 1350/1427: Loss=5.4469 (C:4.3611, R:0.0099, T:3.6159(w:0.300)🚀)
Batch 1375/1427: Loss=5.4522 (C:4.3533, R:0.0100, T:3.6596(w:0.300)🚀)
Batch 1400/1427: Loss=5.4837 (C:4.3631, R:0.0100, T:3.7322(w:0.300)🚀)
Batch 1425/1427: Loss=5.5280 (C:4.4077, R:0.0100, T:3.7311(w:0.300)🚀)

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 5.4623
  Contrastive: 4.3718
  Reconstruction: 0.0100
  Topological: 3.6317 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14.9865
  Contrastive: 1.9874
  Reconstruction: 0.0100
  Topological: 43.3272 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (68.6s)
Train Loss: 5.4623 (C:4.3718, R:0.0100, T:3.6317)
Val Loss:   14.9865 (C:1.9874, R:0.0100, T:43.3272)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4977 (C:4.3831, R:0.0100, T:3.7121(w:0.300)🚀)
Batch  25/1427: Loss=5.4346 (C:4.3773, R:0.0099, T:3.5212(w:0.300)🚀)
Batch  50/1427: Loss=5.4473 (C:4.3954, R:0.0099, T:3.5031(w:0.300)🚀)
Batch  75/1427: Loss=5.5043 (C:4.3674, R:0.0100, T:3.7864(w:0.300)🚀)
Batch 100/1427: Loss=5.4261 (C:4.3912, R:0.0099, T:3.4464(w:0.300)🚀)
Batch 125/1427: Loss=5.5067 (C:4.4028, R:0.0100, T:3.6763(w:0.300)🚀)
Batch 150/1427: Loss=5.4566 (C:4.3383, R:0.0099, T:3.7245(w:0.300)🚀)
Batch 175/1427: Loss=5.4681 (C:4.3774, R:0.0100, T:3.6322(w:0.300)🚀)
Batch 200/1427: Loss=5.4200 (C:4.3629, R:0.0099, T:3.5203(w:0.300)🚀)
Batch 225/1427: Loss=5.4297 (C:4.3629, R:0.0099, T:3.5526(w:0.300)🚀)
Batch 250/1427: Loss=5.4830 (C:4.3708, R:0.0099, T:3.7039(w:0.300)🚀)
Batch 275/1427: Loss=5.4841 (C:4.3523, R:0.0100, T:3.7693(w:0.300)🚀)
Batch 300/1427: Loss=5.4632 (C:4.3421, R:0.0100, T:3.7337(w:0.300)🚀)
Batch 325/1427: Loss=5.4485 (C:4.3599, R:0.0100, T:3.6251(w:0.300)🚀)
Batch 350/1427: Loss=5.4481 (C:4.3719, R:0.0100, T:3.5839(w:0.300)🚀)
Batch 375/1427: Loss=5.4854 (C:4.3882, R:0.0100, T:3.6539(w:0.300)🚀)
Batch 400/1427: Loss=5.4859 (C:4.4152, R:0.0099, T:3.5657(w:0.300)🚀)
Batch 425/1427: Loss=5.4861 (C:4.4012, R:0.0099, T:3.6129(w:0.300)🚀)
Batch 450/1427: Loss=5.5258 (C:4.4159, R:0.0100, T:3.6965(w:0.300)🚀)
Batch 475/1427: Loss=5.4471 (C:4.3232, R:0.0100, T:3.7430(w:0.300)🚀)
Batch 500/1427: Loss=5.4902 (C:4.3770, R:0.0100, T:3.7075(w:0.300)🚀)
Batch 525/1427: Loss=5.4668 (C:4.3591, R:0.0099, T:3.6890(w:0.300)🚀)
Batch 550/1427: Loss=5.4847 (C:4.3870, R:0.0100, T:3.6556(w:0.300)🚀)
Batch 575/1427: Loss=5.4769 (C:4.3684, R:0.0100, T:3.6915(w:0.300)🚀)
Batch 600/1427: Loss=5.4839 (C:4.3820, R:0.0100, T:3.6694(w:0.300)🚀)
Batch 625/1427: Loss=5.5002 (C:4.3779, R:0.0100, T:3.7377(w:0.300)🚀)
Batch 650/1427: Loss=5.4573 (C:4.3899, R:0.0099, T:3.5544(w:0.300)🚀)
Batch 675/1427: Loss=5.4425 (C:4.4034, R:0.0099, T:3.4604(w:0.300)🚀)
Batch 700/1427: Loss=5.4342 (C:4.3613, R:0.0099, T:3.5729(w:0.300)🚀)
Batch 725/1427: Loss=5.4661 (C:4.3603, R:0.0100, T:3.6828(w:0.300)🚀)
Batch 750/1427: Loss=5.4680 (C:4.3161, R:0.0099, T:3.8361(w:0.300)🚀)
Batch 775/1427: Loss=5.4460 (C:4.3657, R:0.0099, T:3.5977(w:0.300)🚀)
Batch 800/1427: Loss=5.4582 (C:4.3837, R:0.0099, T:3.5785(w:0.300)🚀)
Batch 825/1427: Loss=5.4618 (C:4.3581, R:0.0099, T:3.6756(w:0.300)🚀)
Batch 850/1427: Loss=5.4885 (C:4.4138, R:0.0100, T:3.5789(w:0.300)🚀)
Batch 875/1427: Loss=5.4314 (C:4.3843, R:0.0099, T:3.4872(w:0.300)🚀)
Batch 900/1427: Loss=5.4595 (C:4.3959, R:0.0100, T:3.5421(w:0.300)🚀)
Batch 925/1427: Loss=5.4656 (C:4.4145, R:0.0099, T:3.5004(w:0.300)🚀)
Batch 950/1427: Loss=5.4252 (C:4.3779, R:0.0099, T:3.4875(w:0.300)🚀)
Batch 975/1427: Loss=5.4495 (C:4.3582, R:0.0099, T:3.6343(w:0.300)🚀)
Batch 1000/1427: Loss=5.4784 (C:4.3918, R:0.0099, T:3.6187(w:0.300)🚀)
Batch 1025/1427: Loss=5.4858 (C:4.3645, R:0.0099, T:3.7344(w:0.300)🚀)
Batch 1050/1427: Loss=5.4613 (C:4.3756, R:0.0099, T:3.6156(w:0.300)🚀)
Batch 1075/1427: Loss=5.4371 (C:4.3645, R:0.0100, T:3.5719(w:0.300)🚀)
Batch 1100/1427: Loss=5.5029 (C:4.3726, R:0.0100, T:3.7642(w:0.300)🚀)
Batch 1125/1427: Loss=5.4709 (C:4.3724, R:0.0100, T:3.6582(w:0.300)🚀)
Batch 1150/1427: Loss=5.5239 (C:4.3293, R:0.0100, T:3.9788(w:0.300)🚀)
Batch 1175/1427: Loss=5.4435 (C:4.3786, R:0.0099, T:3.5464(w:0.300)🚀)
Batch 1200/1427: Loss=5.5120 (C:4.3705, R:0.0100, T:3.8015(w:0.300)🚀)
Batch 1225/1427: Loss=5.4549 (C:4.3749, R:0.0100, T:3.5965(w:0.300)🚀)
Batch 1250/1427: Loss=5.4678 (C:4.3861, R:0.0100, T:3.6023(w:0.300)🚀)
Batch 1275/1427: Loss=5.4861 (C:4.3764, R:0.0100, T:3.6957(w:0.300)🚀)
Batch 1300/1427: Loss=5.4494 (C:4.3772, R:0.0100, T:3.5707(w:0.300)🚀)
Batch 1325/1427: Loss=5.4191 (C:4.3507, R:0.0099, T:3.5582(w:0.300)🚀)
Batch 1350/1427: Loss=5.4572 (C:4.3743, R:0.0100, T:3.6062(w:0.300)🚀)
Batch 1375/1427: Loss=5.4378 (C:4.4203, R:0.0099, T:3.3882(w:0.300)🚀)
Batch 1400/1427: Loss=5.4188 (C:4.3519, R:0.0099, T:3.5533(w:0.300)🚀)
Batch 1425/1427: Loss=5.4517 (C:4.3602, R:0.0100, T:3.6349(w:0.300)🚀)

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 5.4640
  Contrastive: 4.3719
  Reconstruction: 0.0100
  Topological: 3.6367 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.4889
  Contrastive: 1.9906
  Reconstruction: 0.0100
  Topological: 44.9910 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 13/50 COMPLETE (68.0s)
Train Loss: 5.4640 (C:4.3719, R:0.0100, T:3.6367)
Val Loss:   15.4889 (C:1.9906, R:0.0100, T:44.9910)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4593 (C:4.3472, R:0.0099, T:3.7037(w:0.300)🚀)
Batch  25/1427: Loss=5.4829 (C:4.3786, R:0.0100, T:3.6779(w:0.300)🚀)
Batch  50/1427: Loss=5.4694 (C:4.3806, R:0.0100, T:3.6261(w:0.300)🚀)
Batch  75/1427: Loss=5.4596 (C:4.3911, R:0.0099, T:3.5583(w:0.300)🚀)
Batch 100/1427: Loss=5.4763 (C:4.4067, R:0.0100, T:3.5618(w:0.300)🚀)
Batch 125/1427: Loss=5.4424 (C:4.3534, R:0.0099, T:3.6269(w:0.300)🚀)
Batch 150/1427: Loss=5.5012 (C:4.3672, R:0.0100, T:3.7767(w:0.300)🚀)
Batch 175/1427: Loss=5.4703 (C:4.3581, R:0.0099, T:3.7042(w:0.300)🚀)
Batch 200/1427: Loss=5.4543 (C:4.3656, R:0.0099, T:3.6256(w:0.300)🚀)
Batch 225/1427: Loss=5.4579 (C:4.3290, R:0.0099, T:3.7597(w:0.300)🚀)
Batch 250/1427: Loss=5.4759 (C:4.4020, R:0.0100, T:3.5763(w:0.300)🚀)
Batch 275/1427: Loss=5.4811 (C:4.4012, R:0.0100, T:3.5961(w:0.300)🚀)
Batch 300/1427: Loss=5.4770 (C:4.4153, R:0.0100, T:3.5357(w:0.300)🚀)
Batch 325/1427: Loss=5.4988 (C:4.3467, R:0.0100, T:3.8370(w:0.300)🚀)
Batch 350/1427: Loss=5.4526 (C:4.3475, R:0.0099, T:3.6804(w:0.300)🚀)
Batch 375/1427: Loss=5.4756 (C:4.3677, R:0.0100, T:3.6894(w:0.300)🚀)
Batch 400/1427: Loss=5.4506 (C:4.3688, R:0.0100, T:3.6026(w:0.300)🚀)
Batch 425/1427: Loss=5.4882 (C:4.3937, R:0.0100, T:3.6450(w:0.300)🚀)
Batch 450/1427: Loss=5.4686 (C:4.3382, R:0.0099, T:3.7647(w:0.300)🚀)
Batch 475/1427: Loss=5.4753 (C:4.3598, R:0.0100, T:3.7150(w:0.300)🚀)
Batch 500/1427: Loss=5.4002 (C:4.3555, R:0.0099, T:3.4790(w:0.300)🚀)
Batch 525/1427: Loss=5.4372 (C:4.4026, R:0.0099, T:3.4455(w:0.300)🚀)
Batch 550/1427: Loss=5.4728 (C:4.3746, R:0.0100, T:3.6572(w:0.300)🚀)
Batch 575/1427: Loss=5.4095 (C:4.3737, R:0.0099, T:3.4494(w:0.300)🚀)
Batch 600/1427: Loss=5.4765 (C:4.4006, R:0.0100, T:3.5828(w:0.300)🚀)
Batch 625/1427: Loss=5.4551 (C:4.3818, R:0.0099, T:3.5745(w:0.300)🚀)
Batch 650/1427: Loss=5.4228 (C:4.4031, R:0.0100, T:3.3956(w:0.300)🚀)
Batch 675/1427: Loss=5.4359 (C:4.3828, R:0.0099, T:3.5070(w:0.300)🚀)
Batch 700/1427: Loss=5.4726 (C:4.3620, R:0.0100, T:3.6984(w:0.300)🚀)
Batch 725/1427: Loss=5.4541 (C:4.3512, R:0.0100, T:3.6731(w:0.300)🚀)
Batch 750/1427: Loss=5.4559 (C:4.3946, R:0.0100, T:3.5344(w:0.300)🚀)
Batch 775/1427: Loss=5.4600 (C:4.3308, R:0.0100, T:3.7609(w:0.300)🚀)
Batch 800/1427: Loss=5.4854 (C:4.3799, R:0.0100, T:3.6815(w:0.300)🚀)
Batch 825/1427: Loss=5.4150 (C:4.3618, R:0.0099, T:3.5071(w:0.300)🚀)
Batch 850/1427: Loss=5.4674 (C:4.3768, R:0.0100, T:3.6319(w:0.300)🚀)
Batch 875/1427: Loss=5.4412 (C:4.3146, R:0.0099, T:3.7521(w:0.300)🚀)
Batch 900/1427: Loss=5.4560 (C:4.3802, R:0.0100, T:3.5828(w:0.300)🚀)
Batch 925/1427: Loss=5.4332 (C:4.3459, R:0.0100, T:3.6211(w:0.300)🚀)
Batch 950/1427: Loss=5.4286 (C:4.3392, R:0.0099, T:3.6280(w:0.300)🚀)
Batch 975/1427: Loss=5.4892 (C:4.3954, R:0.0099, T:3.6427(w:0.300)🚀)
Batch 1000/1427: Loss=5.4689 (C:4.4032, R:0.0100, T:3.5490(w:0.300)🚀)
Batch 1025/1427: Loss=5.5120 (C:4.3804, R:0.0100, T:3.7687(w:0.300)🚀)
Batch 1050/1427: Loss=5.4717 (C:4.3542, R:0.0100, T:3.7216(w:0.300)🚀)
Batch 1075/1427: Loss=5.4490 (C:4.3529, R:0.0100, T:3.6503(w:0.300)🚀)
Batch 1100/1427: Loss=5.4819 (C:4.3636, R:0.0100, T:3.7244(w:0.300)🚀)
Batch 1125/1427: Loss=5.4936 (C:4.3803, R:0.0100, T:3.7077(w:0.300)🚀)
Batch 1150/1427: Loss=5.4642 (C:4.4034, R:0.0100, T:3.5328(w:0.300)🚀)
Batch 1175/1427: Loss=5.4828 (C:4.3648, R:0.0100, T:3.7232(w:0.300)🚀)
Batch 1200/1427: Loss=5.4580 (C:4.3752, R:0.0099, T:3.6060(w:0.300)🚀)
Batch 1225/1427: Loss=5.4773 (C:4.3644, R:0.0099, T:3.7065(w:0.300)🚀)
Batch 1250/1427: Loss=5.4712 (C:4.3719, R:0.0100, T:3.6610(w:0.300)🚀)
Batch 1275/1427: Loss=5.4622 (C:4.3814, R:0.0099, T:3.5992(w:0.300)🚀)
Batch 1300/1427: Loss=5.4529 (C:4.3809, R:0.0099, T:3.5698(w:0.300)🚀)
Batch 1325/1427: Loss=5.4452 (C:4.3659, R:0.0099, T:3.5944(w:0.300)🚀)
Batch 1350/1427: Loss=5.4908 (C:4.3173, R:0.0100, T:3.9085(w:0.300)🚀)
Batch 1375/1427: Loss=5.4374 (C:4.3742, R:0.0100, T:3.5407(w:0.300)🚀)
Batch 1400/1427: Loss=5.4902 (C:4.3792, R:0.0099, T:3.7001(w:0.300)🚀)
Batch 1425/1427: Loss=5.4636 (C:4.3262, R:0.0100, T:3.7880(w:0.300)🚀)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 5.4650
  Contrastive: 4.3719
  Reconstruction: 0.0100
  Topological: 3.6405 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.3158
  Contrastive: 1.9879
  Reconstruction: 0.0100
  Topological: 44.4228 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 14/50 COMPLETE (68.5s)
Train Loss: 5.4650 (C:4.3719, R:0.0100, T:3.6405)
Val Loss:   15.3158 (C:1.9879, R:0.0100, T:44.4228)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.5181 (C:4.3699, R:0.0101, T:3.8241(w:0.300)🚀)
Batch  25/1427: Loss=5.4975 (C:4.3146, R:0.0100, T:3.9398(w:0.300)🚀)
Batch  50/1427: Loss=5.4638 (C:4.3859, R:0.0100, T:3.5896(w:0.300)🚀)
Batch  75/1427: Loss=5.4760 (C:4.3838, R:0.0099, T:3.6371(w:0.300)🚀)
Batch 100/1427: Loss=5.5138 (C:4.3936, R:0.0100, T:3.7307(w:0.300)🚀)
Batch 125/1427: Loss=5.4853 (C:4.3620, R:0.0100, T:3.7412(w:0.300)🚀)
Batch 150/1427: Loss=5.4469 (C:4.3745, R:0.0100, T:3.5714(w:0.300)🚀)
Batch 175/1427: Loss=5.4838 (C:4.3610, R:0.0099, T:3.7394(w:0.300)🚀)
Batch 200/1427: Loss=5.4645 (C:4.3660, R:0.0100, T:3.6583(w:0.300)🚀)
Batch 225/1427: Loss=5.4745 (C:4.3151, R:0.0100, T:3.8614(w:0.300)🚀)
Batch 250/1427: Loss=5.4513 (C:4.4279, R:0.0100, T:3.4081(w:0.300)🚀)
Batch 275/1427: Loss=5.4847 (C:4.3642, R:0.0100, T:3.7318(w:0.300)🚀)
Batch 300/1427: Loss=5.5204 (C:4.3436, R:0.0100, T:3.9194(w:0.300)🚀)
Batch 325/1427: Loss=5.4879 (C:4.3865, R:0.0100, T:3.6678(w:0.300)🚀)
Batch 350/1427: Loss=5.4174 (C:4.3830, R:0.0099, T:3.4447(w:0.300)🚀)
Batch 375/1427: Loss=5.4615 (C:4.4023, R:0.0099, T:3.5273(w:0.300)🚀)
Batch 400/1427: Loss=5.4513 (C:4.3600, R:0.0100, T:3.6343(w:0.300)🚀)
Batch 425/1427: Loss=5.4730 (C:4.4122, R:0.0100, T:3.5327(w:0.300)🚀)
Batch 450/1427: Loss=5.4280 (C:4.3282, R:0.0099, T:3.6627(w:0.300)🚀)
Batch 475/1427: Loss=5.4605 (C:4.3727, R:0.0100, T:3.6228(w:0.300)🚀)
Batch 500/1427: Loss=5.4807 (C:4.3845, R:0.0100, T:3.6507(w:0.300)🚀)
Batch 525/1427: Loss=5.4700 (C:4.3735, R:0.0100, T:3.6516(w:0.300)🚀)
Batch 550/1427: Loss=5.4488 (C:4.4022, R:0.0099, T:3.4853(w:0.300)🚀)
Batch 575/1427: Loss=5.4747 (C:4.3942, R:0.0100, T:3.5984(w:0.300)🚀)
Batch 600/1427: Loss=5.4563 (C:4.3520, R:0.0100, T:3.6776(w:0.300)🚀)
Batch 625/1427: Loss=5.4566 (C:4.3741, R:0.0099, T:3.6049(w:0.300)🚀)
Batch 650/1427: Loss=5.4291 (C:4.3464, R:0.0099, T:3.6056(w:0.300)🚀)
Batch 675/1427: Loss=5.4393 (C:4.3745, R:0.0099, T:3.5461(w:0.300)🚀)
Batch 700/1427: Loss=5.4274 (C:4.3843, R:0.0099, T:3.4735(w:0.300)🚀)
Batch 725/1427: Loss=5.4337 (C:4.3512, R:0.0099, T:3.6049(w:0.300)🚀)
Batch 750/1427: Loss=5.4596 (C:4.3772, R:0.0100, T:3.6045(w:0.300)🚀)
Batch 775/1427: Loss=5.4491 (C:4.3403, R:0.0100, T:3.6926(w:0.300)🚀)
Batch 800/1427: Loss=5.4396 (C:4.3395, R:0.0100, T:3.6635(w:0.300)🚀)
Batch 825/1427: Loss=5.4514 (C:4.3732, R:0.0100, T:3.5905(w:0.300)🚀)
Batch 850/1427: Loss=5.4831 (C:4.3301, R:0.0099, T:3.8400(w:0.300)🚀)
Batch 875/1427: Loss=5.4819 (C:4.3721, R:0.0100, T:3.6963(w:0.300)🚀)
Batch 900/1427: Loss=5.4720 (C:4.3759, R:0.0100, T:3.6502(w:0.300)🚀)
Batch 925/1427: Loss=5.4667 (C:4.3880, R:0.0100, T:3.5923(w:0.300)🚀)
Batch 950/1427: Loss=5.4447 (C:4.3913, R:0.0100, T:3.5083(w:0.300)🚀)
Batch 975/1427: Loss=5.4523 (C:4.3573, R:0.0099, T:3.6468(w:0.300)🚀)
Batch 1000/1427: Loss=5.4838 (C:4.3467, R:0.0099, T:3.7871(w:0.300)🚀)
Batch 1025/1427: Loss=5.4701 (C:4.3854, R:0.0100, T:3.6122(w:0.300)🚀)
Batch 1050/1427: Loss=5.4477 (C:4.4019, R:0.0099, T:3.4827(w:0.300)🚀)
Batch 1075/1427: Loss=5.4808 (C:4.3469, R:0.0100, T:3.7764(w:0.300)🚀)
Batch 1100/1427: Loss=5.4682 (C:4.3457, R:0.0100, T:3.7383(w:0.300)🚀)
Batch 1125/1427: Loss=5.4805 (C:4.3713, R:0.0100, T:3.6937(w:0.300)🚀)
Batch 1150/1427: Loss=5.4427 (C:4.3896, R:0.0099, T:3.5072(w:0.300)🚀)
Batch 1175/1427: Loss=5.4551 (C:4.3816, R:0.0099, T:3.5750(w:0.300)🚀)
Batch 1200/1427: Loss=5.4554 (C:4.3802, R:0.0099, T:3.5807(w:0.300)🚀)
Batch 1225/1427: Loss=5.4724 (C:4.3950, R:0.0099, T:3.5881(w:0.300)🚀)
Batch 1250/1427: Loss=5.4499 (C:4.3519, R:0.0099, T:3.6567(w:0.300)🚀)
Batch 1275/1427: Loss=5.4368 (C:4.3824, R:0.0099, T:3.5114(w:0.300)🚀)
Batch 1300/1427: Loss=5.4758 (C:4.3515, R:0.0100, T:3.7443(w:0.300)🚀)
Batch 1325/1427: Loss=5.4743 (C:4.3882, R:0.0100, T:3.6168(w:0.300)🚀)
Batch 1350/1427: Loss=5.4549 (C:4.3515, R:0.0100, T:3.6744(w:0.300)🚀)
Batch 1375/1427: Loss=5.4520 (C:4.3974, R:0.0099, T:3.5120(w:0.300)🚀)
Batch 1400/1427: Loss=5.4669 (C:4.3505, R:0.0099, T:3.7182(w:0.300)🚀)
Batch 1425/1427: Loss=5.4446 (C:4.3923, R:0.0099, T:3.5044(w:0.300)🚀)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 5.4651
  Contrastive: 4.3721
  Reconstruction: 0.0100
  Topological: 3.6400 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.2545
  Contrastive: 1.9911
  Reconstruction: 0.0100
  Topological: 44.2077 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 15/50 COMPLETE (70.6s)
Train Loss: 5.4651 (C:4.3721, R:0.0100, T:3.6400)
Val Loss:   15.2545 (C:1.9911, R:0.0100, T:44.2077)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4657 (C:4.3398, R:0.0100, T:3.7495(w:0.300)🚀)
Batch  25/1427: Loss=5.4482 (C:4.3643, R:0.0100, T:3.6094(w:0.300)🚀)
Batch  50/1427: Loss=5.4699 (C:4.3997, R:0.0099, T:3.5639(w:0.300)🚀)
Batch  75/1427: Loss=5.4287 (C:4.3547, R:0.0099, T:3.5768(w:0.300)🚀)
Batch 100/1427: Loss=5.4934 (C:4.3327, R:0.0100, T:3.8659(w:0.300)🚀)
Batch 125/1427: Loss=5.4598 (C:4.3936, R:0.0099, T:3.5509(w:0.300)🚀)
Batch 150/1427: Loss=5.4752 (C:4.3979, R:0.0100, T:3.5879(w:0.300)🚀)
Batch 175/1427: Loss=5.4506 (C:4.3439, R:0.0100, T:3.6857(w:0.300)🚀)
Batch 200/1427: Loss=5.4437 (C:4.3984, R:0.0099, T:3.4812(w:0.300)🚀)
Batch 225/1427: Loss=5.4414 (C:4.3830, R:0.0099, T:3.5246(w:0.300)🚀)
Batch 250/1427: Loss=5.4600 (C:4.3509, R:0.0099, T:3.6936(w:0.300)🚀)
Batch 275/1427: Loss=5.4686 (C:4.3354, R:0.0099, T:3.7742(w:0.300)🚀)
Batch 300/1427: Loss=5.4639 (C:4.3759, R:0.0100, T:3.6234(w:0.300)🚀)
Batch 325/1427: Loss=5.4683 (C:4.3866, R:0.0100, T:3.6026(w:0.300)🚀)
Batch 350/1427: Loss=5.4679 (C:4.3383, R:0.0099, T:3.7619(w:0.300)🚀)
Batch 375/1427: Loss=5.5011 (C:4.3871, R:0.0100, T:3.7099(w:0.300)🚀)
Batch 400/1427: Loss=5.4811 (C:4.3227, R:0.0100, T:3.8580(w:0.300)🚀)
Batch 425/1427: Loss=5.4799 (C:4.3803, R:0.0100, T:3.6621(w:0.300)🚀)
Batch 450/1427: Loss=5.4761 (C:4.3733, R:0.0100, T:3.6728(w:0.300)🚀)
Batch 475/1427: Loss=5.4596 (C:4.3624, R:0.0099, T:3.6540(w:0.300)🚀)
Batch 500/1427: Loss=5.4443 (C:4.4133, R:0.0099, T:3.4335(w:0.300)🚀)
Batch 525/1427: Loss=5.4987 (C:4.3641, R:0.0100, T:3.7789(w:0.300)🚀)
Batch 550/1427: Loss=5.5143 (C:4.3583, R:0.0100, T:3.8499(w:0.300)🚀)
Batch 575/1427: Loss=5.4710 (C:4.3489, R:0.0100, T:3.7371(w:0.300)🚀)
Batch 600/1427: Loss=5.4890 (C:4.3977, R:0.0099, T:3.6344(w:0.300)🚀)
Batch 625/1427: Loss=5.4632 (C:4.3740, R:0.0099, T:3.6275(w:0.300)🚀)
Batch 650/1427: Loss=5.4958 (C:4.3598, R:0.0100, T:3.7831(w:0.300)🚀)
Batch 675/1427: Loss=5.4181 (C:4.3692, R:0.0099, T:3.4928(w:0.300)🚀)
Batch 700/1427: Loss=5.4598 (C:4.3636, R:0.0099, T:3.6509(w:0.300)🚀)
Batch 725/1427: Loss=5.4263 (C:4.3397, R:0.0099, T:3.6185(w:0.300)🚀)
Batch 750/1427: Loss=5.4745 (C:4.3541, R:0.0100, T:3.7313(w:0.300)🚀)
Batch 775/1427: Loss=5.4539 (C:4.3239, R:0.0099, T:3.7635(w:0.300)🚀)
Batch 800/1427: Loss=5.4653 (C:4.3367, R:0.0100, T:3.7586(w:0.300)🚀)
Batch 825/1427: Loss=5.4984 (C:4.3563, R:0.0100, T:3.8035(w:0.300)🚀)
Batch 850/1427: Loss=5.4278 (C:4.3712, R:0.0099, T:3.5188(w:0.300)🚀)
Batch 875/1427: Loss=5.4656 (C:4.3666, R:0.0100, T:3.6600(w:0.300)🚀)
Batch 900/1427: Loss=5.4789 (C:4.3698, R:0.0100, T:3.6937(w:0.300)🚀)
Batch 925/1427: Loss=5.4128 (C:4.3563, R:0.0099, T:3.5183(w:0.300)🚀)
Batch 950/1427: Loss=5.4554 (C:4.3714, R:0.0099, T:3.6100(w:0.300)🚀)
Batch 975/1427: Loss=5.5109 (C:4.3879, R:0.0100, T:3.7398(w:0.300)🚀)
Batch 1000/1427: Loss=5.4858 (C:4.3741, R:0.0100, T:3.7023(w:0.300)🚀)
Batch 1025/1427: Loss=5.4733 (C:4.3963, R:0.0100, T:3.5866(w:0.300)🚀)
Batch 1050/1427: Loss=5.4669 (C:4.3686, R:0.0099, T:3.6576(w:0.300)🚀)
Batch 1075/1427: Loss=5.4706 (C:4.3327, R:0.0099, T:3.7897(w:0.300)🚀)
Batch 1100/1427: Loss=5.4591 (C:4.4002, R:0.0100, T:3.5263(w:0.300)🚀)
Batch 1125/1427: Loss=5.4672 (C:4.3884, R:0.0099, T:3.5925(w:0.300)🚀)
Batch 1150/1427: Loss=5.4598 (C:4.3638, R:0.0100, T:3.6499(w:0.300)🚀)
Batch 1175/1427: Loss=5.4749 (C:4.3540, R:0.0100, T:3.7328(w:0.300)🚀)
Batch 1200/1427: Loss=5.4166 (C:4.3825, R:0.0099, T:3.4436(w:0.300)🚀)
Batch 1225/1427: Loss=5.4701 (C:4.3669, R:0.0100, T:3.6740(w:0.300)🚀)
Batch 1250/1427: Loss=5.4808 (C:4.3497, R:0.0100, T:3.7671(w:0.300)🚀)
Batch 1275/1427: Loss=5.4653 (C:4.3369, R:0.0100, T:3.7581(w:0.300)🚀)
Batch 1300/1427: Loss=5.4593 (C:4.3959, R:0.0100, T:3.5413(w:0.300)🚀)
Batch 1325/1427: Loss=5.4893 (C:4.3812, R:0.0099, T:3.6903(w:0.300)🚀)
Batch 1350/1427: Loss=5.4472 (C:4.3910, R:0.0100, T:3.5175(w:0.300)🚀)
Batch 1375/1427: Loss=5.4266 (C:4.3764, R:0.0100, T:3.4974(w:0.300)🚀)
Batch 1400/1427: Loss=5.4677 (C:4.3833, R:0.0100, T:3.6111(w:0.300)🚀)
Batch 1425/1427: Loss=5.4458 (C:4.3968, R:0.0099, T:3.4935(w:0.300)🚀)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 5.4650
  Contrastive: 4.3708
  Reconstruction: 0.0100
  Topological: 3.6439 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.0980
  Contrastive: 1.9854
  Reconstruction: 0.0100
  Topological: 43.7053 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 16/50 COMPLETE (68.3s)
Train Loss: 5.4650 (C:4.3708, R:0.0100, T:3.6439)
Val Loss:   15.0980 (C:1.9854, R:0.0100, T:43.7053)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4568 (C:4.3677, R:0.0099, T:3.6269(w:0.300)🚀)
Batch  25/1427: Loss=5.4734 (C:4.3739, R:0.0100, T:3.6615(w:0.300)🚀)
Batch  50/1427: Loss=5.4672 (C:4.3678, R:0.0100, T:3.6613(w:0.300)🚀)
Batch  75/1427: Loss=5.4134 (C:4.3513, R:0.0099, T:3.5371(w:0.300)🚀)
Batch 100/1427: Loss=5.4669 (C:4.3775, R:0.0099, T:3.6281(w:0.300)🚀)
Batch 125/1427: Loss=5.4080 (C:4.3689, R:0.0099, T:3.4603(w:0.300)🚀)
Batch 150/1427: Loss=5.4565 (C:4.3724, R:0.0100, T:3.6103(w:0.300)🚀)
Batch 175/1427: Loss=5.4330 (C:4.4041, R:0.0100, T:3.4263(w:0.300)🚀)
Batch 200/1427: Loss=5.4416 (C:4.3584, R:0.0100, T:3.6074(w:0.300)🚀)
Batch 225/1427: Loss=5.4746 (C:4.3326, R:0.0100, T:3.8033(w:0.300)🚀)
Batch 250/1427: Loss=5.4480 (C:4.3588, R:0.0100, T:3.6275(w:0.300)🚀)
Batch 275/1427: Loss=5.4823 (C:4.4310, R:0.0100, T:3.5010(w:0.300)🚀)
Batch 300/1427: Loss=5.4974 (C:4.4174, R:0.0100, T:3.5967(w:0.300)🚀)
Batch 325/1427: Loss=5.4755 (C:4.3741, R:0.0100, T:3.6679(w:0.300)🚀)
Batch 350/1427: Loss=5.4757 (C:4.4006, R:0.0100, T:3.5802(w:0.300)🚀)
Batch 375/1427: Loss=5.4087 (C:4.3360, R:0.0099, T:3.5723(w:0.300)🚀)
Batch 400/1427: Loss=5.4856 (C:4.3586, R:0.0100, T:3.7534(w:0.300)🚀)
Batch 425/1427: Loss=5.4575 (C:4.3781, R:0.0100, T:3.5948(w:0.300)🚀)
Batch 450/1427: Loss=5.4696 (C:4.4161, R:0.0100, T:3.5086(w:0.300)🚀)
Batch 475/1427: Loss=5.4519 (C:4.3528, R:0.0100, T:3.6602(w:0.300)🚀)
Batch 500/1427: Loss=5.4409 (C:4.3554, R:0.0099, T:3.6153(w:0.300)🚀)
Batch 525/1427: Loss=5.4677 (C:4.3964, R:0.0100, T:3.5677(w:0.300)🚀)
Batch 550/1427: Loss=5.4799 (C:4.3834, R:0.0099, T:3.6520(w:0.300)🚀)
Batch 575/1427: Loss=5.5132 (C:4.3841, R:0.0100, T:3.7603(w:0.300)🚀)
Batch 600/1427: Loss=5.4984 (C:4.3761, R:0.0100, T:3.7377(w:0.300)🚀)
Batch 625/1427: Loss=5.5217 (C:4.3787, R:0.0100, T:3.8068(w:0.300)🚀)
Batch 650/1427: Loss=5.4756 (C:4.3586, R:0.0099, T:3.7199(w:0.300)🚀)
Batch 675/1427: Loss=5.4801 (C:4.3715, R:0.0100, T:3.6919(w:0.300)🚀)
Batch 700/1427: Loss=5.4817 (C:4.3177, R:0.0100, T:3.8765(w:0.300)🚀)
Batch 725/1427: Loss=5.4751 (C:4.3783, R:0.0100, T:3.6527(w:0.300)🚀)
Batch 750/1427: Loss=5.4591 (C:4.3584, R:0.0100, T:3.6658(w:0.300)🚀)
Batch 775/1427: Loss=5.4054 (C:4.3820, R:0.0099, T:3.4079(w:0.300)🚀)
Batch 800/1427: Loss=5.4494 (C:4.3733, R:0.0100, T:3.5837(w:0.300)🚀)
Batch 825/1427: Loss=5.4751 (C:4.4018, R:0.0100, T:3.5741(w:0.300)🚀)
Batch 850/1427: Loss=5.4576 (C:4.3741, R:0.0099, T:3.6082(w:0.300)🚀)
Batch 875/1427: Loss=5.4610 (C:4.3808, R:0.0099, T:3.5974(w:0.300)🚀)
Batch 900/1427: Loss=5.4656 (C:4.3566, R:0.0100, T:3.6931(w:0.300)🚀)
Batch 925/1427: Loss=5.4207 (C:4.3743, R:0.0098, T:3.4847(w:0.300)🚀)
Batch 950/1427: Loss=5.4779 (C:4.4027, R:0.0099, T:3.5806(w:0.300)🚀)
Batch 975/1427: Loss=5.5003 (C:4.3836, R:0.0100, T:3.7190(w:0.300)🚀)
Batch 1000/1427: Loss=5.4624 (C:4.4155, R:0.0099, T:3.4864(w:0.300)🚀)
Batch 1025/1427: Loss=5.4262 (C:4.3963, R:0.0099, T:3.4296(w:0.300)🚀)
Batch 1050/1427: Loss=5.4910 (C:4.3637, R:0.0099, T:3.7543(w:0.300)🚀)
Batch 1075/1427: Loss=5.5031 (C:4.3508, R:0.0100, T:3.8376(w:0.300)🚀)
Batch 1100/1427: Loss=5.4409 (C:4.3511, R:0.0099, T:3.6293(w:0.300)🚀)
Batch 1125/1427: Loss=5.5059 (C:4.3747, R:0.0099, T:3.7672(w:0.300)🚀)
Batch 1150/1427: Loss=5.4903 (C:4.3880, R:0.0100, T:3.6711(w:0.300)🚀)
Batch 1175/1427: Loss=5.4453 (C:4.3595, R:0.0099, T:3.6161(w:0.300)🚀)
Batch 1200/1427: Loss=5.4674 (C:4.3773, R:0.0099, T:3.6301(w:0.300)🚀)
Batch 1225/1427: Loss=5.4363 (C:4.3505, R:0.0099, T:3.6160(w:0.300)🚀)
Batch 1250/1427: Loss=5.4390 (C:4.3699, R:0.0100, T:3.5605(w:0.300)🚀)
Batch 1275/1427: Loss=5.4582 (C:4.3018, R:0.0100, T:3.8511(w:0.300)🚀)
Batch 1300/1427: Loss=5.4476 (C:4.3480, R:0.0099, T:3.6622(w:0.300)🚀)
Batch 1325/1427: Loss=5.5140 (C:4.3653, R:0.0099, T:3.8254(w:0.300)🚀)
Batch 1350/1427: Loss=5.4562 (C:4.3538, R:0.0099, T:3.6716(w:0.300)🚀)
Batch 1375/1427: Loss=5.4769 (C:4.3189, R:0.0100, T:3.8568(w:0.300)🚀)
Batch 1400/1427: Loss=5.4715 (C:4.3308, R:0.0099, T:3.7988(w:0.300)🚀)
Batch 1425/1427: Loss=5.4459 (C:4.3399, R:0.0099, T:3.6832(w:0.300)🚀)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 5.4636
  Contrastive: 4.3706
  Reconstruction: 0.0100
  Topological: 3.6399 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.3580
  Contrastive: 1.9857
  Reconstruction: 0.0100
  Topological: 44.5709 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 17/50 COMPLETE (68.9s)
Train Loss: 5.4636 (C:4.3706, R:0.0100, T:3.6399)
Val Loss:   15.3580 (C:1.9857, R:0.0100, T:44.5709)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4796 (C:4.3425, R:0.0100, T:3.7872(w:0.300)🚀)
Batch  25/1427: Loss=5.4614 (C:4.3644, R:0.0100, T:3.6535(w:0.300)🚀)
Batch  50/1427: Loss=5.4892 (C:4.4053, R:0.0099, T:3.6095(w:0.300)🚀)
Batch  75/1427: Loss=5.4844 (C:4.3440, R:0.0100, T:3.7977(w:0.300)🚀)
Batch 100/1427: Loss=5.4832 (C:4.3639, R:0.0100, T:3.7276(w:0.300)🚀)
Batch 125/1427: Loss=5.4561 (C:4.3503, R:0.0100, T:3.6827(w:0.300)🚀)
Batch 150/1427: Loss=5.4728 (C:4.3132, R:0.0100, T:3.8622(w:0.300)🚀)
Batch 175/1427: Loss=5.4954 (C:4.3579, R:0.0100, T:3.7884(w:0.300)🚀)
Batch 200/1427: Loss=5.4700 (C:4.3671, R:0.0100, T:3.6731(w:0.300)🚀)
Batch 225/1427: Loss=5.3929 (C:4.3626, R:0.0099, T:3.4310(w:0.300)🚀)
Batch 250/1427: Loss=5.4634 (C:4.3586, R:0.0099, T:3.6792(w:0.300)🚀)
Batch 275/1427: Loss=5.4642 (C:4.4071, R:0.0099, T:3.5203(w:0.300)🚀)
Batch 300/1427: Loss=5.4602 (C:4.3480, R:0.0100, T:3.7039(w:0.300)🚀)
Batch 325/1427: Loss=5.4485 (C:4.3757, R:0.0099, T:3.5726(w:0.300)🚀)
Batch 350/1427: Loss=5.4861 (C:4.3936, R:0.0100, T:3.6382(w:0.300)🚀)
Batch 375/1427: Loss=5.4936 (C:4.3620, R:0.0100, T:3.7687(w:0.300)🚀)
Batch 400/1427: Loss=5.4477 (C:4.3918, R:0.0099, T:3.5163(w:0.300)🚀)
Batch 425/1427: Loss=5.4576 (C:4.3911, R:0.0100, T:3.5514(w:0.300)🚀)
Batch 450/1427: Loss=5.4666 (C:4.3324, R:0.0099, T:3.7775(w:0.300)🚀)
Batch 475/1427: Loss=5.4850 (C:4.3873, R:0.0100, T:3.6556(w:0.300)🚀)
Batch 500/1427: Loss=5.4742 (C:4.3643, R:0.0100, T:3.6962(w:0.300)🚀)
Batch 525/1427: Loss=5.4578 (C:4.3595, R:0.0100, T:3.6577(w:0.300)🚀)
Batch 550/1427: Loss=5.4519 (C:4.3740, R:0.0100, T:3.5899(w:0.300)🚀)
Batch 575/1427: Loss=5.5046 (C:4.3770, R:0.0100, T:3.7554(w:0.300)🚀)
Batch 600/1427: Loss=5.4491 (C:4.3731, R:0.0100, T:3.5835(w:0.300)🚀)
Batch 625/1427: Loss=5.4783 (C:4.3882, R:0.0100, T:3.6303(w:0.300)🚀)
Batch 650/1427: Loss=5.4704 (C:4.3346, R:0.0100, T:3.7827(w:0.300)🚀)
Batch 675/1427: Loss=5.4576 (C:4.3583, R:0.0100, T:3.6611(w:0.300)🚀)
Batch 700/1427: Loss=5.4474 (C:4.3384, R:0.0099, T:3.6932(w:0.300)🚀)
Batch 725/1427: Loss=5.4122 (C:4.4062, R:0.0100, T:3.3500(w:0.300)🚀)
Batch 750/1427: Loss=5.4099 (C:4.3678, R:0.0099, T:3.4706(w:0.300)🚀)
Batch 775/1427: Loss=5.4790 (C:4.3924, R:0.0100, T:3.6188(w:0.300)🚀)
Batch 800/1427: Loss=5.4310 (C:4.3650, R:0.0100, T:3.5498(w:0.300)🚀)
Batch 825/1427: Loss=5.4287 (C:4.3778, R:0.0099, T:3.4999(w:0.300)🚀)
Batch 850/1427: Loss=5.5103 (C:4.3986, R:0.0100, T:3.7024(w:0.300)🚀)
Batch 875/1427: Loss=5.4448 (C:4.3628, R:0.0100, T:3.6031(w:0.300)🚀)
Batch 900/1427: Loss=5.4361 (C:4.3639, R:0.0100, T:3.5706(w:0.300)🚀)
Batch 925/1427: Loss=5.4851 (C:4.3639, R:0.0100, T:3.7341(w:0.300)🚀)
Batch 950/1427: Loss=5.5012 (C:4.3960, R:0.0100, T:3.6807(w:0.300)🚀)
Batch 975/1427: Loss=5.4601 (C:4.3424, R:0.0099, T:3.7225(w:0.300)🚀)
Batch 1000/1427: Loss=5.5042 (C:4.3569, R:0.0100, T:3.8209(w:0.300)🚀)
Batch 1025/1427: Loss=5.4415 (C:4.3362, R:0.0099, T:3.6808(w:0.300)🚀)
Batch 1050/1427: Loss=5.4091 (C:4.3883, R:0.0099, T:3.3994(w:0.300)🚀)
Batch 1075/1427: Loss=5.4523 (C:4.3874, R:0.0100, T:3.5465(w:0.300)🚀)
Batch 1100/1427: Loss=5.4523 (C:4.3764, R:0.0099, T:3.5832(w:0.300)🚀)
Batch 1125/1427: Loss=5.4753 (C:4.3918, R:0.0100, T:3.6082(w:0.300)🚀)
Batch 1150/1427: Loss=5.4633 (C:4.4144, R:0.0100, T:3.4929(w:0.300)🚀)
Batch 1175/1427: Loss=5.4899 (C:4.4008, R:0.0100, T:3.6271(w:0.300)🚀)
Batch 1200/1427: Loss=5.4370 (C:4.3623, R:0.0100, T:3.5791(w:0.300)🚀)
Batch 1225/1427: Loss=5.4492 (C:4.3627, R:0.0100, T:3.6184(w:0.300)🚀)
Batch 1250/1427: Loss=5.4561 (C:4.4131, R:0.0099, T:3.4734(w:0.300)🚀)
Batch 1275/1427: Loss=5.4804 (C:4.3743, R:0.0100, T:3.6839(w:0.300)🚀)
Batch 1300/1427: Loss=5.4534 (C:4.3986, R:0.0099, T:3.5126(w:0.300)🚀)
Batch 1325/1427: Loss=5.4405 (C:4.3781, R:0.0099, T:3.5382(w:0.300)🚀)
Batch 1350/1427: Loss=5.4407 (C:4.3901, R:0.0100, T:3.4986(w:0.300)🚀)
Batch 1375/1427: Loss=5.4462 (C:4.3622, R:0.0099, T:3.6102(w:0.300)🚀)
Batch 1400/1427: Loss=5.5032 (C:4.3916, R:0.0100, T:3.7019(w:0.300)🚀)
Batch 1425/1427: Loss=5.4143 (C:4.3735, R:0.0099, T:3.4661(w:0.300)🚀)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 5.4644
  Contrastive: 4.3711
  Reconstruction: 0.0100
  Topological: 3.6410 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.5862
  Contrastive: 1.9927
  Reconstruction: 0.0100
  Topological: 45.3084 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 18/50 COMPLETE (70.6s)
Train Loss: 5.4644 (C:4.3711, R:0.0100, T:3.6410)
Val Loss:   15.5862 (C:1.9927, R:0.0100, T:45.3084)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4471 (C:4.3960, R:0.0100, T:3.5003(w:0.300)🚀)
Batch  25/1427: Loss=5.4530 (C:4.3654, R:0.0099, T:3.6222(w:0.300)🚀)
Batch  50/1427: Loss=5.4503 (C:4.3981, R:0.0100, T:3.5041(w:0.300)🚀)
Batch  75/1427: Loss=5.4828 (C:4.3692, R:0.0100, T:3.7085(w:0.300)🚀)
Batch 100/1427: Loss=5.4921 (C:4.3603, R:0.0100, T:3.7692(w:0.300)🚀)
Batch 125/1427: Loss=5.4438 (C:4.3715, R:0.0099, T:3.5709(w:0.300)🚀)
Batch 150/1427: Loss=5.5172 (C:4.3778, R:0.0101, T:3.7946(w:0.300)🚀)
Batch 175/1427: Loss=5.4908 (C:4.3714, R:0.0100, T:3.7278(w:0.300)🚀)
Batch 200/1427: Loss=5.4258 (C:4.3902, R:0.0099, T:3.4485(w:0.300)🚀)
Batch 225/1427: Loss=5.4890 (C:4.3861, R:0.0100, T:3.6731(w:0.300)🚀)
Batch 250/1427: Loss=5.4517 (C:4.3953, R:0.0099, T:3.5179(w:0.300)🚀)
Batch 275/1427: Loss=5.4478 (C:4.3753, R:0.0099, T:3.5717(w:0.300)🚀)
Batch 300/1427: Loss=5.4641 (C:4.3779, R:0.0100, T:3.6174(w:0.300)🚀)
Batch 325/1427: Loss=5.4645 (C:4.3466, R:0.0100, T:3.7227(w:0.300)🚀)
Batch 350/1427: Loss=5.4651 (C:4.3589, R:0.0099, T:3.6840(w:0.300)🚀)
Batch 375/1427: Loss=5.4399 (C:4.3297, R:0.0100, T:3.6971(w:0.300)🚀)
Batch 400/1427: Loss=5.4490 (C:4.3864, R:0.0100, T:3.5387(w:0.300)🚀)
Batch 425/1427: Loss=5.4727 (C:4.3855, R:0.0100, T:3.6210(w:0.300)🚀)
Batch 450/1427: Loss=5.4568 (C:4.3433, R:0.0100, T:3.7084(w:0.300)🚀)
Batch 475/1427: Loss=5.4824 (C:4.3450, R:0.0100, T:3.7881(w:0.300)🚀)
Batch 500/1427: Loss=5.4623 (C:4.3775, R:0.0099, T:3.6127(w:0.300)🚀)
Batch 525/1427: Loss=5.4621 (C:4.3650, R:0.0100, T:3.6540(w:0.300)🚀)
Batch 550/1427: Loss=5.4429 (C:4.3935, R:0.0099, T:3.4946(w:0.300)🚀)
Batch 575/1427: Loss=5.5220 (C:4.4015, R:0.0100, T:3.7315(w:0.300)🚀)
Batch 600/1427: Loss=5.4617 (C:4.3776, R:0.0100, T:3.6103(w:0.300)🚀)
Batch 625/1427: Loss=5.4708 (C:4.3380, R:0.0100, T:3.7726(w:0.300)🚀)
Batch 650/1427: Loss=5.4772 (C:4.3622, R:0.0100, T:3.7133(w:0.300)🚀)
Batch 675/1427: Loss=5.4415 (C:4.4191, R:0.0100, T:3.4046(w:0.300)🚀)
Batch 700/1427: Loss=5.4608 (C:4.3914, R:0.0100, T:3.5615(w:0.300)🚀)
Batch 725/1427: Loss=5.4678 (C:4.4250, R:0.0100, T:3.4729(w:0.300)🚀)
Batch 750/1427: Loss=5.4541 (C:4.3529, R:0.0099, T:3.6674(w:0.300)🚀)
Batch 775/1427: Loss=5.4400 (C:4.3600, R:0.0100, T:3.5965(w:0.300)🚀)
Batch 800/1427: Loss=5.4916 (C:4.3779, R:0.0099, T:3.7091(w:0.300)🚀)
Batch 825/1427: Loss=5.4780 (C:4.3691, R:0.0100, T:3.6930(w:0.300)🚀)
Batch 850/1427: Loss=5.4700 (C:4.3782, R:0.0100, T:3.6359(w:0.300)🚀)
Batch 875/1427: Loss=5.4937 (C:4.3310, R:0.0099, T:3.8725(w:0.300)🚀)
Batch 900/1427: Loss=5.4639 (C:4.3585, R:0.0100, T:3.6812(w:0.300)🚀)
Batch 925/1427: Loss=5.4754 (C:4.3575, R:0.0100, T:3.7230(w:0.300)🚀)
Batch 950/1427: Loss=5.4624 (C:4.3725, R:0.0099, T:3.6298(w:0.300)🚀)
Batch 975/1427: Loss=5.4614 (C:4.3844, R:0.0100, T:3.5868(w:0.300)🚀)
Batch 1000/1427: Loss=5.4751 (C:4.3586, R:0.0100, T:3.7183(w:0.300)🚀)
Batch 1025/1427: Loss=5.4384 (C:4.3609, R:0.0099, T:3.5883(w:0.300)🚀)
Batch 1050/1427: Loss=5.4533 (C:4.3820, R:0.0099, T:3.5675(w:0.300)🚀)
Batch 1075/1427: Loss=5.4782 (C:4.3539, R:0.0100, T:3.7441(w:0.300)🚀)
Batch 1100/1427: Loss=5.4506 (C:4.3714, R:0.0100, T:3.5938(w:0.300)🚀)
Batch 1125/1427: Loss=5.4443 (C:4.3962, R:0.0099, T:3.4903(w:0.300)🚀)
Batch 1150/1427: Loss=5.4696 (C:4.3482, R:0.0100, T:3.7347(w:0.300)🚀)
Batch 1175/1427: Loss=5.4128 (C:4.3544, R:0.0099, T:3.5247(w:0.300)🚀)
Batch 1200/1427: Loss=5.5264 (C:4.3737, R:0.0100, T:3.8389(w:0.300)🚀)
Batch 1225/1427: Loss=5.4752 (C:4.3530, R:0.0100, T:3.7372(w:0.300)🚀)
Batch 1250/1427: Loss=5.4640 (C:4.3715, R:0.0100, T:3.6382(w:0.300)🚀)
Batch 1275/1427: Loss=5.4395 (C:4.3653, R:0.0099, T:3.5774(w:0.300)🚀)
Batch 1300/1427: Loss=5.4874 (C:4.3730, R:0.0100, T:3.7114(w:0.300)🚀)
Batch 1325/1427: Loss=5.4425 (C:4.3510, R:0.0099, T:3.6350(w:0.300)🚀)
Batch 1350/1427: Loss=5.4569 (C:4.3869, R:0.0099, T:3.5635(w:0.300)🚀)
Batch 1375/1427: Loss=5.4924 (C:4.3991, R:0.0100, T:3.6410(w:0.300)🚀)
Batch 1400/1427: Loss=5.4842 (C:4.3392, R:0.0100, T:3.8131(w:0.300)🚀)
Batch 1425/1427: Loss=5.4424 (C:4.3636, R:0.0100, T:3.5925(w:0.300)🚀)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 5.4627
  Contrastive: 4.3711
  Reconstruction: 0.0100
  Topological: 3.6353 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.6999
  Contrastive: 1.9914
  Reconstruction: 0.0100
  Topological: 45.6916 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 19/50 COMPLETE (68.9s)
Train Loss: 5.4627 (C:4.3711, R:0.0100, T:3.6353)
Val Loss:   15.6999 (C:1.9914, R:0.0100, T:45.6916)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4772 (C:4.3745, R:0.0100, T:3.6723(w:0.300)🚀)
Batch  25/1427: Loss=5.4959 (C:4.3586, R:0.0100, T:3.7878(w:0.300)🚀)
Batch  50/1427: Loss=5.5055 (C:4.3850, R:0.0100, T:3.7316(w:0.300)🚀)
Batch  75/1427: Loss=5.4418 (C:4.3279, R:0.0099, T:3.7099(w:0.300)🚀)
Batch 100/1427: Loss=5.4815 (C:4.4028, R:0.0100, T:3.5923(w:0.300)🚀)
Batch 125/1427: Loss=5.4760 (C:4.3248, R:0.0100, T:3.8342(w:0.300)🚀)
Batch 150/1427: Loss=5.4571 (C:4.3432, R:0.0100, T:3.7099(w:0.300)🚀)
Batch 175/1427: Loss=5.4874 (C:4.3524, R:0.0100, T:3.7801(w:0.300)🚀)
Batch 200/1427: Loss=5.4900 (C:4.3783, R:0.0100, T:3.7026(w:0.300)🚀)
Batch 225/1427: Loss=5.4771 (C:4.4144, R:0.0100, T:3.5391(w:0.300)🚀)
Batch 250/1427: Loss=5.4439 (C:4.3578, R:0.0099, T:3.6169(w:0.300)🚀)
Batch 275/1427: Loss=5.4548 (C:4.3778, R:0.0099, T:3.5867(w:0.300)🚀)
Batch 300/1427: Loss=5.4482 (C:4.3439, R:0.0099, T:3.6775(w:0.300)🚀)
Batch 325/1427: Loss=5.4577 (C:4.3554, R:0.0100, T:3.6711(w:0.300)🚀)
Batch 350/1427: Loss=5.4430 (C:4.3655, R:0.0100, T:3.5881(w:0.300)🚀)
Batch 375/1427: Loss=5.4945 (C:4.3646, R:0.0100, T:3.7629(w:0.300)🚀)
Batch 400/1427: Loss=5.4667 (C:4.3876, R:0.0100, T:3.5935(w:0.300)🚀)
Batch 425/1427: Loss=5.4571 (C:4.3286, R:0.0099, T:3.7582(w:0.300)🚀)
Batch 450/1427: Loss=5.4427 (C:4.3965, R:0.0099, T:3.4838(w:0.300)🚀)
Batch 475/1427: Loss=5.5059 (C:4.3715, R:0.0100, T:3.7778(w:0.300)🚀)
Batch 500/1427: Loss=5.4346 (C:4.4099, R:0.0099, T:3.4124(w:0.300)🚀)
Batch 525/1427: Loss=5.4137 (C:4.3633, R:0.0099, T:3.4979(w:0.300)🚀)
Batch 550/1427: Loss=5.4326 (C:4.3439, R:0.0099, T:3.6255(w:0.300)🚀)
Batch 575/1427: Loss=5.4495 (C:4.3679, R:0.0100, T:3.6020(w:0.300)🚀)
Batch 600/1427: Loss=5.4529 (C:4.3911, R:0.0100, T:3.5360(w:0.300)🚀)
Batch 625/1427: Loss=5.4632 (C:4.3474, R:0.0100, T:3.7161(w:0.300)🚀)
Batch 650/1427: Loss=5.4955 (C:4.3711, R:0.0100, T:3.7447(w:0.300)🚀)
Batch 675/1427: Loss=5.5043 (C:4.3616, R:0.0100, T:3.8059(w:0.300)🚀)
Batch 700/1427: Loss=5.4413 (C:4.3608, R:0.0099, T:3.5986(w:0.300)🚀)
Batch 725/1427: Loss=5.4728 (C:4.3515, R:0.0100, T:3.7344(w:0.300)🚀)
Batch 750/1427: Loss=5.4716 (C:4.3588, R:0.0100, T:3.7059(w:0.300)🚀)
Batch 775/1427: Loss=5.4638 (C:4.3796, R:0.0099, T:3.6106(w:0.300)🚀)
Batch 800/1427: Loss=5.4795 (C:4.3999, R:0.0100, T:3.5956(w:0.300)🚀)
Batch 825/1427: Loss=5.4873 (C:4.3693, R:0.0100, T:3.7235(w:0.300)🚀)
Batch 850/1427: Loss=5.3930 (C:4.3745, R:0.0099, T:3.3917(w:0.300)🚀)
Batch 875/1427: Loss=5.4515 (C:4.4112, R:0.0099, T:3.4644(w:0.300)🚀)
Batch 900/1427: Loss=5.5068 (C:4.3541, R:0.0099, T:3.8390(w:0.300)🚀)
Batch 925/1427: Loss=5.4812 (C:4.3846, R:0.0099, T:3.6523(w:0.300)🚀)
Batch 950/1427: Loss=5.4753 (C:4.3980, R:0.0100, T:3.5876(w:0.300)🚀)
Batch 975/1427: Loss=5.4807 (C:4.3683, R:0.0100, T:3.7045(w:0.300)🚀)
Batch 1000/1427: Loss=5.4644 (C:4.3884, R:0.0100, T:3.5835(w:0.300)🚀)
Batch 1025/1427: Loss=5.4861 (C:4.3586, R:0.0100, T:3.7550(w:0.300)🚀)
Batch 1050/1427: Loss=5.5169 (C:4.3481, R:0.0100, T:3.8926(w:0.300)🚀)
Batch 1075/1427: Loss=5.4407 (C:4.3690, R:0.0099, T:3.5693(w:0.300)🚀)
Batch 1100/1427: Loss=5.4483 (C:4.3931, R:0.0100, T:3.5140(w:0.300)🚀)
Batch 1125/1427: Loss=5.4381 (C:4.3838, R:0.0099, T:3.5111(w:0.300)🚀)
Batch 1150/1427: Loss=5.4499 (C:4.3872, R:0.0099, T:3.5387(w:0.300)🚀)
Batch 1175/1427: Loss=5.4487 (C:4.3871, R:0.0099, T:3.5354(w:0.300)🚀)
Batch 1200/1427: Loss=5.4800 (C:4.4177, R:0.0099, T:3.5375(w:0.300)🚀)
Batch 1225/1427: Loss=5.4313 (C:4.3771, R:0.0098, T:3.5105(w:0.300)🚀)
Batch 1250/1427: Loss=5.4286 (C:4.3762, R:0.0099, T:3.5048(w:0.300)🚀)
Batch 1275/1427: Loss=5.4617 (C:4.3778, R:0.0099, T:3.6096(w:0.300)🚀)
Batch 1300/1427: Loss=5.4581 (C:4.3890, R:0.0099, T:3.5601(w:0.300)🚀)
Batch 1325/1427: Loss=5.4599 (C:4.3963, R:0.0099, T:3.5421(w:0.300)🚀)
Batch 1350/1427: Loss=5.4584 (C:4.3651, R:0.0099, T:3.6408(w:0.300)🚀)
Batch 1375/1427: Loss=5.4804 (C:4.3780, R:0.0100, T:3.6714(w:0.300)🚀)
Batch 1400/1427: Loss=5.4448 (C:4.3544, R:0.0099, T:3.6313(w:0.300)🚀)
Batch 1425/1427: Loss=5.4857 (C:4.3605, R:0.0099, T:3.7474(w:0.300)🚀)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 5.4639
  Contrastive: 4.3709
  Reconstruction: 0.0100
  Topological: 3.6398 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.7059
  Contrastive: 1.9910
  Reconstruction: 0.0100
  Topological: 45.7131 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 20/50 COMPLETE (70.9s)
Train Loss: 5.4639 (C:4.3709, R:0.0100, T:3.6398)
Val Loss:   15.7059 (C:1.9910, R:0.0100, T:45.7131)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4864 (C:4.3551, R:0.0100, T:3.7676(w:0.300)🚀)
Batch  25/1427: Loss=5.4403 (C:4.3781, R:0.0099, T:3.5376(w:0.300)🚀)
Batch  50/1427: Loss=5.4695 (C:4.3742, R:0.0100, T:3.6477(w:0.300)🚀)
Batch  75/1427: Loss=5.4308 (C:4.3497, R:0.0099, T:3.6002(w:0.300)🚀)
Batch 100/1427: Loss=5.4373 (C:4.3801, R:0.0098, T:3.5208(w:0.300)🚀)
Batch 125/1427: Loss=5.4826 (C:4.4065, R:0.0100, T:3.5838(w:0.300)🚀)
Batch 150/1427: Loss=5.4475 (C:4.3407, R:0.0099, T:3.6860(w:0.300)🚀)
Batch 175/1427: Loss=5.4940 (C:4.3647, R:0.0100, T:3.7609(w:0.300)🚀)
Batch 200/1427: Loss=5.3933 (C:4.3780, R:0.0099, T:3.3809(w:0.300)🚀)
Batch 225/1427: Loss=5.4930 (C:4.3779, R:0.0100, T:3.7136(w:0.300)🚀)
Batch 250/1427: Loss=5.4658 (C:4.3735, R:0.0099, T:3.6376(w:0.300)🚀)
Batch 275/1427: Loss=5.4653 (C:4.3920, R:0.0100, T:3.5743(w:0.300)🚀)
Batch 300/1427: Loss=5.4682 (C:4.4063, R:0.0099, T:3.5364(w:0.300)🚀)
Batch 325/1427: Loss=5.4539 (C:4.3524, R:0.0100, T:3.6685(w:0.300)🚀)
Batch 350/1427: Loss=5.4689 (C:4.3771, R:0.0100, T:3.6361(w:0.300)🚀)
Batch 375/1427: Loss=5.4904 (C:4.4050, R:0.0100, T:3.6147(w:0.300)🚀)
Batch 400/1427: Loss=5.4751 (C:4.3630, R:0.0100, T:3.7037(w:0.300)🚀)
Batch 425/1427: Loss=5.4827 (C:4.4032, R:0.0100, T:3.5949(w:0.300)🚀)
Batch 450/1427: Loss=5.4848 (C:4.3609, R:0.0100, T:3.7432(w:0.300)🚀)
Batch 475/1427: Loss=5.4327 (C:4.3362, R:0.0099, T:3.6517(w:0.300)🚀)
Batch 500/1427: Loss=5.4548 (C:4.3926, R:0.0100, T:3.5373(w:0.300)🚀)
Batch 525/1427: Loss=5.4671 (C:4.3937, R:0.0100, T:3.5747(w:0.300)🚀)
Batch 550/1427: Loss=5.5272 (C:4.3385, R:0.0100, T:3.9592(w:0.300)🚀)
Batch 575/1427: Loss=5.4970 (C:4.3660, R:0.0100, T:3.7664(w:0.300)🚀)
Batch 600/1427: Loss=5.4310 (C:4.3789, R:0.0100, T:3.5036(w:0.300)🚀)
Batch 625/1427: Loss=5.4659 (C:4.4116, R:0.0099, T:3.5110(w:0.300)🚀)
Batch 650/1427: Loss=5.4596 (C:4.4003, R:0.0099, T:3.5275(w:0.300)🚀)
Batch 675/1427: Loss=5.4662 (C:4.3765, R:0.0099, T:3.6289(w:0.300)🚀)
Batch 700/1427: Loss=5.4629 (C:4.4020, R:0.0100, T:3.5329(w:0.300)🚀)
Batch 725/1427: Loss=5.4491 (C:4.3739, R:0.0099, T:3.5805(w:0.300)🚀)
Batch 750/1427: Loss=5.4856 (C:4.3695, R:0.0099, T:3.7168(w:0.300)🚀)
Batch 775/1427: Loss=5.4582 (C:4.3680, R:0.0099, T:3.6307(w:0.300)🚀)
Batch 800/1427: Loss=5.4652 (C:4.4060, R:0.0100, T:3.5274(w:0.300)🚀)
Batch 825/1427: Loss=5.4565 (C:4.4026, R:0.0099, T:3.5095(w:0.300)🚀)
Batch 850/1427: Loss=5.4648 (C:4.3480, R:0.0100, T:3.7194(w:0.300)🚀)
Batch 875/1427: Loss=5.4578 (C:4.3644, R:0.0099, T:3.6412(w:0.300)🚀)
Batch 900/1427: Loss=5.4578 (C:4.3446, R:0.0100, T:3.7074(w:0.300)🚀)
Batch 925/1427: Loss=5.4532 (C:4.3738, R:0.0099, T:3.5947(w:0.300)🚀)
Batch 950/1427: Loss=5.4769 (C:4.3916, R:0.0099, T:3.6143(w:0.300)🚀)
Batch 975/1427: Loss=5.4824 (C:4.3873, R:0.0100, T:3.6467(w:0.300)🚀)
Batch 1000/1427: Loss=5.5060 (C:4.3546, R:0.0100, T:3.8348(w:0.300)🚀)
Batch 1025/1427: Loss=5.4245 (C:4.3757, R:0.0099, T:3.4929(w:0.300)🚀)
Batch 1050/1427: Loss=5.4706 (C:4.3927, R:0.0100, T:3.5894(w:0.300)🚀)
Batch 1075/1427: Loss=5.4543 (C:4.3773, R:0.0100, T:3.5867(w:0.300)🚀)
Batch 1100/1427: Loss=5.4906 (C:4.3496, R:0.0100, T:3.7999(w:0.300)🚀)
Batch 1125/1427: Loss=5.4708 (C:4.3647, R:0.0100, T:3.6836(w:0.300)🚀)
Batch 1150/1427: Loss=5.4731 (C:4.3544, R:0.0100, T:3.7257(w:0.300)🚀)
Batch 1175/1427: Loss=5.4844 (C:4.3544, R:0.0100, T:3.7634(w:0.300)🚀)
Batch 1200/1427: Loss=5.4841 (C:4.3438, R:0.0099, T:3.7978(w:0.300)🚀)
Batch 1225/1427: Loss=5.4598 (C:4.3491, R:0.0099, T:3.6989(w:0.300)🚀)
Batch 1250/1427: Loss=5.4509 (C:4.3656, R:0.0100, T:3.6142(w:0.300)🚀)
Batch 1275/1427: Loss=5.4511 (C:4.3531, R:0.0099, T:3.6566(w:0.300)🚀)
Batch 1300/1427: Loss=5.4268 (C:4.3705, R:0.0099, T:3.5176(w:0.300)🚀)
Batch 1325/1427: Loss=5.4468 (C:4.3965, R:0.0100, T:3.4979(w:0.300)🚀)
Batch 1350/1427: Loss=5.4558 (C:4.3556, R:0.0099, T:3.6641(w:0.300)🚀)
Batch 1375/1427: Loss=5.4407 (C:4.4073, R:0.0099, T:3.4415(w:0.300)🚀)
Batch 1400/1427: Loss=5.5151 (C:4.3694, R:0.0101, T:3.8157(w:0.300)🚀)
Batch 1425/1427: Loss=5.4526 (C:4.3540, R:0.0099, T:3.6589(w:0.300)🚀)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 5.4637
  Contrastive: 4.3710
  Reconstruction: 0.0100
  Topological: 3.6390 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.5738
  Contrastive: 1.9924
  Reconstruction: 0.0100
  Topological: 45.2680 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 21/50 COMPLETE (75.4s)
Train Loss: 5.4637 (C:4.3710, R:0.0100, T:3.6390)
Val Loss:   15.5738 (C:1.9924, R:0.0100, T:45.2680)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 1427 | Topological Weight: 0.3000
🧠 Full topological learning active
============================================================
Batch   0/1427: Loss=5.4560 (C:4.3434, R:0.0100, T:3.7053(w:0.300)🚀)
Batch  25/1427: Loss=5.4952 (C:4.3683, R:0.0100, T:3.7530(w:0.300)🚀)
Batch  50/1427: Loss=5.4569 (C:4.3654, R:0.0100, T:3.6352(w:0.300)🚀)
Batch  75/1427: Loss=5.4686 (C:4.3384, R:0.0100, T:3.7639(w:0.300)🚀)
Batch 100/1427: Loss=5.4090 (C:4.3474, R:0.0099, T:3.5352(w:0.300)🚀)
Batch 125/1427: Loss=5.4471 (C:4.4104, R:0.0099, T:3.4522(w:0.300)🚀)
Batch 150/1427: Loss=5.4512 (C:4.3759, R:0.0100, T:3.5812(w:0.300)🚀)
Batch 175/1427: Loss=5.5170 (C:4.3323, R:0.0100, T:3.9456(w:0.300)🚀)
Batch 200/1427: Loss=5.4462 (C:4.3538, R:0.0099, T:3.6382(w:0.300)🚀)
Batch 225/1427: Loss=5.4619 (C:4.3945, R:0.0100, T:3.5546(w:0.300)🚀)
Batch 250/1427: Loss=5.4814 (C:4.3493, R:0.0100, T:3.7705(w:0.300)🚀)
Batch 275/1427: Loss=5.4450 (C:4.3546, R:0.0099, T:3.6312(w:0.300)🚀)
Batch 300/1427: Loss=5.4598 (C:4.3564, R:0.0100, T:3.6746(w:0.300)🚀)
Batch 325/1427: Loss=5.4645 (C:4.3993, R:0.0099, T:3.5475(w:0.300)🚀)
Batch 350/1427: Loss=5.4707 (C:4.3894, R:0.0100, T:3.6012(w:0.300)🚀)
Batch 375/1427: Loss=5.4848 (C:4.3545, R:0.0100, T:3.7642(w:0.300)🚀)
Batch 400/1427: Loss=5.4766 (C:4.3557, R:0.0100, T:3.7332(w:0.300)🚀)
Batch 425/1427: Loss=5.4370 (C:4.4089, R:0.0099, T:3.4240(w:0.300)🚀)
Batch 450/1427: Loss=5.4581 (C:4.3736, R:0.0100, T:3.6116(w:0.300)🚀)
Batch 475/1427: Loss=5.4803 (C:4.3211, R:0.0100, T:3.8607(w:0.300)🚀)
Batch 500/1427: Loss=5.4726 (C:4.3610, R:0.0099, T:3.7019(w:0.300)🚀)
Batch 525/1427: Loss=5.4445 (C:4.3545, R:0.0099, T:3.6299(w:0.300)🚀)
Batch 550/1427: Loss=5.4411 (C:4.3605, R:0.0100, T:3.5987(w:0.300)🚀)
Batch 575/1427: Loss=5.4771 (C:4.3689, R:0.0099, T:3.6907(w:0.300)🚀)
Batch 600/1427: Loss=5.4635 (C:4.3900, R:0.0100, T:3.5749(w:0.300)🚀)
Batch 625/1427: Loss=5.4619 (C:4.3826, R:0.0100, T:3.5944(w:0.300)🚀)
Batch 650/1427: Loss=5.4754 (C:4.3262, R:0.0100, T:3.8272(w:0.300)🚀)
Batch 675/1427: Loss=5.4604 (C:4.3949, R:0.0099, T:3.5482(w:0.300)🚀)
Batch 700/1427: Loss=5.4504 (C:4.3702, R:0.0100, T:3.5972(w:0.300)🚀)
Batch 725/1427: Loss=5.4491 (C:4.3560, R:0.0099, T:3.6403(w:0.300)🚀)
Batch 750/1427: Loss=5.4617 (C:4.3804, R:0.0099, T:3.6010(w:0.300)🚀)
Batch 775/1427: Loss=5.4820 (C:4.3849, R:0.0099, T:3.6539(w:0.300)🚀)
Batch 800/1427: Loss=5.4712 (C:4.3756, R:0.0100, T:3.6490(w:0.300)🚀)
Batch 825/1427: Loss=5.4897 (C:4.4343, R:0.0100, T:3.5148(w:0.300)🚀)
Batch 850/1427: Loss=5.4525 (C:4.3784, R:0.0100, T:3.5770(w:0.300)🚀)
Batch 875/1427: Loss=5.4512 (C:4.3812, R:0.0099, T:3.5635(w:0.300)🚀)
Batch 900/1427: Loss=5.4642 (C:4.3488, R:0.0099, T:3.7147(w:0.300)🚀)
Batch 925/1427: Loss=5.4754 (C:4.4015, R:0.0099, T:3.5763(w:0.300)🚀)
Batch 950/1427: Loss=5.4600 (C:4.3685, R:0.0100, T:3.6352(w:0.300)🚀)
Batch 975/1427: Loss=5.4459 (C:4.3530, R:0.0100, T:3.6398(w:0.300)🚀)
Batch 1000/1427: Loss=5.4702 (C:4.3408, R:0.0100, T:3.7616(w:0.300)🚀)
Batch 1025/1427: Loss=5.4959 (C:4.3897, R:0.0100, T:3.6841(w:0.300)🚀)
Batch 1050/1427: Loss=5.4799 (C:4.3873, R:0.0100, T:3.6386(w:0.300)🚀)
Batch 1075/1427: Loss=5.4651 (C:4.3266, R:0.0100, T:3.7917(w:0.300)🚀)
Batch 1100/1427: Loss=5.4459 (C:4.3729, R:0.0099, T:3.5734(w:0.300)🚀)
Batch 1125/1427: Loss=5.4525 (C:4.3586, R:0.0099, T:3.6430(w:0.300)🚀)
Batch 1150/1427: Loss=5.4502 (C:4.3691, R:0.0100, T:3.6004(w:0.300)🚀)
Batch 1175/1427: Loss=5.4558 (C:4.3690, R:0.0099, T:3.6193(w:0.300)🚀)
Batch 1200/1427: Loss=5.4809 (C:4.3391, R:0.0099, T:3.8027(w:0.300)🚀)
Batch 1225/1427: Loss=5.4692 (C:4.3735, R:0.0100, T:3.6491(w:0.300)🚀)
Batch 1250/1427: Loss=5.4491 (C:4.3742, R:0.0100, T:3.5798(w:0.300)🚀)
Batch 1275/1427: Loss=5.4729 (C:4.3804, R:0.0099, T:3.6386(w:0.300)🚀)
Batch 1300/1427: Loss=5.4672 (C:4.3685, R:0.0099, T:3.6592(w:0.300)🚀)
Batch 1325/1427: Loss=5.4709 (C:4.3691, R:0.0100, T:3.6696(w:0.300)🚀)
Batch 1350/1427: Loss=5.4699 (C:4.3978, R:0.0099, T:3.5702(w:0.300)🚀)
Batch 1375/1427: Loss=5.4723 (C:4.3248, R:0.0100, T:3.8215(w:0.300)🚀)
Batch 1400/1427: Loss=5.4590 (C:4.3620, R:0.0099, T:3.6535(w:0.300)🚀)
Batch 1425/1427: Loss=5.4269 (C:4.3581, R:0.0099, T:3.5596(w:0.300)🚀)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 5.4641
  Contrastive: 4.3717
  Reconstruction: 0.0100
  Topological: 3.6382 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15.4440
  Contrastive: 1.9868
  Reconstruction: 0.0100
  Topological: 44.8537 (weight: 0.300)
  Batches with topology: 1427/1427 (100.0%)

🎯 EPOCH 22/50 COMPLETE (70.3s)
Train Loss: 5.4641 (C:4.3717, R:0.0100, T:3.6382)
Val Loss:   15.4440 (C:1.9868, R:0.0100, T:44.8537)
🚀 Good topological learning progress
------------------------------------------------------------

🛑 Early stopping triggered after 22 epochs
Best model was at epoch 12 with Val Loss: 14.9865

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 1
Epochs with topology: 22/22
Max consecutive topology epochs: 22
Best topological loss: 3.6310
Final topological loss: 3.6382
✅ SUCCESS: Topological learning achieved!
🚀 EXCELLENT: Very consistent topological learning (>80%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_141357/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/1431 batches
  Processed 51/1431 batches
  Processed 101/1431 batches
  Processed 151/1431 batches
  Processed 201/1431 batches
  Processed 251/1431 batches
  Processed 301/1431 batches
  Processed 351/1431 batches
  Processed 401/1431 batches
  Processed 451/1431 batches
  Processed 501/1431 batches
  Processed 551/1431 batches
  Processed 601/1431 batches
  Processed 651/1431 batches
  Processed 701/1431 batches
  Processed 751/1431 batches
  Processed 801/1431 batches
  Processed 851/1431 batches
  Processed 901/1431 batches
  Processed 951/1431 batches
  Processed 1001/1431 batches
  Processed 1051/1431 batches
  Processed 1101/1431 batches
  Processed 1151/1431 batches
  Processed 1201/1431 batches
  Processed 1251/1431 batches
  Processed 1301/1431 batches
  Processed 1351/1431 batches
  Processed 1401/1431 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: -0.0446
  Adjusted Rand Score: 0.0164
  Clustering Accuracy: 0.3987
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/1427 batches
  Processed 51/1427 batches
  Processed 101/1427 batches
  Processed 151/1427 batches
  Processed 201/1427 batches
  Processed 251/1427 batches
  Processed 301/1427 batches
  Processed 351/1427 batches
  Processed 401/1427 batches
  Processed 451/1427 batches
  Processed 501/1427 batches
  Processed 551/1427 batches
  Processed 601/1427 batches
  Processed 651/1427 batches
  Processed 701/1427 batches
  Processed 751/1427 batches
  Processed 801/1427 batches
  Processed 851/1427 batches
  Processed 901/1427 batches
  Processed 951/1427 batches
  Processed 1001/1427 batches
  Processed 1051/1427 batches
  Processed 1101/1427 batches
  Processed 1151/1427 batches
  Processed 1201/1427 batches
  Processed 1251/1427 batches
  Processed 1301/1427 batches
  Processed 1351/1427 batches
  Processed 1401/1427 batches
Extracted representations: torch.Size([547968, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/1427 batches
  Processed 51/1427 batches
  Processed 101/1427 batches
  Processed 151/1427 batches
  Processed 201/1427 batches
  Processed 251/1427 batches
  Processed 301/1427 batches
  Processed 351/1427 batches
  Processed 401/1427 batches
  Processed 451/1427 batches
  Processed 501/1427 batches
  Processed 551/1427 batches
  Processed 601/1427 batches
  Processed 651/1427 batches
  Processed 701/1427 batches
  Processed 751/1427 batches
  Processed 801/1427 batches
  Processed 851/1427 batches
  Processed 901/1427 batches
  Processed 951/1427 batches
  Processed 1001/1427 batches
  Processed 1051/1427 batches
  Processed 1101/1427 batches
  Processed 1151/1427 batches
  Processed 1201/1427 batches
  Processed 1251/1427 batches
  Processed 1301/1427 batches
  Processed 1351/1427 batches
  Processed 1401/1427 batches
Extracted representations: torch.Size([547968, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.4555
  Per-class F1: [0.5189551239115874, 0.38763449007425366, 0.45114555256064687]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009955
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 0.384 ± 0.212
  Negative distances: 0.395 ± 0.215
  Separation ratio: 1.03x
  Gap: -2.199
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: -0.0446
  Clustering Accuracy: 0.3987
  Adjusted Rand Score: 0.0164

Classification Performance:
  Accuracy: 0.4555

Separation Quality:
  Separation Ratio: 1.03x
  Gap: -2.199
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009955
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_141357/results/evaluation_results_20250721_144131.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_141357/results/evaluation_results_20250721_144131.json

Key Results:
  Separation ratio: 1.03x
  Perfect separation: False
  Classification accuracy: 0.4555

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 22
  Epochs with topological learning: 22
  Current topological loss: 3.6382
  Current topological weight: 0.3000
  ✅ Topological loss is decreasing (good progress)
🚀 EXCELLENT: Consistent topological learning achieved!
Final topological loss: 3.6382
Epochs with topology: 22/22
⚠️  Poor clustering accuracy: 0.399

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_141357/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_141357

Analysis completed with exit code: 0
Time: Mon 21 Jul 14:41:33 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
