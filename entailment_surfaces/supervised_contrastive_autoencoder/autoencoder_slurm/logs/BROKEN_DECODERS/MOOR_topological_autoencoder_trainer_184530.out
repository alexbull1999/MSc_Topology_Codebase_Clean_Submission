Starting Surface Distance Metric Analysis job...
Job ID: 184530
Node: gpuvm14
Time: Mon 21 Jul 15:46:56 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Mon Jul 21 15:46:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   37C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_154711
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_154711/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 537
  Test batches: 539
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 549367 samples, 537 batches
  Test: 549367 samples, 539 batches
AttentionAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Attention Heads: 5
  Total parameters: 5,881,841
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
MoorTopologicalLoss Initialized: Using 0-dimensional persistence pairings (MST edges).
No prototypes being used for topological loss - whole dataset instead.
TopologicalTrainer initialized on device: cuda
Model parameters: 5,881,841
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.01
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=2.8121 (C:2.8110, R:0.0111, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.9333 (C:1.9323, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.7827 (C:1.7817, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.6152 (C:1.6142, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.5676 (C:1.5666, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.5123 (C:1.5113, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.5616 (C:1.5606, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.5804 (C:1.5794, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.5149 (C:1.5139, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.6129 (C:1.6120, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.5713 (C:1.5703, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.5135 (C:1.5125, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.5640 (C:1.5630, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4840 (C:1.4830, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4302 (C:1.4292, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4456 (C:1.4446, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.5413 (C:1.5403, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4858 (C:1.4848, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.5217 (C:1.5207, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4982 (C:1.4972, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4928 (C:1.4918, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4608 (C:1.4598, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5611
  Contrastive: 1.5601
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4481
  Contrastive: 1.4471
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (22.1s)
Train Loss: 1.5611 (C:1.5601, R:0.0100, T:0.0000)
Val Loss:   1.4481 (C:1.4471, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.5067 (C:1.5057, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4269 (C:1.4259, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4033 (C:1.4023, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4679 (C:1.4669, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.5189 (C:1.5179, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4403 (C:1.4393, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4549 (C:1.4539, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.4844 (C:1.4834, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.5123 (C:1.5113, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4208 (C:1.4198, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.5120 (C:1.5110, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3430 (C:1.3420, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3982 (C:1.3972, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3856 (C:1.3846, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3951 (C:1.3942, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4060 (C:1.4050, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4452 (C:1.4442, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.5623 (C:1.5613, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4787 (C:1.4777, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.5508 (C:1.5498, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3963 (C:1.3953, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4560 (C:1.4550, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.4516
  Contrastive: 1.4506
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4332
  Contrastive: 1.4322
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (21.8s)
Train Loss: 1.4516 (C:1.4506, R:0.0100, T:0.0000)
Val Loss:   1.4332 (C:1.4322, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.4389 (C:1.4379, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4669 (C:1.4659, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4375 (C:1.4365, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.5193 (C:1.5183, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4076 (C:1.4066, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4576 (C:1.4566, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4224 (C:1.4214, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.4341 (C:1.4331, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4813 (C:1.4803, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.3681 (C:1.3671, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4921 (C:1.4911, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3423 (C:1.3414, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4791 (C:1.4781, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.5108 (C:1.5098, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4350 (C:1.4340, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4154 (C:1.4144, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4187 (C:1.4177, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4512 (C:1.4502, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4141 (C:1.4131, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4411 (C:1.4401, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.5556 (C:1.5546, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4509 (C:1.4499, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.4473
  Contrastive: 1.4463
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4325
  Contrastive: 1.4315
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (21.6s)
Train Loss: 1.4473 (C:1.4463, R:0.0100, T:0.0000)
Val Loss:   1.4325 (C:1.4315, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.4624 (C:1.4614, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.5477 (C:1.5467, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4647 (C:1.4637, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4451 (C:1.4441, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4539 (C:1.4529, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4715 (C:1.4705, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4213 (C:1.4203, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3500 (C:1.3490, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.3739 (C:1.3729, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4177 (C:1.4167, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4031 (C:1.4021, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4507 (C:1.4497, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3988 (C:1.3978, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4233 (C:1.4223, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4415 (C:1.4405, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.5289 (C:1.5279, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4124 (C:1.4114, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4995 (C:1.4985, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4928 (C:1.4918, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4229 (C:1.4220, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4349 (C:1.4339, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3277 (C:1.3267, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.4301
  Contrastive: 1.4291
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4177
  Contrastive: 1.4167
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (21.6s)
Train Loss: 1.4301 (C:1.4291, R:0.0100, T:0.0000)
Val Loss:   1.4177 (C:1.4167, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.4968 (C:1.4959, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.3784 (C:1.3774, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3431 (C:1.3421, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3670 (C:1.3660, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4379 (C:1.4369, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4580 (C:1.4570, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4823 (C:1.4813, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.4133 (C:1.4123, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4609 (C:1.4599, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4469 (C:1.4459, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4059 (C:1.4049, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4166 (C:1.4156, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.5082 (C:1.5072, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3817 (C:1.3807, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3860 (C:1.3850, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.3709 (C:1.3699, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4081 (C:1.4071, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3835 (C:1.3825, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4644 (C:1.4634, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3797 (C:1.3787, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3473 (C:1.3463, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4249 (C:1.4239, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.4225
  Contrastive: 1.4215
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3983
  Contrastive: 1.3973
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (21.0s)
Train Loss: 1.4225 (C:1.4215, R:0.0100, T:0.0000)
Val Loss:   1.3983 (C:1.3973, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3812 (C:1.3802, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.5004 (C:1.4994, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4762 (C:1.4752, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4354 (C:1.4344, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4190 (C:1.4180, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4911 (C:1.4901, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4062 (C:1.4052, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.4278 (C:1.4268, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4288 (C:1.4279, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4624 (C:1.4614, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4340 (C:1.4330, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4754 (C:1.4744, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3991 (C:1.3981, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4559 (C:1.4549, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3773 (C:1.3763, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4069 (C:1.4059, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4260 (C:1.4250, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3956 (C:1.3946, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4259 (C:1.4249, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4063 (C:1.4053, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3913 (C:1.3904, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4155 (C:1.4145, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.4228
  Contrastive: 1.4218
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3953
  Contrastive: 1.3943
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (20.6s)
Train Loss: 1.4228 (C:1.4218, R:0.0100, T:0.0000)
Val Loss:   1.3953 (C:1.3943, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.2855 (C:1.2845, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4449 (C:1.4439, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3846 (C:1.3836, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3863 (C:1.3853, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4380 (C:1.4370, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4353 (C:1.4343, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3401 (C:1.3391, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.4321 (C:1.4311, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4038 (C:1.4028, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.3626 (C:1.3616, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3412 (C:1.3402, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4444 (C:1.4434, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3977 (C:1.3967, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3427 (C:1.3417, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3710 (C:1.3701, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.3182 (C:1.3173, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3587 (C:1.3577, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.2680 (C:1.2670, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.3703 (C:1.3693, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3755 (C:1.3745, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3679 (C:1.3669, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3702 (C:1.3692, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.3807
  Contrastive: 1.3797
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3437
  Contrastive: 1.3427
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 7/50 COMPLETE (21.5s)
Train Loss: 1.3807 (C:1.3797, R:0.0100, T:0.0000)
Val Loss:   1.3437 (C:1.3427, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3639 (C:1.3629, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.3603 (C:1.3593, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3358 (C:1.3348, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3193 (C:1.3183, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.3054 (C:1.3044, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4538 (C:1.4528, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3553 (C:1.3543, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3922 (C:1.3912, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4181 (C:1.4171, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.3630 (C:1.3620, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4222 (C:1.4212, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4310 (C:1.4300, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3689 (C:1.3679, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4243 (C:1.4233, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4181 (C:1.4171, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4384 (C:1.4374, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3070 (C:1.3060, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3891 (C:1.3881, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4132 (C:1.4122, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3673 (C:1.3664, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3904 (C:1.3894, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4123 (C:1.4113, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.3919
  Contrastive: 1.3909
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3779
  Contrastive: 1.3769
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

🎯 EPOCH 8/50 COMPLETE (20.7s)
Train Loss: 1.3919 (C:1.3909, R:0.0100, T:0.0000)
Val Loss:   1.3779 (C:1.3769, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 537 | Topological Weight: 0.0100
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.5889 (C:1.4051, R:0.0100, T:18.2816(w:0.010)⚠️)
Batch  25/537: Loss=1.6312 (C:1.4221, R:0.0100, T:20.8133(w:0.010)⚠️)
Batch  50/537: Loss=1.6128 (C:1.4242, R:0.0099, T:18.7580(w:0.010)⚠️)
Batch  75/537: Loss=1.5379 (C:1.3565, R:0.0100, T:18.0396(w:0.010)⚠️)
Batch 100/537: Loss=1.6350 (C:1.4473, R:0.0099, T:18.6628(w:0.010)⚠️)
Batch 125/537: Loss=1.6055 (C:1.4176, R:0.0100, T:18.6970(w:0.010)⚠️)
Batch 150/537: Loss=1.6071 (C:1.4209, R:0.0100, T:18.5185(w:0.010)⚠️)
Batch 175/537: Loss=1.6343 (C:1.4628, R:0.0100, T:17.0554(w:0.010)⚠️)
Batch 200/537: Loss=1.6263 (C:1.4393, R:0.0100, T:18.6046(w:0.010)⚠️)
Batch 225/537: Loss=1.6229 (C:1.4152, R:0.0099, T:20.6704(w:0.010)⚠️)
Batch 250/537: Loss=1.6469 (C:1.4758, R:0.0100, T:17.0121(w:0.010)⚠️)
Batch 275/537: Loss=1.6375 (C:1.4598, R:0.0099, T:17.6632(w:0.010)⚠️)
Batch 300/537: Loss=1.5844 (C:1.3984, R:0.0099, T:18.4989(w:0.010)⚠️)
Batch 325/537: Loss=1.6331 (C:1.4447, R:0.0100, T:18.7410(w:0.010)⚠️)
Batch 350/537: Loss=1.5098 (C:1.3219, R:0.0100, T:18.6922(w:0.010)⚠️)
Batch 375/537: Loss=1.5991 (C:1.4186, R:0.0100, T:17.9546(w:0.010)⚠️)
Batch 400/537: Loss=1.5803 (C:1.3828, R:0.0099, T:19.6494(w:0.010)⚠️)
Batch 425/537: Loss=1.5633 (C:1.3832, R:0.0099, T:17.9095(w:0.010)⚠️)
Batch 450/537: Loss=1.5158 (C:1.3365, R:0.0099, T:17.8315(w:0.010)⚠️)
Batch 475/537: Loss=1.5764 (C:1.3976, R:0.0100, T:17.7764(w:0.010)⚠️)
Batch 500/537: Loss=1.5468 (C:1.3795, R:0.0100, T:16.6245(w:0.010)⚠️)
Batch 525/537: Loss=1.6304 (C:1.4501, R:0.0099, T:17.9241(w:0.010)⚠️)
🎉 MILESTONE: First topological learning detected at epoch 9!
   Initial topological loss: 18.5164
📈 New best topological loss: 18.5164

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.5809
  Contrastive: 1.3947
  Reconstruction: 0.0100
  Topological: 18.5164 (weight: 0.010)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5233
  Contrastive: 1.3552
  Reconstruction: 0.0100
  Topological: 16.7044 (weight: 0.010)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 9/50 COMPLETE (72.5s)
Train Loss: 1.5809 (C:1.3947, R:0.0100, T:18.5164)
Val Loss:   1.5233 (C:1.3552, R:0.0100, T:16.7044)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 537 | Topological Weight: 0.0112
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6268 (C:1.4055, R:0.0100, T:19.5848(w:0.011)⚠️)
Batch  25/537: Loss=1.5715 (C:1.3647, R:0.0100, T:18.2873(w:0.011)⚠️)
Batch  50/537: Loss=1.5923 (C:1.3953, R:0.0100, T:17.4240(w:0.011)⚠️)
Batch  75/537: Loss=1.5343 (C:1.3328, R:0.0099, T:17.8198(w:0.011)⚠️)
Batch 100/537: Loss=1.4635 (C:1.2615, R:0.0100, T:17.8692(w:0.011)⚠️)
Batch 125/537: Loss=1.5770 (C:1.3842, R:0.0099, T:17.0476(w:0.011)⚠️)
Batch 150/537: Loss=1.6131 (C:1.4133, R:0.0099, T:17.6691(w:0.011)⚠️)
Batch 175/537: Loss=1.5483 (C:1.3377, R:0.0100, T:18.6300(w:0.011)⚠️)
Batch 200/537: Loss=1.6116 (C:1.4263, R:0.0100, T:16.3762(w:0.011)⚠️)
Batch 225/537: Loss=1.5400 (C:1.3442, R:0.0100, T:17.3128(w:0.011)⚠️)
Batch 250/537: Loss=1.5291 (C:1.3386, R:0.0100, T:16.8449(w:0.011)⚠️)
Batch 275/537: Loss=1.5690 (C:1.3851, R:0.0099, T:16.2568(w:0.011)⚠️)
Batch 300/537: Loss=1.5261 (C:1.3345, R:0.0100, T:16.9476(w:0.011)⚠️)
Batch 325/537: Loss=1.5750 (C:1.3848, R:0.0100, T:16.8146(w:0.011)⚠️)
Batch 350/537: Loss=1.6234 (C:1.4260, R:0.0100, T:17.4595(w:0.011)⚠️)
Batch 375/537: Loss=1.6492 (C:1.4493, R:0.0099, T:17.6781(w:0.011)⚠️)
Batch 400/537: Loss=1.6149 (C:1.4065, R:0.0100, T:18.4339(w:0.011)⚠️)
Batch 425/537: Loss=1.6370 (C:1.4395, R:0.0100, T:17.4674(w:0.011)⚠️)
Batch 450/537: Loss=1.6091 (C:1.4000, R:0.0099, T:18.4954(w:0.011)⚠️)
Batch 475/537: Loss=1.5653 (C:1.3597, R:0.0100, T:18.1856(w:0.011)⚠️)
Batch 500/537: Loss=1.5129 (C:1.2924, R:0.0100, T:19.5045(w:0.011)⚠️)
Batch 525/537: Loss=1.6114 (C:1.4013, R:0.0100, T:18.5794(w:0.011)⚠️)
📈 New best topological loss: 17.9376

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.5713
  Contrastive: 1.3685
  Reconstruction: 0.0100
  Topological: 17.9376 (weight: 0.011)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5126
  Contrastive: 1.3249
  Reconstruction: 0.0100
  Topological: 16.5918 (weight: 0.011)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 10/50 COMPLETE (74.0s)
Train Loss: 1.5713 (C:1.3685, R:0.0100, T:17.9376)
Val Loss:   1.5126 (C:1.3249, R:0.0100, T:16.5918)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 537 | Topological Weight: 0.0125
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.5744 (C:1.3173, R:0.0100, T:20.4860(w:0.013)⚠️)
Batch  25/537: Loss=1.6622 (C:1.4390, R:0.0099, T:17.7782(w:0.013)⚠️)
Batch  50/537: Loss=1.5857 (C:1.3715, R:0.0099, T:17.0529(w:0.013)⚠️)
Batch  75/537: Loss=1.4965 (C:1.2876, R:0.0100, T:16.6282(w:0.013)⚠️)
Batch 100/537: Loss=1.6170 (C:1.4049, R:0.0099, T:16.8860(w:0.013)⚠️)
Batch 125/537: Loss=1.5871 (C:1.3681, R:0.0099, T:17.4359(w:0.013)⚠️)
Batch 150/537: Loss=1.6889 (C:1.4254, R:0.0099, T:20.9969(w:0.013)⚠️)
Batch 175/537: Loss=1.6109 (C:1.3923, R:0.0099, T:17.4118(w:0.013)⚠️)
Batch 200/537: Loss=1.5850 (C:1.3707, R:0.0099, T:17.0634(w:0.013)⚠️)
Batch 225/537: Loss=1.6957 (C:1.4783, R:0.0100, T:17.3121(w:0.013)⚠️)
Batch 250/537: Loss=1.5917 (C:1.3423, R:0.0099, T:19.8662(w:0.013)⚠️)
Batch 275/537: Loss=1.6325 (C:1.3798, R:0.0100, T:20.1310(w:0.013)⚠️)
Batch 300/537: Loss=1.6074 (C:1.3399, R:0.0100, T:21.3162(w:0.013)⚠️)
Batch 325/537: Loss=1.6217 (C:1.3953, R:0.0099, T:18.0312(w:0.013)⚠️)
Batch 350/537: Loss=1.6314 (C:1.3689, R:0.0100, T:20.9220(w:0.013)⚠️)
Batch 375/537: Loss=1.6791 (C:1.4606, R:0.0099, T:17.3976(w:0.013)⚠️)
Batch 400/537: Loss=1.6114 (C:1.3636, R:0.0100, T:19.7479(w:0.013)⚠️)
Batch 425/537: Loss=1.6246 (C:1.3831, R:0.0100, T:19.2468(w:0.013)⚠️)
Batch 450/537: Loss=1.6675 (C:1.4447, R:0.0099, T:17.7392(w:0.013)⚠️)
Batch 475/537: Loss=1.5211 (C:1.3093, R:0.0100, T:16.8644(w:0.013)⚠️)
Batch 500/537: Loss=1.4988 (C:1.2914, R:0.0100, T:16.5122(w:0.013)⚠️)
Batch 525/537: Loss=1.6097 (C:1.3905, R:0.0099, T:17.4543(w:0.013)⚠️)

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 1.6025
  Contrastive: 1.3733
  Reconstruction: 0.0100
  Topological: 18.2567 (weight: 0.013)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5522
  Contrastive: 1.3415
  Reconstruction: 0.0100
  Topological: 16.7729 (weight: 0.013)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 11/50 COMPLETE (66.3s)
Train Loss: 1.6025 (C:1.3733, R:0.0100, T:18.2567)
Val Loss:   1.5522 (C:1.3415, R:0.0100, T:16.7729)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 537 | Topological Weight: 0.0138
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6353 (C:1.3892, R:0.0100, T:17.8274(w:0.014)⚠️)
Batch  25/537: Loss=1.6364 (C:1.3759, R:0.0099, T:18.8762(w:0.014)⚠️)
Batch  50/537: Loss=1.5831 (C:1.3243, R:0.0100, T:18.7463(w:0.014)⚠️)
Batch  75/537: Loss=1.6516 (C:1.3855, R:0.0099, T:19.2810(w:0.014)⚠️)
Batch 100/537: Loss=1.5953 (C:1.3274, R:0.0100, T:19.4088(w:0.014)⚠️)
Batch 125/537: Loss=1.5637 (C:1.3318, R:0.0099, T:16.7925(w:0.014)⚠️)
Batch 150/537: Loss=1.5995 (C:1.3633, R:0.0099, T:17.1100(w:0.014)⚠️)
Batch 175/537: Loss=1.6346 (C:1.3659, R:0.0099, T:19.4696(w:0.014)⚠️)
Batch 200/537: Loss=1.7397 (C:1.4619, R:0.0099, T:20.1253(w:0.014)⚠️)
Batch 225/537: Loss=1.6144 (C:1.3720, R:0.0100, T:17.5557(w:0.014)⚠️)
Batch 250/537: Loss=1.6639 (C:1.4096, R:0.0100, T:18.4279(w:0.014)⚠️)
Batch 275/537: Loss=1.5951 (C:1.3396, R:0.0099, T:18.5104(w:0.014)⚠️)
Batch 300/537: Loss=1.6876 (C:1.4417, R:0.0099, T:17.8072(w:0.014)⚠️)
Batch 325/537: Loss=1.5816 (C:1.3430, R:0.0099, T:17.2817(w:0.014)⚠️)
Batch 350/537: Loss=1.6688 (C:1.4238, R:0.0099, T:17.7507(w:0.014)⚠️)
Batch 375/537: Loss=1.5955 (C:1.3498, R:0.0100, T:17.7942(w:0.014)⚠️)
Batch 400/537: Loss=1.6042 (C:1.3509, R:0.0100, T:18.3509(w:0.014)⚠️)
Batch 425/537: Loss=1.7088 (C:1.4261, R:0.0100, T:20.4868(w:0.014)⚠️)
Batch 450/537: Loss=1.7869 (C:1.5108, R:0.0100, T:20.0067(w:0.014)⚠️)
Batch 475/537: Loss=1.6867 (C:1.3954, R:0.0099, T:21.1122(w:0.014)⚠️)
Batch 500/537: Loss=1.6490 (C:1.3563, R:0.0100, T:21.2124(w:0.014)⚠️)
Batch 525/537: Loss=1.6479 (C:1.4025, R:0.0100, T:17.7770(w:0.014)⚠️)

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 1.6299
  Contrastive: 1.3748
  Reconstruction: 0.0100
  Topological: 18.4778 (weight: 0.014)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5811
  Contrastive: 1.3499
  Reconstruction: 0.0100
  Topological: 16.7416 (weight: 0.014)
  Batches with topology: 537/537 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (67.3s)
Train Loss: 1.6299 (C:1.3748, R:0.0100, T:18.4778)
Val Loss:   1.5811 (C:1.3499, R:0.0100, T:16.7416)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 537 | Topological Weight: 0.0150
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6757 (C:1.4002, R:0.0100, T:18.2986(w:0.015)⚠️)
Batch  25/537: Loss=1.6507 (C:1.3502, R:0.0099, T:19.9678(w:0.015)⚠️)
Batch  50/537: Loss=1.6601 (C:1.3860, R:0.0100, T:18.2057(w:0.015)⚠️)
Batch  75/537: Loss=1.6051 (C:1.3320, R:0.0100, T:18.1395(w:0.015)⚠️)
Batch 100/537: Loss=1.5903 (C:1.3311, R:0.0099, T:17.2126(w:0.015)⚠️)
Batch 125/537: Loss=1.6693 (C:1.3912, R:0.0100, T:18.4720(w:0.015)⚠️)
Batch 150/537: Loss=1.6324 (C:1.3764, R:0.0099, T:16.9978(w:0.015)⚠️)
Batch 175/537: Loss=1.5865 (C:1.3311, R:0.0100, T:16.9610(w:0.015)⚠️)
Batch 200/537: Loss=1.6570 (C:1.3977, R:0.0100, T:17.2204(w:0.015)⚠️)
Batch 225/537: Loss=1.7650 (C:1.5111, R:0.0099, T:16.8584(w:0.015)⚠️)
Batch 250/537: Loss=1.6557 (C:1.3708, R:0.0099, T:18.9229(w:0.015)⚠️)
Batch 275/537: Loss=1.5921 (C:1.3343, R:0.0100, T:17.1237(w:0.015)⚠️)
Batch 300/537: Loss=1.7077 (C:1.3570, R:0.0100, T:23.3137(w:0.015)⚠️)
Batch 325/537: Loss=1.6455 (C:1.3735, R:0.0099, T:18.0687(w:0.015)⚠️)
Batch 350/537: Loss=1.6365 (C:1.3426, R:0.0099, T:19.5277(w:0.015)⚠️)
Batch 375/537: Loss=1.5838 (C:1.3013, R:0.0099, T:18.7647(w:0.015)⚠️)
Batch 400/537: Loss=1.6060 (C:1.3168, R:0.0100, T:19.2154(w:0.015)⚠️)
Batch 425/537: Loss=1.6357 (C:1.3754, R:0.0099, T:17.2876(w:0.015)⚠️)
Batch 450/537: Loss=1.6415 (C:1.3745, R:0.0099, T:17.7295(w:0.015)⚠️)
Batch 475/537: Loss=1.6576 (C:1.3701, R:0.0100, T:19.0944(w:0.015)⚠️)
Batch 500/537: Loss=1.7849 (C:1.4545, R:0.0100, T:21.9639(w:0.015)⚠️)
Batch 525/537: Loss=1.6640 (C:1.3898, R:0.0099, T:18.2147(w:0.015)⚠️)

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 1.6434
  Contrastive: 1.3638
  Reconstruction: 0.0100
  Topological: 18.5738 (weight: 0.015)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5952
  Contrastive: 1.3434
  Reconstruction: 0.0100
  Topological: 16.7196 (weight: 0.015)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 13/50 COMPLETE (67.3s)
Train Loss: 1.6434 (C:1.3638, R:0.0100, T:18.5738)
Val Loss:   1.5952 (C:1.3434, R:0.0100, T:16.7196)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 537 | Topological Weight: 0.0163
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6112 (C:1.3053, R:0.0099, T:18.7636(w:0.016)⚠️)
Batch  25/537: Loss=1.7140 (C:1.4233, R:0.0099, T:17.8237(w:0.016)⚠️)
Batch  50/537: Loss=1.6462 (C:1.3458, R:0.0100, T:18.4216(w:0.016)⚠️)
Batch  75/537: Loss=1.7071 (C:1.3709, R:0.0099, T:20.6260(w:0.016)⚠️)
Batch 100/537: Loss=1.6439 (C:1.3092, R:0.0100, T:20.5375(w:0.016)⚠️)
Batch 125/537: Loss=1.7950 (C:1.4197, R:0.0099, T:23.0327(w:0.016)⚠️)
Batch 150/537: Loss=1.6673 (C:1.3796, R:0.0099, T:17.6418(w:0.016)⚠️)
Batch 175/537: Loss=1.7470 (C:1.4695, R:0.0100, T:17.0142(w:0.016)⚠️)
Batch 200/537: Loss=1.9464 (C:1.4461, R:0.0100, T:30.7226(w:0.016)⚠️)
Batch 225/537: Loss=1.9221 (C:1.3932, R:0.0099, T:32.4850(w:0.016)⚠️)
Batch 250/537: Loss=1.6772 (C:1.3253, R:0.0100, T:21.5934(w:0.016)⚠️)
Batch 275/537: Loss=1.6026 (C:1.3241, R:0.0100, T:17.0821(w:0.016)⚠️)
Batch 300/537: Loss=1.6560 (C:1.3763, R:0.0100, T:17.1537(w:0.016)⚠️)
Batch 325/537: Loss=1.5953 (C:1.3083, R:0.0100, T:17.5967(w:0.016)⚠️)
Batch 350/537: Loss=1.6754 (C:1.3941, R:0.0099, T:17.2521(w:0.016)⚠️)
Batch 375/537: Loss=1.6906 (C:1.3895, R:0.0100, T:18.4672(w:0.016)⚠️)
Batch 400/537: Loss=1.6645 (C:1.3888, R:0.0100, T:16.9043(w:0.016)⚠️)
Batch 425/537: Loss=1.6456 (C:1.3611, R:0.0099, T:17.4493(w:0.016)⚠️)
Batch 450/537: Loss=1.6348 (C:1.3515, R:0.0099, T:17.3737(w:0.016)⚠️)
Batch 475/537: Loss=1.6363 (C:1.3458, R:0.0100, T:17.8202(w:0.016)⚠️)
Batch 500/537: Loss=1.6367 (C:1.3641, R:0.0100, T:16.7141(w:0.016)⚠️)
Batch 525/537: Loss=1.6544 (C:1.3817, R:0.0100, T:16.7181(w:0.016)⚠️)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 1.6877
  Contrastive: 1.3716
  Reconstruction: 0.0100
  Topological: 19.3909 (weight: 0.016)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6215
  Contrastive: 1.3471
  Reconstruction: 0.0100
  Topological: 16.8267 (weight: 0.016)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 14/50 COMPLETE (67.6s)
Train Loss: 1.6877 (C:1.3716, R:0.0100, T:19.3909)
Val Loss:   1.6215 (C:1.3471, R:0.0100, T:16.8267)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 537 | Topological Weight: 0.0175
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7274 (C:1.4282, R:0.0099, T:17.0383(w:0.018)⚠️)
Batch  25/537: Loss=1.6650 (C:1.3658, R:0.0099, T:17.0418(w:0.018)⚠️)
Batch  50/537: Loss=1.6775 (C:1.3775, R:0.0099, T:17.0896(w:0.018)⚠️)
Batch  75/537: Loss=1.6573 (C:1.3375, R:0.0100, T:18.2124(w:0.018)⚠️)
Batch 100/537: Loss=1.6803 (C:1.3571, R:0.0100, T:18.4102(w:0.018)⚠️)
Batch 125/537: Loss=1.7294 (C:1.4150, R:0.0100, T:17.9095(w:0.018)⚠️)
Batch 150/537: Loss=1.6159 (C:1.3123, R:0.0099, T:17.2933(w:0.018)⚠️)
Batch 175/537: Loss=1.6052 (C:1.3012, R:0.0099, T:17.3130(w:0.018)⚠️)
Batch 200/537: Loss=1.6186 (C:1.3080, R:0.0100, T:17.6931(w:0.018)⚠️)
Batch 225/537: Loss=1.6930 (C:1.3850, R:0.0099, T:17.5384(w:0.018)⚠️)
Batch 250/537: Loss=1.7230 (C:1.4187, R:0.0099, T:17.3318(w:0.018)⚠️)
Batch 275/537: Loss=1.6951 (C:1.3784, R:0.0100, T:18.0360(w:0.018)⚠️)
Batch 300/537: Loss=1.6056 (C:1.2926, R:0.0099, T:17.8275(w:0.018)⚠️)
Batch 325/537: Loss=1.6959 (C:1.3453, R:0.0099, T:19.9723(w:0.018)⚠️)
Batch 350/537: Loss=1.6531 (C:1.2178, R:0.0100, T:24.8171(w:0.018)⚠️)
Batch 375/537: Loss=1.6877 (C:1.3398, R:0.0100, T:19.8206(w:0.018)⚠️)
Batch 400/537: Loss=1.7067 (C:1.4004, R:0.0099, T:17.4437(w:0.018)⚠️)
Batch 425/537: Loss=1.7180 (C:1.4231, R:0.0099, T:16.7964(w:0.018)⚠️)
Batch 450/537: Loss=1.7288 (C:1.4163, R:0.0100, T:17.7977(w:0.018)⚠️)
Batch 475/537: Loss=1.6994 (C:1.3753, R:0.0099, T:18.4636(w:0.018)⚠️)
Batch 500/537: Loss=1.7132 (C:1.4285, R:0.0100, T:16.2095(w:0.018)⚠️)
Batch 525/537: Loss=1.6348 (C:1.3363, R:0.0100, T:16.9948(w:0.018)⚠️)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 1.6775
  Contrastive: 1.3623
  Reconstruction: 0.0100
  Topological: 17.9550 (weight: 0.018)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6301
  Contrastive: 1.3365
  Reconstruction: 0.0100
  Topological: 16.7202 (weight: 0.018)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 15/50 COMPLETE (68.4s)
Train Loss: 1.6775 (C:1.3623, R:0.0100, T:17.9550)
Val Loss:   1.6301 (C:1.3365, R:0.0100, T:16.7202)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 537 | Topological Weight: 0.0187
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6666 (C:1.3543, R:0.0100, T:16.6026(w:0.019)⚠️)
Batch  25/537: Loss=1.6600 (C:1.3412, R:0.0100, T:16.9471(w:0.019)⚠️)
Batch  50/537: Loss=1.7106 (C:1.3744, R:0.0100, T:17.8788(w:0.019)⚠️)
Batch  75/537: Loss=1.7683 (C:1.4455, R:0.0099, T:17.1658(w:0.019)⚠️)
Batch 100/537: Loss=1.8041 (C:1.4202, R:0.0100, T:20.4192(w:0.019)⚠️)
Batch 125/537: Loss=1.9301 (C:1.3120, R:0.0100, T:32.9137(w:0.019)⚠️)
Batch 150/537: Loss=1.9483 (C:1.3289, R:0.0099, T:32.9813(w:0.019)⚠️)
Batch 175/537: Loss=2.0029 (C:1.3903, R:0.0099, T:32.6200(w:0.019)⚠️)
Batch 200/537: Loss=1.9280 (C:1.3509, R:0.0100, T:30.7246(w:0.019)⚠️)
Batch 225/537: Loss=1.8460 (C:1.3248, R:0.0099, T:27.7415(w:0.019)⚠️)
Batch 250/537: Loss=1.9153 (C:1.3215, R:0.0099, T:31.6176(w:0.019)⚠️)
Batch 275/537: Loss=1.9094 (C:1.3518, R:0.0099, T:29.6847(w:0.019)⚠️)
Batch 300/537: Loss=1.9528 (C:1.4091, R:0.0099, T:28.9493(w:0.019)⚠️)
Batch 325/537: Loss=1.8619 (C:1.3296, R:0.0100, T:28.3330(w:0.019)⚠️)
Batch 350/537: Loss=1.9008 (C:1.3929, R:0.0100, T:27.0359(w:0.019)⚠️)
Batch 375/537: Loss=2.0113 (C:1.3884, R:0.0100, T:33.1700(w:0.019)⚠️)
Batch 400/537: Loss=1.8886 (C:1.2431, R:0.0100, T:34.3751(w:0.019)⚠️)
Batch 425/537: Loss=1.9500 (C:1.3181, R:0.0100, T:33.6476(w:0.019)⚠️)
Batch 450/537: Loss=2.0440 (C:1.4921, R:0.0099, T:29.3850(w:0.019)⚠️)
Batch 475/537: Loss=1.9850 (C:1.3677, R:0.0100, T:32.8724(w:0.019)⚠️)
Batch 500/537: Loss=1.9881 (C:1.3399, R:0.0100, T:34.5221(w:0.019)⚠️)
Batch 525/537: Loss=1.8895 (C:1.2642, R:0.0099, T:33.2964(w:0.019)⚠️)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 1.8953
  Contrastive: 1.3562
  Reconstruction: 0.0100
  Topological: 28.6987 (weight: 0.019)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6314
  Contrastive: 1.3178
  Reconstruction: 0.0100
  Topological: 16.6718 (weight: 0.019)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 16/50 COMPLETE (67.4s)
Train Loss: 1.8953 (C:1.3562, R:0.0100, T:28.6987)
Val Loss:   1.6314 (C:1.3178, R:0.0100, T:16.6718)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 537 | Topological Weight: 0.0200
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9544 (C:1.3191, R:0.0099, T:31.7124(w:0.020)⚠️)
Batch  25/537: Loss=1.9801 (C:1.3399, R:0.0100, T:31.9574(w:0.020)⚠️)
Batch  50/537: Loss=2.0851 (C:1.4343, R:0.0100, T:32.4927(w:0.020)⚠️)
Batch  75/537: Loss=2.0301 (C:1.3367, R:0.0099, T:34.6201(w:0.020)⚠️)
Batch 100/537: Loss=2.1290 (C:1.4393, R:0.0100, T:34.4365(w:0.020)⚠️)
Batch 125/537: Loss=2.0646 (C:1.3638, R:0.0099, T:34.9877(w:0.020)⚠️)
Batch 150/537: Loss=2.0764 (C:1.3704, R:0.0100, T:35.2495(w:0.020)⚠️)
Batch 175/537: Loss=1.9831 (C:1.3059, R:0.0099, T:33.8147(w:0.020)⚠️)
Batch 200/537: Loss=2.0224 (C:1.3134, R:0.0100, T:35.4009(w:0.020)⚠️)
Batch 225/537: Loss=2.0963 (C:1.4082, R:0.0100, T:34.3527(w:0.020)⚠️)
Batch 250/537: Loss=2.0731 (C:1.4011, R:0.0100, T:33.5464(w:0.020)⚠️)
Batch 275/537: Loss=2.0308 (C:1.4011, R:0.0100, T:31.4334(w:0.020)⚠️)
Batch 300/537: Loss=2.0282 (C:1.3322, R:0.0099, T:34.7504(w:0.020)⚠️)
Batch 325/537: Loss=2.0476 (C:1.3442, R:0.0099, T:35.1171(w:0.020)⚠️)
Batch 350/537: Loss=2.0412 (C:1.3504, R:0.0100, T:34.4923(w:0.020)⚠️)
Batch 375/537: Loss=2.0692 (C:1.3794, R:0.0099, T:34.4418(w:0.020)⚠️)
Batch 400/537: Loss=2.0468 (C:1.3615, R:0.0099, T:34.2143(w:0.020)⚠️)
Batch 425/537: Loss=2.0071 (C:1.3070, R:0.0100, T:34.9577(w:0.020)⚠️)
Batch 450/537: Loss=1.9937 (C:1.2724, R:0.0100, T:36.0158(w:0.020)⚠️)
Batch 475/537: Loss=2.0051 (C:1.3071, R:0.0099, T:34.8490(w:0.020)⚠️)
Batch 500/537: Loss=1.9730 (C:1.2979, R:0.0100, T:33.7049(w:0.020)⚠️)
Batch 525/537: Loss=2.0102 (C:1.3420, R:0.0100, T:33.3639(w:0.020)⚠️)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 2.0279
  Contrastive: 1.3428
  Reconstruction: 0.0100
  Topological: 34.2042 (weight: 0.020)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6501
  Contrastive: 1.2810
  Reconstruction: 0.0100
  Topological: 18.4084 (weight: 0.020)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 17/50 COMPLETE (68.5s)
Train Loss: 2.0279 (C:1.3428, R:0.0100, T:34.2042)
Val Loss:   1.6501 (C:1.2810, R:0.0100, T:18.4084)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 537 | Topological Weight: 0.0213
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0431 (C:1.3069, R:0.0100, T:34.5994(w:0.021)⚠️)
Batch  25/537: Loss=2.0785 (C:1.3413, R:0.0100, T:34.6435(w:0.021)⚠️)
Batch  50/537: Loss=2.0561 (C:1.3358, R:0.0099, T:33.8515(w:0.021)⚠️)
Batch  75/537: Loss=2.0530 (C:1.2848, R:0.0100, T:36.1072(w:0.021)⚠️)
Batch 100/537: Loss=2.0540 (C:1.3027, R:0.0099, T:35.3092(w:0.021)⚠️)
Batch 125/537: Loss=2.0081 (C:1.2816, R:0.0100, T:34.1413(w:0.021)⚠️)
Batch 150/537: Loss=2.0314 (C:1.2834, R:0.0100, T:35.1522(w:0.021)⚠️)
Batch 175/537: Loss=2.0230 (C:1.3193, R:0.0099, T:33.0702(w:0.021)⚠️)
Batch 200/537: Loss=2.0664 (C:1.3452, R:0.0099, T:33.8935(w:0.021)⚠️)
Batch 225/537: Loss=1.9890 (C:1.2749, R:0.0099, T:33.5586(w:0.021)⚠️)
Batch 250/537: Loss=2.0144 (C:1.2993, R:0.0099, T:33.6026(w:0.021)⚠️)
Batch 275/537: Loss=2.0277 (C:1.2997, R:0.0099, T:34.2141(w:0.021)⚠️)
Batch 300/537: Loss=2.0295 (C:1.3006, R:0.0100, T:34.2534(w:0.021)⚠️)
Batch 325/537: Loss=2.0395 (C:1.3082, R:0.0099, T:34.3674(w:0.021)⚠️)
Batch 350/537: Loss=2.0611 (C:1.3282, R:0.0099, T:34.4430(w:0.021)⚠️)
Batch 375/537: Loss=1.9562 (C:1.2794, R:0.0099, T:31.8033(w:0.021)⚠️)
Batch 400/537: Loss=1.9442 (C:1.2367, R:0.0099, T:33.2477(w:0.021)⚠️)
Batch 425/537: Loss=2.0619 (C:1.3222, R:0.0100, T:34.7620(w:0.021)⚠️)
Batch 450/537: Loss=2.0350 (C:1.3327, R:0.0099, T:33.0007(w:0.021)⚠️)
Batch 475/537: Loss=1.9844 (C:1.2778, R:0.0100, T:33.2050(w:0.021)⚠️)
Batch 500/537: Loss=2.0249 (C:1.3143, R:0.0099, T:33.3945(w:0.021)⚠️)
Batch 525/537: Loss=1.9711 (C:1.2443, R:0.0099, T:34.1552(w:0.021)⚠️)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 2.0304
  Contrastive: 1.3054
  Reconstruction: 0.0100
  Topological: 34.0697 (weight: 0.021)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6969
  Contrastive: 1.2650
  Reconstruction: 0.0100
  Topological: 20.2747 (weight: 0.021)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 18/50 COMPLETE (67.7s)
Train Loss: 2.0304 (C:1.3054, R:0.0100, T:34.0697)
Val Loss:   1.6969 (C:1.2650, R:0.0100, T:20.2747)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 537 | Topological Weight: 0.0225
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0344 (C:1.2639, R:0.0100, T:34.1992(w:0.022)⚠️)
Batch  25/537: Loss=2.0701 (C:1.3193, R:0.0100, T:33.3237(w:0.022)⚠️)
Batch  50/537: Loss=1.9957 (C:1.2594, R:0.0099, T:32.6810(w:0.022)⚠️)
Batch  75/537: Loss=2.0225 (C:1.2598, R:0.0100, T:33.8494(w:0.022)⚠️)
Batch 100/537: Loss=2.0291 (C:1.2768, R:0.0099, T:33.3895(w:0.022)⚠️)
Batch 125/537: Loss=2.1301 (C:1.3668, R:0.0099, T:33.8807(w:0.022)⚠️)
Batch 150/537: Loss=2.0184 (C:1.2932, R:0.0100, T:32.1852(w:0.022)⚠️)
Batch 175/537: Loss=2.0218 (C:1.2748, R:0.0099, T:33.1541(w:0.022)⚠️)
Batch 200/537: Loss=2.0980 (C:1.3404, R:0.0100, T:33.6272(w:0.022)⚠️)
Batch 225/537: Loss=2.0413 (C:1.2671, R:0.0100, T:34.3623(w:0.022)⚠️)
Batch 250/537: Loss=2.0335 (C:1.2716, R:0.0100, T:33.8214(w:0.022)⚠️)
Batch 275/537: Loss=2.0937 (C:1.3151, R:0.0099, T:34.5610(w:0.022)⚠️)
Batch 300/537: Loss=2.0259 (C:1.2774, R:0.0100, T:33.2251(w:0.022)⚠️)
Batch 325/537: Loss=2.0423 (C:1.3090, R:0.0100, T:32.5447(w:0.022)⚠️)
Batch 350/537: Loss=2.0702 (C:1.3106, R:0.0100, T:33.7159(w:0.022)⚠️)
Batch 375/537: Loss=2.0034 (C:1.2600, R:0.0100, T:32.9943(w:0.022)⚠️)
Batch 400/537: Loss=2.0609 (C:1.3011, R:0.0100, T:33.7244(w:0.022)⚠️)
Batch 425/537: Loss=2.0015 (C:1.2456, R:0.0100, T:33.5499(w:0.022)⚠️)
Batch 450/537: Loss=2.0296 (C:1.2817, R:0.0099, T:33.1980(w:0.022)⚠️)
Batch 475/537: Loss=2.0333 (C:1.2782, R:0.0099, T:33.5156(w:0.022)⚠️)
Batch 500/537: Loss=2.0630 (C:1.2940, R:0.0100, T:34.1345(w:0.022)⚠️)
Batch 525/537: Loss=2.1311 (C:1.3635, R:0.0100, T:34.0703(w:0.022)⚠️)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 2.0476
  Contrastive: 1.2918
  Reconstruction: 0.0100
  Topological: 33.5486 (weight: 0.022)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6918
  Contrastive: 1.2425
  Reconstruction: 0.0100
  Topological: 19.9256 (weight: 0.022)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 19/50 COMPLETE (72.6s)
Train Loss: 2.0476 (C:1.2918, R:0.0100, T:33.5486)
Val Loss:   1.6918 (C:1.2425, R:0.0100, T:19.9256)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 537 | Topological Weight: 0.0238
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0774 (C:1.2708, R:0.0100, T:33.9177(w:0.024)⚠️)
Batch  25/537: Loss=2.0281 (C:1.2331, R:0.0100, T:33.4287(w:0.024)⚠️)
Batch  50/537: Loss=2.1169 (C:1.3195, R:0.0100, T:33.5333(w:0.024)⚠️)
Batch  75/537: Loss=2.0731 (C:1.3078, R:0.0100, T:32.1811(w:0.024)⚠️)
Batch 100/537: Loss=2.0665 (C:1.2983, R:0.0099, T:32.3046(w:0.024)⚠️)
Batch 125/537: Loss=2.1018 (C:1.3098, R:0.0100, T:33.3052(w:0.024)⚠️)
Batch 150/537: Loss=2.0927 (C:1.2896, R:0.0099, T:33.7767(w:0.024)⚠️)
Batch 175/537: Loss=2.0578 (C:1.2496, R:0.0099, T:33.9877(w:0.024)⚠️)
Batch 200/537: Loss=2.0985 (C:1.3076, R:0.0100, T:33.2592(w:0.024)⚠️)
Batch 225/537: Loss=2.1327 (C:1.3351, R:0.0100, T:33.5426(w:0.024)⚠️)
Batch 250/537: Loss=2.0876 (C:1.3043, R:0.0099, T:32.9424(w:0.024)⚠️)
Batch 275/537: Loss=2.0684 (C:1.2694, R:0.0099, T:33.6026(w:0.024)⚠️)
Batch 300/537: Loss=2.1185 (C:1.3274, R:0.0099, T:33.2679(w:0.024)⚠️)
Batch 325/537: Loss=2.0576 (C:1.2729, R:0.0100, T:32.9973(w:0.024)⚠️)
Batch 350/537: Loss=2.0491 (C:1.2868, R:0.0100, T:32.0532(w:0.024)⚠️)
Batch 375/537: Loss=2.0368 (C:1.2825, R:0.0100, T:31.7195(w:0.024)⚠️)
Batch 400/537: Loss=2.0989 (C:1.3331, R:0.0099, T:32.2057(w:0.024)⚠️)
Batch 425/537: Loss=2.0704 (C:1.3127, R:0.0100, T:31.8610(w:0.024)⚠️)
Batch 450/537: Loss=2.0755 (C:1.2907, R:0.0099, T:32.9998(w:0.024)⚠️)
Batch 475/537: Loss=2.0591 (C:1.2886, R:0.0100, T:32.3985(w:0.024)⚠️)
Batch 500/537: Loss=2.0770 (C:1.2773, R:0.0100, T:33.6301(w:0.024)⚠️)
Batch 525/537: Loss=2.0112 (C:1.2442, R:0.0100, T:32.2501(w:0.024)⚠️)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 2.0811
  Contrastive: 1.2952
  Reconstruction: 0.0100
  Topological: 33.0464 (weight: 0.024)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7610
  Contrastive: 1.2132
  Reconstruction: 0.0100
  Topological: 23.0254 (weight: 0.024)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 20/50 COMPLETE (72.6s)
Train Loss: 2.0811 (C:1.2952, R:0.0100, T:33.0464)
Val Loss:   1.7610 (C:1.2132, R:0.0100, T:23.0254)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 537 | Topological Weight: 0.0250
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1030 (C:1.2898, R:0.0099, T:32.4881(w:0.025)⚠️)
Batch  25/537: Loss=2.1643 (C:1.3383, R:0.0100, T:32.9995(w:0.025)⚠️)
Batch  50/537: Loss=2.0482 (C:1.2257, R:0.0100, T:32.8613(w:0.025)⚠️)
Batch  75/537: Loss=2.0583 (C:1.2235, R:0.0100, T:33.3535(w:0.025)⚠️)
Batch 100/537: Loss=2.0855 (C:1.2791, R:0.0100, T:32.2145(w:0.025)⚠️)
Batch 125/537: Loss=2.1115 (C:1.2917, R:0.0099, T:32.7537(w:0.025)⚠️)
Batch 150/537: Loss=2.1227 (C:1.2824, R:0.0099, T:33.5724(w:0.025)⚠️)
Batch 175/537: Loss=2.1042 (C:1.2782, R:0.0100, T:33.0009(w:0.025)⚠️)
Batch 200/537: Loss=2.1076 (C:1.2965, R:0.0099, T:32.4065(w:0.025)⚠️)
Batch 225/537: Loss=2.0429 (C:1.2300, R:0.0100, T:32.4765(w:0.025)⚠️)
Batch 250/537: Loss=2.0562 (C:1.2321, R:0.0100, T:32.9232(w:0.025)⚠️)
Batch 275/537: Loss=2.1478 (C:1.3034, R:0.0100, T:33.7369(w:0.025)⚠️)
Batch 300/537: Loss=2.1005 (C:1.2812, R:0.0100, T:32.7338(w:0.025)⚠️)
Batch 325/537: Loss=2.1412 (C:1.3180, R:0.0099, T:32.8859(w:0.025)⚠️)
Batch 350/537: Loss=2.0563 (C:1.2411, R:0.0099, T:32.5647(w:0.025)⚠️)
Batch 375/537: Loss=2.0504 (C:1.2427, R:0.0100, T:32.2699(w:0.025)⚠️)
Batch 400/537: Loss=2.0372 (C:1.2284, R:0.0100, T:32.3083(w:0.025)⚠️)
Batch 425/537: Loss=2.0955 (C:1.2876, R:0.0100, T:32.2739(w:0.025)⚠️)
Batch 450/537: Loss=2.0306 (C:1.2155, R:0.0099, T:32.5666(w:0.025)⚠️)
Batch 475/537: Loss=1.9665 (C:1.1546, R:0.0099, T:32.4367(w:0.025)⚠️)
Batch 500/537: Loss=2.0264 (C:1.1913, R:0.0100, T:33.3660(w:0.025)⚠️)
Batch 525/537: Loss=2.0297 (C:1.2164, R:0.0100, T:32.4926(w:0.025)⚠️)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 2.0722
  Contrastive: 1.2524
  Reconstruction: 0.0100
  Topological: 32.7499 (weight: 0.025)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7138
  Contrastive: 1.1503
  Reconstruction: 0.0100
  Topological: 22.4994 (weight: 0.025)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 21/50 COMPLETE (69.9s)
Train Loss: 2.0722 (C:1.2524, R:0.0100, T:32.7499)
Val Loss:   1.7138 (C:1.1503, R:0.0100, T:22.4994)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 537 | Topological Weight: 0.0262
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0798 (C:1.2118, R:0.0099, T:33.0274(w:0.026)⚠️)
Batch  25/537: Loss=2.0445 (C:1.1972, R:0.0100, T:32.2402(w:0.026)⚠️)
Batch  50/537: Loss=2.0447 (C:1.2015, R:0.0099, T:32.0830(w:0.026)⚠️)
Batch  75/537: Loss=2.0434 (C:1.1713, R:0.0100, T:33.1866(w:0.026)⚠️)
Batch 100/537: Loss=2.0212 (C:1.1720, R:0.0100, T:32.3147(w:0.026)⚠️)
Batch 125/537: Loss=2.1158 (C:1.2776, R:0.0100, T:31.8941(w:0.026)⚠️)
Batch 150/537: Loss=1.9871 (C:1.1245, R:0.0099, T:32.8235(w:0.026)⚠️)
Batch 175/537: Loss=2.0659 (C:1.2106, R:0.0099, T:32.5458(w:0.026)⚠️)
Batch 200/537: Loss=2.0798 (C:1.2014, R:0.0100, T:33.4247(w:0.026)⚠️)
Batch 225/537: Loss=2.0755 (C:1.2182, R:0.0100, T:32.6223(w:0.026)⚠️)
Batch 250/537: Loss=2.0361 (C:1.1726, R:0.0100, T:32.8595(w:0.026)⚠️)
Batch 275/537: Loss=2.0639 (C:1.2116, R:0.0100, T:32.4304(w:0.026)⚠️)
Batch 300/537: Loss=2.0219 (C:1.1646, R:0.0099, T:32.6211(w:0.026)⚠️)
Batch 325/537: Loss=2.0410 (C:1.2080, R:0.0099, T:31.6965(w:0.026)⚠️)
Batch 350/537: Loss=2.0445 (C:1.2215, R:0.0099, T:31.3147(w:0.026)⚠️)
Batch 375/537: Loss=2.0674 (C:1.2037, R:0.0099, T:32.8637(w:0.026)⚠️)
Batch 400/537: Loss=2.0884 (C:1.2183, R:0.0099, T:33.1109(w:0.026)⚠️)
Batch 425/537: Loss=2.0898 (C:1.2344, R:0.0100, T:32.5495(w:0.026)⚠️)
Batch 450/537: Loss=2.0567 (C:1.2305, R:0.0099, T:31.4363(w:0.026)⚠️)
Batch 475/537: Loss=1.9053 (C:1.1173, R:0.0100, T:29.9812(w:0.026)⚠️)
Batch 500/537: Loss=1.9999 (C:1.1539, R:0.0099, T:32.1891(w:0.026)⚠️)
Batch 525/537: Loss=2.1278 (C:1.2937, R:0.0100, T:31.7380(w:0.026)⚠️)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 2.0483
  Contrastive: 1.1938
  Reconstruction: 0.0100
  Topological: 32.5148 (weight: 0.026)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6782
  Contrastive: 1.1009
  Reconstruction: 0.0100
  Topological: 21.9551 (weight: 0.026)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 22/50 COMPLETE (67.1s)
Train Loss: 2.0483 (C:1.1938, R:0.0100, T:32.5148)
Val Loss:   1.6782 (C:1.1009, R:0.0100, T:21.9551)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

🛑 Early stopping triggered after 22 epochs
Best model was at epoch 12 with Val Loss: 1.5811

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 9
Epochs with topology: 14/22
Max consecutive topology epochs: 14
Best topological loss: 17.9376
Final topological loss: 32.5148
✅ SUCCESS: Topological learning achieved!
👍 GOOD: Fairly consistent topological learning (>50%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_154711/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/539 batches
  Processed 51/539 batches
  Processed 101/539 batches
  Processed 151/539 batches
  Processed 201/539 batches
  Processed 251/539 batches
  Processed 301/539 batches
  Processed 351/539 batches
  Processed 401/539 batches
  Processed 451/539 batches
  Processed 501/539 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: 0.1256
  Adjusted Rand Score: 0.2862
  Clustering Accuracy: 0.5880
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.5986
  Per-class F1: [0.5782351282774174, 0.3908839779005525, 0.7880236154062412]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009953
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 0.483 ± 0.843
  Negative distances: 1.136 ± 0.974
  Separation ratio: 2.35x
  Gap: -1.983
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.1256
  Clustering Accuracy: 0.5880
  Adjusted Rand Score: 0.2862

Classification Performance:
  Accuracy: 0.5986

Separation Quality:
  Separation Ratio: 2.35x
  Gap: -1.983
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009953
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_154711/results/evaluation_results_20250721_160747.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_154711/results/evaluation_results_20250721_160747.json

Key Results:
  Separation ratio: 2.35x
  Perfect separation: False
  Classification accuracy: 0.5986

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 22
  Epochs with topological learning: 14
  Current topological loss: 32.5148
  Current topological weight: 0.0262
  ✅ Topological loss is decreasing (good progress)
✅ GOOD: Reasonable topological learning
Final topological loss: 32.5148
Epochs with topology: 14/22
⚠️  Poor clustering accuracy: 0.588

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_154711/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_154711

Analysis completed with exit code: 0
Time: Mon 21 Jul 16:07:49 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
