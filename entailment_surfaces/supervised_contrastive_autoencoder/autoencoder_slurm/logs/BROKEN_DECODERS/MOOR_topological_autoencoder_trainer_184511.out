Starting Surface Distance Metric Analysis job...
Job ID: 184511
Node: gpuvm18
Time: Mon 21 Jul 15:14:22 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Mon Jul 21 15:14:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   36C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_151438
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_151438/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 537
  Test batches: 539
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 549367 samples, 537 batches
  Test: 549367 samples, 539 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,858,891
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
MoorTopologicalLoss Initialized: Using 0-dimensional persistence pairings (MST edges).
No prototypes being used for topological loss - whole dataset instead.
TopologicalTrainer initialized on device: cuda
Model parameters: 5,858,891
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.01
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=2.0010 (C:1.9999, R:0.0110, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.7122 (C:1.7112, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.6985 (C:1.6975, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.6434 (C:1.6424, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.5536 (C:1.5526, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.5892 (C:1.5882, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.5609 (C:1.5599, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.5411 (C:1.5401, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4683 (C:1.4674, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.5065 (C:1.5055, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4887 (C:1.4877, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4789 (C:1.4779, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4436 (C:1.4426, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3914 (C:1.3904, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3646 (C:1.3636, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4409 (C:1.4399, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4065 (C:1.4055, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4683 (C:1.4673, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.3726 (C:1.3716, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4473 (C:1.4463, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.5246 (C:1.5236, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4196 (C:1.4186, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5107
  Contrastive: 1.5097
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3966
  Contrastive: 1.3956
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (25.9s)
Train Loss: 1.5107 (C:1.5097, R:0.0100, T:0.0000)
Val Loss:   1.3966 (C:1.3956, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.4347 (C:1.4337, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4504 (C:1.4494, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3614 (C:1.3604, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4036 (C:1.4026, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4133 (C:1.4123, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.3044 (C:1.3034, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4020 (C:1.4011, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3729 (C:1.3719, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4109 (C:1.4099, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4338 (C:1.4328, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3481 (C:1.3471, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4075 (C:1.4065, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4078 (C:1.4068, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3534 (C:1.3524, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4588 (C:1.4578, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4245 (C:1.4235, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3610 (C:1.3600, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3640 (C:1.3630, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4648 (C:1.4638, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3037 (C:1.3027, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4630 (C:1.4620, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4412 (C:1.4402, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.3954
  Contrastive: 1.3944
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3239
  Contrastive: 1.3229
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (25.7s)
Train Loss: 1.3954 (C:1.3944, R:0.0100, T:0.0000)
Val Loss:   1.3239 (C:1.3229, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3530 (C:1.3520, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.3442 (C:1.3432, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3152 (C:1.3142, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4088 (C:1.4078, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.3452 (C:1.3442, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.3891 (C:1.3881, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3729 (C:1.3719, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3437 (C:1.3427, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4280 (C:1.4270, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.3936 (C:1.3926, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3281 (C:1.3271, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3908 (C:1.3898, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3792 (C:1.3782, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4664 (C:1.4654, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3073 (C:1.3063, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.3598 (C:1.3588, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3780 (C:1.3770, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3342 (C:1.3332, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4356 (C:1.4346, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3154 (C:1.3144, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4092 (C:1.4082, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3178 (C:1.3168, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.3680
  Contrastive: 1.3670
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2883
  Contrastive: 1.2873
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (26.1s)
Train Loss: 1.3680 (C:1.3670, R:0.0100, T:0.0000)
Val Loss:   1.2883 (C:1.2873, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3893 (C:1.3883, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.2620 (C:1.2610, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3064 (C:1.3054, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3971 (C:1.3962, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.2850 (C:1.2840, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.3778 (C:1.3768, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3060 (C:1.3050, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3669 (C:1.3659, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.3821 (C:1.3811, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4367 (C:1.4357, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3066 (C:1.3056, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3367 (C:1.3357, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3055 (C:1.3045, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.2874 (C:1.2864, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3616 (C:1.3606, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.2400 (C:1.2390, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.2934 (C:1.2924, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3439 (C:1.3429, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.2764 (C:1.2754, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.2887 (C:1.2877, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4238 (C:1.4228, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.2784 (C:1.2774, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.3409
  Contrastive: 1.3399
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2676
  Contrastive: 1.2666
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (25.6s)
Train Loss: 1.3409 (C:1.3399, R:0.0100, T:0.0000)
Val Loss:   1.2676 (C:1.2666, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.2705 (C:1.2695, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.2995 (C:1.2985, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3434 (C:1.3425, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3756 (C:1.3746, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.3412 (C:1.3402, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.2816 (C:1.2806, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3617 (C:1.3607, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3594 (C:1.3584, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.3826 (C:1.3816, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.2851 (C:1.2841, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3438 (C:1.3428, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3346 (C:1.3336, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3582 (C:1.3572, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.2840 (C:1.2830, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3786 (C:1.3776, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.2812 (C:1.2802, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3208 (C:1.3198, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3221 (C:1.3211, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.3611 (C:1.3601, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3569 (C:1.3559, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3594 (C:1.3585, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3433 (C:1.3423, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.3260
  Contrastive: 1.3250
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2525
  Contrastive: 1.2515
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (26.1s)
Train Loss: 1.3260 (C:1.3250, R:0.0100, T:0.0000)
Val Loss:   1.2525 (C:1.2515, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3130 (C:1.3120, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.3245 (C:1.3235, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3985 (C:1.3975, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3040 (C:1.3030, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.3517 (C:1.3507, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.3853 (C:1.3843, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.2822 (C:1.2812, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.2930 (C:1.2920, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.2626 (C:1.2617, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.3028 (C:1.3018, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3474 (C:1.3464, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3234 (C:1.3225, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.2997 (C:1.2987, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3477 (C:1.3467, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3386 (C:1.3376, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.3374 (C:1.3364, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3233 (C:1.3223, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.2999 (C:1.2989, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.3454 (C:1.3445, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3682 (C:1.3672, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.2441 (C:1.2431, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3042 (C:1.3032, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.3129
  Contrastive: 1.3120
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2267
  Contrastive: 1.2257
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (26.4s)
Train Loss: 1.3129 (C:1.3120, R:0.0100, T:0.0000)
Val Loss:   1.2267 (C:1.2257, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3225 (C:1.3215, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.3112 (C:1.3102, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.2485 (C:1.2475, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.2802 (C:1.2792, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.2870 (C:1.2860, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.3109 (C:1.3099, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3065 (C:1.3056, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.2996 (C:1.2986, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.2911 (C:1.2902, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.2514 (C:1.2504, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4115 (C:1.4105, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3667 (C:1.3657, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3697 (C:1.3687, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3052 (C:1.3042, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3577 (C:1.3567, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.2565 (C:1.2555, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.2601 (C:1.2591, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.2962 (C:1.2952, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.2577 (C:1.2567, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.2971 (C:1.2961, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3266 (C:1.3256, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3259 (C:1.3249, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.3005
  Contrastive: 1.2995
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2179
  Contrastive: 1.2169
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 7/50 COMPLETE (26.1s)
Train Loss: 1.3005 (C:1.2995, R:0.0100, T:0.0000)
Val Loss:   1.2179 (C:1.2169, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3636 (C:1.3626, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.2339 (C:1.2329, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.3069 (C:1.3059, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.2835 (C:1.2825, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.2867 (C:1.2857, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.2599 (C:1.2589, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3306 (C:1.3296, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.2949 (C:1.2939, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.2196 (C:1.2186, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.2813 (C:1.2803, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3457 (C:1.3447, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.3100 (C:1.3090, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.2612 (C:1.2602, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.2217 (C:1.2207, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3515 (C:1.3505, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.2534 (C:1.2524, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3368 (C:1.3358, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3456 (C:1.3446, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.3033 (C:1.3023, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.2645 (C:1.2635, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.2482 (C:1.2472, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3321 (C:1.3311, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.2868
  Contrastive: 1.2858
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2154
  Contrastive: 1.2144
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 8/50 COMPLETE (24.1s)
Train Loss: 1.2868 (C:1.2858, R:0.0100, T:0.0000)
Val Loss:   1.2154 (C:1.2144, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 537 | Topological Weight: 0.0100
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6378 (C:1.2464, R:0.0099, T:39.0423(w:0.010)⚠️)
Batch  25/537: Loss=1.6999 (C:1.2396, R:0.0100, T:45.9313(w:0.010)⚠️)
Batch  50/537: Loss=1.7315 (C:1.2766, R:0.0100, T:45.3901(w:0.010)⚠️)
Batch  75/537: Loss=1.6668 (C:1.2045, R:0.0100, T:46.1220(w:0.010)⚠️)
Batch 100/537: Loss=1.7996 (C:1.3401, R:0.0099, T:45.8489(w:0.010)⚠️)
Batch 125/537: Loss=1.6753 (C:1.2124, R:0.0100, T:46.1936(w:0.010)⚠️)
Batch 150/537: Loss=1.7559 (C:1.3020, R:0.0100, T:45.2855(w:0.010)⚠️)
Batch 175/537: Loss=1.7357 (C:1.2782, R:0.0099, T:45.6487(w:0.010)⚠️)
Batch 200/537: Loss=1.7242 (C:1.2678, R:0.0099, T:45.5411(w:0.010)⚠️)
Batch 225/537: Loss=1.7588 (C:1.3004, R:0.0100, T:45.7406(w:0.010)⚠️)
Batch 250/537: Loss=1.6882 (C:1.2313, R:0.0099, T:45.5909(w:0.010)⚠️)
Batch 275/537: Loss=1.7630 (C:1.3074, R:0.0100, T:45.4527(w:0.010)⚠️)
Batch 300/537: Loss=1.6900 (C:1.2293, R:0.0100, T:45.9690(w:0.010)⚠️)
Batch 325/537: Loss=1.7505 (C:1.2896, R:0.0100, T:45.9834(w:0.010)⚠️)
Batch 350/537: Loss=1.7256 (C:1.2696, R:0.0100, T:45.5026(w:0.010)⚠️)
Batch 375/537: Loss=1.7158 (C:1.2595, R:0.0099, T:45.5329(w:0.010)⚠️)
Batch 400/537: Loss=1.7641 (C:1.3072, R:0.0100, T:45.5875(w:0.010)⚠️)
Batch 425/537: Loss=1.7356 (C:1.2796, R:0.0100, T:45.5018(w:0.010)⚠️)
Batch 450/537: Loss=1.6994 (C:1.2427, R:0.0099, T:45.5708(w:0.010)⚠️)
Batch 475/537: Loss=1.7140 (C:1.2587, R:0.0099, T:45.4278(w:0.010)⚠️)
Batch 500/537: Loss=1.6950 (C:1.2381, R:0.0099, T:45.5948(w:0.010)⚠️)
Batch 525/537: Loss=1.7343 (C:1.2764, R:0.0099, T:45.6920(w:0.010)⚠️)
🎉 MILESTONE: First topological learning detected at epoch 9!
   Initial topological loss: 45.6209
📈 New best topological loss: 45.6209

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.7329
  Contrastive: 1.2757
  Reconstruction: 0.0100
  Topological: 45.6209 (weight: 0.010)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4953
  Contrastive: 1.2195
  Reconstruction: 0.0100
  Topological: 27.4729 (weight: 0.010)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 9/50 COMPLETE (73.0s)
Train Loss: 1.7329 (C:1.2757, R:0.0100, T:45.6209)
Val Loss:   1.4953 (C:1.2195, R:0.0100, T:27.4729)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 537 | Topological Weight: 0.0112
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.8515 (C:1.3382, R:0.0099, T:45.5330(w:0.011)⚠️)
Batch  25/537: Loss=1.7663 (C:1.2540, R:0.0099, T:45.4443(w:0.011)⚠️)
Batch  50/537: Loss=1.8539 (C:1.3433, R:0.0099, T:45.3030(w:0.011)⚠️)
Batch  75/537: Loss=1.7623 (C:1.2454, R:0.0100, T:45.8574(w:0.011)⚠️)
Batch 100/537: Loss=1.7752 (C:1.2568, R:0.0100, T:45.9912(w:0.011)⚠️)
Batch 125/537: Loss=1.8168 (C:1.3063, R:0.0100, T:45.2942(w:0.011)⚠️)
Batch 150/537: Loss=1.7320 (C:1.2179, R:0.0100, T:45.6085(w:0.011)⚠️)
Batch 175/537: Loss=1.8101 (C:1.3009, R:0.0099, T:45.1699(w:0.011)⚠️)
Batch 200/537: Loss=1.7271 (C:1.2117, R:0.0099, T:45.7290(w:0.011)⚠️)
Batch 225/537: Loss=1.7693 (C:1.2512, R:0.0100, T:45.9670(w:0.011)⚠️)
Batch 250/537: Loss=1.7896 (C:1.2741, R:0.0100, T:45.7418(w:0.011)⚠️)
Batch 275/537: Loss=1.8334 (C:1.3179, R:0.0099, T:45.7383(w:0.011)⚠️)
Batch 300/537: Loss=1.7949 (C:1.2806, R:0.0100, T:45.6269(w:0.011)⚠️)
Batch 325/537: Loss=1.8131 (C:1.2967, R:0.0100, T:45.8140(w:0.011)⚠️)
Batch 350/537: Loss=1.7423 (C:1.2278, R:0.0100, T:45.6429(w:0.011)⚠️)
Batch 375/537: Loss=1.8120 (C:1.2968, R:0.0100, T:45.7118(w:0.011)⚠️)
Batch 400/537: Loss=1.7864 (C:1.2674, R:0.0100, T:46.0482(w:0.011)⚠️)
Batch 425/537: Loss=1.7455 (C:1.2297, R:0.0099, T:45.7624(w:0.011)⚠️)
Batch 450/537: Loss=1.8405 (C:1.3266, R:0.0099, T:45.5842(w:0.011)⚠️)
Batch 475/537: Loss=1.8340 (C:1.3195, R:0.0100, T:45.6448(w:0.011)⚠️)
Batch 500/537: Loss=1.7436 (C:1.2285, R:0.0099, T:45.6976(w:0.011)⚠️)
Batch 525/537: Loss=1.8100 (C:1.2992, R:0.0100, T:45.3158(w:0.011)⚠️)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.7855
  Contrastive: 1.2705
  Reconstruction: 0.0100
  Topological: 45.6947 (weight: 0.011)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4844
  Contrastive: 1.1820
  Reconstruction: 0.0100
  Topological: 26.7960 (weight: 0.011)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 10/50 COMPLETE (74.3s)
Train Loss: 1.7855 (C:1.2705, R:0.0100, T:45.6947)
Val Loss:   1.4844 (C:1.1820, R:0.0100, T:26.7960)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 537 | Topological Weight: 0.0125
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.8549 (C:1.2786, R:0.0100, T:46.0258(w:0.013)⚠️)
Batch  25/537: Loss=1.8540 (C:1.2819, R:0.0099, T:45.6842(w:0.013)⚠️)
Batch  50/537: Loss=1.8640 (C:1.2920, R:0.0100, T:45.6746(w:0.013)⚠️)
Batch  75/537: Loss=1.8477 (C:1.2735, R:0.0100, T:45.8511(w:0.013)⚠️)
Batch 100/537: Loss=1.8106 (C:1.2356, R:0.0100, T:45.9172(w:0.013)⚠️)
Batch 125/537: Loss=1.7806 (C:1.2117, R:0.0099, T:45.4318(w:0.013)⚠️)
Batch 150/537: Loss=1.8583 (C:1.2901, R:0.0100, T:45.3805(w:0.013)⚠️)
Batch 175/537: Loss=1.8868 (C:1.3215, R:0.0099, T:45.1444(w:0.013)⚠️)
Batch 200/537: Loss=1.8047 (C:1.2397, R:0.0099, T:45.1178(w:0.013)⚠️)
Batch 225/537: Loss=1.8103 (C:1.2391, R:0.0099, T:45.6213(w:0.013)⚠️)
Batch 250/537: Loss=1.8102 (C:1.2389, R:0.0099, T:45.6285(w:0.013)⚠️)
Batch 275/537: Loss=1.8878 (C:1.3200, R:0.0099, T:45.3441(w:0.013)⚠️)
Batch 300/537: Loss=1.7907 (C:1.2143, R:0.0100, T:46.0380(w:0.013)⚠️)
Batch 325/537: Loss=1.8305 (C:1.2604, R:0.0099, T:45.5297(w:0.013)⚠️)
Batch 350/537: Loss=1.8349 (C:1.2635, R:0.0100, T:45.6297(w:0.013)⚠️)
Batch 375/537: Loss=1.7995 (C:1.2280, R:0.0100, T:45.6398(w:0.013)⚠️)
Batch 400/537: Loss=1.8466 (C:1.2768, R:0.0100, T:45.4988(w:0.013)⚠️)
Batch 425/537: Loss=1.7775 (C:1.2084, R:0.0100, T:45.4500(w:0.013)⚠️)
Batch 450/537: Loss=1.8287 (C:1.2566, R:0.0100, T:45.6899(w:0.013)⚠️)
Batch 475/537: Loss=1.9163 (C:1.3434, R:0.0100, T:45.7540(w:0.013)⚠️)
Batch 500/537: Loss=1.8561 (C:1.2885, R:0.0099, T:45.3295(w:0.013)⚠️)
Batch 525/537: Loss=1.8291 (C:1.2605, R:0.0099, T:45.4087(w:0.013)⚠️)

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 1.8298
  Contrastive: 1.2583
  Reconstruction: 0.0100
  Topological: 45.6387 (weight: 0.013)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5366
  Contrastive: 1.1859
  Reconstruction: 0.0100
  Topological: 27.9725 (weight: 0.013)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 11/50 COMPLETE (71.8s)
Train Loss: 1.8298 (C:1.2583, R:0.0100, T:45.6387)
Val Loss:   1.5366 (C:1.1859, R:0.0100, T:27.9725)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 537 | Topological Weight: 0.0138
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.8700 (C:1.2441, R:0.0099, T:45.4449(w:0.014)⚠️)
Batch  25/537: Loss=1.8880 (C:1.2605, R:0.0099, T:45.5626(w:0.014)⚠️)
Batch  50/537: Loss=1.8987 (C:1.2725, R:0.0100, T:45.4712(w:0.014)⚠️)
Batch  75/537: Loss=1.8909 (C:1.2617, R:0.0100, T:45.6909(w:0.014)⚠️)
Batch 100/537: Loss=1.9481 (C:1.3313, R:0.0100, T:44.7840(w:0.014)⚠️)
Batch 125/537: Loss=1.8516 (C:1.2254, R:0.0100, T:45.4679(w:0.014)⚠️)
Batch 150/537: Loss=1.8851 (C:1.2639, R:0.0099, T:45.1083(w:0.014)⚠️)
Batch 175/537: Loss=1.9041 (C:1.2749, R:0.0100, T:45.6846(w:0.014)⚠️)
Batch 200/537: Loss=1.8615 (C:1.2392, R:0.0100, T:45.1855(w:0.014)⚠️)
Batch 225/537: Loss=1.9236 (C:1.2959, R:0.0099, T:45.5827(w:0.014)⚠️)
Batch 250/537: Loss=1.8952 (C:1.2686, R:0.0099, T:45.4941(w:0.014)⚠️)
Batch 275/537: Loss=1.8656 (C:1.2368, R:0.0100, T:45.6608(w:0.014)⚠️)
Batch 300/537: Loss=1.9251 (C:1.3047, R:0.0100, T:45.0450(w:0.014)⚠️)
Batch 325/537: Loss=1.8812 (C:1.2512, R:0.0100, T:45.7413(w:0.014)⚠️)
Batch 350/537: Loss=1.8118 (C:1.1832, R:0.0099, T:45.6429(w:0.014)⚠️)
Batch 375/537: Loss=1.8796 (C:1.2534, R:0.0100, T:45.4698(w:0.014)⚠️)
Batch 400/537: Loss=1.8614 (C:1.2387, R:0.0099, T:45.2183(w:0.014)⚠️)
Batch 425/537: Loss=1.9049 (C:1.2792, R:0.0100, T:45.4291(w:0.014)⚠️)
Batch 450/537: Loss=1.8504 (C:1.2198, R:0.0100, T:45.7894(w:0.014)⚠️)
Batch 475/537: Loss=1.9169 (C:1.2883, R:0.0100, T:45.6473(w:0.014)⚠️)
Batch 500/537: Loss=1.9179 (C:1.2893, R:0.0099, T:45.6426(w:0.014)⚠️)
Batch 525/537: Loss=1.8761 (C:1.2483, R:0.0100, T:45.5874(w:0.014)⚠️)
📈 New best topological loss: 45.5706

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 1.8789
  Contrastive: 1.2514
  Reconstruction: 0.0100
  Topological: 45.5706 (weight: 0.014)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5552
  Contrastive: 1.1786
  Reconstruction: 0.0100
  Topological: 27.3154 (weight: 0.014)
  Batches with topology: 537/537 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (79.2s)
Train Loss: 1.8789 (C:1.2514, R:0.0100, T:45.5706)
Val Loss:   1.5552 (C:1.1786, R:0.0100, T:27.3154)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 537 | Topological Weight: 0.0150
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9412 (C:1.2646, R:0.0099, T:45.0413(w:0.015)⚠️)
Batch  25/537: Loss=1.8585 (C:1.1670, R:0.0099, T:46.0345(w:0.015)⚠️)
Batch  50/537: Loss=1.8898 (C:1.2022, R:0.0100, T:45.7764(w:0.015)⚠️)
Batch  75/537: Loss=1.8967 (C:1.2088, R:0.0100, T:45.7973(w:0.015)⚠️)
Batch 100/537: Loss=1.9478 (C:1.2708, R:0.0099, T:45.0682(w:0.015)⚠️)
Batch 125/537: Loss=1.9194 (C:1.2319, R:0.0100, T:45.7654(w:0.015)⚠️)
Batch 150/537: Loss=1.8730 (C:1.1897, R:0.0099, T:45.4893(w:0.015)⚠️)
Batch 175/537: Loss=1.9620 (C:1.2775, R:0.0100, T:45.5675(w:0.015)⚠️)
Batch 200/537: Loss=1.8904 (C:1.2066, R:0.0099, T:45.5195(w:0.015)⚠️)
Batch 225/537: Loss=1.9290 (C:1.2448, R:0.0100, T:45.5487(w:0.015)⚠️)
Batch 250/537: Loss=1.9281 (C:1.2448, R:0.0100, T:45.4856(w:0.015)⚠️)
Batch 275/537: Loss=1.8950 (C:1.2135, R:0.0099, T:45.3675(w:0.015)⚠️)
Batch 300/537: Loss=1.8798 (C:1.1993, R:0.0100, T:45.3026(w:0.015)⚠️)
Batch 325/537: Loss=1.9163 (C:1.2390, R:0.0099, T:45.0925(w:0.015)⚠️)
Batch 350/537: Loss=1.9181 (C:1.2407, R:0.0099, T:45.0993(w:0.015)⚠️)
Batch 375/537: Loss=1.9831 (C:1.3081, R:0.0100, T:44.9344(w:0.015)⚠️)
Batch 400/537: Loss=1.9767 (C:1.2892, R:0.0100, T:45.7668(w:0.015)⚠️)
Batch 425/537: Loss=1.8599 (C:1.1727, R:0.0100, T:45.7512(w:0.015)⚠️)
Batch 450/537: Loss=1.9343 (C:1.2537, R:0.0099, T:45.3089(w:0.015)⚠️)
Batch 475/537: Loss=1.9425 (C:1.2569, R:0.0100, T:45.6427(w:0.015)⚠️)
Batch 500/537: Loss=1.8695 (C:1.1900, R:0.0099, T:45.2308(w:0.015)⚠️)
Batch 525/537: Loss=1.8505 (C:1.1686, R:0.0100, T:45.3961(w:0.015)⚠️)
📈 New best topological loss: 45.4779

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 1.9283
  Contrastive: 1.2451
  Reconstruction: 0.0100
  Topological: 45.4779 (weight: 0.015)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6303
  Contrastive: 1.1697
  Reconstruction: 0.0100
  Topological: 30.6422 (weight: 0.015)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 13/50 COMPLETE (80.0s)
Train Loss: 1.9283 (C:1.2451, R:0.0100, T:45.4779)
Val Loss:   1.6303 (C:1.1697, R:0.0100, T:30.6422)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 537 | Topological Weight: 0.0163
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9603 (C:1.2220, R:0.0099, T:45.3698(w:0.016)⚠️)
Batch  25/537: Loss=2.0227 (C:1.2947, R:0.0099, T:44.7346(w:0.016)⚠️)
Batch  50/537: Loss=1.9535 (C:1.2117, R:0.0100, T:45.5827(w:0.016)⚠️)
Batch  75/537: Loss=1.9758 (C:1.2390, R:0.0100, T:45.2814(w:0.016)⚠️)
Batch 100/537: Loss=1.9784 (C:1.2389, R:0.0100, T:45.4458(w:0.016)⚠️)
Batch 125/537: Loss=1.9469 (C:1.2048, R:0.0100, T:45.6044(w:0.016)⚠️)
Batch 150/537: Loss=2.0122 (C:1.2803, R:0.0099, T:44.9814(w:0.016)⚠️)
Batch 175/537: Loss=1.9914 (C:1.2482, R:0.0100, T:45.6760(w:0.016)⚠️)
Batch 200/537: Loss=1.9199 (C:1.1750, R:0.0100, T:45.7788(w:0.016)⚠️)
Batch 225/537: Loss=1.9772 (C:1.2483, R:0.0100, T:44.7912(w:0.016)⚠️)
Batch 250/537: Loss=2.0385 (C:1.3050, R:0.0099, T:45.0780(w:0.016)⚠️)
Batch 275/537: Loss=1.9682 (C:1.2371, R:0.0100, T:44.9258(w:0.016)⚠️)
Batch 300/537: Loss=1.9629 (C:1.2280, R:0.0100, T:45.1657(w:0.016)⚠️)
Batch 325/537: Loss=1.9872 (C:1.2505, R:0.0100, T:45.2762(w:0.016)⚠️)
Batch 350/537: Loss=2.0015 (C:1.2640, R:0.0100, T:45.3231(w:0.016)⚠️)
Batch 375/537: Loss=1.9393 (C:1.2002, R:0.0100, T:45.4224(w:0.016)⚠️)
Batch 400/537: Loss=1.9884 (C:1.2512, R:0.0100, T:45.3080(w:0.016)⚠️)
Batch 425/537: Loss=2.0391 (C:1.3100, R:0.0100, T:44.8077(w:0.016)⚠️)
Batch 450/537: Loss=2.0724 (C:1.3408, R:0.0099, T:44.9603(w:0.016)⚠️)
Batch 475/537: Loss=2.0324 (C:1.2989, R:0.0100, T:45.0776(w:0.016)⚠️)
Batch 500/537: Loss=1.9884 (C:1.2553, R:0.0100, T:45.0494(w:0.016)⚠️)
Batch 525/537: Loss=1.9297 (C:1.1949, R:0.0100, T:45.1593(w:0.016)⚠️)
📈 New best topological loss: 45.2794

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 1.9807
  Contrastive: 1.2439
  Reconstruction: 0.0100
  Topological: 45.2794 (weight: 0.016)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6893
  Contrastive: 1.1570
  Reconstruction: 0.0100
  Topological: 32.6968 (weight: 0.016)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 14/50 COMPLETE (71.5s)
Train Loss: 1.9807 (C:1.2439, R:0.0100, T:45.2794)
Val Loss:   1.6893 (C:1.1570, R:0.0100, T:32.6968)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 537 | Topological Weight: 0.0175
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0565 (C:1.2619, R:0.0099, T:45.3500(w:0.018)⚠️)
Batch  25/537: Loss=1.9941 (C:1.2014, R:0.0099, T:45.2434(w:0.018)⚠️)
Batch  50/537: Loss=1.9743 (C:1.1870, R:0.0099, T:44.9306(w:0.018)⚠️)
Batch  75/537: Loss=2.0605 (C:1.2730, R:0.0099, T:44.9421(w:0.018)⚠️)
Batch 100/537: Loss=2.0226 (C:1.2312, R:0.0099, T:45.1643(w:0.018)⚠️)
Batch 125/537: Loss=2.0941 (C:1.3083, R:0.0100, T:44.8515(w:0.018)⚠️)
Batch 150/537: Loss=2.0662 (C:1.2792, R:0.0100, T:44.9141(w:0.018)⚠️)
Batch 175/537: Loss=1.9647 (C:1.1757, R:0.0100, T:45.0278(w:0.018)⚠️)
Batch 200/537: Loss=2.0294 (C:1.2445, R:0.0099, T:44.7906(w:0.018)⚠️)
Batch 225/537: Loss=1.9643 (C:1.1726, R:0.0099, T:45.1795(w:0.018)⚠️)
Batch 250/537: Loss=1.9992 (C:1.2053, R:0.0100, T:45.3098(w:0.018)⚠️)
Batch 275/537: Loss=2.0811 (C:1.2985, R:0.0100, T:44.6663(w:0.018)⚠️)
Batch 300/537: Loss=2.0138 (C:1.2291, R:0.0100, T:44.7827(w:0.018)⚠️)
Batch 325/537: Loss=2.0480 (C:1.2638, R:0.0099, T:44.7521(w:0.018)⚠️)
Batch 350/537: Loss=2.0742 (C:1.2915, R:0.0099, T:44.6693(w:0.018)⚠️)
Batch 375/537: Loss=1.9822 (C:1.1952, R:0.0100, T:44.9122(w:0.018)⚠️)
Batch 400/537: Loss=1.9738 (C:1.1867, R:0.0100, T:44.9209(w:0.018)⚠️)
Batch 425/537: Loss=2.0539 (C:1.2712, R:0.0099, T:44.6685(w:0.018)⚠️)
Batch 450/537: Loss=1.9422 (C:1.1624, R:0.0099, T:44.5054(w:0.018)⚠️)
Batch 475/537: Loss=2.0617 (C:1.2832, R:0.0100, T:44.4278(w:0.018)⚠️)
Batch 500/537: Loss=2.0841 (C:1.3039, R:0.0100, T:44.5247(w:0.018)⚠️)
Batch 525/537: Loss=1.9749 (C:1.1874, R:0.0100, T:44.9418(w:0.018)⚠️)
📈 New best topological loss: 44.9242

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 2.0228
  Contrastive: 1.2357
  Reconstruction: 0.0100
  Topological: 44.9242 (weight: 0.018)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7613
  Contrastive: 1.1383
  Reconstruction: 0.0100
  Topological: 35.5399 (weight: 0.018)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 15/50 COMPLETE (70.4s)
Train Loss: 2.0228 (C:1.2357, R:0.0100, T:44.9242)
Val Loss:   1.7613 (C:1.1383, R:0.0100, T:35.5399)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 537 | Topological Weight: 0.0187
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0149 (C:1.1713, R:0.0100, T:44.9391(w:0.019)⚠️)
Batch  25/537: Loss=2.0613 (C:1.2217, R:0.0100, T:44.7209(w:0.019)⚠️)
Batch  50/537: Loss=2.0653 (C:1.2304, R:0.0100, T:44.4713(w:0.019)⚠️)
Batch  75/537: Loss=2.1173 (C:1.2899, R:0.0099, T:44.0748(w:0.019)⚠️)
Batch 100/537: Loss=2.0543 (C:1.2206, R:0.0100, T:44.4130(w:0.019)⚠️)
Batch 125/537: Loss=2.0763 (C:1.2477, R:0.0100, T:44.1407(w:0.019)⚠️)
Batch 150/537: Loss=2.0061 (C:1.1812, R:0.0099, T:43.9447(w:0.019)⚠️)
Batch 175/537: Loss=2.0120 (C:1.1918, R:0.0099, T:43.6922(w:0.019)⚠️)
Batch 200/537: Loss=2.0301 (C:1.2068, R:0.0099, T:43.8544(w:0.019)⚠️)
Batch 225/537: Loss=2.0222 (C:1.2013, R:0.0099, T:43.7308(w:0.019)⚠️)
Batch 250/537: Loss=2.0322 (C:1.2197, R:0.0099, T:43.2818(w:0.019)⚠️)
Batch 275/537: Loss=2.0124 (C:1.1851, R:0.0100, T:44.0700(w:0.019)⚠️)
Batch 300/537: Loss=2.0716 (C:1.2428, R:0.0100, T:44.1528(w:0.019)⚠️)
Batch 325/537: Loss=2.0468 (C:1.2360, R:0.0100, T:43.1861(w:0.019)⚠️)
Batch 350/537: Loss=2.0113 (C:1.1959, R:0.0099, T:43.4339(w:0.019)⚠️)
Batch 375/537: Loss=1.9670 (C:1.1527, R:0.0100, T:43.3760(w:0.019)⚠️)
Batch 400/537: Loss=2.0114 (C:1.2057, R:0.0099, T:42.9162(w:0.019)⚠️)
Batch 425/537: Loss=2.0507 (C:1.2365, R:0.0100, T:43.3720(w:0.019)⚠️)
Batch 450/537: Loss=1.9987 (C:1.1964, R:0.0100, T:42.7363(w:0.019)⚠️)
Batch 475/537: Loss=2.0006 (C:1.1987, R:0.0099, T:42.7169(w:0.019)⚠️)
Batch 500/537: Loss=2.0175 (C:1.2139, R:0.0100, T:42.8031(w:0.019)⚠️)
Batch 525/537: Loss=2.0292 (C:1.2364, R:0.0100, T:42.2299(w:0.019)⚠️)
📈 New best topological loss: 43.7056

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 2.0357
  Contrastive: 1.2152
  Reconstruction: 0.0100
  Topological: 43.7056 (weight: 0.019)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7403
  Contrastive: 1.0812
  Reconstruction: 0.0100
  Topological: 35.0982 (weight: 0.019)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 16/50 COMPLETE (69.4s)
Train Loss: 2.0357 (C:1.2152, R:0.0100, T:43.7056)
Val Loss:   1.7403 (C:1.0812, R:0.0100, T:35.0982)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 537 | Topological Weight: 0.0200
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0818 (C:1.2241, R:0.0099, T:42.8383(w:0.020)⚠️)
Batch  25/537: Loss=2.0541 (C:1.2067, R:0.0099, T:42.3228(w:0.020)⚠️)
Batch  50/537: Loss=2.0534 (C:1.2106, R:0.0099, T:42.0921(w:0.020)⚠️)
Batch  75/537: Loss=2.0537 (C:1.2008, R:0.0100, T:42.5924(w:0.020)⚠️)
Batch 100/537: Loss=2.0362 (C:1.1807, R:0.0100, T:42.7282(w:0.020)⚠️)
Batch 125/537: Loss=2.0375 (C:1.1838, R:0.0100, T:42.6340(w:0.020)⚠️)
Batch 150/537: Loss=2.0741 (C:1.2301, R:0.0100, T:42.1531(w:0.020)⚠️)
Batch 175/537: Loss=2.0234 (C:1.1694, R:0.0100, T:42.6466(w:0.020)⚠️)
Batch 200/537: Loss=2.0859 (C:1.2367, R:0.0099, T:42.4099(w:0.020)⚠️)
Batch 225/537: Loss=2.0356 (C:1.1847, R:0.0099, T:42.4944(w:0.020)⚠️)
Batch 250/537: Loss=2.0646 (C:1.2119, R:0.0099, T:42.5838(w:0.020)⚠️)
Batch 275/537: Loss=2.0329 (C:1.1780, R:0.0100, T:42.6955(w:0.020)⚠️)
Batch 300/537: Loss=2.0352 (C:1.1875, R:0.0099, T:42.3365(w:0.020)⚠️)
Batch 325/537: Loss=1.9896 (C:1.1290, R:0.0100, T:42.9766(w:0.020)⚠️)
Batch 350/537: Loss=2.0442 (C:1.1902, R:0.0099, T:42.6512(w:0.020)⚠️)
Batch 375/537: Loss=1.9748 (C:1.1045, R:0.0099, T:43.4643(w:0.020)⚠️)
Batch 400/537: Loss=2.0486 (C:1.1928, R:0.0100, T:42.7417(w:0.020)⚠️)
Batch 425/537: Loss=2.0213 (C:1.1537, R:0.0100, T:43.3312(w:0.020)⚠️)
Batch 450/537: Loss=2.0620 (C:1.1967, R:0.0100, T:43.2132(w:0.020)⚠️)
Batch 475/537: Loss=1.9709 (C:1.1074, R:0.0100, T:43.1270(w:0.020)⚠️)
Batch 500/537: Loss=1.9794 (C:1.1229, R:0.0100, T:42.7750(w:0.020)⚠️)
Batch 525/537: Loss=2.0004 (C:1.1283, R:0.0100, T:43.5569(w:0.020)⚠️)
📈 New best topological loss: 42.7743

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 2.0250
  Contrastive: 1.1685
  Reconstruction: 0.0100
  Topological: 42.7743 (weight: 0.020)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6538
  Contrastive: 1.0071
  Reconstruction: 0.0100
  Topological: 32.2852 (weight: 0.020)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 17/50 COMPLETE (71.5s)
Train Loss: 2.0250 (C:1.1685, R:0.0100, T:42.7743)
Val Loss:   1.6538 (C:1.0071, R:0.0100, T:32.2852)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 537 | Topological Weight: 0.0213
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9980 (C:1.0772, R:0.0099, T:43.2839(w:0.021)⚠️)
Batch  25/537: Loss=2.0420 (C:1.1220, R:0.0099, T:43.2516(w:0.021)⚠️)
Batch  50/537: Loss=2.0148 (C:1.0943, R:0.0100, T:43.2693(w:0.021)⚠️)
Batch  75/537: Loss=2.0314 (C:1.1195, R:0.0099, T:42.8689(w:0.021)⚠️)
Batch 100/537: Loss=2.0533 (C:1.1391, R:0.0099, T:42.9772(w:0.021)⚠️)
Batch 125/537: Loss=2.0420 (C:1.1285, R:0.0099, T:42.9397(w:0.021)⚠️)
Batch 150/537: Loss=2.0010 (C:1.0749, R:0.0100, T:43.5360(w:0.021)⚠️)
Batch 175/537: Loss=2.0797 (C:1.1665, R:0.0099, T:42.9270(w:0.021)⚠️)
Batch 200/537: Loss=2.0265 (C:1.0948, R:0.0100, T:43.7987(w:0.021)⚠️)
Batch 225/537: Loss=2.0850 (C:1.1816, R:0.0100, T:42.4638(w:0.021)⚠️)
Batch 250/537: Loss=2.0812 (C:1.1661, R:0.0099, T:43.0169(w:0.021)⚠️)
Batch 275/537: Loss=2.0201 (C:1.1020, R:0.0099, T:43.1547(w:0.021)⚠️)
Batch 300/537: Loss=2.0223 (C:1.1120, R:0.0099, T:42.7945(w:0.021)⚠️)
Batch 325/537: Loss=2.0017 (C:1.0752, R:0.0100, T:43.5542(w:0.021)⚠️)
Batch 350/537: Loss=2.0620 (C:1.1410, R:0.0100, T:43.2980(w:0.021)⚠️)
Batch 375/537: Loss=1.9893 (C:1.0714, R:0.0100, T:43.1492(w:0.021)⚠️)
Batch 400/537: Loss=2.0701 (C:1.1429, R:0.0100, T:43.5855(w:0.021)⚠️)
Batch 425/537: Loss=2.0259 (C:1.0968, R:0.0100, T:43.6727(w:0.021)⚠️)
Batch 450/537: Loss=2.0087 (C:1.0831, R:0.0100, T:43.5115(w:0.021)⚠️)
Batch 475/537: Loss=2.1869 (C:1.2882, R:0.0099, T:42.2419(w:0.021)⚠️)
Batch 500/537: Loss=2.0587 (C:1.1453, R:0.0100, T:42.9366(w:0.021)⚠️)
Batch 525/537: Loss=1.9975 (C:1.0709, R:0.0100, T:43.5586(w:0.021)⚠️)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 2.0468
  Contrastive: 1.1305
  Reconstruction: 0.0100
  Topological: 43.0750 (weight: 0.021)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6778
  Contrastive: 0.9726
  Reconstruction: 0.0100
  Topological: 33.1410 (weight: 0.021)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 18/50 COMPLETE (86.8s)
Train Loss: 2.0468 (C:1.1305, R:0.0100, T:43.0750)
Val Loss:   1.6778 (C:0.9726, R:0.0100, T:33.1410)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 537 | Topological Weight: 0.0225
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1264 (C:1.1523, R:0.0099, T:43.2510(w:0.022)⚠️)
Batch  25/537: Loss=2.0884 (C:1.1217, R:0.0099, T:42.9189(w:0.022)⚠️)
Batch  50/537: Loss=2.0087 (C:1.0255, R:0.0100, T:43.6537(w:0.022)⚠️)
Batch  75/537: Loss=2.0137 (C:1.0325, R:0.0100, T:43.5667(w:0.022)⚠️)
Batch 100/537: Loss=2.1201 (C:1.1503, R:0.0100, T:43.0554(w:0.022)⚠️)
Batch 125/537: Loss=2.0207 (C:1.0483, R:0.0100, T:43.1739(w:0.022)⚠️)
Batch 150/537: Loss=2.1091 (C:1.1368, R:0.0100, T:43.1723(w:0.022)⚠️)
Batch 175/537: Loss=2.0568 (C:1.0801, R:0.0100, T:43.3664(w:0.022)⚠️)
Batch 200/537: Loss=2.0591 (C:1.0958, R:0.0099, T:42.7729(w:0.022)⚠️)
Batch 225/537: Loss=2.0542 (C:1.0825, R:0.0100, T:43.1423(w:0.022)⚠️)
Batch 250/537: Loss=2.1348 (C:1.1748, R:0.0100, T:42.6199(w:0.022)⚠️)
Batch 275/537: Loss=2.1078 (C:1.1388, R:0.0099, T:43.0247(w:0.022)⚠️)
Batch 300/537: Loss=2.0850 (C:1.1120, R:0.0100, T:43.2003(w:0.022)⚠️)
Batch 325/537: Loss=2.0782 (C:1.1084, R:0.0099, T:43.0581(w:0.022)⚠️)
Batch 350/537: Loss=2.1652 (C:1.1971, R:0.0100, T:42.9855(w:0.022)⚠️)
Batch 375/537: Loss=2.0359 (C:1.0574, R:0.0099, T:43.4443(w:0.022)⚠️)
Batch 400/537: Loss=2.0774 (C:1.1160, R:0.0099, T:42.6810(w:0.022)⚠️)
Batch 425/537: Loss=2.0434 (C:1.0661, R:0.0100, T:43.3915(w:0.022)⚠️)
Batch 450/537: Loss=2.0353 (C:1.0645, R:0.0099, T:43.1040(w:0.022)⚠️)
Batch 475/537: Loss=2.0733 (C:1.1024, R:0.0100, T:43.1072(w:0.022)⚠️)
Batch 500/537: Loss=2.1121 (C:1.1419, R:0.0100, T:43.0742(w:0.022)⚠️)
Batch 525/537: Loss=2.0553 (C:1.0902, R:0.0099, T:42.8514(w:0.022)⚠️)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 2.0815
  Contrastive: 1.1101
  Reconstruction: 0.0100
  Topological: 43.1322 (weight: 0.022)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7385
  Contrastive: 0.9596
  Reconstruction: 0.0100
  Topological: 34.5734 (weight: 0.022)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 19/50 COMPLETE (83.9s)
Train Loss: 2.0815 (C:1.1101, R:0.0100, T:43.1322)
Val Loss:   1.7385 (C:0.9596, R:0.0100, T:34.5734)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 537 | Topological Weight: 0.0238
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1347 (C:1.1010, R:0.0100, T:43.4813(w:0.024)⚠️)
Batch  25/537: Loss=2.0257 (C:0.9956, R:0.0100, T:43.3283(w:0.024)⚠️)
Batch  50/537: Loss=2.1278 (C:1.1034, R:0.0100, T:43.0877(w:0.024)⚠️)
Batch  75/537: Loss=2.0871 (C:1.0461, R:0.0100, T:43.7909(w:0.024)⚠️)
Batch 100/537: Loss=2.1946 (C:1.1811, R:0.0099, T:42.6326(w:0.024)⚠️)
Batch 125/537: Loss=2.0856 (C:1.0601, R:0.0100, T:43.1349(w:0.024)⚠️)
Batch 150/537: Loss=2.0676 (C:1.0247, R:0.0099, T:43.8717(w:0.024)⚠️)
Batch 175/537: Loss=2.0771 (C:1.0449, R:0.0100, T:43.4206(w:0.024)⚠️)
Batch 200/537: Loss=2.1782 (C:1.1663, R:0.0099, T:42.5607(w:0.024)⚠️)
Batch 225/537: Loss=2.1221 (C:1.0906, R:0.0099, T:43.3876(w:0.024)⚠️)
Batch 250/537: Loss=2.1897 (C:1.1578, R:0.0099, T:43.4074(w:0.024)⚠️)
Batch 275/537: Loss=2.0628 (C:1.0381, R:0.0100, T:43.1023(w:0.024)⚠️)
Batch 300/537: Loss=2.1067 (C:1.0699, R:0.0100, T:43.6139(w:0.024)⚠️)
Batch 325/537: Loss=2.1001 (C:1.0676, R:0.0099, T:43.4324(w:0.024)⚠️)
Batch 350/537: Loss=2.0641 (C:1.0440, R:0.0100, T:42.9088(w:0.024)⚠️)
Batch 375/537: Loss=2.1782 (C:1.1671, R:0.0099, T:42.5304(w:0.024)⚠️)
Batch 400/537: Loss=2.1181 (C:1.0897, R:0.0100, T:43.2601(w:0.024)⚠️)
Batch 425/537: Loss=2.0513 (C:1.0239, R:0.0099, T:43.2142(w:0.024)⚠️)
Batch 450/537: Loss=2.1573 (C:1.1372, R:0.0100, T:42.9102(w:0.024)⚠️)
Batch 475/537: Loss=2.1241 (C:1.0979, R:0.0099, T:43.1658(w:0.024)⚠️)
Batch 500/537: Loss=2.0969 (C:1.0818, R:0.0099, T:42.6963(w:0.024)⚠️)
Batch 525/537: Loss=2.1600 (C:1.1345, R:0.0100, T:43.1354(w:0.024)⚠️)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 2.1172
  Contrastive: 1.0917
  Reconstruction: 0.0100
  Topological: 43.1356 (weight: 0.024)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7861
  Contrastive: 0.9365
  Reconstruction: 0.0100
  Topological: 35.7307 (weight: 0.024)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 20/50 COMPLETE (86.1s)
Train Loss: 2.1172 (C:1.0917, R:0.0100, T:43.1356)
Val Loss:   1.7861 (C:0.9365, R:0.0100, T:35.7307)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 537 | Topological Weight: 0.0250
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1203 (C:1.0383, R:0.0100, T:43.2385(w:0.025)⚠️)
Batch  25/537: Loss=2.1344 (C:1.0497, R:0.0099, T:43.3464(w:0.025)⚠️)
Batch  50/537: Loss=2.1530 (C:1.0707, R:0.0100, T:43.2497(w:0.025)⚠️)
Batch  75/537: Loss=2.1914 (C:1.1058, R:0.0100, T:43.3858(w:0.025)⚠️)
Batch 100/537: Loss=2.1282 (C:1.0517, R:0.0099, T:43.0214(w:0.025)⚠️)
Batch 125/537: Loss=2.1324 (C:1.0522, R:0.0099, T:43.1681(w:0.025)⚠️)
Batch 150/537: Loss=2.1841 (C:1.1056, R:0.0100, T:43.1024(w:0.025)⚠️)
Batch 175/537: Loss=2.1169 (C:1.0360, R:0.0099, T:43.1961(w:0.025)⚠️)
Batch 200/537: Loss=2.2186 (C:1.1353, R:0.0100, T:43.2914(w:0.025)⚠️)
Batch 225/537: Loss=2.1603 (C:1.0828, R:0.0100, T:43.0603(w:0.025)⚠️)
Batch 250/537: Loss=2.1202 (C:1.0337, R:0.0099, T:43.4217(w:0.025)⚠️)
Batch 275/537: Loss=2.1333 (C:1.0563, R:0.0099, T:43.0426(w:0.025)⚠️)
Batch 300/537: Loss=2.2300 (C:1.1521, R:0.0100, T:43.0759(w:0.025)⚠️)
Batch 325/537: Loss=2.1382 (C:1.0650, R:0.0099, T:42.8906(w:0.025)⚠️)
Batch 350/537: Loss=2.1527 (C:1.0638, R:0.0100, T:43.5141(w:0.025)⚠️)
Batch 375/537: Loss=2.1217 (C:1.0347, R:0.0100, T:43.4400(w:0.025)⚠️)
Batch 400/537: Loss=2.1389 (C:1.0476, R:0.0100, T:43.6141(w:0.025)⚠️)
Batch 425/537: Loss=2.1395 (C:1.0487, R:0.0100, T:43.5918(w:0.025)⚠️)
Batch 450/537: Loss=2.1481 (C:1.0775, R:0.0099, T:42.7844(w:0.025)⚠️)
Batch 475/537: Loss=2.2136 (C:1.1521, R:0.0099, T:42.4202(w:0.025)⚠️)
Batch 500/537: Loss=2.1023 (C:1.0185, R:0.0100, T:43.3155(w:0.025)⚠️)
Batch 525/537: Loss=2.1207 (C:1.0363, R:0.0099, T:43.3351(w:0.025)⚠️)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 2.1560
  Contrastive: 1.0765
  Reconstruction: 0.0100
  Topological: 43.1378 (weight: 0.025)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7399
  Contrastive: 0.9136
  Reconstruction: 0.0100
  Topological: 33.0118 (weight: 0.025)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 21/50 COMPLETE (71.1s)
Train Loss: 2.1560 (C:1.0765, R:0.0100, T:43.1378)
Val Loss:   1.7399 (C:0.9136, R:0.0100, T:33.0118)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 537 | Topological Weight: 0.0262
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1918 (C:1.0493, R:0.0099, T:43.4881(w:0.026)⚠️)
Batch  25/537: Loss=2.1699 (C:1.0295, R:0.0100, T:43.4084(w:0.026)⚠️)
Batch  50/537: Loss=2.2003 (C:1.0676, R:0.0100, T:43.1136(w:0.026)⚠️)
Batch  75/537: Loss=2.2205 (C:1.0812, R:0.0100, T:43.3619(w:0.026)⚠️)
Batch 100/537: Loss=2.1919 (C:1.0531, R:0.0100, T:43.3452(w:0.026)⚠️)
Batch 125/537: Loss=2.2395 (C:1.1111, R:0.0099, T:42.9497(w:0.026)⚠️)
Batch 150/537: Loss=2.1618 (C:1.0317, R:0.0099, T:43.0142(w:0.026)⚠️)
Batch 175/537: Loss=2.1085 (C:0.9697, R:0.0099, T:43.3455(w:0.026)⚠️)
Batch 200/537: Loss=2.2319 (C:1.0942, R:0.0100, T:43.3022(w:0.026)⚠️)
Batch 225/537: Loss=2.2373 (C:1.1205, R:0.0099, T:42.5090(w:0.026)⚠️)
Batch 250/537: Loss=2.1097 (C:0.9688, R:0.0099, T:43.4243(w:0.026)⚠️)
Batch 275/537: Loss=2.2131 (C:1.0865, R:0.0100, T:42.8825(w:0.026)⚠️)
Batch 300/537: Loss=2.2284 (C:1.0950, R:0.0100, T:43.1402(w:0.026)⚠️)
Batch 325/537: Loss=2.1965 (C:1.0719, R:0.0100, T:42.8058(w:0.026)⚠️)
Batch 350/537: Loss=2.3076 (C:1.1872, R:0.0099, T:42.6461(w:0.026)⚠️)
Batch 375/537: Loss=2.1912 (C:1.0503, R:0.0100, T:43.4251(w:0.026)⚠️)
Batch 400/537: Loss=2.2084 (C:1.0827, R:0.0100, T:42.8460(w:0.026)⚠️)
Batch 425/537: Loss=2.1564 (C:1.0229, R:0.0099, T:43.1416(w:0.026)⚠️)
Batch 450/537: Loss=2.1831 (C:1.0579, R:0.0099, T:42.8275(w:0.026)⚠️)
Batch 475/537: Loss=2.1864 (C:1.0558, R:0.0100, T:43.0315(w:0.026)⚠️)
Batch 500/537: Loss=2.2383 (C:1.1052, R:0.0100, T:43.1306(w:0.026)⚠️)
Batch 525/537: Loss=2.1473 (C:1.0188, R:0.0099, T:42.9528(w:0.026)⚠️)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 2.1946
  Contrastive: 1.0636
  Reconstruction: 0.0100
  Topological: 43.0497 (weight: 0.026)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7711
  Contrastive: 0.8995
  Reconstruction: 0.0100
  Topological: 33.1665 (weight: 0.026)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 22/50 COMPLETE (71.0s)
Train Loss: 2.1946 (C:1.0636, R:0.0100, T:43.0497)
Val Loss:   1.7711 (C:0.8995, R:0.0100, T:33.1665)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

🛑 Early stopping triggered after 22 epochs
Best model was at epoch 12 with Val Loss: 1.5552

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 9
Epochs with topology: 14/22
Max consecutive topology epochs: 14
Best topological loss: 42.7743
Final topological loss: 43.0497
✅ SUCCESS: Topological learning achieved!
👍 GOOD: Fairly consistent topological learning (>50%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_151438/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/539 batches
  Processed 51/539 batches
  Processed 101/539 batches
  Processed 151/539 batches
  Processed 201/539 batches
  Processed 251/539 batches
  Processed 301/539 batches
  Processed 351/539 batches
  Processed 401/539 batches
  Processed 451/539 batches
  Processed 501/539 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: 0.2061
  Adjusted Rand Score: 0.3617
  Clustering Accuracy: 0.6202
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.6650
  Per-class F1: [0.6447887323943662, 0.5200124107973938, 0.8320421444065697]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009953
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 0.401 ± 0.768
  Negative distances: 1.279 ± 0.973
  Separation ratio: 3.19x
  Gap: -2.178
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.2061
  Clustering Accuracy: 0.6202
  Adjusted Rand Score: 0.3617

Classification Performance:
  Accuracy: 0.6650

Separation Quality:
  Separation Ratio: 3.19x
  Gap: -2.178
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009953
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_151438/results/evaluation_results_20250721_153717.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_151438/results/evaluation_results_20250721_153717.json

Key Results:
  Separation ratio: 3.19x
  Perfect separation: False
  Classification accuracy: 0.6650

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 22
  Epochs with topological learning: 14
  Current topological loss: 43.0497
  Current topological weight: 0.0262
  ✅ Topological loss is decreasing (good progress)
✅ GOOD: Reasonable topological learning
Final topological loss: 43.0497
Epochs with topology: 14/22
⚠️  Poor clustering accuracy: 0.620

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_151438/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_151438

Analysis completed with exit code: 0
Time: Mon 21 Jul 15:37:19 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
