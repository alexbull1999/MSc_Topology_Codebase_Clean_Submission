Starting Surface Distance Metric Analysis job...
Job ID: 182700
Node: gpuvm19
Time: Tue 15 Jul 15:14:42 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Tue Jul 15 15:14:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   33C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-15 15:14:56.525404
Using device: cuda

Configuration:
  Embedding type: cosine_concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 200
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'cosine_concat'
Output dimension will be: 1537
GlobalDataLoader initialized:
  Embedding type: cosine_concat
  Output dimension: 1537
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating cosine_concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated cosine_concat embeddings: torch.Size([549367, 1537])
Generating embeddings for validation...
Generating cosine_concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9842, 1537])
Generating embeddings for test...
Generating cosine_concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9824, 1537])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1537])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1537])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1537])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1537
Updated model input_dim to: 1537
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1537
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,877,580
Model created with 1,877,580 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,877,580
Starting training...
========================================
Starting Global Dataset Training...
============================================================

üåç Updating global dataset at epoch 1
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.090 ¬± 0.010
    Neg distances: 0.090 ¬± 0.010
    Separation ratio: 1.00x
    Gap: -0.120
    ‚ùå Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9997 (C:1.9997, R:0.0117)
Batch  25/537: Loss=1.9950 (C:1.9950, R:0.0115)
Batch  50/537: Loss=1.9736 (C:1.9736, R:0.0113)
Batch  75/537: Loss=1.9690 (C:1.9690, R:0.0112)
Batch 100/537: Loss=1.9500 (C:1.9500, R:0.0111)
Batch 125/537: Loss=1.9404 (C:1.9404, R:0.0110)
Batch 150/537: Loss=1.9241 (C:1.9241, R:0.0109)
Batch 175/537: Loss=1.9180 (C:1.9180, R:0.0109)
Batch 200/537: Loss=1.9139 (C:1.9139, R:0.0109)
Batch 225/537: Loss=1.9162 (C:1.9162, R:0.0108)
Batch 250/537: Loss=1.9066 (C:1.9066, R:0.0108)
Batch 275/537: Loss=1.8979 (C:1.8979, R:0.0108)
Batch 300/537: Loss=1.9051 (C:1.9051, R:0.0108)
Batch 325/537: Loss=1.9071 (C:1.9071, R:0.0108)
Batch 350/537: Loss=1.9156 (C:1.9156, R:0.0107)
Batch 375/537: Loss=1.9069 (C:1.9069, R:0.0108)
Batch 400/537: Loss=1.8929 (C:1.8929, R:0.0107)
Batch 425/537: Loss=1.9019 (C:1.9019, R:0.0107)
Batch 450/537: Loss=1.8966 (C:1.8966, R:0.0107)
Batch 475/537: Loss=1.8929 (C:1.8929, R:0.0107)
Batch 500/537: Loss=1.8975 (C:1.8975, R:0.0107)
Batch 525/537: Loss=1.8853 (C:1.8853, R:0.0108)

============================================================
Epoch 1/200 completed in 40.0s
Train: Loss=1.9219 (C:1.9219, R:0.0109) Ratio=1.76x
Val:   Loss=1.8911 (C:1.8911, R:0.0106) Ratio=2.19x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8911)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.8867 (C:1.8867, R:0.0107)
Batch  25/537: Loss=1.8855 (C:1.8855, R:0.0107)
Batch  50/537: Loss=1.8936 (C:1.8936, R:0.0107)
Batch  75/537: Loss=1.8919 (C:1.8919, R:0.0108)
Batch 100/537: Loss=1.8967 (C:1.8967, R:0.0107)
Batch 125/537: Loss=1.8915 (C:1.8915, R:0.0107)
Batch 150/537: Loss=1.8951 (C:1.8951, R:0.0108)
Batch 175/537: Loss=1.8943 (C:1.8943, R:0.0107)
Batch 200/537: Loss=1.8912 (C:1.8912, R:0.0107)
Batch 225/537: Loss=1.8926 (C:1.8926, R:0.0107)
Batch 250/537: Loss=1.8947 (C:1.8947, R:0.0107)
Batch 275/537: Loss=1.8885 (C:1.8885, R:0.0107)
Batch 300/537: Loss=1.8946 (C:1.8946, R:0.0107)
Batch 325/537: Loss=1.8747 (C:1.8747, R:0.0107)
Batch 350/537: Loss=1.8722 (C:1.8722, R:0.0107)
Batch 375/537: Loss=1.8830 (C:1.8830, R:0.0107)
Batch 400/537: Loss=1.8860 (C:1.8860, R:0.0107)
Batch 425/537: Loss=1.8819 (C:1.8819, R:0.0107)
Batch 450/537: Loss=1.8986 (C:1.8986, R:0.0107)
Batch 475/537: Loss=1.8742 (C:1.8742, R:0.0107)
Batch 500/537: Loss=1.8870 (C:1.8870, R:0.0107)
Batch 525/537: Loss=1.8891 (C:1.8891, R:0.0107)

============================================================
Epoch 2/200 completed in 22.3s
Train: Loss=1.8873 (C:1.8873, R:0.0107) Ratio=2.20x
Val:   Loss=1.8751 (C:1.8751, R:0.0106) Ratio=2.42x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8751)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8816 (C:1.8816, R:0.0108)
Batch  25/537: Loss=1.8828 (C:1.8828, R:0.0107)
Batch  50/537: Loss=1.8839 (C:1.8839, R:0.0107)
Batch  75/537: Loss=1.8783 (C:1.8783, R:0.0107)
Batch 100/537: Loss=1.8757 (C:1.8757, R:0.0107)
Batch 125/537: Loss=1.8799 (C:1.8799, R:0.0107)
Batch 150/537: Loss=1.8779 (C:1.8779, R:0.0107)
Batch 175/537: Loss=1.8791 (C:1.8791, R:0.0107)
Batch 200/537: Loss=1.8775 (C:1.8775, R:0.0107)
Batch 225/537: Loss=1.8754 (C:1.8754, R:0.0107)
Batch 250/537: Loss=1.8691 (C:1.8691, R:0.0107)
Batch 275/537: Loss=1.8696 (C:1.8696, R:0.0107)
Batch 300/537: Loss=1.8744 (C:1.8744, R:0.0108)
Batch 325/537: Loss=1.8752 (C:1.8752, R:0.0107)
Batch 350/537: Loss=1.8673 (C:1.8673, R:0.0107)
Batch 375/537: Loss=1.8725 (C:1.8725, R:0.0108)
Batch 400/537: Loss=1.8793 (C:1.8793, R:0.0107)
Batch 425/537: Loss=1.8796 (C:1.8796, R:0.0107)
Batch 450/537: Loss=1.8712 (C:1.8712, R:0.0107)
Batch 475/537: Loss=1.8814 (C:1.8814, R:0.0107)
Batch 500/537: Loss=1.8795 (C:1.8795, R:0.0107)
Batch 525/537: Loss=1.8805 (C:1.8805, R:0.0107)

============================================================
Epoch 3/200 completed in 21.5s
Train: Loss=1.8766 (C:1.8766, R:0.0107) Ratio=2.40x
Val:   Loss=1.8717 (C:1.8717, R:0.0106) Ratio=2.52x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8717)
============================================================

üåç Updating global dataset at epoch 4
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.611 ¬± 0.596
    Neg distances: 1.637 ¬± 0.893
    Separation ratio: 2.68x
    Gap: -3.672
    ‚úÖ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.2013 (C:1.2013, R:0.0107)
Batch  25/537: Loss=1.2459 (C:1.2459, R:0.0107)
Batch  50/537: Loss=1.2206 (C:1.2206, R:0.0107)
Batch  75/537: Loss=1.2301 (C:1.2301, R:0.0108)
Batch 100/537: Loss=1.2539 (C:1.2539, R:0.0107)
Batch 125/537: Loss=1.1995 (C:1.1995, R:0.0107)
Batch 150/537: Loss=1.2002 (C:1.2002, R:0.0108)
Batch 175/537: Loss=1.1623 (C:1.1623, R:0.0107)
Batch 200/537: Loss=1.2081 (C:1.2081, R:0.0107)
Batch 225/537: Loss=1.2297 (C:1.2297, R:0.0107)
Batch 250/537: Loss=1.2350 (C:1.2350, R:0.0107)
Batch 275/537: Loss=1.2378 (C:1.2378, R:0.0107)
Batch 300/537: Loss=1.2252 (C:1.2252, R:0.0107)
Batch 325/537: Loss=1.2125 (C:1.2125, R:0.0107)
Batch 350/537: Loss=1.2105 (C:1.2105, R:0.0107)
Batch 375/537: Loss=1.2217 (C:1.2217, R:0.0107)
Batch 400/537: Loss=1.2067 (C:1.2067, R:0.0107)
Batch 425/537: Loss=1.2317 (C:1.2317, R:0.0107)
Batch 450/537: Loss=1.2225 (C:1.2225, R:0.0107)
Batch 475/537: Loss=1.2300 (C:1.2300, R:0.0107)
Batch 500/537: Loss=1.2160 (C:1.2160, R:0.0107)
Batch 525/537: Loss=1.2143 (C:1.2143, R:0.0107)

============================================================
Epoch 4/200 completed in 28.5s
Train: Loss=1.2173 (C:1.2173, R:0.0107) Ratio=2.52x
Val:   Loss=1.1942 (C:1.1942, R:0.0106) Ratio=2.67x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1942)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.1410 (C:1.1410, R:0.0107)
Batch  25/537: Loss=1.2616 (C:1.2616, R:0.0107)
Batch  50/537: Loss=1.2238 (C:1.2238, R:0.0107)
Batch  75/537: Loss=1.2131 (C:1.2131, R:0.0107)
Batch 100/537: Loss=1.2221 (C:1.2221, R:0.0107)
Batch 125/537: Loss=1.1997 (C:1.1997, R:0.0107)
Batch 150/537: Loss=1.2036 (C:1.2036, R:0.0107)
Batch 175/537: Loss=1.2153 (C:1.2153, R:0.0107)
Batch 200/537: Loss=1.1783 (C:1.1783, R:0.0107)
Batch 225/537: Loss=1.1958 (C:1.1958, R:0.0107)
Batch 250/537: Loss=1.2016 (C:1.2016, R:0.0107)
Batch 275/537: Loss=1.2051 (C:1.2051, R:0.0107)
Batch 300/537: Loss=1.1669 (C:1.1669, R:0.0107)
Batch 325/537: Loss=1.2025 (C:1.2025, R:0.0107)
Batch 350/537: Loss=1.2285 (C:1.2285, R:0.0106)
Batch 375/537: Loss=1.1963 (C:1.1963, R:0.0107)
Batch 400/537: Loss=1.1930 (C:1.1930, R:0.0107)
Batch 425/537: Loss=1.1719 (C:1.1719, R:0.0107)
Batch 450/537: Loss=1.1966 (C:1.1966, R:0.0107)
Batch 475/537: Loss=1.1856 (C:1.1856, R:0.0107)
Batch 500/537: Loss=1.2192 (C:1.2192, R:0.0107)
Batch 525/537: Loss=1.2020 (C:1.2020, R:0.0107)

============================================================
Epoch 5/200 completed in 21.3s
Train: Loss=1.1941 (C:1.1941, R:0.0107) Ratio=2.70x
Val:   Loss=1.1852 (C:1.1852, R:0.0106) Ratio=2.74x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1852)
Checkpoint saved at epoch 5
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.1637 (C:1.1637, R:0.0107)
Batch  25/537: Loss=1.1684 (C:1.1684, R:0.0107)
Batch  50/537: Loss=1.1859 (C:1.1859, R:0.0107)
Batch  75/537: Loss=1.1709 (C:1.1709, R:0.0107)
Batch 100/537: Loss=1.2000 (C:1.2000, R:0.0108)
Batch 125/537: Loss=1.2056 (C:1.2056, R:0.0107)
Batch 150/537: Loss=1.1885 (C:1.1885, R:0.0107)
Batch 175/537: Loss=1.1658 (C:1.1658, R:0.0107)
Batch 200/537: Loss=1.1357 (C:1.1357, R:0.0107)
Batch 225/537: Loss=1.2065 (C:1.2065, R:0.0107)
Batch 250/537: Loss=1.1813 (C:1.1813, R:0.0107)
Batch 275/537: Loss=1.1826 (C:1.1826, R:0.0107)
Batch 300/537: Loss=1.1767 (C:1.1767, R:0.0107)
Batch 325/537: Loss=1.1820 (C:1.1820, R:0.0107)
Batch 350/537: Loss=1.1489 (C:1.1489, R:0.0107)
Batch 375/537: Loss=1.1794 (C:1.1794, R:0.0107)
Batch 400/537: Loss=1.1775 (C:1.1775, R:0.0107)
Batch 425/537: Loss=1.1891 (C:1.1891, R:0.0107)
Batch 450/537: Loss=1.2158 (C:1.2158, R:0.0107)
Batch 475/537: Loss=1.1536 (C:1.1536, R:0.0107)
Batch 500/537: Loss=1.2045 (C:1.2045, R:0.0107)
Batch 525/537: Loss=1.1841 (C:1.1841, R:0.0107)

============================================================
Epoch 6/200 completed in 21.3s
Train: Loss=1.1813 (C:1.1813, R:0.0107) Ratio=2.87x
Val:   Loss=1.1838 (C:1.1838, R:0.0106) Ratio=2.84x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1838)
============================================================

üåç Updating global dataset at epoch 7
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.539 ¬± 0.599
    Neg distances: 1.738 ¬± 0.901
    Separation ratio: 3.23x
    Gap: -3.470
    ‚úÖ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.0857 (C:1.0857, R:0.0107)
Batch  25/537: Loss=1.0871 (C:1.0871, R:0.0107)
Batch  50/537: Loss=1.1069 (C:1.1069, R:0.0107)
Batch  75/537: Loss=1.0728 (C:1.0728, R:0.0107)
Batch 100/537: Loss=1.1082 (C:1.1082, R:0.0107)
Batch 125/537: Loss=1.1150 (C:1.1150, R:0.0107)
Batch 150/537: Loss=1.0763 (C:1.0763, R:0.0107)
Batch 175/537: Loss=1.1033 (C:1.1033, R:0.0107)
Batch 200/537: Loss=1.0937 (C:1.0937, R:0.0107)
Batch 225/537: Loss=1.0738 (C:1.0738, R:0.0108)
Batch 250/537: Loss=1.0952 (C:1.0952, R:0.0107)
Batch 275/537: Loss=1.0588 (C:1.0588, R:0.0107)
Batch 300/537: Loss=1.1229 (C:1.1229, R:0.0107)
Batch 325/537: Loss=1.0934 (C:1.0934, R:0.0107)
Batch 350/537: Loss=1.0799 (C:1.0799, R:0.0107)
Batch 375/537: Loss=1.0936 (C:1.0936, R:0.0107)
Batch 400/537: Loss=1.0764 (C:1.0764, R:0.0107)
Batch 425/537: Loss=1.1329 (C:1.1329, R:0.0107)
Batch 450/537: Loss=1.1164 (C:1.1164, R:0.0107)
Batch 475/537: Loss=1.0930 (C:1.0930, R:0.0107)
Batch 500/537: Loss=1.0933 (C:1.0933, R:0.0107)
Batch 525/537: Loss=1.1072 (C:1.1072, R:0.0107)

============================================================
Epoch 7/200 completed in 27.1s
Train: Loss=1.0937 (C:1.0937, R:0.0107) Ratio=2.96x
Val:   Loss=1.1106 (C:1.1106, R:0.0106) Ratio=2.92x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1106)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.0761 (C:1.0761, R:0.0108)
Batch  25/537: Loss=1.0538 (C:1.0538, R:0.0107)
Batch  50/537: Loss=1.0813 (C:1.0813, R:0.0107)
Batch  75/537: Loss=1.0909 (C:1.0909, R:0.0107)
Batch 100/537: Loss=1.1064 (C:1.1064, R:0.0107)
Batch 125/537: Loss=1.1056 (C:1.1056, R:0.0107)
Batch 150/537: Loss=1.0504 (C:1.0504, R:0.0107)
Batch 175/537: Loss=1.0690 (C:1.0690, R:0.0107)
Batch 200/537: Loss=1.0967 (C:1.0967, R:0.0107)
Batch 225/537: Loss=1.1253 (C:1.1253, R:0.0107)
Batch 250/537: Loss=1.0919 (C:1.0919, R:0.0107)
Batch 275/537: Loss=1.1031 (C:1.1031, R:0.0107)
Batch 300/537: Loss=1.0829 (C:1.0829, R:0.0107)
Batch 325/537: Loss=1.0968 (C:1.0968, R:0.0107)
Batch 350/537: Loss=1.0548 (C:1.0548, R:0.0107)
Batch 375/537: Loss=1.0863 (C:1.0863, R:0.0107)
Batch 400/537: Loss=1.0904 (C:1.0904, R:0.0107)
Batch 425/537: Loss=1.0843 (C:1.0843, R:0.0107)
Batch 450/537: Loss=1.0980 (C:1.0980, R:0.0107)
Batch 475/537: Loss=1.0798 (C:1.0798, R:0.0107)
Batch 500/537: Loss=1.0427 (C:1.0427, R:0.0107)
Batch 525/537: Loss=1.1028 (C:1.1028, R:0.0107)

============================================================
Epoch 8/200 completed in 21.3s
Train: Loss=1.0843 (C:1.0843, R:0.0107) Ratio=3.04x
Val:   Loss=1.0995 (C:1.0995, R:0.0106) Ratio=2.94x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0995)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0815 (C:1.0815, R:0.0107)
Batch  25/537: Loss=1.0638 (C:1.0638, R:0.0107)
Batch  50/537: Loss=1.0568 (C:1.0568, R:0.0107)
Batch  75/537: Loss=1.0836 (C:1.0836, R:0.0107)
Batch 100/537: Loss=1.0450 (C:1.0450, R:0.0107)
Batch 125/537: Loss=1.0818 (C:1.0818, R:0.0107)
Batch 150/537: Loss=1.0751 (C:1.0751, R:0.0107)
Batch 175/537: Loss=1.0562 (C:1.0562, R:0.0107)
Batch 200/537: Loss=1.0615 (C:1.0615, R:0.0107)
Batch 225/537: Loss=1.0675 (C:1.0675, R:0.0107)
Batch 250/537: Loss=1.0762 (C:1.0762, R:0.0107)
Batch 275/537: Loss=1.0779 (C:1.0779, R:0.0107)
Batch 300/537: Loss=1.0729 (C:1.0729, R:0.0107)
Batch 325/537: Loss=1.0576 (C:1.0576, R:0.0108)
Batch 350/537: Loss=1.0738 (C:1.0738, R:0.0107)
Batch 375/537: Loss=1.0971 (C:1.0971, R:0.0107)
Batch 400/537: Loss=1.0800 (C:1.0800, R:0.0107)
Batch 425/537: Loss=1.0934 (C:1.0934, R:0.0107)
Batch 450/537: Loss=1.0544 (C:1.0544, R:0.0107)
Batch 475/537: Loss=1.1128 (C:1.1128, R:0.0107)
Batch 500/537: Loss=1.0846 (C:1.0846, R:0.0107)
Batch 525/537: Loss=1.0826 (C:1.0826, R:0.0107)

============================================================
Epoch 9/200 completed in 21.4s
Train: Loss=1.0770 (C:1.0770, R:0.0107) Ratio=3.14x
Val:   Loss=1.1024 (C:1.1024, R:0.0106) Ratio=2.96x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 10
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.493 ¬± 0.585
    Neg distances: 1.789 ¬± 0.894
    Separation ratio: 3.63x
    Gap: -3.261
    ‚úÖ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.0060 (C:1.0060, R:0.0107)
Batch  25/537: Loss=1.0169 (C:1.0169, R:0.0107)
Batch  50/537: Loss=0.9997 (C:0.9997, R:0.0107)
Batch  75/537: Loss=1.0191 (C:1.0191, R:0.0107)
Batch 100/537: Loss=1.0433 (C:1.0433, R:0.0107)
Batch 125/537: Loss=1.0336 (C:1.0336, R:0.0107)
Batch 150/537: Loss=1.0020 (C:1.0020, R:0.0107)
Batch 175/537: Loss=1.0218 (C:1.0218, R:0.0107)
Batch 200/537: Loss=1.0233 (C:1.0233, R:0.0107)
Batch 225/537: Loss=1.0189 (C:1.0189, R:0.0107)
Batch 250/537: Loss=1.0323 (C:1.0323, R:0.0107)
Batch 275/537: Loss=0.9998 (C:0.9998, R:0.0107)
Batch 300/537: Loss=1.0024 (C:1.0024, R:0.0107)
Batch 325/537: Loss=1.0179 (C:1.0179, R:0.0108)
Batch 350/537: Loss=1.0097 (C:1.0097, R:0.0107)
Batch 375/537: Loss=1.0415 (C:1.0415, R:0.0107)
Batch 400/537: Loss=1.0055 (C:1.0055, R:0.0107)
Batch 425/537: Loss=1.0423 (C:1.0423, R:0.0107)
Batch 450/537: Loss=1.0051 (C:1.0051, R:0.0107)
Batch 475/537: Loss=1.0094 (C:1.0094, R:0.0107)
Batch 500/537: Loss=1.0429 (C:1.0429, R:0.0107)
Batch 525/537: Loss=1.0236 (C:1.0236, R:0.0107)

============================================================
Epoch 10/200 completed in 27.3s
Train: Loss=1.0243 (C:1.0243, R:0.0107) Ratio=3.22x
Val:   Loss=1.0534 (C:1.0534, R:0.0106) Ratio=2.98x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0534)
Checkpoint saved at epoch 10
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.0142 (C:1.0142, R:0.0107)
Batch  25/537: Loss=1.0011 (C:1.0011, R:0.0107)
Batch  50/537: Loss=1.0258 (C:1.0258, R:0.0108)
Batch  75/537: Loss=1.0141 (C:1.0141, R:0.0107)
Batch 100/537: Loss=1.0123 (C:1.0123, R:0.0107)
Batch 125/537: Loss=1.0070 (C:1.0070, R:0.0107)
Batch 150/537: Loss=1.0295 (C:1.0295, R:0.0107)
Batch 175/537: Loss=1.0218 (C:1.0218, R:0.0107)
Batch 200/537: Loss=1.0352 (C:1.0352, R:0.0107)
Batch 225/537: Loss=1.0032 (C:1.0032, R:0.0107)
Batch 250/537: Loss=1.0169 (C:1.0169, R:0.0107)
Batch 275/537: Loss=0.9871 (C:0.9871, R:0.0108)
Batch 300/537: Loss=1.0370 (C:1.0370, R:0.0108)
Batch 325/537: Loss=1.0538 (C:1.0538, R:0.0107)
Batch 350/537: Loss=1.0359 (C:1.0359, R:0.0107)
Batch 375/537: Loss=0.9975 (C:0.9975, R:0.0107)
Batch 400/537: Loss=1.0213 (C:1.0213, R:0.0107)
Batch 425/537: Loss=1.0106 (C:1.0106, R:0.0107)
Batch 450/537: Loss=1.0313 (C:1.0313, R:0.0107)
Batch 475/537: Loss=1.0677 (C:1.0677, R:0.0107)
Batch 500/537: Loss=1.0181 (C:1.0181, R:0.0107)
Batch 525/537: Loss=0.9951 (C:0.9951, R:0.0107)

============================================================
Epoch 11/200 completed in 21.4s
Train: Loss=1.0162 (C:1.0162, R:0.0107) Ratio=3.29x
Val:   Loss=1.0480 (C:1.0480, R:0.0106) Ratio=3.00x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0480)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.0125 (C:1.0125, R:0.0107)
Batch  25/537: Loss=0.9949 (C:0.9949, R:0.0107)
Batch  50/537: Loss=1.0109 (C:1.0109, R:0.0107)
Batch  75/537: Loss=1.0072 (C:1.0072, R:0.0107)
Batch 100/537: Loss=0.9975 (C:0.9975, R:0.0107)
Batch 125/537: Loss=1.0104 (C:1.0104, R:0.0107)
Batch 150/537: Loss=1.0184 (C:1.0184, R:0.0107)
Batch 175/537: Loss=1.0268 (C:1.0268, R:0.0107)
Batch 200/537: Loss=1.0131 (C:1.0131, R:0.0107)
Batch 225/537: Loss=1.0330 (C:1.0330, R:0.0107)
Batch 250/537: Loss=1.0283 (C:1.0283, R:0.0107)
Batch 275/537: Loss=1.0237 (C:1.0237, R:0.0107)
Batch 300/537: Loss=0.9758 (C:0.9758, R:0.0107)
Batch 325/537: Loss=1.0030 (C:1.0030, R:0.0107)
Batch 350/537: Loss=1.0245 (C:1.0245, R:0.0107)
Batch 375/537: Loss=1.0178 (C:1.0178, R:0.0107)
Batch 400/537: Loss=0.9754 (C:0.9754, R:0.0107)
Batch 425/537: Loss=1.0026 (C:1.0026, R:0.0107)
Batch 450/537: Loss=1.0467 (C:1.0467, R:0.0107)
Batch 475/537: Loss=1.0371 (C:1.0371, R:0.0107)
Batch 500/537: Loss=0.9902 (C:0.9902, R:0.0107)
Batch 525/537: Loss=0.9944 (C:0.9944, R:0.0108)

============================================================
Epoch 12/200 completed in 21.5s
Train: Loss=1.0107 (C:1.0107, R:0.0107) Ratio=3.30x
Val:   Loss=1.0448 (C:1.0448, R:0.0106) Ratio=3.00x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0448)
============================================================

üåç Updating global dataset at epoch 13
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.446 ¬± 0.549
    Neg distances: 1.882 ¬± 0.897
    Separation ratio: 4.22x
    Gap: -3.360
    ‚úÖ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9567 (C:0.9567, R:0.0107)
Batch  25/537: Loss=0.9305 (C:0.9305, R:0.0107)
Batch  50/537: Loss=0.9141 (C:0.9141, R:0.0107)
Batch  75/537: Loss=0.9515 (C:0.9515, R:0.0107)
Batch 100/537: Loss=0.9660 (C:0.9660, R:0.0107)
Batch 125/537: Loss=0.9428 (C:0.9428, R:0.0107)
Batch 150/537: Loss=0.9322 (C:0.9322, R:0.0107)
Batch 175/537: Loss=0.9579 (C:0.9579, R:0.0107)
Batch 200/537: Loss=0.9575 (C:0.9575, R:0.0107)
Batch 225/537: Loss=0.9580 (C:0.9580, R:0.0107)
Batch 250/537: Loss=0.9200 (C:0.9200, R:0.0107)
Batch 275/537: Loss=0.9332 (C:0.9332, R:0.0107)
Batch 300/537: Loss=0.9572 (C:0.9572, R:0.0107)
Batch 325/537: Loss=0.9711 (C:0.9711, R:0.0107)
Batch 350/537: Loss=0.9499 (C:0.9499, R:0.0107)
Batch 375/537: Loss=0.9387 (C:0.9387, R:0.0107)
Batch 400/537: Loss=0.9091 (C:0.9091, R:0.0107)
Batch 425/537: Loss=0.9422 (C:0.9422, R:0.0108)
Batch 450/537: Loss=0.9371 (C:0.9371, R:0.0107)
Batch 475/537: Loss=0.9320 (C:0.9320, R:0.0107)
Batch 500/537: Loss=0.9330 (C:0.9330, R:0.0107)
Batch 525/537: Loss=0.9961 (C:0.9961, R:0.0107)

============================================================
Epoch 13/200 completed in 27.5s
Train: Loss=0.9448 (C:0.9448, R:0.0107) Ratio=3.41x
Val:   Loss=0.9844 (C:0.9844, R:0.0106) Ratio=3.01x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9844)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9555 (C:0.9555, R:0.0107)
Batch  25/537: Loss=0.9315 (C:0.9315, R:0.0107)
Batch  50/537: Loss=0.9399 (C:0.9399, R:0.0107)
Batch  75/537: Loss=0.9448 (C:0.9448, R:0.0107)
Batch 100/537: Loss=0.9055 (C:0.9055, R:0.0107)
Batch 125/537: Loss=0.9372 (C:0.9372, R:0.0107)
Batch 150/537: Loss=0.9243 (C:0.9243, R:0.0107)
Batch 175/537: Loss=0.9537 (C:0.9537, R:0.0107)
Batch 200/537: Loss=0.9718 (C:0.9718, R:0.0107)
Batch 225/537: Loss=0.9226 (C:0.9226, R:0.0107)
Batch 250/537: Loss=0.9548 (C:0.9548, R:0.0107)
Batch 275/537: Loss=0.9112 (C:0.9112, R:0.0107)
Batch 300/537: Loss=0.8961 (C:0.8961, R:0.0107)
Batch 325/537: Loss=0.9802 (C:0.9802, R:0.0107)
Batch 350/537: Loss=0.9596 (C:0.9596, R:0.0107)
Batch 375/537: Loss=0.9257 (C:0.9257, R:0.0107)
Batch 400/537: Loss=0.9168 (C:0.9168, R:0.0107)
Batch 425/537: Loss=0.9310 (C:0.9310, R:0.0107)
Batch 450/537: Loss=0.9833 (C:0.9833, R:0.0107)
Batch 475/537: Loss=0.9546 (C:0.9546, R:0.0107)
Batch 500/537: Loss=0.9662 (C:0.9662, R:0.0107)
Batch 525/537: Loss=0.9409 (C:0.9409, R:0.0107)

============================================================
Epoch 14/200 completed in 21.4s
Train: Loss=0.9400 (C:0.9400, R:0.0107) Ratio=3.45x
Val:   Loss=0.9762 (C:0.9762, R:0.0106) Ratio=3.03x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9762)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.9450 (C:0.9450, R:0.0107)
Batch  25/537: Loss=0.9345 (C:0.9345, R:0.0108)
Batch  50/537: Loss=0.9228 (C:0.9228, R:0.0107)
Batch  75/537: Loss=0.9309 (C:0.9309, R:0.0107)
Batch 100/537: Loss=0.8876 (C:0.8876, R:0.0107)
Batch 125/537: Loss=0.9438 (C:0.9438, R:0.0107)
Batch 150/537: Loss=0.9324 (C:0.9324, R:0.0107)
Batch 175/537: Loss=0.9130 (C:0.9130, R:0.0107)
Batch 200/537: Loss=0.9705 (C:0.9705, R:0.0107)
Batch 225/537: Loss=0.9269 (C:0.9269, R:0.0107)
Batch 250/537: Loss=0.9476 (C:0.9476, R:0.0107)
Batch 275/537: Loss=0.9235 (C:0.9235, R:0.0107)
Batch 300/537: Loss=0.9647 (C:0.9647, R:0.0107)
Batch 325/537: Loss=0.9308 (C:0.9308, R:0.0107)
Batch 350/537: Loss=0.9243 (C:0.9243, R:0.0107)
Batch 375/537: Loss=0.9188 (C:0.9188, R:0.0107)
Batch 400/537: Loss=0.9357 (C:0.9357, R:0.0107)
Batch 425/537: Loss=0.9548 (C:0.9548, R:0.0107)
Batch 450/537: Loss=0.9220 (C:0.9220, R:0.0107)
Batch 475/537: Loss=0.9353 (C:0.9353, R:0.0107)
Batch 500/537: Loss=0.9605 (C:0.9605, R:0.0107)
Batch 525/537: Loss=0.9281 (C:0.9281, R:0.0107)

============================================================
Epoch 15/200 completed in 21.6s
Train: Loss=0.9352 (C:0.9352, R:0.0107) Ratio=3.57x
Val:   Loss=0.9768 (C:0.9768, R:0.0106) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 15
============================================================

üåç Updating global dataset at epoch 16
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.460 ¬± 0.593
    Neg distances: 1.962 ¬± 0.933
    Separation ratio: 4.26x
    Gap: -3.552
    ‚úÖ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.9268 (C:0.9268, R:0.0107)
Batch  25/537: Loss=0.9413 (C:0.9413, R:0.0107)
Batch  50/537: Loss=0.9221 (C:0.9221, R:0.0107)
Batch  75/537: Loss=0.9021 (C:0.9021, R:0.0107)
Batch 100/537: Loss=0.9458 (C:0.9458, R:0.0107)
Batch 125/537: Loss=0.9144 (C:0.9144, R:0.0107)
Batch 150/537: Loss=0.9044 (C:0.9044, R:0.0107)
Batch 175/537: Loss=0.9002 (C:0.9002, R:0.0107)
Batch 200/537: Loss=0.9114 (C:0.9114, R:0.0107)
Batch 225/537: Loss=0.9159 (C:0.9159, R:0.0107)
Batch 250/537: Loss=0.9590 (C:0.9590, R:0.0107)
Batch 275/537: Loss=0.8978 (C:0.8978, R:0.0107)
Batch 300/537: Loss=0.9091 (C:0.9091, R:0.0107)
Batch 325/537: Loss=0.9085 (C:0.9085, R:0.0107)
Batch 350/537: Loss=0.9283 (C:0.9283, R:0.0107)
Batch 375/537: Loss=0.9292 (C:0.9292, R:0.0107)
Batch 400/537: Loss=0.9156 (C:0.9156, R:0.0106)
Batch 425/537: Loss=0.9005 (C:0.9005, R:0.0107)
Batch 450/537: Loss=0.9247 (C:0.9247, R:0.0107)
Batch 475/537: Loss=0.9294 (C:0.9294, R:0.0107)
Batch 500/537: Loss=0.9322 (C:0.9322, R:0.0107)
Batch 525/537: Loss=0.9213 (C:0.9213, R:0.0107)

============================================================
Epoch 16/200 completed in 28.7s
Train: Loss=0.9192 (C:0.9192, R:0.0107) Ratio=3.56x
Val:   Loss=0.9700 (C:0.9700, R:0.0106) Ratio=3.04x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9700)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.8966 (C:0.8966, R:0.0107)
Batch  25/537: Loss=0.9085 (C:0.9085, R:0.0107)
Batch  50/537: Loss=0.8945 (C:0.8945, R:0.0107)
Batch  75/537: Loss=0.8839 (C:0.8839, R:0.0107)
Batch 100/537: Loss=0.9057 (C:0.9057, R:0.0107)
Batch 125/537: Loss=0.9547 (C:0.9547, R:0.0107)
Batch 150/537: Loss=0.9128 (C:0.9128, R:0.0107)
Batch 175/537: Loss=0.9485 (C:0.9485, R:0.0107)
Batch 200/537: Loss=0.9051 (C:0.9051, R:0.0108)
Batch 225/537: Loss=0.9088 (C:0.9088, R:0.0107)
Batch 250/537: Loss=0.9310 (C:0.9310, R:0.0108)
Batch 275/537: Loss=0.9302 (C:0.9302, R:0.0107)
Batch 300/537: Loss=0.9479 (C:0.9479, R:0.0107)
Batch 325/537: Loss=0.8810 (C:0.8810, R:0.0107)
Batch 350/537: Loss=0.9525 (C:0.9525, R:0.0107)
Batch 375/537: Loss=0.9257 (C:0.9257, R:0.0107)
Batch 400/537: Loss=0.9327 (C:0.9327, R:0.0107)
Batch 425/537: Loss=0.9099 (C:0.9099, R:0.0107)
Batch 450/537: Loss=0.9190 (C:0.9190, R:0.0107)
Batch 475/537: Loss=0.9152 (C:0.9152, R:0.0107)
Batch 500/537: Loss=0.8958 (C:0.8958, R:0.0107)
Batch 525/537: Loss=0.9191 (C:0.9191, R:0.0107)

============================================================
Epoch 17/200 completed in 21.7s
Train: Loss=0.9131 (C:0.9131, R:0.0107) Ratio=3.58x
Val:   Loss=0.9774 (C:0.9774, R:0.0106) Ratio=3.05x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.8848 (C:0.8848, R:0.0107)
Batch  25/537: Loss=0.9027 (C:0.9027, R:0.0107)
Batch  50/537: Loss=0.8937 (C:0.8937, R:0.0107)
Batch  75/537: Loss=0.9036 (C:0.9036, R:0.0107)
Batch 100/537: Loss=0.9118 (C:0.9118, R:0.0107)
Batch 125/537: Loss=0.8936 (C:0.8936, R:0.0107)
Batch 150/537: Loss=0.8853 (C:0.8853, R:0.0107)
Batch 175/537: Loss=0.9167 (C:0.9167, R:0.0107)
Batch 200/537: Loss=0.9381 (C:0.9381, R:0.0107)
Batch 225/537: Loss=0.9247 (C:0.9247, R:0.0107)
Batch 250/537: Loss=0.9188 (C:0.9188, R:0.0107)
Batch 275/537: Loss=0.9087 (C:0.9087, R:0.0107)
Batch 300/537: Loss=0.9225 (C:0.9225, R:0.0107)
Batch 325/537: Loss=0.9249 (C:0.9249, R:0.0107)
Batch 350/537: Loss=0.9103 (C:0.9103, R:0.0107)
Batch 375/537: Loss=0.8973 (C:0.8973, R:0.0107)
Batch 400/537: Loss=0.9221 (C:0.9221, R:0.0107)
Batch 425/537: Loss=0.9035 (C:0.9035, R:0.0107)
Batch 450/537: Loss=0.8968 (C:0.8968, R:0.0107)
Batch 475/537: Loss=0.9047 (C:0.9047, R:0.0107)
Batch 500/537: Loss=0.9042 (C:0.9042, R:0.0107)
Batch 525/537: Loss=0.8947 (C:0.8947, R:0.0107)

============================================================
Epoch 18/200 completed in 21.8s
Train: Loss=0.9094 (C:0.9094, R:0.0107) Ratio=3.74x
Val:   Loss=0.9668 (C:0.9668, R:0.0106) Ratio=3.08x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9668)
============================================================

üåç Updating global dataset at epoch 19
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.443 ¬± 0.583
    Neg distances: 2.062 ¬± 0.957
    Separation ratio: 4.65x
    Gap: -3.723
    ‚úÖ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.8707 (C:0.8707, R:0.0107)
Batch  25/537: Loss=0.8706 (C:0.8706, R:0.0107)
Batch  50/537: Loss=0.8676 (C:0.8676, R:0.0107)
Batch  75/537: Loss=0.8494 (C:0.8494, R:0.0107)
Batch 100/537: Loss=0.8697 (C:0.8697, R:0.0107)
Batch 125/537: Loss=0.8278 (C:0.8278, R:0.0107)
Batch 150/537: Loss=0.8820 (C:0.8820, R:0.0107)
Batch 175/537: Loss=0.8487 (C:0.8487, R:0.0107)
Batch 200/537: Loss=0.8690 (C:0.8690, R:0.0107)
Batch 225/537: Loss=0.8664 (C:0.8664, R:0.0107)
Batch 250/537: Loss=0.8388 (C:0.8388, R:0.0107)
Batch 275/537: Loss=0.8761 (C:0.8761, R:0.0107)
Batch 300/537: Loss=0.8846 (C:0.8846, R:0.0107)
Batch 325/537: Loss=0.8316 (C:0.8316, R:0.0107)
Batch 350/537: Loss=0.8769 (C:0.8769, R:0.0107)
Batch 375/537: Loss=0.8542 (C:0.8542, R:0.0108)
Batch 400/537: Loss=0.8717 (C:0.8717, R:0.0108)
Batch 425/537: Loss=0.8338 (C:0.8338, R:0.0107)
Batch 450/537: Loss=0.8699 (C:0.8699, R:0.0107)
Batch 475/537: Loss=0.8630 (C:0.8630, R:0.0107)
Batch 500/537: Loss=0.8445 (C:0.8445, R:0.0107)
Batch 525/537: Loss=0.8632 (C:0.8632, R:0.0107)

============================================================
Epoch 19/200 completed in 28.7s
Train: Loss=0.8646 (C:0.8646, R:0.0107) Ratio=3.76x
Val:   Loss=0.9214 (C:0.9214, R:0.0106) Ratio=3.09x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9214)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.8621 (C:0.8621, R:0.0107)
Batch  25/537: Loss=0.8518 (C:0.8518, R:0.0107)
Batch  50/537: Loss=0.8665 (C:0.8665, R:0.0107)
Batch  75/537: Loss=0.8419 (C:0.8419, R:0.0107)
Batch 100/537: Loss=0.8504 (C:0.8504, R:0.0107)
Batch 125/537: Loss=0.8475 (C:0.8475, R:0.0107)
Batch 150/537: Loss=0.8821 (C:0.8821, R:0.0107)
Batch 175/537: Loss=0.8554 (C:0.8554, R:0.0107)
Batch 200/537: Loss=0.8383 (C:0.8383, R:0.0107)
Batch 225/537: Loss=0.8797 (C:0.8797, R:0.0107)
Batch 250/537: Loss=0.8792 (C:0.8792, R:0.0107)
Batch 275/537: Loss=0.8667 (C:0.8667, R:0.0107)
Batch 300/537: Loss=0.8870 (C:0.8870, R:0.0107)
Batch 325/537: Loss=0.8879 (C:0.8879, R:0.0107)
Batch 350/537: Loss=0.8717 (C:0.8717, R:0.0108)
Batch 375/537: Loss=0.8727 (C:0.8727, R:0.0107)
Batch 400/537: Loss=0.8553 (C:0.8553, R:0.0108)
Batch 425/537: Loss=0.8341 (C:0.8341, R:0.0107)
Batch 450/537: Loss=0.8259 (C:0.8259, R:0.0107)
Batch 475/537: Loss=0.8720 (C:0.8720, R:0.0107)
Batch 500/537: Loss=0.8321 (C:0.8321, R:0.0108)
Batch 525/537: Loss=0.8704 (C:0.8704, R:0.0107)

============================================================
Epoch 20/200 completed in 21.6s
Train: Loss=0.8597 (C:0.8597, R:0.0107) Ratio=3.80x
Val:   Loss=0.9239 (C:0.9239, R:0.0106) Ratio=3.11x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8668 (C:0.8668, R:0.0107)
Batch  25/537: Loss=0.8334 (C:0.8334, R:0.0107)
Batch  50/537: Loss=0.8555 (C:0.8555, R:0.0107)
Batch  75/537: Loss=0.8422 (C:0.8422, R:0.0107)
Batch 100/537: Loss=0.8402 (C:0.8402, R:0.0107)
Batch 125/537: Loss=0.8689 (C:0.8689, R:0.0107)
Batch 150/537: Loss=0.8542 (C:0.8542, R:0.0107)
Batch 175/537: Loss=0.8481 (C:0.8481, R:0.0107)
Batch 200/537: Loss=0.8412 (C:0.8412, R:0.0107)
Batch 225/537: Loss=0.8629 (C:0.8629, R:0.0107)
Batch 250/537: Loss=0.8170 (C:0.8170, R:0.0107)
Batch 275/537: Loss=0.8608 (C:0.8608, R:0.0107)
Batch 300/537: Loss=0.8738 (C:0.8738, R:0.0107)
Batch 325/537: Loss=0.9034 (C:0.9034, R:0.0107)
Batch 350/537: Loss=0.8670 (C:0.8670, R:0.0107)
Batch 375/537: Loss=0.8777 (C:0.8777, R:0.0107)
Batch 400/537: Loss=0.8455 (C:0.8455, R:0.0107)
Batch 425/537: Loss=0.8702 (C:0.8702, R:0.0107)
Batch 450/537: Loss=0.8605 (C:0.8605, R:0.0107)
Batch 475/537: Loss=0.8617 (C:0.8617, R:0.0107)
Batch 500/537: Loss=0.8643 (C:0.8643, R:0.0107)
Batch 525/537: Loss=0.8631 (C:0.8631, R:0.0107)

============================================================
Epoch 21/200 completed in 21.6s
Train: Loss=0.8561 (C:0.8561, R:0.0107) Ratio=3.81x
Val:   Loss=0.9255 (C:0.9255, R:0.0106) Ratio=3.09x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 22
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.423 ¬± 0.598
    Neg distances: 2.173 ¬± 0.985
    Separation ratio: 5.14x
    Gap: -3.830
    ‚úÖ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8239 (C:0.8239, R:0.0107)
Batch  25/537: Loss=0.7780 (C:0.7780, R:0.0107)
Batch  50/537: Loss=0.8102 (C:0.8102, R:0.0107)
Batch  75/537: Loss=0.7960 (C:0.7960, R:0.0107)
Batch 100/537: Loss=0.8469 (C:0.8469, R:0.0107)
Batch 125/537: Loss=0.8234 (C:0.8234, R:0.0107)
Batch 150/537: Loss=0.8022 (C:0.8022, R:0.0107)
Batch 175/537: Loss=0.7730 (C:0.7730, R:0.0107)
Batch 200/537: Loss=0.7922 (C:0.7922, R:0.0107)
Batch 225/537: Loss=0.7891 (C:0.7891, R:0.0107)
Batch 250/537: Loss=0.8007 (C:0.8007, R:0.0107)
Batch 275/537: Loss=0.8199 (C:0.8199, R:0.0107)
Batch 300/537: Loss=0.8231 (C:0.8231, R:0.0107)
Batch 325/537: Loss=0.8520 (C:0.8520, R:0.0107)
Batch 350/537: Loss=0.7646 (C:0.7646, R:0.0107)
Batch 375/537: Loss=0.8546 (C:0.8546, R:0.0107)
Batch 400/537: Loss=0.8186 (C:0.8186, R:0.0107)
Batch 425/537: Loss=0.8296 (C:0.8296, R:0.0107)
Batch 450/537: Loss=0.8017 (C:0.8017, R:0.0107)
Batch 475/537: Loss=0.8191 (C:0.8191, R:0.0107)
Batch 500/537: Loss=0.8029 (C:0.8029, R:0.0107)
Batch 525/537: Loss=0.8317 (C:0.8317, R:0.0107)

============================================================
Epoch 22/200 completed in 28.0s
Train: Loss=0.8096 (C:0.8096, R:0.0107) Ratio=3.82x
Val:   Loss=0.8805 (C:0.8805, R:0.0106) Ratio=3.07x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8805)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.7797 (C:0.7797, R:0.0107)
Batch  25/537: Loss=0.8098 (C:0.8098, R:0.0107)
Batch  50/537: Loss=0.7778 (C:0.7778, R:0.0108)
Batch  75/537: Loss=0.8199 (C:0.8199, R:0.0107)
Batch 100/537: Loss=0.7887 (C:0.7887, R:0.0107)
Batch 125/537: Loss=0.8244 (C:0.8244, R:0.0107)
Batch 150/537: Loss=0.7844 (C:0.7844, R:0.0107)
Batch 175/537: Loss=0.7972 (C:0.7972, R:0.0107)
Batch 200/537: Loss=0.8261 (C:0.8261, R:0.0107)
Batch 225/537: Loss=0.7781 (C:0.7781, R:0.0107)
Batch 250/537: Loss=0.7539 (C:0.7539, R:0.0107)
Batch 275/537: Loss=0.8401 (C:0.8401, R:0.0107)
Batch 300/537: Loss=0.8015 (C:0.8015, R:0.0107)
Batch 325/537: Loss=0.8145 (C:0.8145, R:0.0107)
Batch 350/537: Loss=0.7857 (C:0.7857, R:0.0107)
Batch 375/537: Loss=0.8337 (C:0.8337, R:0.0107)
Batch 400/537: Loss=0.8281 (C:0.8281, R:0.0107)
Batch 425/537: Loss=0.8293 (C:0.8293, R:0.0107)
Batch 450/537: Loss=0.7834 (C:0.7834, R:0.0107)
Batch 475/537: Loss=0.8091 (C:0.8091, R:0.0107)
Batch 500/537: Loss=0.7707 (C:0.7707, R:0.0107)
Batch 525/537: Loss=0.8183 (C:0.8183, R:0.0107)

============================================================
Epoch 23/200 completed in 22.1s
Train: Loss=0.8038 (C:0.8038, R:0.0107) Ratio=3.92x
Val:   Loss=0.8890 (C:0.8890, R:0.0106) Ratio=3.13x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8131 (C:0.8131, R:0.0107)
Batch  25/537: Loss=0.8150 (C:0.8150, R:0.0107)
Batch  50/537: Loss=0.8159 (C:0.8159, R:0.0107)
Batch  75/537: Loss=0.8081 (C:0.8081, R:0.0107)
Batch 100/537: Loss=0.8084 (C:0.8084, R:0.0107)
Batch 125/537: Loss=0.7707 (C:0.7707, R:0.0107)
Batch 150/537: Loss=0.8091 (C:0.8091, R:0.0107)
Batch 175/537: Loss=0.7902 (C:0.7902, R:0.0107)
Batch 200/537: Loss=0.7935 (C:0.7935, R:0.0107)
Batch 225/537: Loss=0.7881 (C:0.7881, R:0.0107)
Batch 250/537: Loss=0.7508 (C:0.7508, R:0.0107)
Batch 275/537: Loss=0.7932 (C:0.7932, R:0.0107)
Batch 300/537: Loss=0.7652 (C:0.7652, R:0.0107)
Batch 325/537: Loss=0.8041 (C:0.8041, R:0.0107)
Batch 350/537: Loss=0.8252 (C:0.8252, R:0.0107)
Batch 375/537: Loss=0.8025 (C:0.8025, R:0.0107)
Batch 400/537: Loss=0.8186 (C:0.8186, R:0.0107)
Batch 425/537: Loss=0.8165 (C:0.8165, R:0.0107)
Batch 450/537: Loss=0.7756 (C:0.7756, R:0.0107)
Batch 475/537: Loss=0.7957 (C:0.7957, R:0.0107)
Batch 500/537: Loss=0.8265 (C:0.8265, R:0.0107)
Batch 525/537: Loss=0.7863 (C:0.7863, R:0.0107)

============================================================
Epoch 24/200 completed in 22.0s
Train: Loss=0.8010 (C:0.8010, R:0.0107) Ratio=3.99x
Val:   Loss=0.8848 (C:0.8848, R:0.0106) Ratio=3.10x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 25
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.416 ¬± 0.599
    Neg distances: 2.248 ¬± 1.008
    Separation ratio: 5.40x
    Gap: -3.922
    ‚úÖ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.7957 (C:0.7957, R:0.0107)
Batch  25/537: Loss=0.7553 (C:0.7553, R:0.0107)
Batch  50/537: Loss=0.7692 (C:0.7692, R:0.0107)
Batch  75/537: Loss=0.7992 (C:0.7992, R:0.0107)
Batch 100/537: Loss=0.7348 (C:0.7348, R:0.0107)
Batch 125/537: Loss=0.7545 (C:0.7545, R:0.0107)
Batch 150/537: Loss=0.7803 (C:0.7803, R:0.0107)
Batch 175/537: Loss=0.7672 (C:0.7672, R:0.0107)
Batch 200/537: Loss=0.7991 (C:0.7991, R:0.0107)
Batch 225/537: Loss=0.7305 (C:0.7305, R:0.0107)
Batch 250/537: Loss=0.7530 (C:0.7530, R:0.0107)
Batch 275/537: Loss=0.7822 (C:0.7822, R:0.0107)
Batch 300/537: Loss=0.7878 (C:0.7878, R:0.0107)
Batch 325/537: Loss=0.7754 (C:0.7754, R:0.0107)
Batch 350/537: Loss=0.7730 (C:0.7730, R:0.0107)
Batch 375/537: Loss=0.7855 (C:0.7855, R:0.0107)
Batch 400/537: Loss=0.7616 (C:0.7616, R:0.0107)
Batch 425/537: Loss=0.7764 (C:0.7764, R:0.0107)
Batch 450/537: Loss=0.7718 (C:0.7718, R:0.0107)
Batch 475/537: Loss=0.7744 (C:0.7744, R:0.0107)
Batch 500/537: Loss=0.7846 (C:0.7846, R:0.0107)
Batch 525/537: Loss=0.7902 (C:0.7902, R:0.0107)

============================================================
Epoch 25/200 completed in 27.7s
Train: Loss=0.7708 (C:0.7708, R:0.0107) Ratio=4.01x
Val:   Loss=0.8598 (C:0.8598, R:0.0106) Ratio=3.10x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8598)
Checkpoint saved at epoch 25
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.7751 (C:0.7751, R:0.0107)
Batch  25/537: Loss=0.7378 (C:0.7378, R:0.0108)
Batch  50/537: Loss=0.7080 (C:0.7080, R:0.0107)
Batch  75/537: Loss=0.7713 (C:0.7713, R:0.0107)
Batch 100/537: Loss=0.7451 (C:0.7451, R:0.0107)
Batch 125/537: Loss=0.7852 (C:0.7852, R:0.0107)
Batch 150/537: Loss=0.7904 (C:0.7904, R:0.0107)
Batch 175/537: Loss=0.7543 (C:0.7543, R:0.0107)
Batch 200/537: Loss=0.7658 (C:0.7658, R:0.0107)
Batch 225/537: Loss=0.7289 (C:0.7289, R:0.0107)
Batch 250/537: Loss=0.7757 (C:0.7757, R:0.0107)
Batch 275/537: Loss=0.7845 (C:0.7845, R:0.0107)
Batch 300/537: Loss=0.7879 (C:0.7879, R:0.0107)
Batch 325/537: Loss=0.7752 (C:0.7752, R:0.0107)
Batch 350/537: Loss=0.7958 (C:0.7958, R:0.0107)
Batch 375/537: Loss=0.8143 (C:0.8143, R:0.0107)
Batch 400/537: Loss=0.7762 (C:0.7762, R:0.0107)
Batch 425/537: Loss=0.7818 (C:0.7818, R:0.0108)
Batch 450/537: Loss=0.7725 (C:0.7725, R:0.0107)
Batch 475/537: Loss=0.7863 (C:0.7863, R:0.0107)
Batch 500/537: Loss=0.7744 (C:0.7744, R:0.0107)
Batch 525/537: Loss=0.7272 (C:0.7272, R:0.0107)

============================================================
Epoch 26/200 completed in 22.8s
Train: Loss=0.7682 (C:0.7682, R:0.0107) Ratio=4.02x
Val:   Loss=0.8527 (C:0.8527, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8527)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.7563 (C:0.7563, R:0.0107)
Batch  25/537: Loss=0.7923 (C:0.7923, R:0.0107)
Batch  50/537: Loss=0.7702 (C:0.7702, R:0.0107)
Batch  75/537: Loss=0.7739 (C:0.7739, R:0.0107)
Batch 100/537: Loss=0.7777 (C:0.7777, R:0.0107)
Batch 125/537: Loss=0.7810 (C:0.7810, R:0.0107)
Batch 150/537: Loss=0.7217 (C:0.7217, R:0.0107)
Batch 175/537: Loss=0.7822 (C:0.7822, R:0.0107)
Batch 200/537: Loss=0.7451 (C:0.7451, R:0.0107)
Batch 225/537: Loss=0.7929 (C:0.7929, R:0.0107)
Batch 250/537: Loss=0.7843 (C:0.7843, R:0.0107)
Batch 275/537: Loss=0.7773 (C:0.7773, R:0.0107)
Batch 300/537: Loss=0.7503 (C:0.7503, R:0.0107)
Batch 325/537: Loss=0.7913 (C:0.7913, R:0.0107)
Batch 350/537: Loss=0.7706 (C:0.7706, R:0.0107)
Batch 375/537: Loss=0.7611 (C:0.7611, R:0.0107)
Batch 400/537: Loss=0.7885 (C:0.7885, R:0.0107)
Batch 425/537: Loss=0.7529 (C:0.7529, R:0.0107)
Batch 450/537: Loss=0.7439 (C:0.7439, R:0.0107)
Batch 475/537: Loss=0.7875 (C:0.7875, R:0.0107)
Batch 500/537: Loss=0.8128 (C:0.8128, R:0.0107)
Batch 525/537: Loss=0.7766 (C:0.7766, R:0.0107)

============================================================
Epoch 27/200 completed in 23.0s
Train: Loss=0.7649 (C:0.7649, R:0.0107) Ratio=4.08x
Val:   Loss=0.8436 (C:0.8436, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8436)
============================================================

üåç Updating global dataset at epoch 28
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.404 ¬± 0.600
    Neg distances: 2.307 ¬± 1.026
    Separation ratio: 5.71x
    Gap: -4.027
    ‚úÖ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.7348 (C:0.7348, R:0.0107)
Batch  25/537: Loss=0.6999 (C:0.6999, R:0.0107)
Batch  50/537: Loss=0.7446 (C:0.7446, R:0.0107)
Batch  75/537: Loss=0.7012 (C:0.7012, R:0.0107)
Batch 100/537: Loss=0.7138 (C:0.7138, R:0.0107)
Batch 125/537: Loss=0.6766 (C:0.6766, R:0.0107)
Batch 150/537: Loss=0.7640 (C:0.7640, R:0.0107)
Batch 175/537: Loss=0.7507 (C:0.7507, R:0.0107)
Batch 200/537: Loss=0.7291 (C:0.7291, R:0.0107)
Batch 225/537: Loss=0.7575 (C:0.7575, R:0.0107)
Batch 250/537: Loss=0.7122 (C:0.7122, R:0.0107)
Batch 275/537: Loss=0.7340 (C:0.7340, R:0.0107)
Batch 300/537: Loss=0.7431 (C:0.7431, R:0.0107)
Batch 325/537: Loss=0.7525 (C:0.7525, R:0.0107)
Batch 350/537: Loss=0.7437 (C:0.7437, R:0.0107)
Batch 375/537: Loss=0.7173 (C:0.7173, R:0.0107)
Batch 400/537: Loss=0.7443 (C:0.7443, R:0.0107)
Batch 425/537: Loss=0.7435 (C:0.7435, R:0.0107)
Batch 450/537: Loss=0.7422 (C:0.7422, R:0.0107)
Batch 475/537: Loss=0.7424 (C:0.7424, R:0.0107)
Batch 500/537: Loss=0.7185 (C:0.7185, R:0.0107)
Batch 525/537: Loss=0.7459 (C:0.7459, R:0.0107)

============================================================
Epoch 28/200 completed in 29.6s
Train: Loss=0.7351 (C:0.7351, R:0.0107) Ratio=4.18x
Val:   Loss=0.8219 (C:0.8219, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8219)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.7560 (C:0.7560, R:0.0107)
Batch  25/537: Loss=0.7321 (C:0.7321, R:0.0107)
Batch  50/537: Loss=0.7224 (C:0.7224, R:0.0107)
Batch  75/537: Loss=0.6985 (C:0.6985, R:0.0107)
Batch 100/537: Loss=0.7327 (C:0.7327, R:0.0107)
Batch 125/537: Loss=0.7676 (C:0.7676, R:0.0108)
Batch 150/537: Loss=0.7513 (C:0.7513, R:0.0107)
Batch 175/537: Loss=0.7298 (C:0.7298, R:0.0107)
Batch 200/537: Loss=0.7462 (C:0.7462, R:0.0107)
Batch 225/537: Loss=0.7257 (C:0.7257, R:0.0107)
Batch 250/537: Loss=0.7281 (C:0.7281, R:0.0107)
Batch 275/537: Loss=0.7248 (C:0.7248, R:0.0107)
Batch 300/537: Loss=0.7159 (C:0.7159, R:0.0107)
Batch 325/537: Loss=0.7385 (C:0.7385, R:0.0107)
Batch 350/537: Loss=0.7597 (C:0.7597, R:0.0107)
Batch 375/537: Loss=0.7462 (C:0.7462, R:0.0107)
Batch 400/537: Loss=0.7442 (C:0.7442, R:0.0107)
Batch 425/537: Loss=0.7618 (C:0.7618, R:0.0107)
Batch 450/537: Loss=0.7285 (C:0.7285, R:0.0107)
Batch 475/537: Loss=0.7476 (C:0.7476, R:0.0107)
Batch 500/537: Loss=0.7261 (C:0.7261, R:0.0107)
Batch 525/537: Loss=0.7626 (C:0.7626, R:0.0107)

============================================================
Epoch 29/200 completed in 22.4s
Train: Loss=0.7324 (C:0.7324, R:0.0107) Ratio=4.13x
Val:   Loss=0.8266 (C:0.8266, R:0.0106) Ratio=3.14x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7332 (C:0.7332, R:0.0107)
Batch  25/537: Loss=0.7119 (C:0.7119, R:0.0107)
Batch  50/537: Loss=0.7134 (C:0.7134, R:0.0107)
Batch  75/537: Loss=0.7298 (C:0.7298, R:0.0107)
Batch 100/537: Loss=0.6876 (C:0.6876, R:0.0107)
Batch 125/537: Loss=0.7358 (C:0.7358, R:0.0107)
Batch 150/537: Loss=0.7358 (C:0.7358, R:0.0107)
Batch 175/537: Loss=0.7121 (C:0.7121, R:0.0107)
Batch 200/537: Loss=0.7057 (C:0.7057, R:0.0107)
Batch 225/537: Loss=0.7650 (C:0.7650, R:0.0107)
Batch 250/537: Loss=0.7334 (C:0.7334, R:0.0107)
Batch 275/537: Loss=0.7037 (C:0.7037, R:0.0107)
Batch 300/537: Loss=0.7678 (C:0.7678, R:0.0107)
Batch 325/537: Loss=0.7312 (C:0.7312, R:0.0107)
Batch 350/537: Loss=0.7534 (C:0.7534, R:0.0107)
Batch 375/537: Loss=0.7158 (C:0.7158, R:0.0107)
Batch 400/537: Loss=0.7161 (C:0.7161, R:0.0107)
Batch 425/537: Loss=0.7490 (C:0.7490, R:0.0107)
Batch 450/537: Loss=0.7062 (C:0.7062, R:0.0107)
Batch 475/537: Loss=0.7211 (C:0.7211, R:0.0107)
Batch 500/537: Loss=0.7139 (C:0.7139, R:0.0107)
Batch 525/537: Loss=0.7404 (C:0.7404, R:0.0107)

============================================================
Epoch 30/200 completed in 22.5s
Train: Loss=0.7281 (C:0.7281, R:0.0107) Ratio=4.22x
Val:   Loss=0.8333 (C:0.8333, R:0.0106) Ratio=3.12x
Reconstruction weight: 0.000
No improvement for 2 epochs
Checkpoint saved at epoch 30
============================================================

üåç Updating global dataset at epoch 31
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.404 ¬± 0.606
    Neg distances: 2.359 ¬± 1.046
    Separation ratio: 5.83x
    Gap: -4.065
    ‚úÖ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.7045 (C:0.7045, R:0.0107)
Batch  25/537: Loss=0.7085 (C:0.7085, R:0.0107)
Batch  50/537: Loss=0.7236 (C:0.7236, R:0.0107)
Batch  75/537: Loss=0.7056 (C:0.7056, R:0.0107)
Batch 100/537: Loss=0.7263 (C:0.7263, R:0.0107)
Batch 125/537: Loss=0.7149 (C:0.7149, R:0.0107)
Batch 150/537: Loss=0.7311 (C:0.7311, R:0.0107)
Batch 175/537: Loss=0.7178 (C:0.7178, R:0.0107)
Batch 200/537: Loss=0.7093 (C:0.7093, R:0.0107)
Batch 225/537: Loss=0.6848 (C:0.6848, R:0.0107)
Batch 250/537: Loss=0.6907 (C:0.6907, R:0.0107)
Batch 275/537: Loss=0.6991 (C:0.6991, R:0.0108)
Batch 300/537: Loss=0.6884 (C:0.6884, R:0.0107)
Batch 325/537: Loss=0.6773 (C:0.6773, R:0.0107)
Batch 350/537: Loss=0.6646 (C:0.6646, R:0.0107)
Batch 375/537: Loss=0.7337 (C:0.7337, R:0.0107)
Batch 400/537: Loss=0.7201 (C:0.7201, R:0.0107)
Batch 425/537: Loss=0.7499 (C:0.7499, R:0.0107)
Batch 450/537: Loss=0.7112 (C:0.7112, R:0.0107)
Batch 475/537: Loss=0.7674 (C:0.7674, R:0.0107)
Batch 500/537: Loss=0.7060 (C:0.7060, R:0.0107)
Batch 525/537: Loss=0.7314 (C:0.7314, R:0.0107)

============================================================
Epoch 31/200 completed in 29.3s
Train: Loss=0.7140 (C:0.7140, R:0.0107) Ratio=4.25x
Val:   Loss=0.8017 (C:0.8017, R:0.0106) Ratio=3.18x
Reconstruction weight: 0.015
‚úÖ New best model saved (Val Loss: 0.8017)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.6826 (C:0.6826, R:0.0107)
Batch  25/537: Loss=0.6988 (C:0.6988, R:0.0107)
Batch  50/537: Loss=0.6928 (C:0.6928, R:0.0108)
Batch  75/537: Loss=0.7407 (C:0.7407, R:0.0108)
Batch 100/537: Loss=0.7029 (C:0.7029, R:0.0107)
Batch 125/537: Loss=0.7165 (C:0.7165, R:0.0107)
Batch 150/537: Loss=0.6881 (C:0.6881, R:0.0107)
Batch 175/537: Loss=0.6881 (C:0.6881, R:0.0107)
Batch 200/537: Loss=0.7407 (C:0.7407, R:0.0107)
Batch 225/537: Loss=0.6883 (C:0.6883, R:0.0107)
Batch 250/537: Loss=0.7114 (C:0.7114, R:0.0107)
Batch 275/537: Loss=0.7132 (C:0.7132, R:0.0107)
Batch 300/537: Loss=0.6906 (C:0.6906, R:0.0107)
Batch 325/537: Loss=0.7306 (C:0.7306, R:0.0107)
Batch 350/537: Loss=0.6986 (C:0.6986, R:0.0107)
Batch 375/537: Loss=0.7626 (C:0.7626, R:0.0107)
Batch 400/537: Loss=0.6984 (C:0.6984, R:0.0107)
Batch 425/537: Loss=0.7080 (C:0.7080, R:0.0107)
Batch 450/537: Loss=0.7502 (C:0.7502, R:0.0107)
Batch 475/537: Loss=0.7065 (C:0.7065, R:0.0107)
Batch 500/537: Loss=0.7185 (C:0.7185, R:0.0107)
Batch 525/537: Loss=0.7082 (C:0.7082, R:0.0107)

============================================================
Epoch 32/200 completed in 22.4s
Train: Loss=0.7098 (C:0.7098, R:0.0107) Ratio=4.28x
Val:   Loss=0.8022 (C:0.8022, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.6976 (C:0.6976, R:0.0107)
Batch  25/537: Loss=0.6755 (C:0.6755, R:0.0107)
Batch  50/537: Loss=0.6820 (C:0.6820, R:0.0107)
Batch  75/537: Loss=0.7359 (C:0.7359, R:0.0107)
Batch 100/537: Loss=0.6954 (C:0.6954, R:0.0107)
Batch 125/537: Loss=0.6682 (C:0.6682, R:0.0107)
Batch 150/537: Loss=0.7036 (C:0.7036, R:0.0107)
Batch 175/537: Loss=0.7076 (C:0.7076, R:0.0107)
Batch 200/537: Loss=0.7636 (C:0.7636, R:0.0107)
Batch 225/537: Loss=0.7090 (C:0.7090, R:0.0107)
Batch 250/537: Loss=0.7214 (C:0.7214, R:0.0107)
Batch 275/537: Loss=0.6913 (C:0.6913, R:0.0107)
Batch 300/537: Loss=0.7213 (C:0.7213, R:0.0107)
Batch 325/537: Loss=0.7535 (C:0.7535, R:0.0107)
Batch 350/537: Loss=0.7129 (C:0.7129, R:0.0107)
Batch 375/537: Loss=0.6774 (C:0.6774, R:0.0107)
Batch 400/537: Loss=0.7325 (C:0.7325, R:0.0107)
Batch 425/537: Loss=0.6717 (C:0.6717, R:0.0107)
Batch 450/537: Loss=0.7156 (C:0.7156, R:0.0107)
Batch 475/537: Loss=0.7394 (C:0.7394, R:0.0107)
Batch 500/537: Loss=0.7117 (C:0.7117, R:0.0107)
Batch 525/537: Loss=0.7020 (C:0.7020, R:0.0107)

============================================================
Epoch 33/200 completed in 21.8s
Train: Loss=0.7069 (C:0.7069, R:0.0107) Ratio=4.35x
Val:   Loss=0.8172 (C:0.8172, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.045
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 34
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.386 ¬± 0.608
    Neg distances: 2.418 ¬± 1.055
    Separation ratio: 6.27x
    Gap: -4.171
    ‚úÖ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.6804 (C:0.6804, R:0.0107)
Batch  25/537: Loss=0.6620 (C:0.6620, R:0.0107)
Batch  50/537: Loss=0.6325 (C:0.6325, R:0.0107)
Batch  75/537: Loss=0.6583 (C:0.6583, R:0.0107)
Batch 100/537: Loss=0.6426 (C:0.6426, R:0.0107)
Batch 125/537: Loss=0.6678 (C:0.6678, R:0.0107)
Batch 150/537: Loss=0.6766 (C:0.6766, R:0.0107)
Batch 175/537: Loss=0.7025 (C:0.7025, R:0.0107)
Batch 200/537: Loss=0.7279 (C:0.7279, R:0.0107)
Batch 225/537: Loss=0.7033 (C:0.7033, R:0.0107)
Batch 250/537: Loss=0.6132 (C:0.6132, R:0.0107)
Batch 275/537: Loss=0.6595 (C:0.6595, R:0.0108)
Batch 300/537: Loss=0.6798 (C:0.6798, R:0.0107)
Batch 325/537: Loss=0.6370 (C:0.6370, R:0.0107)
Batch 350/537: Loss=0.6471 (C:0.6471, R:0.0107)
Batch 375/537: Loss=0.6963 (C:0.6963, R:0.0107)
Batch 400/537: Loss=0.6551 (C:0.6551, R:0.0107)
Batch 425/537: Loss=0.6641 (C:0.6641, R:0.0107)
Batch 450/537: Loss=0.6691 (C:0.6691, R:0.0108)
Batch 475/537: Loss=0.6540 (C:0.6540, R:0.0107)
Batch 500/537: Loss=0.6825 (C:0.6825, R:0.0107)
Batch 525/537: Loss=0.6526 (C:0.6526, R:0.0107)

============================================================
Epoch 34/200 completed in 28.1s
Train: Loss=0.6766 (C:0.6766, R:0.0107) Ratio=4.47x
Val:   Loss=0.7890 (C:0.7890, R:0.0106) Ratio=3.12x
Reconstruction weight: 0.060
‚úÖ New best model saved (Val Loss: 0.7890)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.6869 (C:0.6869, R:0.0107)
Batch  25/537: Loss=0.6456 (C:0.6456, R:0.0107)
Batch  50/537: Loss=0.6941 (C:0.6941, R:0.0107)
Batch  75/537: Loss=0.6638 (C:0.6638, R:0.0107)
Batch 100/537: Loss=0.6527 (C:0.6527, R:0.0107)
Batch 125/537: Loss=0.7206 (C:0.7206, R:0.0107)
Batch 150/537: Loss=0.7007 (C:0.7007, R:0.0107)
Batch 175/537: Loss=0.6510 (C:0.6510, R:0.0107)
Batch 200/537: Loss=0.6665 (C:0.6665, R:0.0107)
Batch 225/537: Loss=0.6839 (C:0.6839, R:0.0107)
Batch 250/537: Loss=0.6558 (C:0.6558, R:0.0107)
Batch 275/537: Loss=0.6637 (C:0.6637, R:0.0107)
Batch 300/537: Loss=0.7142 (C:0.7142, R:0.0107)
Batch 325/537: Loss=0.6618 (C:0.6618, R:0.0107)
Batch 350/537: Loss=0.6963 (C:0.6963, R:0.0107)
Batch 375/537: Loss=0.6886 (C:0.6886, R:0.0107)
Batch 400/537: Loss=0.6789 (C:0.6789, R:0.0107)
Batch 425/537: Loss=0.6733 (C:0.6733, R:0.0108)
Batch 450/537: Loss=0.6523 (C:0.6523, R:0.0107)
Batch 475/537: Loss=0.6622 (C:0.6622, R:0.0107)
Batch 500/537: Loss=0.6700 (C:0.6700, R:0.0107)
Batch 525/537: Loss=0.6983 (C:0.6983, R:0.0107)

============================================================
Epoch 35/200 completed in 21.7s
Train: Loss=0.6742 (C:0.6742, R:0.0107) Ratio=4.41x
Val:   Loss=0.7881 (C:0.7881, R:0.0106) Ratio=3.14x
Reconstruction weight: 0.075
‚úÖ New best model saved (Val Loss: 0.7881)
Checkpoint saved at epoch 35
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.6807 (C:0.6807, R:0.0107)
Batch  25/537: Loss=0.6533 (C:0.6533, R:0.0107)
Batch  50/537: Loss=0.6595 (C:0.6595, R:0.0107)
Batch  75/537: Loss=0.6546 (C:0.6546, R:0.0107)
Batch 100/537: Loss=0.6657 (C:0.6657, R:0.0107)
Batch 125/537: Loss=0.6418 (C:0.6418, R:0.0107)
Batch 150/537: Loss=0.6776 (C:0.6776, R:0.0107)
Batch 175/537: Loss=0.6951 (C:0.6951, R:0.0107)
Batch 200/537: Loss=0.6713 (C:0.6713, R:0.0107)
Batch 225/537: Loss=0.6745 (C:0.6745, R:0.0107)
Batch 250/537: Loss=0.6233 (C:0.6233, R:0.0107)
Batch 275/537: Loss=0.6824 (C:0.6824, R:0.0107)
Batch 300/537: Loss=0.6583 (C:0.6583, R:0.0107)
Batch 325/537: Loss=0.6814 (C:0.6814, R:0.0107)
Batch 350/537: Loss=0.6701 (C:0.6701, R:0.0107)
Batch 375/537: Loss=0.6854 (C:0.6854, R:0.0107)
Batch 400/537: Loss=0.6853 (C:0.6853, R:0.0107)
Batch 425/537: Loss=0.6572 (C:0.6572, R:0.0107)
Batch 450/537: Loss=0.6902 (C:0.6902, R:0.0107)
Batch 475/537: Loss=0.6795 (C:0.6795, R:0.0107)
Batch 500/537: Loss=0.6908 (C:0.6908, R:0.0107)
Batch 525/537: Loss=0.6600 (C:0.6600, R:0.0107)

============================================================
Epoch 36/200 completed in 22.0s
Train: Loss=0.6701 (C:0.6701, R:0.0107) Ratio=4.39x
Val:   Loss=0.7860 (C:0.7860, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.090
‚úÖ New best model saved (Val Loss: 0.7860)
============================================================

üåç Updating global dataset at epoch 37
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.370 ¬± 0.602
    Neg distances: 2.459 ¬± 1.069
    Separation ratio: 6.64x
    Gap: -4.191
    ‚úÖ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.6328 (C:0.6328, R:0.0107)
Batch  25/537: Loss=0.6543 (C:0.6543, R:0.0107)
Batch  50/537: Loss=0.6312 (C:0.6312, R:0.0107)
Batch  75/537: Loss=0.6643 (C:0.6643, R:0.0107)
Batch 100/537: Loss=0.6397 (C:0.6397, R:0.0107)
Batch 125/537: Loss=0.6496 (C:0.6496, R:0.0107)
Batch 150/537: Loss=0.6216 (C:0.6216, R:0.0108)
Batch 175/537: Loss=0.6512 (C:0.6512, R:0.0106)
Batch 200/537: Loss=0.6295 (C:0.6295, R:0.0107)
Batch 225/537: Loss=0.6598 (C:0.6598, R:0.0107)
Batch 250/537: Loss=0.6700 (C:0.6700, R:0.0107)
Batch 275/537: Loss=0.6124 (C:0.6124, R:0.0107)
Batch 300/537: Loss=0.6457 (C:0.6457, R:0.0107)
Batch 325/537: Loss=0.6123 (C:0.6123, R:0.0107)
Batch 350/537: Loss=0.6910 (C:0.6910, R:0.0108)
Batch 375/537: Loss=0.6588 (C:0.6588, R:0.0107)
Batch 400/537: Loss=0.6429 (C:0.6429, R:0.0108)
Batch 425/537: Loss=0.6519 (C:0.6519, R:0.0107)
Batch 450/537: Loss=0.6894 (C:0.6894, R:0.0107)
Batch 475/537: Loss=0.6727 (C:0.6727, R:0.0107)
Batch 500/537: Loss=0.6284 (C:0.6284, R:0.0107)
Batch 525/537: Loss=0.6494 (C:0.6494, R:0.0107)

============================================================
Epoch 37/200 completed in 28.3s
Train: Loss=0.6507 (C:0.6507, R:0.0107) Ratio=4.47x
Val:   Loss=0.7642 (C:0.7642, R:0.0106) Ratio=3.13x
Reconstruction weight: 0.105
‚úÖ New best model saved (Val Loss: 0.7642)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.6431 (C:0.6431, R:0.0107)
Batch  25/537: Loss=0.6372 (C:0.6372, R:0.0107)
Batch  50/537: Loss=0.6261 (C:0.6261, R:0.0107)
Batch  75/537: Loss=0.6467 (C:0.6467, R:0.0107)
Batch 100/537: Loss=0.6482 (C:0.6482, R:0.0108)
Batch 125/537: Loss=0.6480 (C:0.6480, R:0.0108)
Batch 150/537: Loss=0.6984 (C:0.6984, R:0.0107)
Batch 175/537: Loss=0.6533 (C:0.6533, R:0.0107)
Batch 200/537: Loss=0.6293 (C:0.6293, R:0.0107)
Batch 225/537: Loss=0.6490 (C:0.6490, R:0.0107)
Batch 250/537: Loss=0.6352 (C:0.6352, R:0.0107)
Batch 275/537: Loss=0.6169 (C:0.6169, R:0.0107)
Batch 300/537: Loss=0.6727 (C:0.6727, R:0.0107)
Batch 325/537: Loss=0.6679 (C:0.6679, R:0.0107)
Batch 350/537: Loss=0.6465 (C:0.6465, R:0.0107)
Batch 375/537: Loss=0.6159 (C:0.6159, R:0.0107)
Batch 400/537: Loss=0.6532 (C:0.6532, R:0.0107)
Batch 425/537: Loss=0.6771 (C:0.6771, R:0.0107)
Batch 450/537: Loss=0.6363 (C:0.6363, R:0.0107)
Batch 475/537: Loss=0.6251 (C:0.6251, R:0.0107)
Batch 500/537: Loss=0.5950 (C:0.5950, R:0.0107)
Batch 525/537: Loss=0.6175 (C:0.6175, R:0.0107)

============================================================
Epoch 38/200 completed in 21.7s
Train: Loss=0.6480 (C:0.6480, R:0.0107) Ratio=4.55x
Val:   Loss=0.7675 (C:0.7675, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.6338 (C:0.6338, R:0.0107)
Batch  25/537: Loss=0.6247 (C:0.6247, R:0.0107)
Batch  50/537: Loss=0.6381 (C:0.6381, R:0.0107)
Batch  75/537: Loss=0.6712 (C:0.6712, R:0.0107)
Batch 100/537: Loss=0.6244 (C:0.6244, R:0.0108)
Batch 125/537: Loss=0.6839 (C:0.6839, R:0.0108)
Batch 150/537: Loss=0.6572 (C:0.6572, R:0.0107)
Batch 175/537: Loss=0.6302 (C:0.6302, R:0.0107)
Batch 200/537: Loss=0.6230 (C:0.6230, R:0.0107)
Batch 225/537: Loss=0.6530 (C:0.6530, R:0.0107)
Batch 250/537: Loss=0.6595 (C:0.6595, R:0.0107)
Batch 275/537: Loss=0.6366 (C:0.6366, R:0.0107)
Batch 300/537: Loss=0.6443 (C:0.6443, R:0.0107)
Batch 325/537: Loss=0.6424 (C:0.6424, R:0.0107)
Batch 350/537: Loss=0.6607 (C:0.6607, R:0.0107)
Batch 375/537: Loss=0.6752 (C:0.6752, R:0.0107)
Batch 400/537: Loss=0.6671 (C:0.6671, R:0.0107)
Batch 425/537: Loss=0.6964 (C:0.6964, R:0.0107)
Batch 450/537: Loss=0.6422 (C:0.6422, R:0.0107)
Batch 475/537: Loss=0.6624 (C:0.6624, R:0.0107)
Batch 500/537: Loss=0.6365 (C:0.6365, R:0.0107)
Batch 525/537: Loss=0.6375 (C:0.6375, R:0.0107)

============================================================
Epoch 39/200 completed in 21.6s
Train: Loss=0.6465 (C:0.6465, R:0.0107) Ratio=4.51x
Val:   Loss=0.7697 (C:0.7697, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 40
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.369 ¬± 0.598
    Neg distances: 2.500 ¬± 1.083
    Separation ratio: 6.77x
    Gap: -4.351
    ‚úÖ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.6259 (C:0.6259, R:0.0107)
Batch  25/537: Loss=0.6204 (C:0.6204, R:0.0107)
Batch  50/537: Loss=0.6271 (C:0.6271, R:0.0107)
Batch  75/537: Loss=0.5797 (C:0.5797, R:0.0107)
Batch 100/537: Loss=0.6548 (C:0.6548, R:0.0107)
Batch 125/537: Loss=0.6014 (C:0.6014, R:0.0107)
Batch 150/537: Loss=0.6111 (C:0.6111, R:0.0107)
Batch 175/537: Loss=0.6326 (C:0.6326, R:0.0107)
Batch 200/537: Loss=0.6728 (C:0.6728, R:0.0107)
Batch 225/537: Loss=0.6102 (C:0.6102, R:0.0107)
Batch 250/537: Loss=0.6323 (C:0.6323, R:0.0107)
Batch 275/537: Loss=0.6354 (C:0.6354, R:0.0108)
Batch 300/537: Loss=0.6169 (C:0.6169, R:0.0107)
Batch 325/537: Loss=0.5978 (C:0.5978, R:0.0107)
Batch 350/537: Loss=0.6573 (C:0.6573, R:0.0107)
Batch 375/537: Loss=0.6632 (C:0.6632, R:0.0107)
Batch 400/537: Loss=0.6676 (C:0.6676, R:0.0107)
Batch 425/537: Loss=0.6528 (C:0.6528, R:0.0107)
Batch 450/537: Loss=0.6432 (C:0.6432, R:0.0107)
Batch 475/537: Loss=0.5965 (C:0.5965, R:0.0107)
Batch 500/537: Loss=0.6108 (C:0.6108, R:0.0107)
Batch 525/537: Loss=0.6276 (C:0.6276, R:0.0107)

============================================================
Epoch 40/200 completed in 27.7s
Train: Loss=0.6365 (C:0.6365, R:0.0107) Ratio=4.69x
Val:   Loss=0.7521 (C:0.7521, R:0.0106) Ratio=3.19x
Reconstruction weight: 0.150
‚úÖ New best model saved (Val Loss: 0.7521)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6478 (C:0.6478, R:0.0107)
Batch  25/537: Loss=0.6048 (C:0.6048, R:0.0107)
Batch  50/537: Loss=0.6388 (C:0.6388, R:0.0107)
Batch  75/537: Loss=0.6701 (C:0.6701, R:0.0107)
Batch 100/537: Loss=0.6086 (C:0.6086, R:0.0108)
Batch 125/537: Loss=0.6238 (C:0.6238, R:0.0107)
Batch 150/537: Loss=0.6023 (C:0.6023, R:0.0107)
Batch 175/537: Loss=0.6390 (C:0.6390, R:0.0107)
Batch 200/537: Loss=0.6815 (C:0.6815, R:0.0107)
Batch 225/537: Loss=0.6019 (C:0.6019, R:0.0107)
Batch 250/537: Loss=0.6371 (C:0.6371, R:0.0107)
Batch 275/537: Loss=0.6024 (C:0.6024, R:0.0107)
Batch 300/537: Loss=0.6603 (C:0.6603, R:0.0107)
Batch 325/537: Loss=0.6411 (C:0.6411, R:0.0107)
Batch 350/537: Loss=0.6398 (C:0.6398, R:0.0107)
Batch 375/537: Loss=0.6602 (C:0.6602, R:0.0107)
Batch 400/537: Loss=0.6537 (C:0.6537, R:0.0107)
Batch 425/537: Loss=0.6287 (C:0.6287, R:0.0107)
Batch 450/537: Loss=0.6712 (C:0.6712, R:0.0107)
Batch 475/537: Loss=0.6614 (C:0.6614, R:0.0107)
Batch 500/537: Loss=0.6538 (C:0.6538, R:0.0107)
Batch 525/537: Loss=0.6601 (C:0.6601, R:0.0107)

============================================================
Epoch 41/200 completed in 21.6s
Train: Loss=0.6347 (C:0.6347, R:0.0107) Ratio=4.54x
Val:   Loss=0.7634 (C:0.7634, R:0.0106) Ratio=3.14x
Reconstruction weight: 0.165
No improvement for 1 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.6308 (C:0.6308, R:0.0107)
Batch  25/537: Loss=0.6615 (C:0.6615, R:0.0107)
Batch  50/537: Loss=0.6314 (C:0.6314, R:0.0107)
Batch  75/537: Loss=0.6259 (C:0.6259, R:0.0107)
Batch 100/537: Loss=0.6649 (C:0.6649, R:0.0107)
Batch 125/537: Loss=0.6243 (C:0.6243, R:0.0107)
Batch 150/537: Loss=0.6298 (C:0.6298, R:0.0107)
Batch 175/537: Loss=0.6390 (C:0.6390, R:0.0107)
Batch 200/537: Loss=0.6621 (C:0.6621, R:0.0107)
Batch 225/537: Loss=0.6354 (C:0.6354, R:0.0107)
Batch 250/537: Loss=0.6040 (C:0.6040, R:0.0107)
Batch 275/537: Loss=0.5934 (C:0.5934, R:0.0107)
Batch 300/537: Loss=0.6546 (C:0.6546, R:0.0107)
Batch 325/537: Loss=0.6217 (C:0.6217, R:0.0107)
Batch 350/537: Loss=0.6827 (C:0.6827, R:0.0108)
Batch 375/537: Loss=0.6177 (C:0.6177, R:0.0107)
Batch 400/537: Loss=0.5995 (C:0.5995, R:0.0107)
Batch 425/537: Loss=0.6751 (C:0.6751, R:0.0107)
Batch 450/537: Loss=0.6322 (C:0.6322, R:0.0107)
Batch 475/537: Loss=0.6499 (C:0.6499, R:0.0107)
Batch 500/537: Loss=0.6631 (C:0.6631, R:0.0107)
Batch 525/537: Loss=0.6104 (C:0.6104, R:0.0107)

============================================================
Epoch 42/200 completed in 22.0s
Train: Loss=0.6338 (C:0.6338, R:0.0107) Ratio=4.61x
Val:   Loss=0.7711 (C:0.7711, R:0.0106) Ratio=3.12x
Reconstruction weight: 0.180
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 43
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.364 ¬± 0.634
    Neg distances: 2.531 ¬± 1.087
    Separation ratio: 6.95x
    Gap: -4.327
    ‚úÖ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.5913 (C:0.5913, R:0.0107)
Batch  25/537: Loss=0.6100 (C:0.6100, R:0.0107)
Batch  50/537: Loss=0.5880 (C:0.5880, R:0.0107)
Batch  75/537: Loss=0.5769 (C:0.5769, R:0.0107)
Batch 100/537: Loss=0.6293 (C:0.6293, R:0.0108)
Batch 125/537: Loss=0.6167 (C:0.6167, R:0.0107)
Batch 150/537: Loss=0.6467 (C:0.6467, R:0.0107)
Batch 175/537: Loss=0.6300 (C:0.6300, R:0.0107)
Batch 200/537: Loss=0.6201 (C:0.6201, R:0.0107)
Batch 225/537: Loss=0.6529 (C:0.6529, R:0.0107)
Batch 250/537: Loss=0.6277 (C:0.6277, R:0.0107)
Batch 275/537: Loss=0.6075 (C:0.6075, R:0.0107)
Batch 300/537: Loss=0.6084 (C:0.6084, R:0.0107)
Batch 325/537: Loss=0.6368 (C:0.6368, R:0.0107)
Batch 350/537: Loss=0.6101 (C:0.6101, R:0.0107)
Batch 375/537: Loss=0.6055 (C:0.6055, R:0.0107)
Batch 400/537: Loss=0.6023 (C:0.6023, R:0.0107)
Batch 425/537: Loss=0.6111 (C:0.6111, R:0.0107)
Batch 450/537: Loss=0.6436 (C:0.6436, R:0.0107)
Batch 475/537: Loss=0.6493 (C:0.6493, R:0.0108)
Batch 500/537: Loss=0.6450 (C:0.6450, R:0.0107)
Batch 525/537: Loss=0.5979 (C:0.5979, R:0.0107)

============================================================
Epoch 43/200 completed in 28.2s
Train: Loss=0.6212 (C:0.6212, R:0.0107) Ratio=4.74x
Val:   Loss=0.7497 (C:0.7497, R:0.0106) Ratio=3.12x
Reconstruction weight: 0.195
‚úÖ New best model saved (Val Loss: 0.7497)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.5854 (C:0.5854, R:0.0107)
Batch  25/537: Loss=0.5820 (C:0.5820, R:0.0107)
Batch  50/537: Loss=0.6289 (C:0.6289, R:0.0107)
Batch  75/537: Loss=0.6072 (C:0.6072, R:0.0107)
Batch 100/537: Loss=0.5718 (C:0.5718, R:0.0107)
Batch 125/537: Loss=0.6208 (C:0.6208, R:0.0108)
Batch 150/537: Loss=0.6174 (C:0.6174, R:0.0108)
Batch 175/537: Loss=0.5895 (C:0.5895, R:0.0107)
Batch 200/537: Loss=0.5995 (C:0.5995, R:0.0107)
Batch 225/537: Loss=0.6013 (C:0.6013, R:0.0107)
Batch 250/537: Loss=0.6271 (C:0.6271, R:0.0107)
Batch 275/537: Loss=0.6327 (C:0.6327, R:0.0107)
Batch 300/537: Loss=0.6125 (C:0.6125, R:0.0107)
Batch 325/537: Loss=0.6397 (C:0.6397, R:0.0107)
Batch 350/537: Loss=0.6579 (C:0.6579, R:0.0107)
Batch 375/537: Loss=0.5871 (C:0.5871, R:0.0107)
Batch 400/537: Loss=0.6220 (C:0.6220, R:0.0107)
Batch 425/537: Loss=0.6208 (C:0.6208, R:0.0107)
Batch 450/537: Loss=0.6349 (C:0.6349, R:0.0107)
Batch 475/537: Loss=0.5900 (C:0.5900, R:0.0107)
Batch 500/537: Loss=0.6087 (C:0.6087, R:0.0107)
Batch 525/537: Loss=0.6144 (C:0.6144, R:0.0107)

============================================================
Epoch 44/200 completed in 21.6s
Train: Loss=0.6167 (C:0.6167, R:0.0107) Ratio=4.78x
Val:   Loss=0.7460 (C:0.7460, R:0.0106) Ratio=3.16x
Reconstruction weight: 0.210
‚úÖ New best model saved (Val Loss: 0.7460)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.6053 (C:0.6053, R:0.0107)
Batch  25/537: Loss=0.5810 (C:0.5810, R:0.0108)
Batch  50/537: Loss=0.5855 (C:0.5855, R:0.0108)
Batch  75/537: Loss=0.6556 (C:0.6556, R:0.0107)
Batch 100/537: Loss=0.5991 (C:0.5991, R:0.0107)
Batch 125/537: Loss=0.5684 (C:0.5684, R:0.0107)
Batch 150/537: Loss=0.6208 (C:0.6208, R:0.0107)
Batch 175/537: Loss=0.6006 (C:0.6006, R:0.0107)
Batch 200/537: Loss=0.6508 (C:0.6508, R:0.0107)
Batch 225/537: Loss=0.5919 (C:0.5919, R:0.0107)
Batch 250/537: Loss=0.6118 (C:0.6118, R:0.0107)
Batch 275/537: Loss=0.6033 (C:0.6033, R:0.0107)
Batch 300/537: Loss=0.6513 (C:0.6513, R:0.0107)
Batch 325/537: Loss=0.5862 (C:0.5862, R:0.0107)
Batch 350/537: Loss=0.6342 (C:0.6342, R:0.0107)
Batch 375/537: Loss=0.5966 (C:0.5966, R:0.0107)
Batch 400/537: Loss=0.5793 (C:0.5793, R:0.0107)
Batch 425/537: Loss=0.6163 (C:0.6163, R:0.0107)
Batch 450/537: Loss=0.6491 (C:0.6491, R:0.0108)
Batch 475/537: Loss=0.6085 (C:0.6085, R:0.0107)
Batch 500/537: Loss=0.6040 (C:0.6040, R:0.0107)
Batch 525/537: Loss=0.6520 (C:0.6520, R:0.0107)

============================================================
Epoch 45/200 completed in 21.9s
Train: Loss=0.6156 (C:0.6156, R:0.0107) Ratio=4.81x
Val:   Loss=0.7552 (C:0.7552, R:0.0106) Ratio=3.13x
Reconstruction weight: 0.225
No improvement for 1 epochs
Checkpoint saved at epoch 45
============================================================

üåç Updating global dataset at epoch 46
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.376 ¬± 0.645
    Neg distances: 2.556 ¬± 1.107
    Separation ratio: 6.79x
    Gap: -4.500
    ‚úÖ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.5926 (C:0.5926, R:0.0107)
Batch  25/537: Loss=0.6445 (C:0.6445, R:0.0107)
Batch  50/537: Loss=0.6120 (C:0.6120, R:0.0107)
Batch  75/537: Loss=0.6587 (C:0.6587, R:0.0107)
Batch 100/537: Loss=0.6347 (C:0.6347, R:0.0108)
Batch 125/537: Loss=0.6229 (C:0.6229, R:0.0107)
Batch 150/537: Loss=0.6501 (C:0.6501, R:0.0107)
Batch 175/537: Loss=0.5903 (C:0.5903, R:0.0107)
Batch 200/537: Loss=0.6698 (C:0.6698, R:0.0107)
Batch 225/537: Loss=0.6912 (C:0.6912, R:0.0107)
Batch 250/537: Loss=0.5859 (C:0.5859, R:0.0107)
Batch 275/537: Loss=0.6379 (C:0.6379, R:0.0107)
Batch 300/537: Loss=0.6018 (C:0.6018, R:0.0107)
Batch 325/537: Loss=0.6239 (C:0.6239, R:0.0107)
Batch 350/537: Loss=0.6282 (C:0.6282, R:0.0108)
Batch 375/537: Loss=0.6333 (C:0.6333, R:0.0107)
Batch 400/537: Loss=0.6131 (C:0.6131, R:0.0107)
Batch 425/537: Loss=0.6469 (C:0.6469, R:0.0107)
Batch 450/537: Loss=0.6157 (C:0.6157, R:0.0107)
Batch 475/537: Loss=0.6616 (C:0.6616, R:0.0107)
Batch 500/537: Loss=0.6050 (C:0.6050, R:0.0107)
Batch 525/537: Loss=0.5930 (C:0.5930, R:0.0107)

============================================================
Epoch 46/200 completed in 28.2s
Train: Loss=0.6224 (C:0.6224, R:0.0107) Ratio=4.68x
Val:   Loss=0.7554 (C:0.7554, R:0.0106) Ratio=3.16x
Reconstruction weight: 0.240
No improvement for 2 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.6100 (C:0.6100, R:0.0107)
Batch  25/537: Loss=0.6077 (C:0.6077, R:0.0107)
Batch  50/537: Loss=0.5798 (C:0.5798, R:0.0107)
Batch  75/537: Loss=0.6462 (C:0.6462, R:0.0107)
Batch 100/537: Loss=0.6509 (C:0.6509, R:0.0107)
Batch 125/537: Loss=0.6101 (C:0.6101, R:0.0107)
Batch 150/537: Loss=0.6263 (C:0.6263, R:0.0107)
Batch 175/537: Loss=0.6110 (C:0.6110, R:0.0107)
Batch 200/537: Loss=0.5874 (C:0.5874, R:0.0107)
Batch 225/537: Loss=0.6456 (C:0.6456, R:0.0107)
Batch 250/537: Loss=0.6507 (C:0.6507, R:0.0107)
Batch 275/537: Loss=0.6205 (C:0.6205, R:0.0107)
Batch 300/537: Loss=0.6205 (C:0.6205, R:0.0107)
Batch 325/537: Loss=0.6081 (C:0.6081, R:0.0107)
Batch 350/537: Loss=0.6100 (C:0.6100, R:0.0107)
Batch 375/537: Loss=0.6028 (C:0.6028, R:0.0107)
Batch 400/537: Loss=0.6037 (C:0.6037, R:0.0107)
Batch 425/537: Loss=0.6307 (C:0.6307, R:0.0107)
Batch 450/537: Loss=0.6080 (C:0.6080, R:0.0107)
Batch 475/537: Loss=0.6806 (C:0.6806, R:0.0107)
Batch 500/537: Loss=0.6199 (C:0.6199, R:0.0107)
Batch 525/537: Loss=0.6125 (C:0.6125, R:0.0107)

============================================================
Epoch 47/200 completed in 21.4s
Train: Loss=0.6174 (C:0.6174, R:0.0107) Ratio=4.80x
Val:   Loss=0.7562 (C:0.7562, R:0.0106) Ratio=3.18x
Reconstruction weight: 0.255
No improvement for 3 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.5751 (C:0.5751, R:0.0107)
Batch  25/537: Loss=0.6095 (C:0.6095, R:0.0107)
Batch  50/537: Loss=0.5951 (C:0.5951, R:0.0107)
Batch  75/537: Loss=0.6162 (C:0.6162, R:0.0107)
Batch 100/537: Loss=0.6171 (C:0.6171, R:0.0107)
Batch 125/537: Loss=0.6076 (C:0.6076, R:0.0107)
Batch 150/537: Loss=0.6166 (C:0.6166, R:0.0107)
Batch 175/537: Loss=0.6402 (C:0.6402, R:0.0107)
Batch 200/537: Loss=0.6292 (C:0.6292, R:0.0107)
Batch 225/537: Loss=0.6001 (C:0.6001, R:0.0107)
Batch 250/537: Loss=0.6006 (C:0.6006, R:0.0107)
Batch 275/537: Loss=0.6040 (C:0.6040, R:0.0108)
Batch 300/537: Loss=0.6474 (C:0.6474, R:0.0107)
Batch 325/537: Loss=0.6188 (C:0.6188, R:0.0107)
Batch 350/537: Loss=0.6295 (C:0.6295, R:0.0107)
Batch 375/537: Loss=0.6325 (C:0.6325, R:0.0107)
Batch 400/537: Loss=0.6084 (C:0.6084, R:0.0107)
Batch 425/537: Loss=0.5962 (C:0.5962, R:0.0107)
Batch 450/537: Loss=0.6221 (C:0.6221, R:0.0107)
Batch 475/537: Loss=0.6223 (C:0.6223, R:0.0107)
Batch 500/537: Loss=0.6310 (C:0.6310, R:0.0107)
Batch 525/537: Loss=0.6076 (C:0.6076, R:0.0107)

============================================================
Epoch 48/200 completed in 21.5s
Train: Loss=0.6167 (C:0.6167, R:0.0107) Ratio=4.87x
Val:   Loss=0.7711 (C:0.7711, R:0.0106) Ratio=3.12x
Reconstruction weight: 0.270
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 49
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.323 ¬± 0.599
    Neg distances: 2.637 ¬± 1.104
    Separation ratio: 8.17x
    Gap: -4.440
    ‚úÖ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.5695 (C:0.5695, R:0.0107)
Batch  25/537: Loss=0.5603 (C:0.5603, R:0.0107)
Batch  50/537: Loss=0.5520 (C:0.5520, R:0.0107)
Batch  75/537: Loss=0.5342 (C:0.5342, R:0.0107)
Batch 100/537: Loss=0.5788 (C:0.5788, R:0.0107)
Batch 125/537: Loss=0.5482 (C:0.5482, R:0.0107)
Batch 150/537: Loss=0.6026 (C:0.6026, R:0.0107)
Batch 175/537: Loss=0.5528 (C:0.5528, R:0.0107)
Batch 200/537: Loss=0.5709 (C:0.5709, R:0.0107)
Batch 225/537: Loss=0.5374 (C:0.5374, R:0.0107)
Batch 250/537: Loss=0.5878 (C:0.5878, R:0.0107)
Batch 275/537: Loss=0.5762 (C:0.5762, R:0.0107)
Batch 300/537: Loss=0.5655 (C:0.5655, R:0.0107)
Batch 325/537: Loss=0.5756 (C:0.5756, R:0.0107)
Batch 350/537: Loss=0.5807 (C:0.5807, R:0.0107)
Batch 375/537: Loss=0.6002 (C:0.6002, R:0.0107)
Batch 400/537: Loss=0.6012 (C:0.6012, R:0.0107)
Batch 425/537: Loss=0.5788 (C:0.5788, R:0.0107)
Batch 450/537: Loss=0.5658 (C:0.5658, R:0.0107)
Batch 475/537: Loss=0.5683 (C:0.5683, R:0.0107)
Batch 500/537: Loss=0.5534 (C:0.5534, R:0.0107)
Batch 525/537: Loss=0.5712 (C:0.5712, R:0.0107)

============================================================
Epoch 49/200 completed in 27.5s
Train: Loss=0.5683 (C:0.5683, R:0.0107) Ratio=4.85x
Val:   Loss=0.7089 (C:0.7089, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.285
‚úÖ New best model saved (Val Loss: 0.7089)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.5625 (C:0.5625, R:0.0107)
Batch  25/537: Loss=0.5963 (C:0.5963, R:0.0107)
Batch  50/537: Loss=0.5117 (C:0.5117, R:0.0107)
Batch  75/537: Loss=0.5609 (C:0.5609, R:0.0107)
Batch 100/537: Loss=0.5851 (C:0.5851, R:0.0107)
Batch 125/537: Loss=0.5494 (C:0.5494, R:0.0107)
Batch 150/537: Loss=0.5959 (C:0.5959, R:0.0107)
Batch 175/537: Loss=0.5661 (C:0.5661, R:0.0107)
Batch 200/537: Loss=0.5531 (C:0.5531, R:0.0107)
Batch 225/537: Loss=0.5421 (C:0.5421, R:0.0107)
Batch 250/537: Loss=0.5489 (C:0.5489, R:0.0107)
Batch 275/537: Loss=0.5844 (C:0.5844, R:0.0107)
Batch 300/537: Loss=0.6019 (C:0.6019, R:0.0107)
Batch 325/537: Loss=0.5530 (C:0.5530, R:0.0107)
Batch 350/537: Loss=0.6003 (C:0.6003, R:0.0107)
Batch 375/537: Loss=0.5474 (C:0.5474, R:0.0107)
Batch 400/537: Loss=0.5930 (C:0.5930, R:0.0107)
Batch 425/537: Loss=0.5417 (C:0.5417, R:0.0107)
Batch 450/537: Loss=0.5474 (C:0.5474, R:0.0107)
Batch 475/537: Loss=0.5742 (C:0.5742, R:0.0107)
Batch 500/537: Loss=0.5672 (C:0.5672, R:0.0107)
Batch 525/537: Loss=0.5641 (C:0.5641, R:0.0107)

============================================================
Epoch 50/200 completed in 21.5s
Train: Loss=0.5661 (C:0.5661, R:0.0107) Ratio=4.91x
Val:   Loss=0.7224 (C:0.7224, R:0.0106) Ratio=3.17x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 50
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.5555 (C:0.5555, R:0.0107)
Batch  25/537: Loss=0.5540 (C:0.5540, R:0.0107)
Batch  50/537: Loss=0.5544 (C:0.5544, R:0.0107)
Batch  75/537: Loss=0.5575 (C:0.5575, R:0.0107)
Batch 100/537: Loss=0.5373 (C:0.5373, R:0.0107)
Batch 125/537: Loss=0.6024 (C:0.6024, R:0.0107)
Batch 150/537: Loss=0.6026 (C:0.6026, R:0.0107)
Batch 175/537: Loss=0.5600 (C:0.5600, R:0.0107)
Batch 200/537: Loss=0.5793 (C:0.5793, R:0.0108)
Batch 225/537: Loss=0.5442 (C:0.5442, R:0.0108)
Batch 250/537: Loss=0.5569 (C:0.5569, R:0.0107)
Batch 275/537: Loss=0.5754 (C:0.5754, R:0.0107)
Batch 300/537: Loss=0.5986 (C:0.5986, R:0.0107)
Batch 325/537: Loss=0.6080 (C:0.6080, R:0.0107)
Batch 350/537: Loss=0.5520 (C:0.5520, R:0.0107)
Batch 375/537: Loss=0.5652 (C:0.5652, R:0.0107)
Batch 400/537: Loss=0.5564 (C:0.5564, R:0.0107)
Batch 425/537: Loss=0.5661 (C:0.5661, R:0.0107)
Batch 450/537: Loss=0.5544 (C:0.5544, R:0.0107)
Batch 475/537: Loss=0.5971 (C:0.5971, R:0.0107)
Batch 500/537: Loss=0.5553 (C:0.5553, R:0.0107)
Batch 525/537: Loss=0.5644 (C:0.5644, R:0.0107)

============================================================
Epoch 51/200 completed in 21.5s
Train: Loss=0.5659 (C:0.5659, R:0.0107) Ratio=4.90x
Val:   Loss=0.7245 (C:0.7245, R:0.0106) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 52
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.333 ¬± 0.608
    Neg distances: 2.593 ¬± 1.102
    Separation ratio: 7.79x
    Gap: -4.544
    ‚úÖ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.5366 (C:0.5366, R:0.0107)
Batch  25/537: Loss=0.5891 (C:0.5891, R:0.0107)
Batch  50/537: Loss=0.5513 (C:0.5513, R:0.0107)
Batch  75/537: Loss=0.5551 (C:0.5551, R:0.0107)
Batch 100/537: Loss=0.5579 (C:0.5579, R:0.0107)
Batch 125/537: Loss=0.5770 (C:0.5770, R:0.0107)
Batch 150/537: Loss=0.6014 (C:0.6014, R:0.0107)
Batch 175/537: Loss=0.5844 (C:0.5844, R:0.0107)
Batch 200/537: Loss=0.5811 (C:0.5811, R:0.0107)
Batch 225/537: Loss=0.6094 (C:0.6094, R:0.0108)
Batch 250/537: Loss=0.5686 (C:0.5686, R:0.0107)
Batch 275/537: Loss=0.5931 (C:0.5931, R:0.0107)
Batch 300/537: Loss=0.5755 (C:0.5755, R:0.0107)
Batch 325/537: Loss=0.5820 (C:0.5820, R:0.0107)
Batch 350/537: Loss=0.5638 (C:0.5638, R:0.0107)
Batch 375/537: Loss=0.5888 (C:0.5888, R:0.0107)
Batch 400/537: Loss=0.5586 (C:0.5586, R:0.0107)
Batch 425/537: Loss=0.5533 (C:0.5533, R:0.0107)
Batch 450/537: Loss=0.5781 (C:0.5781, R:0.0107)
Batch 475/537: Loss=0.5628 (C:0.5628, R:0.0107)
Batch 500/537: Loss=0.5978 (C:0.5978, R:0.0108)
Batch 525/537: Loss=0.5944 (C:0.5944, R:0.0107)

============================================================
Epoch 52/200 completed in 27.6s
Train: Loss=0.5725 (C:0.5725, R:0.0107) Ratio=5.01x
Val:   Loss=0.7215 (C:0.7215, R:0.0106) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.5770 (C:0.5770, R:0.0107)
Batch  25/537: Loss=0.5717 (C:0.5717, R:0.0107)
Batch  50/537: Loss=0.5734 (C:0.5734, R:0.0107)
Batch  75/537: Loss=0.6138 (C:0.6138, R:0.0107)
Batch 100/537: Loss=0.5970 (C:0.5970, R:0.0107)
Batch 125/537: Loss=0.5850 (C:0.5850, R:0.0107)
Batch 150/537: Loss=0.5476 (C:0.5476, R:0.0107)
Batch 175/537: Loss=0.5344 (C:0.5344, R:0.0107)
Batch 200/537: Loss=0.5412 (C:0.5412, R:0.0107)
Batch 225/537: Loss=0.5565 (C:0.5565, R:0.0107)
Batch 250/537: Loss=0.5963 (C:0.5963, R:0.0107)
Batch 275/537: Loss=0.5438 (C:0.5438, R:0.0107)
Batch 300/537: Loss=0.5959 (C:0.5959, R:0.0108)
Batch 325/537: Loss=0.5561 (C:0.5561, R:0.0107)
Batch 350/537: Loss=0.5390 (C:0.5390, R:0.0107)
Batch 375/537: Loss=0.5746 (C:0.5746, R:0.0107)
Batch 400/537: Loss=0.5862 (C:0.5862, R:0.0107)
Batch 425/537: Loss=0.5654 (C:0.5654, R:0.0107)
Batch 450/537: Loss=0.6013 (C:0.6013, R:0.0107)
Batch 475/537: Loss=0.5906 (C:0.5906, R:0.0107)
Batch 500/537: Loss=0.6056 (C:0.6056, R:0.0107)
Batch 525/537: Loss=0.5603 (C:0.5603, R:0.0107)

============================================================
Epoch 53/200 completed in 21.5s
Train: Loss=0.5714 (C:0.5714, R:0.0107) Ratio=4.97x
Val:   Loss=0.7266 (C:0.7266, R:0.0106) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.5891 (C:0.5891, R:0.0107)
Batch  25/537: Loss=0.6375 (C:0.6375, R:0.0107)
Batch  50/537: Loss=0.5402 (C:0.5402, R:0.0107)
Batch  75/537: Loss=0.5864 (C:0.5864, R:0.0108)
Batch 100/537: Loss=0.5967 (C:0.5967, R:0.0107)
Batch 125/537: Loss=0.5411 (C:0.5411, R:0.0107)
Batch 150/537: Loss=0.5288 (C:0.5288, R:0.0107)
Batch 175/537: Loss=0.5976 (C:0.5976, R:0.0107)
Batch 200/537: Loss=0.5609 (C:0.5609, R:0.0107)
Batch 225/537: Loss=0.5677 (C:0.5677, R:0.0107)
Batch 250/537: Loss=0.5949 (C:0.5949, R:0.0107)
Batch 275/537: Loss=0.5745 (C:0.5745, R:0.0107)
Batch 300/537: Loss=0.6132 (C:0.6132, R:0.0107)
Batch 325/537: Loss=0.5725 (C:0.5725, R:0.0106)
Batch 350/537: Loss=0.5660 (C:0.5660, R:0.0107)
Batch 375/537: Loss=0.5771 (C:0.5771, R:0.0107)
Batch 400/537: Loss=0.5838 (C:0.5838, R:0.0107)
Batch 425/537: Loss=0.5706 (C:0.5706, R:0.0108)
Batch 450/537: Loss=0.5827 (C:0.5827, R:0.0107)
Batch 475/537: Loss=0.5758 (C:0.5758, R:0.0107)
Batch 500/537: Loss=0.5764 (C:0.5764, R:0.0107)
Batch 525/537: Loss=0.5636 (C:0.5636, R:0.0107)

============================================================
Epoch 54/200 completed in 21.5s
Train: Loss=0.5704 (C:0.5704, R:0.0107) Ratio=4.95x
Val:   Loss=0.7239 (C:0.7239, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 55
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.324 ¬± 0.601
    Neg distances: 2.630 ¬± 1.105
    Separation ratio: 8.12x
    Gap: -4.539
    ‚úÖ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.5631 (C:0.5631, R:0.0107)
Batch  25/537: Loss=0.5794 (C:0.5794, R:0.0107)
Batch  50/537: Loss=0.5527 (C:0.5527, R:0.0107)
Batch  75/537: Loss=0.5441 (C:0.5441, R:0.0107)
Batch 100/537: Loss=0.5725 (C:0.5725, R:0.0107)
Batch 125/537: Loss=0.5698 (C:0.5698, R:0.0107)
Batch 150/537: Loss=0.5770 (C:0.5770, R:0.0107)
Batch 175/537: Loss=0.5730 (C:0.5730, R:0.0107)
Batch 200/537: Loss=0.5784 (C:0.5784, R:0.0107)
Batch 225/537: Loss=0.5578 (C:0.5578, R:0.0107)
Batch 250/537: Loss=0.5450 (C:0.5450, R:0.0107)
Batch 275/537: Loss=0.5541 (C:0.5541, R:0.0107)
Batch 300/537: Loss=0.5553 (C:0.5553, R:0.0107)
Batch 325/537: Loss=0.5598 (C:0.5598, R:0.0107)
Batch 350/537: Loss=0.5415 (C:0.5415, R:0.0107)
Batch 375/537: Loss=0.5725 (C:0.5725, R:0.0107)
Batch 400/537: Loss=0.5579 (C:0.5579, R:0.0107)
Batch 425/537: Loss=0.5718 (C:0.5718, R:0.0108)
Batch 450/537: Loss=0.5589 (C:0.5589, R:0.0107)
Batch 475/537: Loss=0.5742 (C:0.5742, R:0.0107)
Batch 500/537: Loss=0.5677 (C:0.5677, R:0.0107)
Batch 525/537: Loss=0.5786 (C:0.5786, R:0.0107)

============================================================
Epoch 55/200 completed in 27.3s
Train: Loss=0.5564 (C:0.5564, R:0.0107) Ratio=4.99x
Val:   Loss=0.7192 (C:0.7192, R:0.0106) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 6 epochs
Checkpoint saved at epoch 55
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.6023 (C:0.6023, R:0.0107)
Batch  25/537: Loss=0.5364 (C:0.5364, R:0.0108)
Batch  50/537: Loss=0.5247 (C:0.5247, R:0.0107)
Batch  75/537: Loss=0.5381 (C:0.5381, R:0.0107)
Batch 100/537: Loss=0.5299 (C:0.5299, R:0.0107)
Batch 125/537: Loss=0.5457 (C:0.5457, R:0.0107)
Batch 150/537: Loss=0.5641 (C:0.5641, R:0.0107)
Batch 175/537: Loss=0.5246 (C:0.5246, R:0.0107)
Batch 200/537: Loss=0.5483 (C:0.5483, R:0.0107)
Batch 225/537: Loss=0.5470 (C:0.5470, R:0.0107)
Batch 250/537: Loss=0.5559 (C:0.5559, R:0.0107)
Batch 275/537: Loss=0.5350 (C:0.5350, R:0.0107)
Batch 300/537: Loss=0.5815 (C:0.5815, R:0.0107)
Batch 325/537: Loss=0.5385 (C:0.5385, R:0.0108)
Batch 350/537: Loss=0.5549 (C:0.5549, R:0.0107)
Batch 375/537: Loss=0.5619 (C:0.5619, R:0.0107)
Batch 400/537: Loss=0.5419 (C:0.5419, R:0.0107)
Batch 425/537: Loss=0.5919 (C:0.5919, R:0.0107)
Batch 450/537: Loss=0.5716 (C:0.5716, R:0.0108)
Batch 475/537: Loss=0.5946 (C:0.5946, R:0.0107)
Batch 500/537: Loss=0.5707 (C:0.5707, R:0.0107)
Batch 525/537: Loss=0.5903 (C:0.5903, R:0.0107)

============================================================
Epoch 56/200 completed in 21.4s
Train: Loss=0.5552 (C:0.5552, R:0.0107) Ratio=5.06x
Val:   Loss=0.7159 (C:0.7159, R:0.0106) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.5629 (C:0.5629, R:0.0107)
Batch  25/537: Loss=0.5537 (C:0.5537, R:0.0107)
Batch  50/537: Loss=0.5212 (C:0.5212, R:0.0107)
Batch  75/537: Loss=0.5586 (C:0.5586, R:0.0107)
Batch 100/537: Loss=0.5296 (C:0.5296, R:0.0107)
Batch 125/537: Loss=0.5557 (C:0.5557, R:0.0107)
Batch 150/537: Loss=0.5249 (C:0.5249, R:0.0107)
Batch 175/537: Loss=0.5671 (C:0.5671, R:0.0107)
Batch 200/537: Loss=0.5488 (C:0.5488, R:0.0107)
Batch 225/537: Loss=0.5661 (C:0.5661, R:0.0107)
Batch 250/537: Loss=0.5853 (C:0.5853, R:0.0107)
Batch 275/537: Loss=0.5426 (C:0.5426, R:0.0107)
Batch 300/537: Loss=0.5564 (C:0.5564, R:0.0107)
Batch 325/537: Loss=0.5381 (C:0.5381, R:0.0107)
Batch 350/537: Loss=0.5258 (C:0.5258, R:0.0107)
Batch 375/537: Loss=0.5732 (C:0.5732, R:0.0107)
Batch 400/537: Loss=0.5827 (C:0.5827, R:0.0107)
Batch 425/537: Loss=0.5608 (C:0.5608, R:0.0107)
Batch 450/537: Loss=0.5234 (C:0.5234, R:0.0107)
Batch 475/537: Loss=0.5269 (C:0.5269, R:0.0107)
Batch 500/537: Loss=0.5317 (C:0.5317, R:0.0107)
Batch 525/537: Loss=0.5540 (C:0.5540, R:0.0107)

============================================================
Epoch 57/200 completed in 21.4s
Train: Loss=0.5542 (C:0.5542, R:0.0107) Ratio=5.17x
Val:   Loss=0.7160 (C:0.7160, R:0.0106) Ratio=3.15x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 57 epochs
Best model was at epoch 49 with Val Loss: 0.7089

Global Dataset Training Completed!
Best epoch: 49
Best validation loss: 0.7089
Final separation ratios: Train=5.17x, Val=3.15x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4670
  Adjusted Rand Score: 0.5359
  Clustering Accuracy: 0.8177
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8208
  Per-class F1: [0.8460253841015363, 0.7585007116874901, 0.8609687551661431]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010623
Evaluating separation quality...
Separation Results:
  Positive distances: 0.758 ¬± 0.904
  Negative distances: 2.325 ¬± 1.236
  Separation ratio: 3.07x
  Gap: -4.685
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4670
  Clustering Accuracy: 0.8177
  Adjusted Rand Score: 0.5359

Classification Performance:
  Accuracy: 0.8208

Separation Quality:
  Separation Ratio: 3.07x
  Gap: -4.685
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010623
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456/results/evaluation_results_20250715_153959.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456/results/evaluation_results_20250715_153959.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_20250715_151456/final_results.json

Key Results:
  Separation ratio: 3.07x
  Perfect separation: False
  Classification accuracy: 0.8208

Analysis completed with exit code: 0
Time: Tue 15 Jul 15:40:01 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
