Starting Surface Distance Metric Analysis job...
Job ID: 184560
Node: gpuvm14
Time: Mon 21 Jul 16:24:45 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Mon Jul 21 16:24:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   43C    P8             14W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_162459
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_162459/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 537
  Test batches: 539
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 549367 samples, 537 batches
  Test: 549367 samples, 539 batches
AttentionAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Attention Heads: 5
  Total parameters: 5,881,841
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
MoorTopologicalLoss Initialized: Using 0-dimensional persistence pairings (MST edges).
No prototypes being used for topological loss - whole dataset instead.
TopologicalTrainer initialized on device: cuda
Model parameters: 5,881,841
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.01
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=3.0696 (C:3.0685, R:0.0110, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.8944 (C:1.8934, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.7651 (C:1.7641, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.6511 (C:1.6501, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.6172 (C:1.6162, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.5981 (C:1.5971, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.6207 (C:1.6197, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.5842 (C:1.5832, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4828 (C:1.4818, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.5179 (C:1.5169, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.5308 (C:1.5298, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4984 (C:1.4974, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4569 (C:1.4559, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4839 (C:1.4829, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4507 (C:1.4497, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4446 (C:1.4436, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4797 (C:1.4787, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4572 (C:1.4562, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4320 (C:1.4310, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4613 (C:1.4604, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4254 (C:1.4244, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4278 (C:1.4268, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5609
  Contrastive: 1.5599
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4054
  Contrastive: 1.4044
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/100 COMPLETE (22.4s)
Train Loss: 1.5609 (C:1.5599, R:0.0100, T:0.0000)
Val Loss:   1.4054 (C:1.4044, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.4326 (C:1.4316, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.5029 (C:1.5019, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4811 (C:1.4801, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4858 (C:1.4849, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4163 (C:1.4153, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4633 (C:1.4623, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4043 (C:1.4033, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.5123 (C:1.5113, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4360 (C:1.4350, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4257 (C:1.4248, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3699 (C:1.3689, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4779 (C:1.4769, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.3751 (C:1.3741, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4152 (C:1.4142, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4423 (C:1.4414, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4342 (C:1.4332, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.3386 (C:1.3376, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4891 (C:1.4881, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4191 (C:1.4181, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3690 (C:1.3680, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.3440 (C:1.3430, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.3330 (C:1.3320, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.4246
  Contrastive: 1.4236
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3905
  Contrastive: 1.3895
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 2/100 COMPLETE (21.5s)
Train Loss: 1.4246 (C:1.4236, R:0.0100, T:0.0000)
Val Loss:   1.3905 (C:1.3895, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3823 (C:1.3813, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4354 (C:1.4344, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4817 (C:1.4807, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4054 (C:1.4044, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.3553 (C:1.3543, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4582 (C:1.4573, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3736 (C:1.3726, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.4291 (C:1.4281, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4928 (C:1.4918, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4444 (C:1.4434, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3718 (C:1.3708, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4002 (C:1.3992, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4026 (C:1.4016, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4089 (C:1.4079, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3915 (C:1.3905, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.3900 (C:1.3890, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4780 (C:1.4770, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.3801 (C:1.3791, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.3682 (C:1.3672, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.3975 (C:1.3965, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4014 (C:1.4005, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4339 (C:1.4329, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.4189
  Contrastive: 1.4180
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3736
  Contrastive: 1.3726
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)
✅ New best model saved!

🎯 EPOCH 3/100 COMPLETE (24.8s)
Train Loss: 1.4189 (C:1.4180, R:0.0100, T:0.0000)
Val Loss:   1.3736 (C:1.3726, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.3185 (C:1.3175, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4038 (C:1.4029, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.5334 (C:1.5324, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3854 (C:1.3844, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4973 (C:1.4963, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4424 (C:1.4414, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.3705 (C:1.3695, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3910 (C:1.3900, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4226 (C:1.4216, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.3801 (C:1.3791, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3966 (C:1.3956, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.5168 (C:1.5159, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4410 (C:1.4400, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3608 (C:1.3598, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4586 (C:1.4576, R:0.0100, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4584 (C:1.4574, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4427 (C:1.4417, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4809 (C:1.4800, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4192 (C:1.4182, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4943 (C:1.4933, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4599 (C:1.4589, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4576 (C:1.4566, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.4305
  Contrastive: 1.4295
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4112
  Contrastive: 1.4102
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

🎯 EPOCH 4/100 COMPLETE (24.4s)
Train Loss: 1.4305 (C:1.4295, R:0.0100, T:0.0000)
Val Loss:   1.4112 (C:1.4102, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.4621 (C:1.4611, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4077 (C:1.4067, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4140 (C:1.4130, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.3745 (C:1.3735, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4008 (C:1.3998, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4461 (C:1.4451, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4178 (C:1.4169, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3884 (C:1.3874, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.3781 (C:1.3771, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.3892 (C:1.3882, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4576 (C:1.4566, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4258 (C:1.4248, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4277 (C:1.4267, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.3912 (C:1.3902, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.3307 (C:1.3297, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4458 (C:1.4448, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.4246 (C:1.4236, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4906 (C:1.4896, R:0.0100, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4491 (C:1.4481, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.5210 (C:1.5200, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.5050 (C:1.5040, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4944 (C:1.4934, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.4246
  Contrastive: 1.4236
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4482
  Contrastive: 1.4472
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

🎯 EPOCH 5/100 COMPLETE (25.0s)
Train Loss: 1.4246 (C:1.4236, R:0.0100, T:0.0000)
Val Loss:   1.4482 (C:1.4472, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.5160 (C:1.5150, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.5164 (C:1.5154, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.5277 (C:1.5267, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4579 (C:1.4569, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4192 (C:1.4182, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4142 (C:1.4132, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.5747 (C:1.5737, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.4362 (C:1.4352, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.5820 (C:1.5810, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4802 (C:1.4793, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4274 (C:1.4264, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4587 (C:1.4577, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4260 (C:1.4250, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4638 (C:1.4628, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4721 (C:1.4711, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4496 (C:1.4486, R:0.0100, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.5576 (C:1.5566, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4865 (C:1.4855, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4501 (C:1.4491, R:0.0100, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4679 (C:1.4669, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.5509 (C:1.5499, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4703 (C:1.4693, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.4649
  Contrastive: 1.4639
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4404
  Contrastive: 1.4394
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

🎯 EPOCH 6/100 COMPLETE (22.0s)
Train Loss: 1.4649 (C:1.4639, R:0.0100, T:0.0000)
Val Loss:   1.4404 (C:1.4394, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.4999 (C:1.4989, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.6113 (C:1.6103, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4860 (C:1.4850, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4836 (C:1.4826, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.4750 (C:1.4740, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.5176 (C:1.5166, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4404 (C:1.4394, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.3806 (C:1.3796, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4630 (C:1.4621, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.4033 (C:1.4023, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.3818 (C:1.3808, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4453 (C:1.4443, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4380 (C:1.4370, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4328 (C:1.4318, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.5374 (C:1.5364, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4202 (C:1.4192, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.5002 (C:1.4992, R:0.0100, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.5618 (C:1.5608, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4436 (C:1.4427, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.5005 (C:1.4995, R:0.0099, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4577 (C:1.4567, R:0.0099, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4826 (C:1.4816, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.4657
  Contrastive: 1.4647
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4388
  Contrastive: 1.4378
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

🎯 EPOCH 7/100 COMPLETE (21.6s)
Train Loss: 1.4657 (C:1.4647, R:0.0100, T:0.0000)
Val Loss:   1.4388 (C:1.4378, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 537 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/537: Loss=1.5028 (C:1.5018, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/537: Loss=1.4571 (C:1.4561, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/537: Loss=1.4473 (C:1.4463, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/537: Loss=1.4678 (C:1.4668, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/537: Loss=1.5152 (C:1.5142, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/537: Loss=1.4915 (C:1.4905, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/537: Loss=1.4652 (C:1.4642, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/537: Loss=1.5256 (C:1.5246, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/537: Loss=1.4188 (C:1.4178, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/537: Loss=1.6005 (C:1.5995, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/537: Loss=1.4921 (C:1.4911, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/537: Loss=1.4575 (C:1.4565, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/537: Loss=1.4039 (C:1.4029, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/537: Loss=1.4211 (C:1.4201, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/537: Loss=1.4513 (C:1.4503, R:0.0099, T:0.0000(w:0.000)❌)
Batch 375/537: Loss=1.4456 (C:1.4446, R:0.0099, T:0.0000(w:0.000)❌)
Batch 400/537: Loss=1.5525 (C:1.5515, R:0.0099, T:0.0000(w:0.000)❌)
Batch 425/537: Loss=1.4664 (C:1.4654, R:0.0099, T:0.0000(w:0.000)❌)
Batch 450/537: Loss=1.4807 (C:1.4798, R:0.0099, T:0.0000(w:0.000)❌)
Batch 475/537: Loss=1.4876 (C:1.4866, R:0.0100, T:0.0000(w:0.000)❌)
Batch 500/537: Loss=1.4465 (C:1.4455, R:0.0100, T:0.0000(w:0.000)❌)
Batch 525/537: Loss=1.4552 (C:1.4542, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.4737
  Contrastive: 1.4727
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.4424
  Contrastive: 1.4414
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/537 (0.0%)

🎯 EPOCH 8/100 COMPLETE (24.4s)
Train Loss: 1.4737 (C:1.4727, R:0.0100, T:0.0000)
Val Loss:   1.4424 (C:1.4414, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 537 | Topological Weight: 0.0100
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6377 (C:1.4656, R:0.0099, T:17.1165(w:0.010)⚠️)
Batch  25/537: Loss=1.5752 (C:1.4018, R:0.0099, T:17.2420(w:0.010)⚠️)
Batch  50/537: Loss=1.6401 (C:1.4704, R:0.0099, T:16.8643(w:0.010)⚠️)
Batch  75/537: Loss=1.6423 (C:1.4695, R:0.0100, T:17.1737(w:0.010)⚠️)
Batch 100/537: Loss=1.6935 (C:1.5112, R:0.0100, T:18.1294(w:0.010)⚠️)
Batch 125/537: Loss=1.6278 (C:1.4445, R:0.0100, T:18.2312(w:0.010)⚠️)
Batch 150/537: Loss=1.6942 (C:1.5131, R:0.0100, T:18.0074(w:0.010)⚠️)
Batch 175/537: Loss=1.6849 (C:1.4955, R:0.0100, T:18.8463(w:0.010)⚠️)
Batch 200/537: Loss=1.6113 (C:1.4371, R:0.0100, T:17.3211(w:0.010)⚠️)
Batch 225/537: Loss=1.6336 (C:1.4682, R:0.0099, T:16.4399(w:0.010)⚠️)
Batch 250/537: Loss=1.6223 (C:1.4553, R:0.0100, T:16.6042(w:0.010)⚠️)
Batch 275/537: Loss=1.6513 (C:1.4768, R:0.0100, T:17.3498(w:0.010)⚠️)
Batch 300/537: Loss=1.6596 (C:1.4928, R:0.0100, T:16.5869(w:0.010)⚠️)
Batch 325/537: Loss=1.5891 (C:1.4133, R:0.0100, T:17.4763(w:0.010)⚠️)
Batch 350/537: Loss=1.6320 (C:1.4541, R:0.0099, T:17.6950(w:0.010)⚠️)
Batch 375/537: Loss=1.5893 (C:1.4149, R:0.0099, T:17.3388(w:0.010)⚠️)
Batch 400/537: Loss=1.5570 (C:1.3878, R:0.0100, T:16.8248(w:0.010)⚠️)
Batch 425/537: Loss=1.6877 (C:1.5119, R:0.0100, T:17.4830(w:0.010)⚠️)
Batch 450/537: Loss=1.6354 (C:1.4662, R:0.0100, T:16.8215(w:0.010)⚠️)
Batch 475/537: Loss=1.6319 (C:1.4441, R:0.0100, T:18.6817(w:0.010)⚠️)
Batch 500/537: Loss=1.6758 (C:1.4828, R:0.0099, T:19.1974(w:0.010)⚠️)
Batch 525/537: Loss=1.5998 (C:1.4220, R:0.0099, T:17.6804(w:0.010)⚠️)
🎉 MILESTONE: First topological learning detected at epoch 9!
   Initial topological loss: 17.5807
📈 New best topological loss: 17.5807

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.6294
  Contrastive: 1.4526
  Reconstruction: 0.0100
  Topological: 17.5807 (weight: 0.010)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.5964
  Contrastive: 1.4343
  Reconstruction: 0.0100
  Topological: 16.1074 (weight: 0.010)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 9/100 COMPLETE (81.7s)
Train Loss: 1.6294 (C:1.4526, R:0.0100, T:17.5807)
Val Loss:   1.5964 (C:1.4343, R:0.0100, T:16.1074)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 537 | Topological Weight: 0.0112
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7037 (C:1.5122, R:0.0100, T:16.9369(w:0.011)⚠️)
Batch  25/537: Loss=1.6652 (C:1.4723, R:0.0100, T:17.0594(w:0.011)⚠️)
Batch  50/537: Loss=1.6383 (C:1.4483, R:0.0100, T:16.7980(w:0.011)⚠️)
Batch  75/537: Loss=1.6273 (C:1.4340, R:0.0100, T:17.0928(w:0.011)⚠️)
Batch 100/537: Loss=1.6058 (C:1.4100, R:0.0100, T:17.3078(w:0.011)⚠️)
Batch 125/537: Loss=1.5870 (C:1.4046, R:0.0099, T:16.1278(w:0.011)⚠️)
Batch 150/537: Loss=1.6045 (C:1.4138, R:0.0100, T:16.8618(w:0.011)⚠️)
Batch 175/537: Loss=1.5361 (C:1.3448, R:0.0100, T:16.9218(w:0.011)⚠️)
Batch 200/537: Loss=1.5918 (C:1.3872, R:0.0100, T:18.0982(w:0.011)⚠️)
Batch 225/537: Loss=1.6016 (C:1.4063, R:0.0100, T:17.2669(w:0.011)⚠️)
Batch 250/537: Loss=1.6086 (C:1.4149, R:0.0100, T:17.1311(w:0.011)⚠️)
Batch 275/537: Loss=1.6601 (C:1.4675, R:0.0099, T:17.0323(w:0.011)⚠️)
Batch 300/537: Loss=1.6692 (C:1.4745, R:0.0100, T:17.2236(w:0.011)⚠️)
Batch 325/537: Loss=1.5444 (C:1.3521, R:0.0100, T:17.0045(w:0.011)⚠️)
Batch 350/537: Loss=1.5650 (C:1.3707, R:0.0100, T:17.1810(w:0.011)⚠️)
Batch 375/537: Loss=1.5429 (C:1.3470, R:0.0099, T:17.3311(w:0.011)⚠️)
Batch 400/537: Loss=1.5933 (C:1.3960, R:0.0099, T:17.4426(w:0.011)⚠️)
Batch 425/537: Loss=1.6659 (C:1.4761, R:0.0099, T:16.7823(w:0.011)⚠️)
Batch 450/537: Loss=1.6657 (C:1.4750, R:0.0099, T:16.8593(w:0.011)⚠️)
Batch 475/537: Loss=1.7083 (C:1.5038, R:0.0100, T:18.0885(w:0.011)⚠️)
Batch 500/537: Loss=1.6110 (C:1.4177, R:0.0099, T:17.0917(w:0.011)⚠️)
Batch 525/537: Loss=1.5906 (C:1.3937, R:0.0099, T:17.4078(w:0.011)⚠️)
📈 New best topological loss: 17.2788

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.6289
  Contrastive: 1.4335
  Reconstruction: 0.0100
  Topological: 17.2788 (weight: 0.011)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6052
  Contrastive: 1.4212
  Reconstruction: 0.0100
  Topological: 16.2700 (weight: 0.011)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 10/100 COMPLETE (82.2s)
Train Loss: 1.6289 (C:1.4335, R:0.0100, T:17.2788)
Val Loss:   1.6052 (C:1.4212, R:0.0100, T:16.2700)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 537 | Topological Weight: 0.0125
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7130 (C:1.4961, R:0.0099, T:17.2765(w:0.013)⚠️)
Batch  25/537: Loss=1.6974 (C:1.4873, R:0.0099, T:16.7255(w:0.013)⚠️)
Batch  50/537: Loss=1.6894 (C:1.4782, R:0.0099, T:16.8156(w:0.013)⚠️)
Batch  75/537: Loss=1.5907 (C:1.3855, R:0.0100, T:16.3405(w:0.013)⚠️)
Batch 100/537: Loss=1.7159 (C:1.4899, R:0.0100, T:17.9976(w:0.013)⚠️)
Batch 125/537: Loss=1.6906 (C:1.4750, R:0.0100, T:17.1732(w:0.013)⚠️)
Batch 150/537: Loss=1.7222 (C:1.5084, R:0.0100, T:17.0274(w:0.013)⚠️)
Batch 175/537: Loss=1.6432 (C:1.4196, R:0.0100, T:17.8149(w:0.013)⚠️)
Batch 200/537: Loss=1.6575 (C:1.4389, R:0.0100, T:17.4147(w:0.013)⚠️)
Batch 225/537: Loss=1.6343 (C:1.4160, R:0.0099, T:17.3829(w:0.013)⚠️)
Batch 250/537: Loss=1.6183 (C:1.4085, R:0.0100, T:16.7011(w:0.013)⚠️)
Batch 275/537: Loss=1.6627 (C:1.4531, R:0.0100, T:16.6849(w:0.013)⚠️)
Batch 300/537: Loss=1.7084 (C:1.4949, R:0.0100, T:17.0007(w:0.013)⚠️)
Batch 325/537: Loss=1.6783 (C:1.4609, R:0.0100, T:17.3145(w:0.013)⚠️)
Batch 350/537: Loss=1.6094 (C:1.3836, R:0.0099, T:17.9822(w:0.013)⚠️)
Batch 375/537: Loss=1.6598 (C:1.4299, R:0.0099, T:18.3166(w:0.013)⚠️)
Batch 400/537: Loss=1.6344 (C:1.3992, R:0.0100, T:18.7380(w:0.013)⚠️)
Batch 425/537: Loss=1.7111 (C:1.5032, R:0.0100, T:16.5557(w:0.013)⚠️)
Batch 450/537: Loss=1.6524 (C:1.4380, R:0.0100, T:17.0705(w:0.013)⚠️)
Batch 475/537: Loss=1.5888 (C:1.3800, R:0.0099, T:16.6213(w:0.013)⚠️)
Batch 500/537: Loss=1.6433 (C:1.4421, R:0.0100, T:16.0148(w:0.013)⚠️)
Batch 525/537: Loss=1.5972 (C:1.3872, R:0.0099, T:16.7212(w:0.013)⚠️)
📈 New best topological loss: 17.2477

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 1.6708
  Contrastive: 1.4542
  Reconstruction: 0.0100
  Topological: 17.2477 (weight: 0.013)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6283
  Contrastive: 1.4237
  Reconstruction: 0.0100
  Topological: 16.2904 (weight: 0.013)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 11/100 COMPLETE (81.6s)
Train Loss: 1.6708 (C:1.4542, R:0.0100, T:17.2477)
Val Loss:   1.6283 (C:1.4237, R:0.0100, T:16.2904)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 537 | Topological Weight: 0.0138
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6974 (C:1.4645, R:0.0099, T:16.8665(w:0.014)⚠️)
Batch  25/537: Loss=1.6764 (C:1.4447, R:0.0100, T:16.7817(w:0.014)⚠️)
Batch  50/537: Loss=1.6577 (C:1.4226, R:0.0100, T:17.0236(w:0.014)⚠️)
Batch  75/537: Loss=1.7317 (C:1.4965, R:0.0100, T:17.0305(w:0.014)⚠️)
Batch 100/537: Loss=1.7145 (C:1.4878, R:0.0100, T:16.4199(w:0.014)⚠️)
Batch 125/537: Loss=1.7032 (C:1.4594, R:0.0100, T:17.6639(w:0.014)⚠️)
Batch 150/537: Loss=1.7193 (C:1.4894, R:0.0099, T:16.6490(w:0.014)⚠️)
Batch 175/537: Loss=1.6591 (C:1.4227, R:0.0099, T:17.1240(w:0.014)⚠️)
Batch 200/537: Loss=1.7121 (C:1.4797, R:0.0100, T:16.8282(w:0.014)⚠️)
Batch 225/537: Loss=1.7388 (C:1.5089, R:0.0099, T:16.6445(w:0.014)⚠️)
Batch 250/537: Loss=1.7361 (C:1.5071, R:0.0099, T:16.5829(w:0.014)⚠️)
Batch 275/537: Loss=1.7184 (C:1.4807, R:0.0100, T:17.2173(w:0.014)⚠️)
Batch 300/537: Loss=1.7192 (C:1.4888, R:0.0099, T:16.6895(w:0.014)⚠️)
Batch 325/537: Loss=1.7399 (C:1.5117, R:0.0100, T:16.5234(w:0.014)⚠️)
Batch 350/537: Loss=1.6517 (C:1.4212, R:0.0099, T:16.6897(w:0.014)⚠️)
Batch 375/537: Loss=1.6856 (C:1.4502, R:0.0099, T:17.0481(w:0.014)⚠️)
Batch 400/537: Loss=1.7494 (C:1.4917, R:0.0099, T:18.6725(w:0.014)⚠️)
Batch 425/537: Loss=1.6469 (C:1.3827, R:0.0100, T:19.1441(w:0.014)⚠️)
Batch 450/537: Loss=1.7087 (C:1.4714, R:0.0099, T:17.1840(w:0.014)⚠️)
Batch 475/537: Loss=1.7129 (C:1.4714, R:0.0099, T:17.4931(w:0.014)⚠️)
Batch 500/537: Loss=1.6556 (C:1.4226, R:0.0099, T:16.8716(w:0.014)⚠️)
Batch 525/537: Loss=1.7807 (C:1.5388, R:0.0099, T:17.5265(w:0.014)⚠️)
📈 New best topological loss: 17.2414

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 1.7083
  Contrastive: 1.4702
  Reconstruction: 0.0100
  Topological: 17.2414 (weight: 0.014)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6855
  Contrastive: 1.4630
  Reconstruction: 0.0100
  Topological: 16.1105 (weight: 0.014)
  Batches with topology: 537/537 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/100 COMPLETE (81.8s)
Train Loss: 1.7083 (C:1.4702, R:0.0100, T:17.2414)
Val Loss:   1.6855 (C:1.4630, R:0.0100, T:16.1105)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 537 | Topological Weight: 0.0150
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7125 (C:1.4369, R:0.0100, T:18.3096(w:0.015)⚠️)
Batch  25/537: Loss=1.6359 (C:1.3840, R:0.0099, T:16.7263(w:0.015)⚠️)
Batch  50/537: Loss=1.6948 (C:1.4295, R:0.0100, T:17.6173(w:0.015)⚠️)
Batch  75/537: Loss=1.7257 (C:1.4753, R:0.0100, T:16.6262(w:0.015)⚠️)
Batch 100/537: Loss=1.7503 (C:1.4950, R:0.0100, T:16.9523(w:0.015)⚠️)
Batch 125/537: Loss=1.7138 (C:1.4664, R:0.0099, T:16.4312(w:0.015)⚠️)
Batch 150/537: Loss=1.7038 (C:1.4560, R:0.0099, T:16.4584(w:0.015)⚠️)
Batch 175/537: Loss=1.6955 (C:1.4387, R:0.0100, T:17.0483(w:0.015)⚠️)
Batch 200/537: Loss=1.6970 (C:1.4376, R:0.0100, T:17.2270(w:0.015)⚠️)
Batch 225/537: Loss=1.8253 (C:1.5699, R:0.0099, T:16.9563(w:0.015)⚠️)
Batch 250/537: Loss=1.6295 (C:1.3533, R:0.0100, T:18.3492(w:0.015)⚠️)
Batch 275/537: Loss=1.6417 (C:1.3868, R:0.0100, T:16.9230(w:0.015)⚠️)
Batch 300/537: Loss=1.7624 (C:1.4873, R:0.0099, T:18.2743(w:0.015)⚠️)
Batch 325/537: Loss=1.7759 (C:1.5169, R:0.0099, T:17.2015(w:0.015)⚠️)
Batch 350/537: Loss=1.7642 (C:1.5165, R:0.0100, T:16.4497(w:0.015)⚠️)
Batch 375/537: Loss=1.7681 (C:1.5066, R:0.0099, T:17.3653(w:0.015)⚠️)
Batch 400/537: Loss=1.6979 (C:1.4437, R:0.0100, T:16.8806(w:0.015)⚠️)
Batch 425/537: Loss=1.6984 (C:1.4513, R:0.0100, T:16.4044(w:0.015)⚠️)
Batch 450/537: Loss=1.7792 (C:1.5326, R:0.0100, T:16.3751(w:0.015)⚠️)
Batch 475/537: Loss=1.6515 (C:1.3912, R:0.0100, T:17.2835(w:0.015)⚠️)
Batch 500/537: Loss=1.7730 (C:1.5155, R:0.0099, T:17.0982(w:0.015)⚠️)
Batch 525/537: Loss=1.6983 (C:1.4503, R:0.0100, T:16.4668(w:0.015)⚠️)
📈 New best topological loss: 17.0738

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 1.7213
  Contrastive: 1.4642
  Reconstruction: 0.0100
  Topological: 17.0738 (weight: 0.015)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6780
  Contrastive: 1.4365
  Reconstruction: 0.0100
  Topological: 16.0351 (weight: 0.015)
  Batches with topology: 537/537 (100.0%)
✅ New best model saved!

🎯 EPOCH 13/100 COMPLETE (82.5s)
Train Loss: 1.7213 (C:1.4642, R:0.0100, T:17.0738)
Val Loss:   1.6780 (C:1.4365, R:0.0100, T:16.0351)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 537 | Topological Weight: 0.0163
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.6725 (C:1.3976, R:0.0099, T:16.8513(w:0.016)⚠️)
Batch  25/537: Loss=1.7416 (C:1.4586, R:0.0099, T:17.3545(w:0.016)⚠️)
Batch  50/537: Loss=1.7175 (C:1.4368, R:0.0100, T:17.2172(w:0.016)⚠️)
Batch  75/537: Loss=1.7531 (C:1.4745, R:0.0100, T:17.0831(w:0.016)⚠️)
Batch 100/537: Loss=1.7215 (C:1.4585, R:0.0099, T:16.1265(w:0.016)⚠️)
Batch 125/537: Loss=1.7136 (C:1.4451, R:0.0100, T:16.4613(w:0.016)⚠️)
Batch 150/537: Loss=1.7508 (C:1.4696, R:0.0100, T:17.2408(w:0.016)⚠️)
Batch 175/537: Loss=1.7650 (C:1.4773, R:0.0099, T:17.6452(w:0.016)⚠️)
Batch 200/537: Loss=1.6554 (C:1.3806, R:0.0099, T:16.8508(w:0.016)⚠️)
Batch 225/537: Loss=1.7491 (C:1.4628, R:0.0099, T:17.5597(w:0.016)⚠️)
Batch 250/537: Loss=1.7607 (C:1.4791, R:0.0100, T:17.2700(w:0.016)⚠️)
Batch 275/537: Loss=1.6615 (C:1.3817, R:0.0100, T:17.1577(w:0.016)⚠️)
Batch 300/537: Loss=1.6791 (C:1.4133, R:0.0099, T:16.2980(w:0.016)⚠️)
Batch 325/537: Loss=1.6341 (C:1.3657, R:0.0100, T:16.4560(w:0.016)⚠️)
Batch 350/537: Loss=1.7166 (C:1.4464, R:0.0099, T:16.5646(w:0.016)⚠️)
Batch 375/537: Loss=1.6557 (C:1.3781, R:0.0099, T:17.0212(w:0.016)⚠️)
Batch 400/537: Loss=1.7252 (C:1.4529, R:0.0100, T:16.6940(w:0.016)⚠️)
Batch 425/537: Loss=1.6850 (C:1.4090, R:0.0099, T:16.9260(w:0.016)⚠️)
Batch 450/537: Loss=1.6526 (C:1.3763, R:0.0100, T:16.9405(w:0.016)⚠️)
Batch 475/537: Loss=1.8035 (C:1.5100, R:0.0099, T:17.9981(w:0.016)⚠️)
Batch 500/537: Loss=1.7275 (C:1.4213, R:0.0100, T:18.7801(w:0.016)⚠️)
Batch 525/537: Loss=1.6574 (C:1.3691, R:0.0100, T:17.6819(w:0.016)⚠️)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 1.7185
  Contrastive: 1.4383
  Reconstruction: 0.0100
  Topological: 17.1842 (weight: 0.016)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7176
  Contrastive: 1.4492
  Reconstruction: 0.0100
  Topological: 16.4565 (weight: 0.016)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 14/100 COMPLETE (87.8s)
Train Loss: 1.7185 (C:1.4383, R:0.0100, T:17.1842)
Val Loss:   1.7176 (C:1.4492, R:0.0100, T:16.4565)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 537 | Topological Weight: 0.0175
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7444 (C:1.4566, R:0.0100, T:16.3874(w:0.018)⚠️)
Batch  25/537: Loss=1.7287 (C:1.4280, R:0.0099, T:17.1290(w:0.018)⚠️)
Batch  50/537: Loss=1.7442 (C:1.4324, R:0.0100, T:17.7580(w:0.018)⚠️)
Batch  75/537: Loss=1.6474 (C:1.3381, R:0.0100, T:17.6153(w:0.018)⚠️)
Batch 100/537: Loss=1.7219 (C:1.4137, R:0.0100, T:17.5538(w:0.018)⚠️)
Batch 125/537: Loss=1.7581 (C:1.4695, R:0.0099, T:16.4342(w:0.018)⚠️)
Batch 150/537: Loss=1.7776 (C:1.4906, R:0.0099, T:16.3431(w:0.018)⚠️)
Batch 175/537: Loss=1.7319 (C:1.4190, R:0.0100, T:17.8259(w:0.018)⚠️)
Batch 200/537: Loss=1.7870 (C:1.4773, R:0.0099, T:17.6373(w:0.018)⚠️)
Batch 225/537: Loss=1.8038 (C:1.5043, R:0.0100, T:17.0567(w:0.018)⚠️)
Batch 250/537: Loss=1.7271 (C:1.4157, R:0.0099, T:17.7387(w:0.018)⚠️)
Batch 275/537: Loss=1.8334 (C:1.5280, R:0.0099, T:17.3943(w:0.018)⚠️)
Batch 300/537: Loss=1.7656 (C:1.4453, R:0.0100, T:18.2496(w:0.018)⚠️)
Batch 325/537: Loss=1.8011 (C:1.4776, R:0.0100, T:18.4333(w:0.018)⚠️)
Batch 350/537: Loss=1.7142 (C:1.3834, R:0.0100, T:18.8414(w:0.018)⚠️)
Batch 375/537: Loss=1.6999 (C:1.3790, R:0.0100, T:18.2812(w:0.018)⚠️)
Batch 400/537: Loss=1.7262 (C:1.4104, R:0.0100, T:17.9910(w:0.018)⚠️)
Batch 425/537: Loss=1.6915 (C:1.3848, R:0.0099, T:17.4711(w:0.018)⚠️)
Batch 450/537: Loss=1.7508 (C:1.4319, R:0.0100, T:18.1648(w:0.018)⚠️)
Batch 475/537: Loss=1.7918 (C:1.4818, R:0.0100, T:17.6605(w:0.018)⚠️)
Batch 500/537: Loss=1.7307 (C:1.4208, R:0.0100, T:17.6528(w:0.018)⚠️)
Batch 525/537: Loss=1.7316 (C:1.4157, R:0.0099, T:17.9949(w:0.018)⚠️)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 1.7504
  Contrastive: 1.4402
  Reconstruction: 0.0100
  Topological: 17.6687 (weight: 0.018)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6830
  Contrastive: 1.3867
  Reconstruction: 0.0100
  Topological: 16.8772 (weight: 0.018)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 15/100 COMPLETE (86.6s)
Train Loss: 1.7504 (C:1.4402, R:0.0100, T:17.6687)
Val Loss:   1.6830 (C:1.3867, R:0.0100, T:16.8772)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 537 | Topological Weight: 0.0187
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7817 (C:1.4448, R:0.0100, T:17.9142(w:0.019)⚠️)
Batch  25/537: Loss=1.8239 (C:1.4712, R:0.0100, T:18.7601(w:0.019)⚠️)
Batch  50/537: Loss=1.8347 (C:1.4542, R:0.0100, T:20.2417(w:0.019)⚠️)
Batch  75/537: Loss=1.8065 (C:1.4468, R:0.0100, T:19.1264(w:0.019)⚠️)
Batch 100/537: Loss=1.6766 (C:1.3294, R:0.0100, T:18.4650(w:0.019)⚠️)
Batch 125/537: Loss=1.8115 (C:1.4523, R:0.0099, T:19.1054(w:0.019)⚠️)
Batch 150/537: Loss=1.7786 (C:1.4062, R:0.0099, T:19.8085(w:0.019)⚠️)
Batch 175/537: Loss=1.8053 (C:1.4745, R:0.0099, T:17.5904(w:0.019)⚠️)
Batch 200/537: Loss=1.8095 (C:1.4617, R:0.0099, T:18.4988(w:0.019)⚠️)
Batch 225/537: Loss=1.7771 (C:1.4017, R:0.0100, T:19.9644(w:0.019)⚠️)
Batch 250/537: Loss=1.7428 (C:1.3627, R:0.0100, T:20.2202(w:0.019)⚠️)
Batch 275/537: Loss=1.7326 (C:1.3715, R:0.0100, T:19.2078(w:0.019)⚠️)
Batch 300/537: Loss=1.7225 (C:1.3840, R:0.0099, T:17.9988(w:0.019)⚠️)
Batch 325/537: Loss=1.7154 (C:1.3438, R:0.0100, T:19.7675(w:0.019)⚠️)
Batch 350/537: Loss=1.7372 (C:1.3753, R:0.0099, T:19.2504(w:0.019)⚠️)
Batch 375/537: Loss=1.6800 (C:1.3138, R:0.0100, T:19.4763(w:0.019)⚠️)
Batch 400/537: Loss=1.7561 (C:1.4207, R:0.0100, T:17.8372(w:0.019)⚠️)
Batch 425/537: Loss=1.8145 (C:1.4291, R:0.0099, T:20.5003(w:0.019)⚠️)
Batch 450/537: Loss=1.8025 (C:1.4147, R:0.0100, T:20.6272(w:0.019)⚠️)
Batch 475/537: Loss=1.7333 (C:1.3680, R:0.0100, T:19.4284(w:0.019)⚠️)
Batch 500/537: Loss=1.7721 (C:1.4027, R:0.0099, T:19.6471(w:0.019)⚠️)
Batch 525/537: Loss=1.7229 (C:1.3577, R:0.0100, T:19.4272(w:0.019)⚠️)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 1.7578
  Contrastive: 1.3968
  Reconstruction: 0.0100
  Topological: 19.1987 (weight: 0.019)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6799
  Contrastive: 1.3603
  Reconstruction: 0.0100
  Topological: 16.9957 (weight: 0.019)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 16/100 COMPLETE (85.0s)
Train Loss: 1.7578 (C:1.3968, R:0.0100, T:19.1987)
Val Loss:   1.6799 (C:1.3603, R:0.0100, T:16.9957)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 537 | Topological Weight: 0.0200
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7268 (C:1.3467, R:0.0099, T:18.9580(w:0.020)⚠️)
Batch  25/537: Loss=1.8380 (C:1.4248, R:0.0099, T:20.6122(w:0.020)⚠️)
Batch  50/537: Loss=1.7673 (C:1.3650, R:0.0100, T:20.0653(w:0.020)⚠️)
Batch  75/537: Loss=1.8322 (C:1.4190, R:0.0099, T:20.6103(w:0.020)⚠️)
Batch 100/537: Loss=1.7744 (C:1.3561, R:0.0099, T:20.8628(w:0.020)⚠️)
Batch 125/537: Loss=1.7763 (C:1.3963, R:0.0100, T:18.9473(w:0.020)⚠️)
Batch 150/537: Loss=1.7831 (C:1.3642, R:0.0099, T:20.8918(w:0.020)⚠️)
Batch 175/537: Loss=1.7669 (C:1.3734, R:0.0100, T:19.6284(w:0.020)⚠️)
Batch 200/537: Loss=1.7613 (C:1.3746, R:0.0100, T:19.2883(w:0.020)⚠️)
Batch 225/537: Loss=1.7916 (C:1.3658, R:0.0099, T:21.2398(w:0.020)⚠️)
Batch 250/537: Loss=1.7486 (C:1.3609, R:0.0100, T:19.3353(w:0.020)⚠️)
Batch 275/537: Loss=1.7624 (C:1.3674, R:0.0100, T:19.6994(w:0.020)⚠️)
Batch 300/537: Loss=1.6823 (C:1.3018, R:0.0100, T:18.9758(w:0.020)⚠️)
Batch 325/537: Loss=1.7956 (C:1.3780, R:0.0099, T:20.8295(w:0.020)⚠️)
Batch 350/537: Loss=1.8033 (C:1.3957, R:0.0099, T:20.3268(w:0.020)⚠️)
Batch 375/537: Loss=1.7681 (C:1.3744, R:0.0100, T:19.6307(w:0.020)⚠️)
Batch 400/537: Loss=1.7241 (C:1.3586, R:0.0099, T:18.2241(w:0.020)⚠️)
Batch 425/537: Loss=1.8516 (C:1.4628, R:0.0100, T:19.3895(w:0.020)⚠️)
Batch 450/537: Loss=1.7638 (C:1.3800, R:0.0100, T:19.1405(w:0.020)⚠️)
Batch 475/537: Loss=1.7143 (C:1.3208, R:0.0099, T:19.6248(w:0.020)⚠️)
Batch 500/537: Loss=1.7788 (C:1.3548, R:0.0100, T:21.1514(w:0.020)⚠️)
Batch 525/537: Loss=1.7527 (C:1.3373, R:0.0099, T:20.7223(w:0.020)⚠️)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 1.7702
  Contrastive: 1.3686
  Reconstruction: 0.0100
  Topological: 20.0289 (weight: 0.020)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.6761
  Contrastive: 1.3168
  Reconstruction: 0.0100
  Topological: 17.9143 (weight: 0.020)
  Batches with topology: 537/537 (100.0%)
✅ New best model saved!

🎯 EPOCH 17/100 COMPLETE (85.1s)
Train Loss: 1.7702 (C:1.3686, R:0.0100, T:20.0289)
Val Loss:   1.6761 (C:1.3168, R:0.0100, T:17.9143)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 537 | Topological Weight: 0.0213
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.8150 (C:1.3997, R:0.0100, T:19.4991(w:0.021)⚠️)
Batch  25/537: Loss=1.7892 (C:1.3594, R:0.0100, T:20.1794(w:0.021)⚠️)
Batch  50/537: Loss=1.8157 (C:1.3732, R:0.0099, T:20.7781(w:0.021)⚠️)
Batch  75/537: Loss=1.7614 (C:1.3543, R:0.0100, T:19.1120(w:0.021)⚠️)
Batch 100/537: Loss=1.7623 (C:1.3370, R:0.0100, T:19.9674(w:0.021)⚠️)
Batch 125/537: Loss=1.7999 (C:1.3841, R:0.0099, T:19.5180(w:0.021)⚠️)
Batch 150/537: Loss=1.7311 (C:1.3412, R:0.0100, T:18.2987(w:0.021)⚠️)
Batch 175/537: Loss=1.7509 (C:1.3261, R:0.0100, T:19.9424(w:0.021)⚠️)
Batch 200/537: Loss=1.6895 (C:1.2779, R:0.0100, T:19.3253(w:0.021)⚠️)
Batch 225/537: Loss=1.8344 (C:1.4010, R:0.0099, T:20.3446(w:0.021)⚠️)
Batch 250/537: Loss=1.7979 (C:1.3763, R:0.0100, T:19.7925(w:0.021)⚠️)
Batch 275/537: Loss=1.7733 (C:1.3344, R:0.0100, T:20.6061(w:0.021)⚠️)
Batch 300/537: Loss=1.7555 (C:1.3155, R:0.0100, T:20.6570(w:0.021)⚠️)
Batch 325/537: Loss=1.7926 (C:1.3559, R:0.0100, T:20.5053(w:0.021)⚠️)
Batch 350/537: Loss=1.7504 (C:1.3336, R:0.0099, T:19.5685(w:0.021)⚠️)
Batch 375/537: Loss=1.7334 (C:1.3115, R:0.0099, T:19.8077(w:0.021)⚠️)
Batch 400/537: Loss=1.6900 (C:1.2997, R:0.0099, T:18.3188(w:0.021)⚠️)
Batch 425/537: Loss=1.7219 (C:1.3126, R:0.0099, T:19.2175(w:0.021)⚠️)
Batch 450/537: Loss=1.8127 (C:1.3652, R:0.0099, T:21.0127(w:0.021)⚠️)
Batch 475/537: Loss=1.7992 (C:1.3687, R:0.0100, T:20.2145(w:0.021)⚠️)
Batch 500/537: Loss=1.7969 (C:1.3486, R:0.0100, T:21.0494(w:0.021)⚠️)
Batch 525/537: Loss=1.7257 (C:1.3005, R:0.0100, T:19.9642(w:0.021)⚠️)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 1.7724
  Contrastive: 1.3439
  Reconstruction: 0.0100
  Topological: 20.1168 (weight: 0.021)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7004
  Contrastive: 1.2965
  Reconstruction: 0.0100
  Topological: 18.9585 (weight: 0.021)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 18/100 COMPLETE (86.3s)
Train Loss: 1.7724 (C:1.3439, R:0.0100, T:20.1168)
Val Loss:   1.7004 (C:1.2965, R:0.0100, T:18.9585)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 537 | Topological Weight: 0.0225
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.8344 (C:1.3809, R:0.0099, T:20.1112(w:0.022)⚠️)
Batch  25/537: Loss=1.8195 (C:1.3499, R:0.0100, T:20.8245(w:0.022)⚠️)
Batch  50/537: Loss=1.7681 (C:1.3114, R:0.0099, T:20.2560(w:0.022)⚠️)
Batch  75/537: Loss=1.8199 (C:1.3487, R:0.0099, T:20.8963(w:0.022)⚠️)
Batch 100/537: Loss=1.7455 (C:1.2773, R:0.0099, T:20.7657(w:0.022)⚠️)
Batch 125/537: Loss=1.8015 (C:1.3323, R:0.0100, T:20.8098(w:0.022)⚠️)
Batch 150/537: Loss=1.8175 (C:1.3537, R:0.0099, T:20.5704(w:0.022)⚠️)
Batch 175/537: Loss=1.8948 (C:1.3830, R:0.0099, T:22.7064(w:0.022)⚠️)
Batch 200/537: Loss=1.8143 (C:1.3303, R:0.0100, T:21.4699(w:0.022)⚠️)
Batch 225/537: Loss=1.7583 (C:1.2861, R:0.0100, T:20.9416(w:0.022)⚠️)
Batch 250/537: Loss=1.8126 (C:1.3229, R:0.0100, T:21.7174(w:0.022)⚠️)
Batch 275/537: Loss=1.7435 (C:1.2988, R:0.0099, T:19.7180(w:0.022)⚠️)
Batch 300/537: Loss=1.7664 (C:1.3130, R:0.0099, T:20.1059(w:0.022)⚠️)
Batch 325/537: Loss=1.8320 (C:1.3446, R:0.0099, T:21.6162(w:0.022)⚠️)
Batch 350/537: Loss=1.7840 (C:1.3217, R:0.0099, T:20.5020(w:0.022)⚠️)
Batch 375/537: Loss=1.7946 (C:1.3193, R:0.0100, T:21.0797(w:0.022)⚠️)
Batch 400/537: Loss=1.7010 (C:1.2418, R:0.0100, T:20.3636(w:0.022)⚠️)
Batch 425/537: Loss=1.8285 (C:1.3358, R:0.0100, T:21.8549(w:0.022)⚠️)
Batch 450/537: Loss=1.8057 (C:1.3079, R:0.0100, T:22.0791(w:0.022)⚠️)
Batch 475/537: Loss=1.7573 (C:1.2874, R:0.0100, T:20.8380(w:0.022)⚠️)
Batch 500/537: Loss=1.7292 (C:1.2658, R:0.0099, T:20.5528(w:0.022)⚠️)
Batch 525/537: Loss=1.8080 (C:1.3448, R:0.0099, T:20.5420(w:0.022)⚠️)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 1.7910
  Contrastive: 1.3212
  Reconstruction: 0.0100
  Topological: 20.8344 (weight: 0.022)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7214
  Contrastive: 1.2750
  Reconstruction: 0.0100
  Topological: 19.7964 (weight: 0.022)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 19/100 COMPLETE (86.7s)
Train Loss: 1.7910 (C:1.3212, R:0.0100, T:20.8344)
Val Loss:   1.7214 (C:1.2750, R:0.0100, T:19.7964)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 537 | Topological Weight: 0.0238
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.7466 (C:1.2624, R:0.0100, T:20.3470(w:0.024)⚠️)
Batch  25/537: Loss=1.8294 (C:1.3199, R:0.0100, T:21.4139(w:0.024)⚠️)
Batch  50/537: Loss=1.8120 (C:1.3035, R:0.0099, T:21.3657(w:0.024)⚠️)
Batch  75/537: Loss=1.8376 (C:1.3569, R:0.0099, T:20.1992(w:0.024)⚠️)
Batch 100/537: Loss=1.8206 (C:1.3352, R:0.0099, T:20.3940(w:0.024)⚠️)
Batch 125/537: Loss=1.7526 (C:1.2936, R:0.0100, T:19.2820(w:0.024)⚠️)
Batch 150/537: Loss=1.7865 (C:1.3108, R:0.0099, T:19.9879(w:0.024)⚠️)
Batch 175/537: Loss=1.7541 (C:1.2918, R:0.0100, T:19.4250(w:0.024)⚠️)
Batch 200/537: Loss=1.8047 (C:1.3340, R:0.0099, T:19.7768(w:0.024)⚠️)
Batch 225/537: Loss=1.7933 (C:1.3145, R:0.0099, T:20.1186(w:0.024)⚠️)
Batch 250/537: Loss=1.7922 (C:1.3094, R:0.0100, T:20.2864(w:0.024)⚠️)
Batch 275/537: Loss=1.7864 (C:1.2916, R:0.0099, T:20.7930(w:0.024)⚠️)
Batch 300/537: Loss=1.8177 (C:1.3674, R:0.0100, T:18.9182(w:0.024)⚠️)
Batch 325/537: Loss=1.7996 (C:1.3050, R:0.0099, T:20.7854(w:0.024)⚠️)
Batch 350/537: Loss=1.7414 (C:1.2760, R:0.0100, T:19.5540(w:0.024)⚠️)
Batch 375/537: Loss=1.7464 (C:1.2682, R:0.0099, T:20.0914(w:0.024)⚠️)
Batch 400/537: Loss=1.8185 (C:1.2936, R:0.0100, T:22.0601(w:0.024)⚠️)
Batch 425/537: Loss=1.8120 (C:1.2944, R:0.0099, T:21.7542(w:0.024)⚠️)
Batch 450/537: Loss=1.8879 (C:1.3732, R:0.0100, T:21.6299(w:0.024)⚠️)
Batch 475/537: Loss=1.8197 (C:1.3047, R:0.0100, T:21.6460(w:0.024)⚠️)
Batch 500/537: Loss=1.8168 (C:1.3279, R:0.0100, T:20.5432(w:0.024)⚠️)
Batch 525/537: Loss=1.8186 (C:1.3339, R:0.0100, T:20.3645(w:0.024)⚠️)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 1.7987
  Contrastive: 1.3052
  Reconstruction: 0.0100
  Topological: 20.7376 (weight: 0.024)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7403
  Contrastive: 1.2599
  Reconstruction: 0.0100
  Topological: 20.1849 (weight: 0.024)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 20/100 COMPLETE (86.7s)
Train Loss: 1.7987 (C:1.3052, R:0.0100, T:20.7376)
Val Loss:   1.7403 (C:1.2599, R:0.0100, T:20.1849)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 537 | Topological Weight: 0.0250
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.8472 (C:1.3072, R:0.0099, T:21.5598(w:0.025)⚠️)
Batch  25/537: Loss=1.7810 (C:1.2722, R:0.0099, T:20.3130(w:0.025)⚠️)
Batch  50/537: Loss=1.8068 (C:1.2804, R:0.0099, T:21.0153(w:0.025)⚠️)
Batch  75/537: Loss=1.8095 (C:1.2771, R:0.0100, T:21.2582(w:0.025)⚠️)
Batch 100/537: Loss=1.8439 (C:1.3277, R:0.0100, T:20.6047(w:0.025)⚠️)
Batch 125/537: Loss=1.7656 (C:1.2648, R:0.0099, T:19.9933(w:0.025)⚠️)
Batch 150/537: Loss=1.8718 (C:1.3120, R:0.0100, T:22.3494(w:0.025)⚠️)
Batch 175/537: Loss=1.8153 (C:1.2964, R:0.0099, T:20.7154(w:0.025)⚠️)
Batch 200/537: Loss=1.8993 (C:1.3505, R:0.0099, T:21.9128(w:0.025)⚠️)
Batch 225/537: Loss=1.8477 (C:1.3055, R:0.0100, T:21.6507(w:0.025)⚠️)
Batch 250/537: Loss=1.8300 (C:1.2950, R:0.0099, T:21.3627(w:0.025)⚠️)
Batch 275/537: Loss=1.8735 (C:1.3144, R:0.0099, T:22.3274(w:0.025)⚠️)
Batch 300/537: Loss=1.8575 (C:1.2786, R:0.0099, T:23.1173(w:0.025)⚠️)
Batch 325/537: Loss=1.8622 (C:1.2880, R:0.0099, T:22.9252(w:0.025)⚠️)
Batch 350/537: Loss=1.8397 (C:1.2649, R:0.0100, T:22.9534(w:0.025)⚠️)
Batch 375/537: Loss=1.8738 (C:1.2904, R:0.0100, T:23.2947(w:0.025)⚠️)
Batch 400/537: Loss=1.9344 (C:1.3387, R:0.0099, T:23.7917(w:0.025)⚠️)
Batch 425/537: Loss=1.8280 (C:1.2919, R:0.0100, T:21.4066(w:0.025)⚠️)
Batch 450/537: Loss=1.9470 (C:1.3277, R:0.0100, T:24.7323(w:0.025)⚠️)
Batch 475/537: Loss=1.8907 (C:1.2888, R:0.0100, T:24.0348(w:0.025)⚠️)
Batch 500/537: Loss=1.8684 (C:1.2550, R:0.0099, T:24.4945(w:0.025)⚠️)
Batch 525/537: Loss=1.8802 (C:1.2670, R:0.0100, T:24.4899(w:0.025)⚠️)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 1.8588
  Contrastive: 1.3003
  Reconstruction: 0.0100
  Topological: 22.2983 (weight: 0.025)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7713
  Contrastive: 1.1992
  Reconstruction: 0.0100
  Topological: 22.8423 (weight: 0.025)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 21/100 COMPLETE (86.2s)
Train Loss: 1.8588 (C:1.3003, R:0.0100, T:22.2983)
Val Loss:   1.7713 (C:1.1992, R:0.0100, T:22.8423)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 537 | Topological Weight: 0.0262
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9523 (C:1.2907, R:0.0099, T:25.1673(w:0.026)⚠️)
Batch  25/537: Loss=2.0163 (C:1.3567, R:0.0100, T:25.0874(w:0.026)⚠️)
Batch  50/537: Loss=2.0111 (C:1.2843, R:0.0099, T:27.6506(w:0.026)⚠️)
Batch  75/537: Loss=1.9991 (C:1.2924, R:0.0100, T:26.8837(w:0.026)⚠️)
Batch 100/537: Loss=1.9244 (C:1.2655, R:0.0100, T:25.0655(w:0.026)⚠️)
Batch 125/537: Loss=1.9148 (C:1.2338, R:0.0100, T:25.9030(w:0.026)⚠️)
Batch 150/537: Loss=1.8629 (C:1.2249, R:0.0100, T:24.2687(w:0.026)⚠️)
Batch 175/537: Loss=1.9246 (C:1.2432, R:0.0099, T:25.9231(w:0.026)⚠️)
Batch 200/537: Loss=1.8982 (C:1.2615, R:0.0100, T:24.2156(w:0.026)⚠️)
Batch 225/537: Loss=1.8673 (C:1.2059, R:0.0099, T:25.1593(w:0.026)⚠️)
Batch 250/537: Loss=1.8954 (C:1.2293, R:0.0099, T:25.3390(w:0.026)⚠️)
Batch 275/537: Loss=1.8690 (C:1.2145, R:0.0100, T:24.8954(w:0.026)⚠️)
Batch 300/537: Loss=1.9489 (C:1.2687, R:0.0100, T:25.8755(w:0.026)⚠️)
Batch 325/537: Loss=1.8847 (C:1.2406, R:0.0099, T:24.4993(w:0.026)⚠️)
Batch 350/537: Loss=2.0022 (C:1.3544, R:0.0100, T:24.6410(w:0.026)⚠️)
Batch 375/537: Loss=1.9000 (C:1.2174, R:0.0100, T:25.9654(w:0.026)⚠️)
Batch 400/537: Loss=1.8484 (C:1.1881, R:0.0100, T:25.1160(w:0.026)⚠️)
Batch 425/537: Loss=1.8994 (C:1.2510, R:0.0099, T:24.6620(w:0.026)⚠️)
Batch 450/537: Loss=1.8093 (C:1.1276, R:0.0100, T:25.9340(w:0.026)⚠️)
Batch 475/537: Loss=1.9110 (C:1.2126, R:0.0100, T:26.5670(w:0.026)⚠️)
Batch 500/537: Loss=1.9179 (C:1.2385, R:0.0100, T:25.8443(w:0.026)⚠️)
Batch 525/537: Loss=1.9179 (C:1.2655, R:0.0099, T:24.8139(w:0.026)⚠️)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 1.9166
  Contrastive: 1.2464
  Reconstruction: 0.0100
  Topological: 25.4932 (weight: 0.026)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7597
  Contrastive: 1.1407
  Reconstruction: 0.0100
  Topological: 23.5410 (weight: 0.026)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 22/100 COMPLETE (84.9s)
Train Loss: 1.9166 (C:1.2464, R:0.0100, T:25.4932)
Val Loss:   1.7597 (C:1.1407, R:0.0100, T:23.5410)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 23 | Batches: 537 | Topological Weight: 0.0275
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9015 (C:1.1789, R:0.0099, T:26.2381(w:0.028)⚠️)
Batch  25/537: Loss=1.9066 (C:1.1875, R:0.0099, T:26.1138(w:0.028)⚠️)
Batch  50/537: Loss=1.8809 (C:1.1808, R:0.0100, T:25.4199(w:0.028)⚠️)
Batch  75/537: Loss=1.8854 (C:1.1703, R:0.0099, T:25.9687(w:0.028)⚠️)
Batch 100/537: Loss=1.8620 (C:1.1817, R:0.0100, T:24.7004(w:0.028)⚠️)
Batch 125/537: Loss=1.8512 (C:1.1965, R:0.0099, T:23.7685(w:0.028)⚠️)
Batch 150/537: Loss=1.8735 (C:1.1785, R:0.0100, T:25.2384(w:0.028)⚠️)
Batch 175/537: Loss=1.9684 (C:1.2800, R:0.0100, T:24.9935(w:0.028)⚠️)
Batch 200/537: Loss=1.9373 (C:1.2280, R:0.0099, T:25.7561(w:0.028)⚠️)
Batch 225/537: Loss=1.8793 (C:1.1730, R:0.0099, T:25.6483(w:0.028)⚠️)
Batch 250/537: Loss=1.8696 (C:1.1548, R:0.0100, T:25.9583(w:0.028)⚠️)
Batch 275/537: Loss=1.9060 (C:1.1854, R:0.0100, T:26.1680(w:0.028)⚠️)
Batch 300/537: Loss=1.9886 (C:1.2475, R:0.0099, T:26.9136(w:0.028)⚠️)
Batch 325/537: Loss=1.8297 (C:1.1450, R:0.0099, T:24.8629(w:0.028)⚠️)
Batch 350/537: Loss=1.9060 (C:1.1835, R:0.0100, T:26.2358(w:0.028)⚠️)
Batch 375/537: Loss=1.9620 (C:1.2405, R:0.0099, T:26.2003(w:0.028)⚠️)
Batch 400/537: Loss=1.8556 (C:1.1481, R:0.0100, T:25.6916(w:0.028)⚠️)
Batch 425/537: Loss=1.8734 (C:1.1665, R:0.0099, T:25.6690(w:0.028)⚠️)
Batch 450/537: Loss=1.8763 (C:1.1639, R:0.0099, T:25.8705(w:0.028)⚠️)
Batch 475/537: Loss=1.9130 (C:1.1636, R:0.0099, T:27.2146(w:0.028)⚠️)
Batch 500/537: Loss=1.8614 (C:1.1345, R:0.0100, T:26.3970(w:0.028)⚠️)
Batch 525/537: Loss=1.9351 (C:1.2044, R:0.0100, T:26.5341(w:0.028)⚠️)

📊 EPOCH 23 TRAINING SUMMARY:
  Total Loss: 1.9102
  Contrastive: 1.1952
  Reconstruction: 0.0100
  Topological: 25.9659 (weight: 0.028)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7637
  Contrastive: 1.0974
  Reconstruction: 0.0100
  Topological: 24.1904 (weight: 0.028)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 23/100 COMPLETE (86.4s)
Train Loss: 1.9102 (C:1.1952, R:0.0100, T:25.9659)
Val Loss:   1.7637 (C:1.0974, R:0.0100, T:24.1904)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 24 | Batches: 537 | Topological Weight: 0.0288
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9533 (C:1.1973, R:0.0100, T:26.2637(w:0.029)⚠️)
Batch  25/537: Loss=1.9178 (C:1.1356, R:0.0099, T:27.1731(w:0.029)⚠️)
Batch  50/537: Loss=1.8887 (C:1.1061, R:0.0100, T:27.1865(w:0.029)⚠️)
Batch  75/537: Loss=1.9741 (C:1.1896, R:0.0100, T:27.2514(w:0.029)⚠️)
Batch 100/537: Loss=1.9357 (C:1.1380, R:0.0100, T:27.7117(w:0.029)⚠️)
Batch 125/537: Loss=1.9875 (C:1.1573, R:0.0100, T:28.8429(w:0.029)⚠️)
Batch 150/537: Loss=1.8828 (C:1.1494, R:0.0100, T:25.4767(w:0.029)⚠️)
Batch 175/537: Loss=1.7857 (C:1.0464, R:0.0099, T:25.6798(w:0.029)⚠️)
Batch 200/537: Loss=1.9416 (C:1.1753, R:0.0100, T:26.6193(w:0.029)⚠️)
Batch 225/537: Loss=1.9339 (C:1.1558, R:0.0099, T:27.0309(w:0.029)⚠️)
Batch 250/537: Loss=1.8445 (C:1.0986, R:0.0099, T:25.9100(w:0.029)⚠️)
Batch 275/537: Loss=1.9336 (C:1.1466, R:0.0099, T:27.3395(w:0.029)⚠️)
Batch 300/537: Loss=1.9565 (C:1.1943, R:0.0100, T:26.4794(w:0.029)⚠️)
Batch 325/537: Loss=1.9002 (C:1.1245, R:0.0100, T:26.9461(w:0.029)⚠️)
Batch 350/537: Loss=1.9539 (C:1.1736, R:0.0100, T:27.1080(w:0.029)⚠️)
Batch 375/537: Loss=1.9557 (C:1.1909, R:0.0099, T:26.5689(w:0.029)⚠️)
Batch 400/537: Loss=1.8440 (C:1.0738, R:0.0100, T:26.7544(w:0.029)⚠️)
Batch 425/537: Loss=1.8940 (C:1.1064, R:0.0099, T:27.3615(w:0.029)⚠️)
Batch 450/537: Loss=1.9631 (C:1.1850, R:0.0100, T:27.0286(w:0.029)⚠️)
Batch 475/537: Loss=1.8984 (C:1.1684, R:0.0100, T:25.3588(w:0.029)⚠️)
Batch 500/537: Loss=1.9091 (C:1.1412, R:0.0099, T:26.6752(w:0.029)⚠️)
Batch 525/537: Loss=1.8860 (C:1.1158, R:0.0100, T:26.7543(w:0.029)⚠️)

📊 EPOCH 24 TRAINING SUMMARY:
  Total Loss: 1.9312
  Contrastive: 1.1582
  Reconstruction: 0.0100
  Topological: 26.8506 (weight: 0.029)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7228
  Contrastive: 1.0597
  Reconstruction: 0.0100
  Topological: 23.0294 (weight: 0.029)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 24/100 COMPLETE (90.2s)
Train Loss: 1.9312 (C:1.1582, R:0.0100, T:26.8506)
Val Loss:   1.7228 (C:1.0597, R:0.0100, T:23.0294)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 25 | Batches: 537 | Topological Weight: 0.0300
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.8498 (C:1.0964, R:0.0100, T:25.0811(w:0.030)⚠️)
Batch  25/537: Loss=1.9780 (C:1.1931, R:0.0100, T:26.1313(w:0.030)⚠️)
Batch  50/537: Loss=1.9442 (C:1.1282, R:0.0099, T:27.1671(w:0.030)⚠️)
Batch  75/537: Loss=1.8675 (C:1.0937, R:0.0099, T:25.7629(w:0.030)⚠️)
Batch 100/537: Loss=1.8390 (C:1.0988, R:0.0099, T:24.6390(w:0.030)⚠️)
Batch 125/537: Loss=1.8987 (C:1.0907, R:0.0100, T:26.8993(w:0.030)⚠️)
Batch 150/537: Loss=2.0094 (C:1.1765, R:0.0099, T:27.7301(w:0.030)⚠️)
Batch 175/537: Loss=2.0213 (C:1.2090, R:0.0100, T:27.0421(w:0.030)⚠️)
Batch 200/537: Loss=1.9994 (C:1.1862, R:0.0100, T:27.0737(w:0.030)⚠️)
Batch 225/537: Loss=1.9375 (C:1.1440, R:0.0100, T:26.4185(w:0.030)⚠️)
Batch 250/537: Loss=1.9592 (C:1.1526, R:0.0100, T:26.8521(w:0.030)⚠️)
Batch 275/537: Loss=1.9339 (C:1.1218, R:0.0099, T:27.0365(w:0.030)⚠️)
Batch 300/537: Loss=1.8862 (C:1.0537, R:0.0100, T:27.7157(w:0.030)⚠️)
Batch 325/537: Loss=1.8667 (C:1.0719, R:0.0100, T:26.4598(w:0.030)⚠️)
Batch 350/537: Loss=1.8501 (C:1.0530, R:0.0100, T:26.5344(w:0.030)⚠️)
Batch 375/537: Loss=1.9508 (C:1.0752, R:0.0099, T:29.1515(w:0.030)⚠️)
Batch 400/537: Loss=1.9968 (C:1.1676, R:0.0099, T:27.6061(w:0.030)⚠️)
Batch 425/537: Loss=1.8959 (C:1.0853, R:0.0099, T:26.9861(w:0.030)⚠️)
Batch 450/537: Loss=1.9320 (C:1.0871, R:0.0099, T:28.1304(w:0.030)⚠️)
Batch 475/537: Loss=1.9691 (C:1.1301, R:0.0100, T:27.9351(w:0.030)⚠️)
Batch 500/537: Loss=1.9344 (C:1.0751, R:0.0100, T:28.6099(w:0.030)⚠️)
Batch 525/537: Loss=1.9850 (C:1.0936, R:0.0100, T:29.6804(w:0.030)⚠️)

📊 EPOCH 25 TRAINING SUMMARY:
  Total Loss: 1.9385
  Contrastive: 1.1236
  Reconstruction: 0.0100
  Topological: 27.1289 (weight: 0.030)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7015
  Contrastive: 1.0249
  Reconstruction: 0.0100
  Topological: 22.5181 (weight: 0.030)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 25/100 COMPLETE (87.8s)
Train Loss: 1.9385 (C:1.1236, R:0.0100, T:27.1289)
Val Loss:   1.7015 (C:1.0249, R:0.0100, T:22.5181)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 26 | Batches: 537 | Topological Weight: 0.0312
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9750 (C:1.0973, R:0.0099, T:28.0561(w:0.031)⚠️)
Batch  25/537: Loss=1.9841 (C:1.1492, R:0.0099, T:26.6841(w:0.031)⚠️)
Batch  50/537: Loss=1.9547 (C:1.0860, R:0.0099, T:27.7644(w:0.031)⚠️)
Batch  75/537: Loss=2.0611 (C:1.1623, R:0.0099, T:28.7317(w:0.031)⚠️)
Batch 100/537: Loss=1.9579 (C:1.0520, R:0.0100, T:28.9597(w:0.031)⚠️)
Batch 125/537: Loss=2.0004 (C:1.1046, R:0.0099, T:28.6322(w:0.031)⚠️)
Batch 150/537: Loss=1.9619 (C:1.0473, R:0.0099, T:29.2366(w:0.031)⚠️)
Batch 175/537: Loss=1.9381 (C:1.0808, R:0.0100, T:27.4034(w:0.031)⚠️)
Batch 200/537: Loss=2.0090 (C:1.1481, R:0.0100, T:27.5189(w:0.031)⚠️)
Batch 225/537: Loss=1.9968 (C:1.1006, R:0.0099, T:28.6448(w:0.031)⚠️)
Batch 250/537: Loss=1.9641 (C:1.1006, R:0.0100, T:27.6013(w:0.031)⚠️)
Batch 275/537: Loss=1.9691 (C:1.0874, R:0.0100, T:28.1832(w:0.031)⚠️)
Batch 300/537: Loss=1.9859 (C:1.1219, R:0.0099, T:27.6168(w:0.031)⚠️)
Batch 325/537: Loss=1.8993 (C:1.0580, R:0.0100, T:26.8923(w:0.031)⚠️)
Batch 350/537: Loss=2.0097 (C:1.1361, R:0.0099, T:27.9251(w:0.031)⚠️)
Batch 375/537: Loss=1.9419 (C:1.0483, R:0.0100, T:28.5638(w:0.031)⚠️)
Batch 400/537: Loss=1.9416 (C:1.0894, R:0.0099, T:27.2366(w:0.031)⚠️)
Batch 425/537: Loss=1.9349 (C:1.0878, R:0.0099, T:27.0759(w:0.031)⚠️)
Batch 450/537: Loss=1.9509 (C:1.0639, R:0.0099, T:28.3513(w:0.031)⚠️)
Batch 475/537: Loss=1.9370 (C:1.0363, R:0.0100, T:28.7910(w:0.031)⚠️)
Batch 500/537: Loss=2.0260 (C:1.1064, R:0.0099, T:29.3975(w:0.031)⚠️)
Batch 525/537: Loss=2.0170 (C:1.1398, R:0.0100, T:28.0376(w:0.031)⚠️)

📊 EPOCH 26 TRAINING SUMMARY:
  Total Loss: 1.9828
  Contrastive: 1.1036
  Reconstruction: 0.0100
  Topological: 28.1040 (weight: 0.031)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7245
  Contrastive: 0.9934
  Reconstruction: 0.0100
  Topological: 23.3638 (weight: 0.031)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 26/100 COMPLETE (73.3s)
Train Loss: 1.9828 (C:1.1036, R:0.0100, T:28.1040)
Val Loss:   1.7245 (C:0.9934, R:0.0100, T:23.3638)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 27 | Batches: 537 | Topological Weight: 0.0325
🌱 Early topological learning
============================================================
Batch   0/537: Loss=1.9411 (C:0.9723, R:0.0100, T:29.7801(w:0.033)⚠️)
Batch  25/537: Loss=2.0356 (C:1.0830, R:0.0100, T:29.2809(w:0.033)⚠️)
Batch  50/537: Loss=1.9879 (C:1.0328, R:0.0099, T:29.3570(w:0.033)⚠️)
Batch  75/537: Loss=1.9999 (C:1.0528, R:0.0100, T:29.1097(w:0.033)⚠️)
Batch 100/537: Loss=2.0365 (C:1.0759, R:0.0100, T:29.5274(w:0.033)⚠️)
Batch 125/537: Loss=1.9519 (C:1.0237, R:0.0100, T:28.5287(w:0.033)⚠️)
Batch 150/537: Loss=1.8876 (C:0.9534, R:0.0100, T:28.7126(w:0.033)⚠️)
Batch 175/537: Loss=2.0799 (C:1.0937, R:0.0100, T:30.3143(w:0.033)⚠️)
Batch 200/537: Loss=2.0890 (C:1.0980, R:0.0099, T:30.4603(w:0.033)⚠️)
Batch 225/537: Loss=2.0426 (C:1.0596, R:0.0100, T:30.2163(w:0.033)⚠️)
Batch 250/537: Loss=1.9902 (C:1.0598, R:0.0100, T:28.5963(w:0.033)⚠️)
Batch 275/537: Loss=2.0330 (C:1.1234, R:0.0099, T:27.9580(w:0.033)⚠️)
Batch 300/537: Loss=1.8932 (C:0.9959, R:0.0100, T:27.5783(w:0.033)⚠️)
Batch 325/537: Loss=1.9863 (C:1.0540, R:0.0100, T:28.6576(w:0.033)⚠️)
Batch 350/537: Loss=1.9490 (C:1.0259, R:0.0100, T:28.3742(w:0.033)⚠️)
Batch 375/537: Loss=1.9947 (C:1.0822, R:0.0100, T:28.0438(w:0.033)⚠️)
Batch 400/537: Loss=1.9972 (C:1.1096, R:0.0100, T:27.2775(w:0.033)⚠️)
Batch 425/537: Loss=2.0382 (C:1.1196, R:0.0099, T:28.2350(w:0.033)⚠️)
Batch 450/537: Loss=2.0635 (C:1.0640, R:0.0100, T:30.7231(w:0.033)⚠️)
Batch 475/537: Loss=1.9817 (C:1.0397, R:0.0099, T:28.9547(w:0.033)⚠️)
Batch 500/537: Loss=1.9942 (C:1.0521, R:0.0100, T:28.9578(w:0.033)⚠️)
Batch 525/537: Loss=2.0235 (C:1.1018, R:0.0099, T:28.3291(w:0.033)⚠️)

📊 EPOCH 27 TRAINING SUMMARY:
  Total Loss: 2.0191
  Contrastive: 1.0774
  Reconstruction: 0.0100
  Topological: 28.9432 (weight: 0.033)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7317
  Contrastive: 0.9764
  Reconstruction: 0.0100
  Topological: 23.2084 (weight: 0.033)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 27/100 COMPLETE (70.3s)
Train Loss: 2.0191 (C:1.0774, R:0.0100, T:28.9432)
Val Loss:   1.7317 (C:0.9764, R:0.0100, T:23.2084)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 28 | Batches: 537 | Topological Weight: 0.0338
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0967 (C:1.0206, R:0.0099, T:31.8527(w:0.034)⚠️)
Batch  25/537: Loss=2.0328 (C:1.0291, R:0.0099, T:29.7104(w:0.034)⚠️)
Batch  50/537: Loss=1.9748 (C:1.0312, R:0.0099, T:27.9283(w:0.034)⚠️)
Batch  75/537: Loss=2.0398 (C:1.0771, R:0.0100, T:28.4947(w:0.034)⚠️)
Batch 100/537: Loss=1.9948 (C:1.0134, R:0.0099, T:29.0489(w:0.034)⚠️)
Batch 125/537: Loss=2.0648 (C:1.0781, R:0.0100, T:29.2055(w:0.034)⚠️)
Batch 150/537: Loss=2.0733 (C:1.0159, R:0.0100, T:31.3001(w:0.034)⚠️)
Batch 175/537: Loss=2.0602 (C:1.0251, R:0.0100, T:30.6422(w:0.034)⚠️)
Batch 200/537: Loss=2.0755 (C:1.0715, R:0.0100, T:29.7163(w:0.034)⚠️)
Batch 225/537: Loss=2.0763 (C:1.0251, R:0.0100, T:31.1175(w:0.034)⚠️)
Batch 250/537: Loss=2.0704 (C:1.0469, R:0.0099, T:30.2955(w:0.034)⚠️)
Batch 275/537: Loss=2.1776 (C:1.1166, R:0.0099, T:31.4055(w:0.034)⚠️)
Batch 300/537: Loss=2.0307 (C:1.0100, R:0.0100, T:30.2130(w:0.034)⚠️)
Batch 325/537: Loss=2.0306 (C:1.0129, R:0.0099, T:30.1258(w:0.034)⚠️)
Batch 350/537: Loss=2.0957 (C:1.0769, R:0.0100, T:30.1592(w:0.034)⚠️)
Batch 375/537: Loss=2.1003 (C:1.0659, R:0.0100, T:30.6187(w:0.034)⚠️)
Batch 400/537: Loss=2.0747 (C:1.0653, R:0.0099, T:29.8787(w:0.034)⚠️)
Batch 425/537: Loss=2.1352 (C:1.1064, R:0.0099, T:30.4552(w:0.034)⚠️)
Batch 450/537: Loss=2.0814 (C:1.0646, R:0.0099, T:30.0977(w:0.034)⚠️)
Batch 475/537: Loss=2.0657 (C:1.0489, R:0.0099, T:30.0957(w:0.034)⚠️)
Batch 500/537: Loss=2.1260 (C:1.0716, R:0.0100, T:31.2112(w:0.034)⚠️)
Batch 525/537: Loss=2.0896 (C:1.0502, R:0.0099, T:30.7671(w:0.034)⚠️)

📊 EPOCH 28 TRAINING SUMMARY:
  Total Loss: 2.0771
  Contrastive: 1.0622
  Reconstruction: 0.0100
  Topological: 30.0427 (weight: 0.034)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7516
  Contrastive: 0.9599
  Reconstruction: 0.0100
  Topological: 23.4294 (weight: 0.034)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 28/100 COMPLETE (74.3s)
Train Loss: 2.0771 (C:1.0622, R:0.0100, T:30.0427)
Val Loss:   1.7516 (C:0.9599, R:0.0100, T:23.4294)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 29 | Batches: 537 | Topological Weight: 0.0350
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.0684 (C:1.0363, R:0.0100, T:29.4619(w:0.035)⚠️)
Batch  25/537: Loss=2.1807 (C:1.0932, R:0.0099, T:31.0408(w:0.035)⚠️)
Batch  50/537: Loss=2.0344 (C:0.9358, R:0.0099, T:31.3599(w:0.035)⚠️)
Batch  75/537: Loss=2.0718 (C:1.0171, R:0.0100, T:30.1046(w:0.035)⚠️)
Batch 100/537: Loss=2.0954 (C:1.0210, R:0.0099, T:30.6702(w:0.035)⚠️)
Batch 125/537: Loss=2.1341 (C:1.0786, R:0.0100, T:30.1299(w:0.035)⚠️)
Batch 150/537: Loss=2.0468 (C:0.9382, R:0.0099, T:31.6476(w:0.035)⚠️)
Batch 175/537: Loss=2.1074 (C:1.0675, R:0.0100, T:29.6825(w:0.035)⚠️)
Batch 200/537: Loss=2.2250 (C:1.1152, R:0.0100, T:31.6798(w:0.035)⚠️)
Batch 225/537: Loss=2.0385 (C:0.9589, R:0.0100, T:30.8178(w:0.035)⚠️)
Batch 250/537: Loss=2.1202 (C:1.0181, R:0.0100, T:31.4619(w:0.035)⚠️)
Batch 275/537: Loss=2.1063 (C:1.0655, R:0.0100, T:29.7064(w:0.035)⚠️)
Batch 300/537: Loss=2.1578 (C:1.0780, R:0.0100, T:30.8235(w:0.035)⚠️)
Batch 325/537: Loss=2.1451 (C:1.0814, R:0.0099, T:30.3629(w:0.035)⚠️)
Batch 350/537: Loss=2.0560 (C:0.9905, R:0.0100, T:30.4136(w:0.035)⚠️)
Batch 375/537: Loss=2.1530 (C:1.0732, R:0.0099, T:30.8236(w:0.035)⚠️)
Batch 400/537: Loss=2.1195 (C:1.0527, R:0.0099, T:30.4499(w:0.035)⚠️)
Batch 425/537: Loss=2.1062 (C:1.0654, R:0.0100, T:29.7096(w:0.035)⚠️)
Batch 450/537: Loss=2.0394 (C:0.9502, R:0.0100, T:31.0909(w:0.035)⚠️)
Batch 475/537: Loss=2.1207 (C:1.0569, R:0.0100, T:30.3633(w:0.035)⚠️)
Batch 500/537: Loss=2.0896 (C:1.0383, R:0.0100, T:30.0095(w:0.035)⚠️)
Batch 525/537: Loss=2.1971 (C:1.1267, R:0.0100, T:30.5545(w:0.035)⚠️)

📊 EPOCH 29 TRAINING SUMMARY:
  Total Loss: 2.1227
  Contrastive: 1.0521
  Reconstruction: 0.0100
  Topological: 30.5600 (weight: 0.035)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7719
  Contrastive: 0.9445
  Reconstruction: 0.0100
  Topological: 23.6128 (weight: 0.035)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 29/100 COMPLETE (70.7s)
Train Loss: 2.1227 (C:1.0521, R:0.0100, T:30.5600)
Val Loss:   1.7719 (C:0.9445, R:0.0100, T:23.6128)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 30 | Batches: 537 | Topological Weight: 0.0362
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1924 (C:1.0783, R:0.0099, T:30.7054(w:0.036)⚠️)
Batch  25/537: Loss=2.1122 (C:1.0107, R:0.0099, T:30.3594(w:0.036)⚠️)
Batch  50/537: Loss=2.1115 (C:1.0011, R:0.0099, T:30.6033(w:0.036)⚠️)
Batch  75/537: Loss=2.1449 (C:1.0524, R:0.0100, T:30.1101(w:0.036)⚠️)
Batch 100/537: Loss=2.0962 (C:1.0115, R:0.0099, T:29.8949(w:0.036)⚠️)
Batch 125/537: Loss=2.1453 (C:1.0540, R:0.0100, T:30.0784(w:0.036)⚠️)
Batch 150/537: Loss=2.1145 (C:1.0284, R:0.0099, T:29.9335(w:0.036)⚠️)
Batch 175/537: Loss=2.0899 (C:1.0332, R:0.0099, T:29.1239(w:0.036)⚠️)
Batch 200/537: Loss=2.1424 (C:1.0382, R:0.0100, T:30.4349(w:0.036)⚠️)
Batch 225/537: Loss=2.2006 (C:1.1160, R:0.0099, T:29.8931(w:0.036)⚠️)
Batch 250/537: Loss=2.0666 (C:0.9884, R:0.0100, T:29.7155(w:0.036)⚠️)
Batch 275/537: Loss=2.0814 (C:1.0532, R:0.0099, T:28.3346(w:0.036)⚠️)
Batch 300/537: Loss=2.0817 (C:1.0337, R:0.0099, T:28.8838(w:0.036)⚠️)
Batch 325/537: Loss=2.1910 (C:1.0815, R:0.0099, T:30.5801(w:0.036)⚠️)
Batch 350/537: Loss=2.1828 (C:1.0858, R:0.0099, T:30.2338(w:0.036)⚠️)
Batch 375/537: Loss=2.1954 (C:1.1590, R:0.0100, T:28.5631(w:0.036)⚠️)
Batch 400/537: Loss=2.0972 (C:1.0361, R:0.0100, T:29.2449(w:0.036)⚠️)
Batch 425/537: Loss=2.2066 (C:1.0658, R:0.0100, T:31.4416(w:0.036)⚠️)
Batch 450/537: Loss=2.1330 (C:1.0387, R:0.0100, T:30.1583(w:0.036)⚠️)
Batch 475/537: Loss=2.0907 (C:1.0232, R:0.0100, T:29.4204(w:0.036)⚠️)
Batch 500/537: Loss=2.2036 (C:1.0551, R:0.0100, T:31.6541(w:0.036)⚠️)
Batch 525/537: Loss=2.1691 (C:1.0687, R:0.0100, T:30.3277(w:0.036)⚠️)

📊 EPOCH 30 TRAINING SUMMARY:
  Total Loss: 2.1257
  Contrastive: 1.0346
  Reconstruction: 0.0100
  Topological: 30.0728 (weight: 0.036)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.7685
  Contrastive: 0.9188
  Reconstruction: 0.0100
  Topological: 23.4115 (weight: 0.036)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 30/100 COMPLETE (71.3s)
Train Loss: 2.1257 (C:1.0346, R:0.0100, T:30.0728)
Val Loss:   1.7685 (C:0.9188, R:0.0100, T:23.4115)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 31 | Batches: 537 | Topological Weight: 0.0375
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1518 (C:1.0276, R:0.0099, T:29.9523(w:0.037)⚠️)
Batch  25/537: Loss=2.1279 (C:0.9847, R:0.0100, T:30.4586(w:0.037)⚠️)
Batch  50/537: Loss=2.1536 (C:0.9636, R:0.0100, T:31.7074(w:0.037)⚠️)
Batch  75/537: Loss=2.1868 (C:1.0269, R:0.0100, T:30.9044(w:0.037)⚠️)
Batch 100/537: Loss=2.1941 (C:1.0124, R:0.0100, T:31.4842(w:0.037)⚠️)
Batch 125/537: Loss=2.1383 (C:1.0033, R:0.0100, T:30.2403(w:0.037)⚠️)
Batch 150/537: Loss=2.2708 (C:1.0975, R:0.0100, T:31.2603(w:0.037)⚠️)
Batch 175/537: Loss=2.1651 (C:1.0145, R:0.0099, T:30.6572(w:0.037)⚠️)
Batch 200/537: Loss=2.1426 (C:0.9849, R:0.0100, T:30.8443(w:0.037)⚠️)
Batch 225/537: Loss=2.3281 (C:1.1797, R:0.0100, T:30.5961(w:0.037)⚠️)
Batch 250/537: Loss=2.2455 (C:1.0581, R:0.0099, T:31.6375(w:0.037)⚠️)
Batch 275/537: Loss=2.1535 (C:1.0041, R:0.0099, T:30.6248(w:0.037)⚠️)
Batch 300/537: Loss=2.1353 (C:1.0091, R:0.0100, T:30.0072(w:0.037)⚠️)
Batch 325/537: Loss=2.2347 (C:1.0347, R:0.0100, T:31.9724(w:0.037)⚠️)
Batch 350/537: Loss=2.1043 (C:0.9846, R:0.0099, T:29.8334(w:0.037)⚠️)
Batch 375/537: Loss=2.2620 (C:1.1443, R:0.0100, T:29.7797(w:0.037)⚠️)
Batch 400/537: Loss=2.2263 (C:1.0621, R:0.0100, T:31.0184(w:0.037)⚠️)
Batch 425/537: Loss=2.1907 (C:1.0123, R:0.0100, T:31.3967(w:0.037)⚠️)
Batch 450/537: Loss=2.1450 (C:1.0086, R:0.0099, T:30.2770(w:0.037)⚠️)
Batch 475/537: Loss=2.1209 (C:0.9789, R:0.0100, T:30.4282(w:0.037)⚠️)
Batch 500/537: Loss=2.2202 (C:1.0299, R:0.0099, T:31.7148(w:0.037)⚠️)
Batch 525/537: Loss=2.2093 (C:1.0446, R:0.0099, T:31.0317(w:0.037)⚠️)

📊 EPOCH 31 TRAINING SUMMARY:
  Total Loss: 2.1803
  Contrastive: 1.0250
  Reconstruction: 0.0100
  Topological: 30.7830 (weight: 0.037)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.8097
  Contrastive: 0.8991
  Reconstruction: 0.0100
  Topological: 24.2559 (weight: 0.037)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 31/100 COMPLETE (70.3s)
Train Loss: 2.1803 (C:1.0250, R:0.0100, T:30.7830)
Val Loss:   1.8097 (C:0.8991, R:0.0100, T:24.2559)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 32 | Batches: 537 | Topological Weight: 0.0387
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.2190 (C:0.9908, R:0.0100, T:31.6680(w:0.039)⚠️)
Batch  25/537: Loss=2.3169 (C:1.0842, R:0.0099, T:31.7861(w:0.039)⚠️)
Batch  50/537: Loss=2.2265 (C:1.0109, R:0.0099, T:31.3441(w:0.039)⚠️)
Batch  75/537: Loss=2.1715 (C:0.9890, R:0.0099, T:30.4916(w:0.039)⚠️)
Batch 100/537: Loss=2.1847 (C:0.9558, R:0.0100, T:31.6877(w:0.039)⚠️)
Batch 125/537: Loss=2.2820 (C:1.1169, R:0.0099, T:30.0403(w:0.039)⚠️)
Batch 150/537: Loss=2.1931 (C:1.0196, R:0.0100, T:30.2576(w:0.039)⚠️)
Batch 175/537: Loss=2.2697 (C:1.0183, R:0.0100, T:32.2687(w:0.039)⚠️)
Batch 200/537: Loss=2.2559 (C:1.0125, R:0.0100, T:32.0596(w:0.039)⚠️)
Batch 225/537: Loss=2.1737 (C:0.9289, R:0.0100, T:32.0982(w:0.039)⚠️)
Batch 250/537: Loss=2.2523 (C:1.0051, R:0.0099, T:32.1604(w:0.039)⚠️)
Batch 275/537: Loss=2.2596 (C:1.0592, R:0.0099, T:30.9528(w:0.039)⚠️)
Batch 300/537: Loss=2.2288 (C:1.0434, R:0.0099, T:30.5666(w:0.039)⚠️)
Batch 325/537: Loss=2.1795 (C:0.9982, R:0.0099, T:30.4599(w:0.039)⚠️)
Batch 350/537: Loss=2.2541 (C:1.0500, R:0.0099, T:31.0489(w:0.039)⚠️)
Batch 375/537: Loss=2.2727 (C:1.0738, R:0.0099, T:30.9135(w:0.039)⚠️)
Batch 400/537: Loss=2.3110 (C:1.0913, R:0.0099, T:31.4517(w:0.039)⚠️)
Batch 425/537: Loss=2.1610 (C:1.0051, R:0.0100, T:29.8032(w:0.039)⚠️)
Batch 450/537: Loss=2.2195 (C:0.9944, R:0.0099, T:31.5884(w:0.039)⚠️)
Batch 475/537: Loss=2.2722 (C:1.0628, R:0.0100, T:31.1835(w:0.039)⚠️)
Batch 500/537: Loss=2.2489 (C:1.0722, R:0.0100, T:30.3423(w:0.039)⚠️)
Batch 525/537: Loss=2.2651 (C:1.0449, R:0.0099, T:31.4640(w:0.039)⚠️)

📊 EPOCH 32 TRAINING SUMMARY:
  Total Loss: 2.2236
  Contrastive: 1.0118
  Reconstruction: 0.0100
  Topological: 31.2462 (weight: 0.039)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.8249
  Contrastive: 0.8873
  Reconstruction: 0.0100
  Topological: 24.1708 (weight: 0.039)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 32/100 COMPLETE (71.5s)
Train Loss: 2.2236 (C:1.0118, R:0.0100, T:31.2462)
Val Loss:   1.8249 (C:0.8873, R:0.0100, T:24.1708)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 33 | Batches: 537 | Topological Weight: 0.0400
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.1826 (C:0.9106, R:0.0099, T:31.7742(w:0.040)⚠️)
Batch  25/537: Loss=2.2480 (C:0.9683, R:0.0100, T:31.9694(w:0.040)⚠️)
Batch  50/537: Loss=2.2283 (C:0.9878, R:0.0100, T:30.9875(w:0.040)⚠️)
Batch  75/537: Loss=2.1601 (C:0.9255, R:0.0099, T:30.8408(w:0.040)⚠️)
Batch 100/537: Loss=2.2536 (C:0.9919, R:0.0100, T:31.5181(w:0.040)⚠️)
Batch 125/537: Loss=2.3426 (C:1.0709, R:0.0100, T:31.7674(w:0.040)⚠️)
Batch 150/537: Loss=2.2847 (C:1.0053, R:0.0100, T:31.9607(w:0.040)⚠️)
Batch 175/537: Loss=2.2812 (C:1.0125, R:0.0099, T:31.6929(w:0.040)⚠️)
Batch 200/537: Loss=2.3096 (C:1.0655, R:0.0099, T:31.0776(w:0.040)⚠️)
Batch 225/537: Loss=2.2448 (C:1.0036, R:0.0099, T:31.0073(w:0.040)⚠️)
Batch 250/537: Loss=2.2588 (C:0.9650, R:0.0100, T:32.3216(w:0.040)⚠️)
Batch 275/537: Loss=2.3092 (C:1.0084, R:0.0099, T:32.4943(w:0.040)⚠️)
Batch 300/537: Loss=2.3441 (C:1.0459, R:0.0099, T:32.4316(w:0.040)⚠️)
Batch 325/537: Loss=2.3153 (C:1.0375, R:0.0100, T:31.9203(w:0.040)⚠️)
Batch 350/537: Loss=2.1849 (C:0.9577, R:0.0099, T:30.6531(w:0.040)⚠️)
Batch 375/537: Loss=2.3677 (C:1.0949, R:0.0100, T:31.7946(w:0.040)⚠️)
Batch 400/537: Loss=2.2886 (C:1.0293, R:0.0100, T:31.4573(w:0.040)⚠️)
Batch 425/537: Loss=2.2667 (C:0.9461, R:0.0099, T:32.9895(w:0.040)⚠️)
Batch 450/537: Loss=2.3279 (C:1.0743, R:0.0099, T:31.3164(w:0.040)⚠️)
Batch 475/537: Loss=2.3113 (C:1.0712, R:0.0100, T:30.9775(w:0.040)⚠️)
Batch 500/537: Loss=2.1979 (C:0.9554, R:0.0099, T:31.0382(w:0.040)⚠️)
Batch 525/537: Loss=2.2580 (C:1.0094, R:0.0099, T:31.1893(w:0.040)⚠️)

📊 EPOCH 33 TRAINING SUMMARY:
  Total Loss: 2.2635
  Contrastive: 1.0081
  Reconstruction: 0.0100
  Topological: 31.3590 (weight: 0.040)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.8704
  Contrastive: 0.8640
  Reconstruction: 0.0100
  Topological: 25.1345 (weight: 0.040)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 33/100 COMPLETE (72.1s)
Train Loss: 2.2635 (C:1.0081, R:0.0100, T:31.3590)
Val Loss:   1.8704 (C:0.8640, R:0.0100, T:25.1345)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 34 | Batches: 537 | Topological Weight: 0.0413
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.2876 (C:0.9519, R:0.0099, T:32.3554(w:0.041)⚠️)
Batch  25/537: Loss=2.3261 (C:0.9980, R:0.0100, T:32.1735(w:0.041)⚠️)
Batch  50/537: Loss=2.2689 (C:0.9530, R:0.0099, T:31.8784(w:0.041)⚠️)
Batch  75/537: Loss=2.3058 (C:1.0203, R:0.0099, T:31.1419(w:0.041)⚠️)
Batch 100/537: Loss=2.5469 (C:1.0196, R:0.0100, T:37.0013(w:0.041)⚠️)
Batch 125/537: Loss=2.4082 (C:1.0799, R:0.0100, T:32.1767(w:0.041)⚠️)
Batch 150/537: Loss=2.3197 (C:1.0225, R:0.0100, T:31.4224(w:0.041)⚠️)
Batch 175/537: Loss=2.2362 (C:0.9471, R:0.0100, T:31.2286(w:0.041)⚠️)
Batch 200/537: Loss=2.3460 (C:1.0624, R:0.0099, T:31.0935(w:0.041)⚠️)
Batch 225/537: Loss=2.2929 (C:0.9757, R:0.0099, T:31.9082(w:0.041)⚠️)
Batch 250/537: Loss=2.2449 (C:0.9938, R:0.0100, T:30.3057(w:0.041)⚠️)
Batch 275/537: Loss=2.2852 (C:1.0005, R:0.0100, T:31.1206(w:0.041)⚠️)
Batch 300/537: Loss=2.3127 (C:1.0061, R:0.0100, T:31.6515(w:0.041)⚠️)
Batch 325/537: Loss=2.1635 (C:0.8954, R:0.0100, T:30.7177(w:0.041)⚠️)
Batch 350/537: Loss=2.3186 (C:1.0083, R:0.0100, T:31.7417(w:0.041)⚠️)
Batch 375/537: Loss=2.3211 (C:1.0237, R:0.0100, T:31.4293(w:0.041)⚠️)
Batch 400/537: Loss=2.3531 (C:1.0393, R:0.0100, T:31.8264(w:0.041)⚠️)
Batch 425/537: Loss=2.2891 (C:0.9912, R:0.0099, T:31.4404(w:0.041)⚠️)
Batch 450/537: Loss=2.3179 (C:1.0456, R:0.0100, T:30.8200(w:0.041)⚠️)
Batch 475/537: Loss=2.3548 (C:1.0383, R:0.0100, T:31.8909(w:0.041)⚠️)
Batch 500/537: Loss=2.3364 (C:1.0479, R:0.0100, T:31.2126(w:0.041)⚠️)
Batch 525/537: Loss=2.3320 (C:1.0251, R:0.0099, T:31.6582(w:0.041)⚠️)

📊 EPOCH 34 TRAINING SUMMARY:
  Total Loss: 2.3039
  Contrastive: 1.0023
  Reconstruction: 0.0100
  Topological: 31.5290 (weight: 0.041)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.9277
  Contrastive: 0.8656
  Reconstruction: 0.0100
  Topological: 25.7248 (weight: 0.041)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 34/100 COMPLETE (73.6s)
Train Loss: 2.3039 (C:1.0023, R:0.0100, T:31.5290)
Val Loss:   1.9277 (C:0.8656, R:0.0100, T:25.7248)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 35 | Batches: 537 | Topological Weight: 0.0425
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.3844 (C:0.9790, R:0.0099, T:33.0464(w:0.043)⚠️)
Batch  25/537: Loss=2.3127 (C:0.9710, R:0.0099, T:31.5450(w:0.043)⚠️)
Batch  50/537: Loss=2.3086 (C:0.9507, R:0.0100, T:31.9276(w:0.043)⚠️)
Batch  75/537: Loss=2.2962 (C:1.0071, R:0.0099, T:30.3071(w:0.043)⚠️)
Batch 100/537: Loss=2.3589 (C:1.0122, R:0.0100, T:31.6638(w:0.043)⚠️)
Batch 125/537: Loss=2.3618 (C:1.0552, R:0.0099, T:30.7203(w:0.043)⚠️)
Batch 150/537: Loss=2.3554 (C:0.9943, R:0.0100, T:32.0031(w:0.043)⚠️)
Batch 175/537: Loss=2.2891 (C:0.9279, R:0.0099, T:32.0027(w:0.043)⚠️)
Batch 200/537: Loss=2.2941 (C:0.9324, R:0.0099, T:32.0167(w:0.043)⚠️)
Batch 225/537: Loss=2.4209 (C:1.0271, R:0.0100, T:32.7699(w:0.043)⚠️)
Batch 250/537: Loss=2.3553 (C:0.9819, R:0.0100, T:32.2910(w:0.043)⚠️)
Batch 275/537: Loss=2.3254 (C:1.0077, R:0.0100, T:30.9804(w:0.043)⚠️)
Batch 300/537: Loss=2.2963 (C:0.9448, R:0.0099, T:31.7774(w:0.043)⚠️)
Batch 325/537: Loss=2.3528 (C:1.0336, R:0.0100, T:31.0165(w:0.043)⚠️)
Batch 350/537: Loss=2.3170 (C:1.0091, R:0.0099, T:30.7519(w:0.043)⚠️)
Batch 375/537: Loss=2.3735 (C:1.0307, R:0.0100, T:31.5704(w:0.043)⚠️)
Batch 400/537: Loss=2.3071 (C:1.0083, R:0.0099, T:30.5360(w:0.043)⚠️)
Batch 425/537: Loss=2.3293 (C:0.9851, R:0.0100, T:31.6063(w:0.043)⚠️)
Batch 450/537: Loss=2.3752 (C:1.0505, R:0.0099, T:31.1474(w:0.043)⚠️)
Batch 475/537: Loss=2.3905 (C:1.0508, R:0.0100, T:31.4984(w:0.043)⚠️)
Batch 500/537: Loss=2.2943 (C:0.9583, R:0.0099, T:31.4112(w:0.043)⚠️)
Batch 525/537: Loss=2.3902 (C:1.0103, R:0.0100, T:32.4436(w:0.043)⚠️)

📊 EPOCH 35 TRAINING SUMMARY:
  Total Loss: 2.3459
  Contrastive: 1.0025
  Reconstruction: 0.0100
  Topological: 31.5874 (weight: 0.043)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.9640
  Contrastive: 0.8570
  Reconstruction: 0.0100
  Topological: 26.0244 (weight: 0.043)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 35/100 COMPLETE (78.9s)
Train Loss: 2.3459 (C:1.0025, R:0.0100, T:31.5874)
Val Loss:   1.9640 (C:0.8570, R:0.0100, T:26.0244)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 36 | Batches: 537 | Topological Weight: 0.0438
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.3698 (C:1.0151, R:0.0100, T:30.9409(w:0.044)⚠️)
Batch  25/537: Loss=2.3642 (C:0.9378, R:0.0100, T:32.5811(w:0.044)⚠️)
Batch  50/537: Loss=2.4188 (C:1.0176, R:0.0100, T:32.0042(w:0.044)⚠️)
Batch  75/537: Loss=2.4347 (C:1.0246, R:0.0100, T:32.2089(w:0.044)⚠️)
Batch 100/537: Loss=2.4544 (C:1.0386, R:0.0100, T:32.3404(w:0.044)⚠️)
Batch 125/537: Loss=2.3908 (C:0.9578, R:0.0100, T:32.7310(w:0.044)⚠️)
Batch 150/537: Loss=2.4321 (C:1.0239, R:0.0100, T:32.1646(w:0.044)⚠️)
Batch 175/537: Loss=2.3542 (C:0.9400, R:0.0100, T:32.3019(w:0.044)⚠️)
Batch 200/537: Loss=2.4295 (C:1.0573, R:0.0099, T:31.3426(w:0.044)⚠️)
Batch 225/537: Loss=2.4665 (C:1.0565, R:0.0099, T:32.2072(w:0.044)⚠️)
Batch 250/537: Loss=2.4263 (C:1.0250, R:0.0099, T:32.0082(w:0.044)⚠️)
Batch 275/537: Loss=2.3488 (C:0.9404, R:0.0100, T:32.1693(w:0.044)⚠️)
Batch 300/537: Loss=2.3701 (C:0.9534, R:0.0100, T:32.3594(w:0.044)⚠️)
Batch 325/537: Loss=2.3544 (C:0.9686, R:0.0100, T:31.6540(w:0.044)⚠️)
Batch 350/537: Loss=2.4584 (C:1.0447, R:0.0100, T:32.2919(w:0.044)⚠️)
Batch 375/537: Loss=2.4631 (C:1.0909, R:0.0100, T:31.3415(w:0.044)⚠️)
Batch 400/537: Loss=2.4156 (C:1.0190, R:0.0100, T:31.8990(w:0.044)⚠️)
Batch 425/537: Loss=2.3875 (C:1.0271, R:0.0099, T:31.0721(w:0.044)⚠️)
Batch 450/537: Loss=2.3787 (C:1.0341, R:0.0099, T:30.7098(w:0.044)⚠️)
Batch 475/537: Loss=2.4587 (C:1.0712, R:0.0100, T:31.6910(w:0.044)⚠️)
Batch 500/537: Loss=2.3841 (C:1.0228, R:0.0099, T:31.0935(w:0.044)⚠️)
Batch 525/537: Loss=2.4069 (C:1.0428, R:0.0099, T:31.1573(w:0.044)⚠️)

📊 EPOCH 36 TRAINING SUMMARY:
  Total Loss: 2.3887
  Contrastive: 1.0025
  Reconstruction: 0.0100
  Topological: 31.6603 (weight: 0.044)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.9622
  Contrastive: 0.8234
  Reconstruction: 0.0100
  Topological: 26.0070 (weight: 0.044)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 36/100 COMPLETE (78.3s)
Train Loss: 2.3887 (C:1.0025, R:0.0100, T:31.6603)
Val Loss:   1.9622 (C:0.8234, R:0.0100, T:26.0070)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 37 | Batches: 537 | Topological Weight: 0.0450
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.3714 (C:0.9922, R:0.0099, T:30.6260(w:0.045)⚠️)
Batch  25/537: Loss=2.3624 (C:0.9347, R:0.0099, T:31.7046(w:0.045)⚠️)
Batch  50/537: Loss=2.3699 (C:0.9124, R:0.0100, T:32.3669(w:0.045)⚠️)
Batch  75/537: Loss=2.5180 (C:1.0648, R:0.0100, T:32.2702(w:0.045)⚠️)
Batch 100/537: Loss=2.4362 (C:0.9940, R:0.0100, T:32.0255(w:0.045)⚠️)
Batch 125/537: Loss=2.4192 (C:0.9579, R:0.0099, T:32.4525(w:0.045)⚠️)
Batch 150/537: Loss=2.4809 (C:1.0124, R:0.0100, T:32.6118(w:0.045)⚠️)
Batch 175/537: Loss=2.4087 (C:0.9749, R:0.0100, T:31.8401(w:0.045)⚠️)
Batch 200/537: Loss=2.3572 (C:0.9902, R:0.0100, T:30.3568(w:0.045)⚠️)
Batch 225/537: Loss=2.4230 (C:0.9986, R:0.0100, T:31.6322(w:0.045)⚠️)
Batch 250/537: Loss=2.4428 (C:1.0091, R:0.0099, T:31.8382(w:0.045)⚠️)
Batch 275/537: Loss=2.4004 (C:1.0079, R:0.0100, T:30.9221(w:0.045)⚠️)
Batch 300/537: Loss=2.4384 (C:1.0674, R:0.0100, T:30.4441(w:0.045)⚠️)
Batch 325/537: Loss=2.3890 (C:1.0077, R:0.0099, T:30.6736(w:0.045)⚠️)
Batch 350/537: Loss=2.4470 (C:0.9818, R:0.0099, T:32.5377(w:0.045)⚠️)
Batch 375/537: Loss=2.4190 (C:1.0583, R:0.0099, T:30.2173(w:0.045)⚠️)
Batch 400/537: Loss=2.4221 (C:0.9779, R:0.0099, T:32.0705(w:0.045)⚠️)
Batch 425/537: Loss=2.3906 (C:0.9541, R:0.0100, T:31.8992(w:0.045)⚠️)
Batch 450/537: Loss=2.4963 (C:1.0854, R:0.0099, T:31.3306(w:0.045)⚠️)
Batch 475/537: Loss=2.4988 (C:1.0556, R:0.0100, T:32.0481(w:0.045)⚠️)
Batch 500/537: Loss=2.4125 (C:1.0188, R:0.0099, T:30.9485(w:0.045)⚠️)
Batch 525/537: Loss=2.4070 (C:1.0444, R:0.0100, T:30.2576(w:0.045)⚠️)

📊 EPOCH 37 TRAINING SUMMARY:
  Total Loss: 2.4157
  Contrastive: 1.0046
  Reconstruction: 0.0100
  Topological: 31.3338 (weight: 0.045)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0371
  Contrastive: 0.8406
  Reconstruction: 0.0100
  Topological: 26.5677 (weight: 0.045)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 37/100 COMPLETE (70.3s)
Train Loss: 2.4157 (C:1.0046, R:0.0100, T:31.3338)
Val Loss:   2.0371 (C:0.8406, R:0.0100, T:26.5677)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 38 | Batches: 537 | Topological Weight: 0.0462
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.4461 (C:0.9584, R:0.0100, T:32.1435(w:0.046)⚠️)
Batch  25/537: Loss=2.3955 (C:0.9628, R:0.0100, T:30.9545(w:0.046)⚠️)
Batch  50/537: Loss=2.4309 (C:0.9750, R:0.0099, T:31.4571(w:0.046)⚠️)
Batch  75/537: Loss=2.4220 (C:0.9673, R:0.0100, T:31.4311(w:0.046)⚠️)
Batch 100/537: Loss=2.5238 (C:1.0659, R:0.0100, T:31.5014(w:0.046)⚠️)
Batch 125/537: Loss=2.4430 (C:0.9956, R:0.0100, T:31.2745(w:0.046)⚠️)
Batch 150/537: Loss=2.5133 (C:1.0435, R:0.0100, T:31.7564(w:0.046)⚠️)
Batch 175/537: Loss=2.5168 (C:1.0290, R:0.0100, T:32.1469(w:0.046)⚠️)
Batch 200/537: Loss=2.4258 (C:0.9605, R:0.0100, T:31.6611(w:0.046)⚠️)
Batch 225/537: Loss=2.4123 (C:0.9664, R:0.0099, T:31.2423(w:0.046)⚠️)
Batch 250/537: Loss=2.5332 (C:1.1006, R:0.0100, T:30.9531(w:0.046)⚠️)
Batch 275/537: Loss=2.4641 (C:0.9969, R:0.0100, T:31.7020(w:0.046)⚠️)
Batch 300/537: Loss=2.5202 (C:1.0923, R:0.0100, T:30.8519(w:0.046)⚠️)
Batch 325/537: Loss=2.4938 (C:1.0284, R:0.0100, T:31.6616(w:0.046)⚠️)
Batch 350/537: Loss=2.5144 (C:1.0618, R:0.0100, T:31.3869(w:0.046)⚠️)
Batch 375/537: Loss=2.4854 (C:1.0508, R:0.0099, T:30.9976(w:0.046)⚠️)
Batch 400/537: Loss=2.4187 (C:0.9732, R:0.0100, T:31.2326(w:0.046)⚠️)
Batch 425/537: Loss=2.4543 (C:1.0358, R:0.0100, T:30.6478(w:0.046)⚠️)
Batch 450/537: Loss=2.4959 (C:1.0533, R:0.0099, T:31.1698(w:0.046)⚠️)
Batch 475/537: Loss=2.4963 (C:1.0461, R:0.0100, T:31.3354(w:0.046)⚠️)
Batch 500/537: Loss=2.5262 (C:1.0693, R:0.0100, T:31.4788(w:0.046)⚠️)
Batch 525/537: Loss=2.4211 (C:1.0391, R:0.0099, T:29.8596(w:0.046)⚠️)

📊 EPOCH 38 TRAINING SUMMARY:
  Total Loss: 2.4605
  Contrastive: 1.0156
  Reconstruction: 0.0100
  Topological: 31.2206 (weight: 0.046)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0809
  Contrastive: 0.8406
  Reconstruction: 0.0100
  Topological: 26.7961 (weight: 0.046)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 38/100 COMPLETE (74.0s)
Train Loss: 2.4605 (C:1.0156, R:0.0100, T:31.2206)
Val Loss:   2.0809 (C:0.8406, R:0.0100, T:26.7961)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 39 | Batches: 537 | Topological Weight: 0.0475
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.4988 (C:1.0303, R:0.0099, T:30.8950(w:0.048)⚠️)
Batch  25/537: Loss=2.4790 (C:0.9909, R:0.0099, T:31.3070(w:0.048)⚠️)
Batch  50/537: Loss=2.4590 (C:0.9821, R:0.0099, T:31.0712(w:0.048)⚠️)
Batch  75/537: Loss=2.4882 (C:1.0061, R:0.0099, T:31.1798(w:0.048)⚠️)
Batch 100/537: Loss=2.5218 (C:1.0490, R:0.0099, T:30.9851(w:0.048)⚠️)
Batch 125/537: Loss=2.4337 (C:0.9191, R:0.0100, T:31.8649(w:0.048)⚠️)
Batch 150/537: Loss=2.5183 (C:0.9700, R:0.0100, T:32.5747(w:0.048)⚠️)
Batch 175/537: Loss=2.5318 (C:0.9955, R:0.0100, T:32.3222(w:0.048)⚠️)
Batch 200/537: Loss=2.4689 (C:1.0326, R:0.0099, T:30.2180(w:0.048)⚠️)
Batch 225/537: Loss=2.5141 (C:1.0116, R:0.0100, T:31.6099(w:0.048)⚠️)
Batch 250/537: Loss=2.4713 (C:0.9616, R:0.0099, T:31.7614(w:0.048)⚠️)
Batch 275/537: Loss=2.5719 (C:1.0155, R:0.0100, T:32.7450(w:0.048)⚠️)
Batch 300/537: Loss=2.4843 (C:0.9605, R:0.0100, T:32.0592(w:0.048)⚠️)
Batch 325/537: Loss=2.5255 (C:0.9747, R:0.0100, T:32.6267(w:0.048)⚠️)
Batch 350/537: Loss=2.5220 (C:0.9968, R:0.0100, T:32.0880(w:0.048)⚠️)
Batch 375/537: Loss=2.4767 (C:0.9680, R:0.0100, T:31.7412(w:0.048)⚠️)
Batch 400/537: Loss=2.5167 (C:1.0128, R:0.0099, T:31.6400(w:0.048)⚠️)
Batch 425/537: Loss=2.4815 (C:0.9829, R:0.0100, T:31.5292(w:0.048)⚠️)
Batch 450/537: Loss=2.5203 (C:1.0321, R:0.0100, T:31.3104(w:0.048)⚠️)
Batch 475/537: Loss=2.4965 (C:1.0464, R:0.0099, T:30.5082(w:0.048)⚠️)
Batch 500/537: Loss=2.5444 (C:1.0214, R:0.0099, T:32.0410(w:0.048)⚠️)
Batch 525/537: Loss=2.5567 (C:1.0720, R:0.0100, T:31.2347(w:0.048)⚠️)

📊 EPOCH 39 TRAINING SUMMARY:
  Total Loss: 2.5166
  Contrastive: 1.0181
  Reconstruction: 0.0100
  Topological: 31.5260 (weight: 0.048)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0791
  Contrastive: 0.8300
  Reconstruction: 0.0100
  Topological: 26.2749 (weight: 0.048)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 39/100 COMPLETE (71.5s)
Train Loss: 2.5166 (C:1.0181, R:0.0100, T:31.5260)
Val Loss:   2.0791 (C:0.8300, R:0.0100, T:26.2749)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 40 | Batches: 537 | Topological Weight: 0.0488
🌱 Early topological learning
============================================================
Batch   0/537: Loss=2.5529 (C:1.0189, R:0.0100, T:31.4469(w:0.049)⚠️)
Batch  25/537: Loss=2.6549 (C:1.1261, R:0.0100, T:31.3407(w:0.049)⚠️)
Batch  50/537: Loss=2.5013 (C:0.9524, R:0.0099, T:31.7508(w:0.049)⚠️)
Batch  75/537: Loss=2.5361 (C:1.0192, R:0.0099, T:31.0961(w:0.049)⚠️)
Batch 100/537: Loss=2.5600 (C:1.0342, R:0.0100, T:31.2799(w:0.049)⚠️)
Batch 125/537: Loss=2.4969 (C:1.0445, R:0.0100, T:29.7735(w:0.049)⚠️)
Batch 150/537: Loss=2.5876 (C:1.0455, R:0.0099, T:31.6117(w:0.049)⚠️)
Batch 175/537: Loss=2.5291 (C:1.0135, R:0.0100, T:31.0689(w:0.049)⚠️)
Batch 200/537: Loss=2.6304 (C:1.0688, R:0.0099, T:32.0137(w:0.049)⚠️)
Batch 225/537: Loss=2.5902 (C:1.0307, R:0.0100, T:31.9692(w:0.049)⚠️)
Batch 250/537: Loss=2.5918 (C:1.0510, R:0.0099, T:31.5847(w:0.049)⚠️)
Batch 275/537: Loss=2.6065 (C:1.0375, R:0.0099, T:32.1624(w:0.049)⚠️)
Batch 300/537: Loss=2.5918 (C:1.0228, R:0.0100, T:32.1634(w:0.049)⚠️)
Batch 325/537: Loss=2.5949 (C:1.0391, R:0.0100, T:31.8942(w:0.049)⚠️)
Batch 350/537: Loss=2.6528 (C:1.0807, R:0.0099, T:32.2285(w:0.049)⚠️)
Batch 375/537: Loss=2.5685 (C:1.0284, R:0.0099, T:31.5713(w:0.049)⚠️)
Batch 400/537: Loss=2.6113 (C:1.1096, R:0.0099, T:30.7834(w:0.049)⚠️)
Batch 425/537: Loss=2.5781 (C:1.0695, R:0.0099, T:30.9263(w:0.049)⚠️)
Batch 450/537: Loss=2.6013 (C:1.0668, R:0.0100, T:31.4572(w:0.049)⚠️)
Batch 475/537: Loss=2.5716 (C:1.0221, R:0.0099, T:31.7657(w:0.049)⚠️)
Batch 500/537: Loss=2.6053 (C:1.0343, R:0.0100, T:32.2064(w:0.049)⚠️)
Batch 525/537: Loss=2.5995 (C:1.0636, R:0.0100, T:31.4839(w:0.049)⚠️)

📊 EPOCH 40 TRAINING SUMMARY:
  Total Loss: 2.5796
  Contrastive: 1.0354
  Reconstruction: 0.0100
  Topological: 31.6550 (weight: 0.049)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1164
  Contrastive: 0.8239
  Reconstruction: 0.0100
  Topological: 26.4934 (weight: 0.049)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 40/100 COMPLETE (72.4s)
Train Loss: 2.5796 (C:1.0354, R:0.0100, T:31.6550)
Val Loss:   2.1164 (C:0.8239, R:0.0100, T:26.4934)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 41 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6109 (C:0.9831, R:0.0100, T:32.5365(w:0.050)⚠️)
Batch  25/537: Loss=2.6339 (C:1.0507, R:0.0099, T:31.6441(w:0.050)⚠️)
Batch  50/537: Loss=2.5549 (C:0.9722, R:0.0099, T:31.6336(w:0.050)⚠️)
Batch  75/537: Loss=2.6019 (C:1.0140, R:0.0100, T:31.7381(w:0.050)⚠️)
Batch 100/537: Loss=2.6633 (C:1.0533, R:0.0100, T:32.1808(w:0.050)⚠️)
Batch 125/537: Loss=2.6378 (C:1.0511, R:0.0099, T:31.7130(w:0.050)⚠️)
Batch 150/537: Loss=2.6580 (C:1.1445, R:0.0099, T:30.2492(w:0.050)⚠️)
Batch 175/537: Loss=2.5535 (C:0.9889, R:0.0100, T:31.2715(w:0.050)⚠️)
Batch 200/537: Loss=2.7153 (C:1.0294, R:0.0100, T:33.6984(w:0.050)⚠️)
Batch 225/537: Loss=2.6246 (C:1.0964, R:0.0099, T:30.5452(w:0.050)⚠️)
Batch 250/537: Loss=2.6863 (C:1.1014, R:0.0099, T:31.6779(w:0.050)⚠️)
Batch 275/537: Loss=2.6413 (C:1.0680, R:0.0100, T:31.4459(w:0.050)⚠️)
Batch 300/537: Loss=2.6567 (C:1.1055, R:0.0100, T:31.0042(w:0.050)⚠️)
Batch 325/537: Loss=2.6141 (C:1.0405, R:0.0100, T:31.4511(w:0.050)⚠️)
Batch 350/537: Loss=2.6451 (C:1.0764, R:0.0099, T:31.3542(w:0.050)⚠️)
Batch 375/537: Loss=2.6261 (C:1.1199, R:0.0099, T:30.1037(w:0.050)⚠️)
Batch 400/537: Loss=2.6389 (C:1.1590, R:0.0100, T:29.5782(w:0.050)⚠️)
Batch 425/537: Loss=2.6271 (C:1.1275, R:0.0099, T:29.9731(w:0.050)⚠️)
Batch 450/537: Loss=2.5769 (C:1.0415, R:0.0100, T:30.6870(w:0.050)⚠️)
Batch 475/537: Loss=2.6257 (C:1.1134, R:0.0099, T:30.2256(w:0.050)⚠️)
Batch 500/537: Loss=2.6862 (C:1.1486, R:0.0099, T:30.7315(w:0.050)⚠️)
Batch 525/537: Loss=2.6336 (C:1.0720, R:0.0099, T:31.2131(w:0.050)⚠️)

📊 EPOCH 41 TRAINING SUMMARY:
  Total Loss: 2.6121
  Contrastive: 1.0538
  Reconstruction: 0.0100
  Topological: 31.1456 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.2080
  Contrastive: 0.8425
  Reconstruction: 0.0100
  Topological: 27.2912 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 41/100 COMPLETE (70.0s)
Train Loss: 2.6121 (C:1.0538, R:0.0100, T:31.1456)
Val Loss:   2.2080 (C:0.8425, R:0.0100, T:27.2912)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 42 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6043 (C:1.0721, R:0.0100, T:30.6246(w:0.050)⚠️)
Batch  25/537: Loss=2.6165 (C:1.0295, R:0.0100, T:31.7198(w:0.050)⚠️)
Batch  50/537: Loss=2.4969 (C:0.9940, R:0.0100, T:30.0380(w:0.050)⚠️)
Batch  75/537: Loss=2.5817 (C:0.9788, R:0.0099, T:32.0379(w:0.050)⚠️)
Batch 100/537: Loss=2.5986 (C:1.0437, R:0.0100, T:31.0779(w:0.050)⚠️)
Batch 125/537: Loss=2.5807 (C:1.0423, R:0.0100, T:30.7483(w:0.050)⚠️)
Batch 150/537: Loss=2.6037 (C:1.0543, R:0.0099, T:30.9686(w:0.050)⚠️)
Batch 175/537: Loss=2.5518 (C:1.0184, R:0.0100, T:30.6486(w:0.050)⚠️)
Batch 200/537: Loss=2.5845 (C:1.0550, R:0.0100, T:30.5703(w:0.050)⚠️)
Batch 225/537: Loss=2.6266 (C:1.0732, R:0.0099, T:31.0494(w:0.050)⚠️)
Batch 250/537: Loss=2.6352 (C:1.0828, R:0.0100, T:31.0271(w:0.050)⚠️)
Batch 275/537: Loss=2.6381 (C:1.0951, R:0.0100, T:30.8414(w:0.050)⚠️)
Batch 300/537: Loss=2.6362 (C:1.0966, R:0.0100, T:30.7732(w:0.050)⚠️)
Batch 325/537: Loss=2.6028 (C:1.0354, R:0.0100, T:31.3283(w:0.050)⚠️)
Batch 350/537: Loss=2.5737 (C:1.0256, R:0.0099, T:30.9437(w:0.050)⚠️)
Batch 375/537: Loss=2.5506 (C:0.9931, R:0.0099, T:31.1299(w:0.050)⚠️)
Batch 400/537: Loss=2.6349 (C:1.0922, R:0.0100, T:30.8334(w:0.050)⚠️)
Batch 425/537: Loss=2.6758 (C:1.1472, R:0.0099, T:30.5514(w:0.050)⚠️)
Batch 450/537: Loss=2.5780 (C:1.0379, R:0.0100, T:30.7826(w:0.050)⚠️)
Batch 475/537: Loss=2.7212 (C:1.1652, R:0.0100, T:31.1002(w:0.050)⚠️)
Batch 500/537: Loss=2.5604 (C:1.0197, R:0.0099, T:30.7951(w:0.050)⚠️)
Batch 525/537: Loss=2.6715 (C:1.1061, R:0.0100, T:31.2875(w:0.050)⚠️)

📊 EPOCH 42 TRAINING SUMMARY:
  Total Loss: 2.6157
  Contrastive: 1.0660
  Reconstruction: 0.0100
  Topological: 30.9728 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.2115
  Contrastive: 0.8220
  Reconstruction: 0.0100
  Topological: 27.7695 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 42/100 COMPLETE (74.4s)
Train Loss: 2.6157 (C:1.0660, R:0.0100, T:30.9728)
Val Loss:   2.2115 (C:0.8220, R:0.0100, T:27.7695)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 43 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5951 (C:1.0095, R:0.0100, T:31.6911(w:0.050)⚠️)
Batch  25/537: Loss=2.6268 (C:1.0562, R:0.0099, T:31.3920(w:0.050)⚠️)
Batch  50/537: Loss=2.6196 (C:1.0577, R:0.0100, T:31.2188(w:0.050)⚠️)
Batch  75/537: Loss=2.6499 (C:1.0888, R:0.0100, T:31.2021(w:0.050)⚠️)
Batch 100/537: Loss=2.5640 (C:1.0209, R:0.0099, T:30.8418(w:0.050)⚠️)
Batch 125/537: Loss=2.6069 (C:1.0568, R:0.0099, T:30.9810(w:0.050)⚠️)
Batch 150/537: Loss=2.5882 (C:1.0304, R:0.0099, T:31.1360(w:0.050)⚠️)
Batch 175/537: Loss=2.5963 (C:1.0537, R:0.0100, T:30.8320(w:0.050)⚠️)
Batch 200/537: Loss=2.5414 (C:1.0436, R:0.0099, T:29.9378(w:0.050)⚠️)
Batch 225/537: Loss=2.6194 (C:1.0817, R:0.0099, T:30.7353(w:0.050)⚠️)
Batch 250/537: Loss=2.5368 (C:0.9590, R:0.0099, T:31.5366(w:0.050)⚠️)
Batch 275/537: Loss=2.5884 (C:1.0204, R:0.0100, T:31.3403(w:0.050)⚠️)
Batch 300/537: Loss=2.5240 (C:0.9522, R:0.0100, T:31.4157(w:0.050)⚠️)
Batch 325/537: Loss=2.6236 (C:1.0597, R:0.0100, T:31.2570(w:0.050)⚠️)
Batch 350/537: Loss=2.6570 (C:1.1371, R:0.0099, T:30.3764(w:0.050)⚠️)
Batch 375/537: Loss=2.6090 (C:1.0897, R:0.0099, T:30.3659(w:0.050)⚠️)
Batch 400/537: Loss=2.6272 (C:1.1064, R:0.0099, T:30.3961(w:0.050)⚠️)
Batch 425/537: Loss=2.6408 (C:1.0663, R:0.0100, T:31.4697(w:0.050)⚠️)
Batch 450/537: Loss=2.6369 (C:1.1086, R:0.0099, T:30.5462(w:0.050)⚠️)
Batch 475/537: Loss=2.6269 (C:1.0620, R:0.0100, T:31.2782(w:0.050)⚠️)
Batch 500/537: Loss=2.6358 (C:1.0830, R:0.0099, T:31.0361(w:0.050)⚠️)
Batch 525/537: Loss=2.6328 (C:1.0897, R:0.0099, T:30.8417(w:0.050)⚠️)

📊 EPOCH 43 TRAINING SUMMARY:
  Total Loss: 2.6154
  Contrastive: 1.0669
  Reconstruction: 0.0100
  Topological: 30.9491 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1640
  Contrastive: 0.8150
  Reconstruction: 0.0100
  Topological: 26.9606 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 43/100 COMPLETE (82.8s)
Train Loss: 2.6154 (C:1.0669, R:0.0100, T:30.9491)
Val Loss:   2.1640 (C:0.8150, R:0.0100, T:26.9606)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 44 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5746 (C:1.0355, R:0.0099, T:30.7627(w:0.050)⚠️)
Batch  25/537: Loss=2.5763 (C:1.0084, R:0.0099, T:31.3380(w:0.050)⚠️)
Batch  50/537: Loss=2.5786 (C:1.0149, R:0.0099, T:31.2548(w:0.050)⚠️)
Batch  75/537: Loss=2.5410 (C:1.0346, R:0.0100, T:30.1087(w:0.050)⚠️)
Batch 100/537: Loss=2.5904 (C:1.0169, R:0.0100, T:31.4484(w:0.050)⚠️)
Batch 125/537: Loss=2.5651 (C:1.0244, R:0.0099, T:30.7935(w:0.050)⚠️)
Batch 150/537: Loss=2.6286 (C:1.1117, R:0.0099, T:30.3184(w:0.050)⚠️)
Batch 175/537: Loss=2.5373 (C:1.0056, R:0.0099, T:30.6144(w:0.050)⚠️)
Batch 200/537: Loss=2.5186 (C:0.9796, R:0.0099, T:30.7594(w:0.050)⚠️)
Batch 225/537: Loss=2.5708 (C:1.0357, R:0.0100, T:30.6814(w:0.050)⚠️)
Batch 250/537: Loss=2.5926 (C:1.0471, R:0.0100, T:30.8894(w:0.050)⚠️)
Batch 275/537: Loss=2.5666 (C:1.0230, R:0.0099, T:30.8540(w:0.050)⚠️)
Batch 300/537: Loss=2.5529 (C:0.9855, R:0.0099, T:31.3279(w:0.050)⚠️)
Batch 325/537: Loss=2.7015 (C:1.1308, R:0.0099, T:31.3932(w:0.050)⚠️)
Batch 350/537: Loss=2.6510 (C:1.1094, R:0.0099, T:30.8124(w:0.050)⚠️)
Batch 375/537: Loss=2.6281 (C:1.0179, R:0.0100, T:32.1831(w:0.050)⚠️)
Batch 400/537: Loss=2.6622 (C:1.1431, R:0.0100, T:30.3616(w:0.050)⚠️)
Batch 425/537: Loss=2.6388 (C:1.1077, R:0.0100, T:30.6021(w:0.050)⚠️)
Batch 450/537: Loss=2.6965 (C:1.1400, R:0.0100, T:31.1105(w:0.050)⚠️)
Batch 475/537: Loss=2.6382 (C:1.1005, R:0.0099, T:30.7344(w:0.050)⚠️)
Batch 500/537: Loss=2.6647 (C:1.0898, R:0.0100, T:31.4790(w:0.050)⚠️)
Batch 525/537: Loss=2.5158 (C:0.9855, R:0.0099, T:30.5862(w:0.050)⚠️)

📊 EPOCH 44 TRAINING SUMMARY:
  Total Loss: 2.6057
  Contrastive: 1.0626
  Reconstruction: 0.0100
  Topological: 30.8421 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1598
  Contrastive: 0.7921
  Reconstruction: 0.0100
  Topological: 27.3329 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 44/100 COMPLETE (91.2s)
Train Loss: 2.6057 (C:1.0626, R:0.0100, T:30.8421)
Val Loss:   2.1598 (C:0.7921, R:0.0100, T:27.3329)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 45 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5892 (C:1.0853, R:0.0099, T:30.0575(w:0.050)⚠️)
Batch  25/537: Loss=2.5736 (C:1.0208, R:0.0099, T:31.0362(w:0.050)⚠️)
Batch  50/537: Loss=2.6165 (C:1.0540, R:0.0099, T:31.2312(w:0.050)⚠️)
Batch  75/537: Loss=2.5720 (C:1.0608, R:0.0100, T:30.2043(w:0.050)⚠️)
Batch 100/537: Loss=2.5938 (C:1.0466, R:0.0100, T:30.9250(w:0.050)⚠️)
Batch 125/537: Loss=2.6095 (C:1.0486, R:0.0099, T:31.1976(w:0.050)⚠️)
Batch 150/537: Loss=2.5506 (C:1.0378, R:0.0099, T:30.2362(w:0.050)⚠️)
Batch 175/537: Loss=2.5076 (C:0.9733, R:0.0099, T:30.6662(w:0.050)⚠️)
Batch 200/537: Loss=2.6448 (C:1.1264, R:0.0100, T:30.3470(w:0.050)⚠️)
Batch 225/537: Loss=2.6160 (C:1.0652, R:0.0099, T:30.9968(w:0.050)⚠️)
Batch 250/537: Loss=2.6240 (C:1.0844, R:0.0100, T:30.7727(w:0.050)⚠️)
Batch 275/537: Loss=2.6647 (C:1.1189, R:0.0100, T:30.8978(w:0.050)⚠️)
Batch 300/537: Loss=2.5900 (C:1.0577, R:0.0099, T:30.6253(w:0.050)⚠️)
Batch 325/537: Loss=2.6552 (C:1.1207, R:0.0100, T:30.6715(w:0.050)⚠️)
Batch 350/537: Loss=2.6399 (C:1.1470, R:0.0099, T:29.8398(w:0.050)⚠️)
Batch 375/537: Loss=2.6177 (C:1.0808, R:0.0099, T:30.7179(w:0.050)⚠️)
Batch 400/537: Loss=2.6074 (C:1.0499, R:0.0099, T:31.1313(w:0.050)⚠️)
Batch 425/537: Loss=2.6288 (C:1.0862, R:0.0099, T:30.8318(w:0.050)⚠️)
Batch 450/537: Loss=2.6489 (C:1.1233, R:0.0099, T:30.4921(w:0.050)⚠️)
Batch 475/537: Loss=2.5819 (C:1.0352, R:0.0100, T:30.9141(w:0.050)⚠️)
Batch 500/537: Loss=2.5875 (C:1.0870, R:0.0100, T:29.9888(w:0.050)⚠️)
Batch 525/537: Loss=2.6346 (C:1.0976, R:0.0099, T:30.7197(w:0.050)⚠️)

📊 EPOCH 45 TRAINING SUMMARY:
  Total Loss: 2.6129
  Contrastive: 1.0640
  Reconstruction: 0.0100
  Topological: 30.9594 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1540
  Contrastive: 0.7895
  Reconstruction: 0.0100
  Topological: 27.2700 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 45/100 COMPLETE (89.0s)
Train Loss: 2.6129 (C:1.0640, R:0.0100, T:30.9594)
Val Loss:   2.1540 (C:0.7895, R:0.0100, T:27.2700)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 46 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5492 (C:1.0239, R:0.0100, T:30.4852(w:0.050)⚠️)
Batch  25/537: Loss=2.5835 (C:1.0530, R:0.0099, T:30.5909(w:0.050)⚠️)
Batch  50/537: Loss=2.6197 (C:1.0460, R:0.0099, T:31.4553(w:0.050)⚠️)
Batch  75/537: Loss=2.7802 (C:1.0398, R:0.0100, T:34.7886(w:0.050)⚠️)
Batch 100/537: Loss=2.6575 (C:1.0956, R:0.0100, T:31.2170(w:0.050)⚠️)
Batch 125/537: Loss=2.6084 (C:1.0706, R:0.0100, T:30.7376(w:0.050)⚠️)
Batch 150/537: Loss=2.5825 (C:0.9777, R:0.0100, T:32.0767(w:0.050)⚠️)
Batch 175/537: Loss=2.5653 (C:1.0153, R:0.0100, T:30.9795(w:0.050)⚠️)
Batch 200/537: Loss=2.6070 (C:1.0335, R:0.0100, T:31.4491(w:0.050)⚠️)
Batch 225/537: Loss=2.5466 (C:1.0147, R:0.0099, T:30.6188(w:0.050)⚠️)
Batch 250/537: Loss=2.6383 (C:1.1190, R:0.0100, T:30.3656(w:0.050)⚠️)
Batch 275/537: Loss=2.6048 (C:1.0479, R:0.0100, T:31.1166(w:0.050)⚠️)
Batch 300/537: Loss=2.6516 (C:1.0729, R:0.0100, T:31.5549(w:0.050)⚠️)
Batch 325/537: Loss=2.5747 (C:1.0132, R:0.0099, T:31.2084(w:0.050)⚠️)
Batch 350/537: Loss=2.5994 (C:1.0150, R:0.0099, T:31.6689(w:0.050)⚠️)
Batch 375/537: Loss=2.6476 (C:1.0972, R:0.0100, T:30.9864(w:0.050)⚠️)
Batch 400/537: Loss=2.5675 (C:1.0724, R:0.0099, T:29.8830(w:0.050)⚠️)
Batch 425/537: Loss=2.6443 (C:1.0727, R:0.0100, T:31.4126(w:0.050)⚠️)
Batch 450/537: Loss=2.6319 (C:1.0534, R:0.0100, T:31.5518(w:0.050)⚠️)
Batch 475/537: Loss=2.6456 (C:1.1302, R:0.0099, T:30.2880(w:0.050)⚠️)
Batch 500/537: Loss=2.6220 (C:1.1030, R:0.0100, T:30.3616(w:0.050)⚠️)
Batch 525/537: Loss=2.4998 (C:0.9810, R:0.0100, T:30.3548(w:0.050)⚠️)

📊 EPOCH 46 TRAINING SUMMARY:
  Total Loss: 2.6086
  Contrastive: 1.0609
  Reconstruction: 0.0100
  Topological: 30.9351 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1672
  Contrastive: 0.7979
  Reconstruction: 0.0100
  Topological: 27.3666 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 46/100 COMPLETE (89.2s)
Train Loss: 2.6086 (C:1.0609, R:0.0100, T:30.9351)
Val Loss:   2.1672 (C:0.7979, R:0.0100, T:27.3666)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 47 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6004 (C:1.1150, R:0.0099, T:29.6888(w:0.050)⚠️)
Batch  25/537: Loss=2.5454 (C:0.9787, R:0.0099, T:31.3140(w:0.050)⚠️)
Batch  50/537: Loss=2.6207 (C:1.0819, R:0.0099, T:30.7574(w:0.050)⚠️)
Batch  75/537: Loss=2.5925 (C:1.0700, R:0.0099, T:30.4305(w:0.050)⚠️)
Batch 100/537: Loss=2.6123 (C:1.0978, R:0.0099, T:30.2696(w:0.050)⚠️)
Batch 125/537: Loss=2.5506 (C:0.9907, R:0.0100, T:31.1779(w:0.050)⚠️)
Batch 150/537: Loss=2.5647 (C:1.0249, R:0.0100, T:30.7776(w:0.050)⚠️)
Batch 175/537: Loss=2.5631 (C:1.0455, R:0.0099, T:30.3305(w:0.050)⚠️)
Batch 200/537: Loss=2.6240 (C:1.0815, R:0.0099, T:30.8299(w:0.050)⚠️)
Batch 225/537: Loss=2.6163 (C:1.1262, R:0.0099, T:29.7812(w:0.050)⚠️)
Batch 250/537: Loss=2.5667 (C:1.0558, R:0.0099, T:30.1977(w:0.050)⚠️)
Batch 275/537: Loss=2.6753 (C:1.1458, R:0.0100, T:30.5693(w:0.050)⚠️)
Batch 300/537: Loss=2.6052 (C:1.0557, R:0.0100, T:30.9715(w:0.050)⚠️)
Batch 325/537: Loss=2.5038 (C:1.0009, R:0.0099, T:30.0382(w:0.050)⚠️)
Batch 350/537: Loss=2.5731 (C:1.0807, R:0.0099, T:29.8271(w:0.050)⚠️)
Batch 375/537: Loss=2.6006 (C:1.0485, R:0.0099, T:31.0217(w:0.050)⚠️)
Batch 400/537: Loss=2.5829 (C:1.0766, R:0.0099, T:30.1060(w:0.050)⚠️)
Batch 425/537: Loss=2.6068 (C:1.0978, R:0.0099, T:30.1604(w:0.050)⚠️)
Batch 450/537: Loss=2.6184 (C:1.1204, R:0.0100, T:29.9407(w:0.050)⚠️)
Batch 475/537: Loss=2.6065 (C:1.1079, R:0.0099, T:29.9528(w:0.050)⚠️)
Batch 500/537: Loss=2.6032 (C:1.0774, R:0.0100, T:30.4943(w:0.050)⚠️)
Batch 525/537: Loss=2.5709 (C:1.0631, R:0.0100, T:30.1360(w:0.050)⚠️)

📊 EPOCH 47 TRAINING SUMMARY:
  Total Loss: 2.5948
  Contrastive: 1.0677
  Reconstruction: 0.0100
  Topological: 30.5219 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1368
  Contrastive: 0.7735
  Reconstruction: 0.0100
  Topological: 27.2456 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 47/100 COMPLETE (89.2s)
Train Loss: 2.5948 (C:1.0677, R:0.0100, T:30.5219)
Val Loss:   2.1368 (C:0.7735, R:0.0100, T:27.2456)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 48 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5992 (C:1.0652, R:0.0100, T:30.6592(w:0.050)⚠️)
Batch  25/537: Loss=2.5617 (C:1.0067, R:0.0100, T:31.0811(w:0.050)⚠️)
Batch  50/537: Loss=2.5505 (C:1.0062, R:0.0099, T:30.8673(w:0.050)⚠️)
Batch  75/537: Loss=2.5973 (C:1.0265, R:0.0099, T:31.3972(w:0.050)⚠️)
Batch 100/537: Loss=2.5456 (C:0.9977, R:0.0100, T:30.9381(w:0.050)⚠️)
Batch 125/537: Loss=2.5774 (C:1.0298, R:0.0099, T:30.9337(w:0.050)⚠️)
Batch 150/537: Loss=2.6467 (C:1.1152, R:0.0099, T:30.6087(w:0.050)⚠️)
Batch 175/537: Loss=2.5948 (C:1.0498, R:0.0100, T:30.8790(w:0.050)⚠️)
Batch 200/537: Loss=2.5622 (C:0.9930, R:0.0100, T:31.3629(w:0.050)⚠️)
Batch 225/537: Loss=2.6109 (C:1.0790, R:0.0100, T:30.6195(w:0.050)⚠️)
Batch 250/537: Loss=2.6049 (C:1.0693, R:0.0099, T:30.6915(w:0.050)⚠️)
Batch 275/537: Loss=2.5877 (C:1.0586, R:0.0099, T:30.5606(w:0.050)⚠️)
Batch 300/537: Loss=2.6150 (C:1.1036, R:0.0100, T:30.2084(w:0.050)⚠️)
Batch 325/537: Loss=2.6472 (C:1.1340, R:0.0099, T:30.2449(w:0.050)⚠️)
Batch 350/537: Loss=2.6019 (C:1.0689, R:0.0099, T:30.6406(w:0.050)⚠️)
Batch 375/537: Loss=2.6565 (C:1.1077, R:0.0099, T:30.9563(w:0.050)⚠️)
Batch 400/537: Loss=2.6344 (C:1.0985, R:0.0099, T:30.6977(w:0.050)⚠️)
Batch 425/537: Loss=2.6737 (C:1.1129, R:0.0100, T:31.1957(w:0.050)⚠️)
Batch 450/537: Loss=2.5538 (C:1.0448, R:0.0099, T:30.1595(w:0.050)⚠️)
Batch 475/537: Loss=2.6084 (C:1.0742, R:0.0100, T:30.6625(w:0.050)⚠️)
Batch 500/537: Loss=2.6337 (C:1.1064, R:0.0099, T:30.5271(w:0.050)⚠️)
Batch 525/537: Loss=2.6407 (C:1.0947, R:0.0099, T:30.8995(w:0.050)⚠️)

📊 EPOCH 48 TRAINING SUMMARY:
  Total Loss: 2.6046
  Contrastive: 1.0680
  Reconstruction: 0.0100
  Topological: 30.7128 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1652
  Contrastive: 0.7782
  Reconstruction: 0.0100
  Topological: 27.7208 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 48/100 COMPLETE (88.0s)
Train Loss: 2.6046 (C:1.0680, R:0.0100, T:30.7128)
Val Loss:   2.1652 (C:0.7782, R:0.0100, T:27.7208)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 49 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6003 (C:1.0505, R:0.0100, T:30.9771(w:0.050)⚠️)
Batch  25/537: Loss=2.5580 (C:1.0328, R:0.0100, T:30.4822(w:0.050)⚠️)
Batch  50/537: Loss=2.5103 (C:0.9385, R:0.0099, T:31.4164(w:0.050)⚠️)
Batch  75/537: Loss=2.5885 (C:1.0244, R:0.0099, T:31.2639(w:0.050)⚠️)
Batch 100/537: Loss=2.6489 (C:1.0679, R:0.0100, T:31.6006(w:0.050)⚠️)
Batch 125/537: Loss=2.5966 (C:1.0229, R:0.0100, T:31.4535(w:0.050)⚠️)
Batch 150/537: Loss=2.6017 (C:1.0540, R:0.0100, T:30.9338(w:0.050)⚠️)
Batch 175/537: Loss=2.5370 (C:1.0246, R:0.0099, T:30.2283(w:0.050)⚠️)
Batch 200/537: Loss=2.5722 (C:1.0304, R:0.0100, T:30.8164(w:0.050)⚠️)
Batch 225/537: Loss=2.5441 (C:0.9875, R:0.0099, T:31.1125(w:0.050)⚠️)
Batch 250/537: Loss=2.6051 (C:1.0689, R:0.0100, T:30.7026(w:0.050)⚠️)
Batch 275/537: Loss=2.5791 (C:1.0325, R:0.0100, T:30.9108(w:0.050)⚠️)
Batch 300/537: Loss=2.6347 (C:1.1255, R:0.0099, T:30.1635(w:0.050)⚠️)
Batch 325/537: Loss=2.6226 (C:1.0888, R:0.0099, T:30.6549(w:0.050)⚠️)
Batch 350/537: Loss=2.6426 (C:1.0983, R:0.0100, T:30.8662(w:0.050)⚠️)
Batch 375/537: Loss=2.5684 (C:1.0276, R:0.0099, T:30.7962(w:0.050)⚠️)
Batch 400/537: Loss=2.5923 (C:1.0674, R:0.0100, T:30.4778(w:0.050)⚠️)
Batch 425/537: Loss=2.6038 (C:1.0876, R:0.0099, T:30.3043(w:0.050)⚠️)
Batch 450/537: Loss=2.6102 (C:1.0291, R:0.0100, T:31.6033(w:0.050)⚠️)
Batch 475/537: Loss=2.6014 (C:1.0451, R:0.0100, T:31.1067(w:0.050)⚠️)
Batch 500/537: Loss=2.6052 (C:1.0791, R:0.0099, T:30.5025(w:0.050)⚠️)
Batch 525/537: Loss=2.5967 (C:1.0647, R:0.0100, T:30.6199(w:0.050)⚠️)

📊 EPOCH 49 TRAINING SUMMARY:
  Total Loss: 2.5987
  Contrastive: 1.0555
  Reconstruction: 0.0100
  Topological: 30.8433 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1241
  Contrastive: 0.7579
  Reconstruction: 0.0100
  Topological: 27.3024 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 49/100 COMPLETE (85.7s)
Train Loss: 2.5987 (C:1.0555, R:0.0100, T:30.8433)
Val Loss:   2.1241 (C:0.7579, R:0.0100, T:27.3024)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 50 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5279 (C:0.9588, R:0.0100, T:31.3629(w:0.050)⚠️)
Batch  25/537: Loss=2.5837 (C:1.0088, R:0.0100, T:31.4783(w:0.050)⚠️)
Batch  50/537: Loss=2.5569 (C:0.9854, R:0.0100, T:31.4106(w:0.050)⚠️)
Batch  75/537: Loss=2.5335 (C:0.9717, R:0.0099, T:31.2151(w:0.050)⚠️)
Batch 100/537: Loss=2.5315 (C:0.9544, R:0.0100, T:31.5227(w:0.050)⚠️)
Batch 125/537: Loss=2.5188 (C:0.9862, R:0.0100, T:30.6336(w:0.050)⚠️)
Batch 150/537: Loss=2.5615 (C:1.0214, R:0.0100, T:30.7812(w:0.050)⚠️)
Batch 175/537: Loss=2.5849 (C:1.0405, R:0.0099, T:30.8683(w:0.050)⚠️)
Batch 200/537: Loss=2.6223 (C:1.1018, R:0.0099, T:30.3893(w:0.050)⚠️)
Batch 225/537: Loss=2.5686 (C:1.0206, R:0.0100, T:30.9407(w:0.050)⚠️)
Batch 250/537: Loss=2.6322 (C:1.1005, R:0.0100, T:30.6148(w:0.050)⚠️)
Batch 275/537: Loss=2.6098 (C:1.0710, R:0.0099, T:30.7573(w:0.050)⚠️)
Batch 300/537: Loss=2.6133 (C:1.0973, R:0.0099, T:30.3009(w:0.050)⚠️)
Batch 325/537: Loss=2.5166 (C:0.9862, R:0.0100, T:30.5884(w:0.050)⚠️)
Batch 350/537: Loss=2.6468 (C:1.1238, R:0.0099, T:30.4391(w:0.050)⚠️)
Batch 375/537: Loss=2.5437 (C:1.0134, R:0.0100, T:30.5866(w:0.050)⚠️)
Batch 400/537: Loss=2.6576 (C:1.1408, R:0.0100, T:30.3144(w:0.050)⚠️)
Batch 425/537: Loss=2.5398 (C:1.0082, R:0.0100, T:30.6119(w:0.050)⚠️)
Batch 450/537: Loss=2.6752 (C:1.1049, R:0.0100, T:31.3860(w:0.050)⚠️)
Batch 475/537: Loss=2.6053 (C:1.0806, R:0.0100, T:30.4737(w:0.050)⚠️)
Batch 500/537: Loss=2.6197 (C:1.0553, R:0.0100, T:31.2682(w:0.050)⚠️)
Batch 525/537: Loss=2.6160 (C:1.0782, R:0.0100, T:30.7376(w:0.050)⚠️)

📊 EPOCH 50 TRAINING SUMMARY:
  Total Loss: 2.5983
  Contrastive: 1.0549
  Reconstruction: 0.0100
  Topological: 30.8479 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1198
  Contrastive: 0.7536
  Reconstruction: 0.0100
  Topological: 27.3045 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 50/100 COMPLETE (84.7s)
Train Loss: 2.5983 (C:1.0549, R:0.0100, T:30.8479)
Val Loss:   2.1198 (C:0.7536, R:0.0100, T:27.3045)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 51 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6128 (C:1.0627, R:0.0100, T:30.9810(w:0.050)⚠️)
Batch  25/537: Loss=2.5819 (C:1.0196, R:0.0099, T:31.2260(w:0.050)⚠️)
Batch  50/537: Loss=2.5532 (C:1.0056, R:0.0100, T:30.9318(w:0.050)⚠️)
Batch  75/537: Loss=2.5325 (C:0.9696, R:0.0100, T:31.2386(w:0.050)⚠️)
Batch 100/537: Loss=2.5510 (C:0.9864, R:0.0100, T:31.2726(w:0.050)⚠️)
Batch 125/537: Loss=2.6076 (C:1.0631, R:0.0100, T:30.8706(w:0.050)⚠️)
Batch 150/537: Loss=2.6569 (C:1.0974, R:0.0099, T:31.1703(w:0.050)⚠️)
Batch 175/537: Loss=2.5941 (C:1.0673, R:0.0100, T:30.5149(w:0.050)⚠️)
Batch 200/537: Loss=2.6371 (C:1.1124, R:0.0100, T:30.4737(w:0.050)⚠️)
Batch 225/537: Loss=2.6184 (C:1.0695, R:0.0100, T:30.9587(w:0.050)⚠️)
Batch 250/537: Loss=2.5860 (C:1.0351, R:0.0099, T:30.9984(w:0.050)⚠️)
Batch 275/537: Loss=2.5037 (C:0.9946, R:0.0099, T:30.1624(w:0.050)⚠️)
Batch 300/537: Loss=2.5335 (C:1.0228, R:0.0100, T:30.1949(w:0.050)⚠️)
Batch 325/537: Loss=2.5845 (C:1.0029, R:0.0099, T:31.6121(w:0.050)⚠️)
Batch 350/537: Loss=2.6533 (C:1.1206, R:0.0099, T:30.6348(w:0.050)⚠️)
Batch 375/537: Loss=2.6266 (C:1.0966, R:0.0099, T:30.5800(w:0.050)⚠️)
Batch 400/537: Loss=2.6073 (C:1.1020, R:0.0099, T:30.0861(w:0.050)⚠️)
Batch 425/537: Loss=2.6801 (C:1.1613, R:0.0099, T:30.3566(w:0.050)⚠️)
Batch 450/537: Loss=2.6075 (C:1.1022, R:0.0100, T:30.0859(w:0.050)⚠️)
Batch 475/537: Loss=2.6017 (C:1.0459, R:0.0099, T:31.0971(w:0.050)⚠️)
Batch 500/537: Loss=2.6045 (C:1.0963, R:0.0100, T:30.1455(w:0.050)⚠️)
Batch 525/537: Loss=2.5837 (C:1.0012, R:0.0100, T:31.6294(w:0.050)⚠️)

📊 EPOCH 51 TRAINING SUMMARY:
  Total Loss: 2.5844
  Contrastive: 1.0433
  Reconstruction: 0.0100
  Topological: 30.8033 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1443
  Contrastive: 0.7665
  Reconstruction: 0.0100
  Topological: 27.5373 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 51/100 COMPLETE (84.4s)
Train Loss: 2.5844 (C:1.0433, R:0.0100, T:30.8033)
Val Loss:   2.1443 (C:0.7665, R:0.0100, T:27.5373)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 52 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5775 (C:1.0486, R:0.0100, T:30.5578(w:0.050)⚠️)
Batch  25/537: Loss=2.5543 (C:0.9938, R:0.0100, T:31.1893(w:0.050)⚠️)
Batch  50/537: Loss=2.6038 (C:1.0591, R:0.0100, T:30.8736(w:0.050)⚠️)
Batch  75/537: Loss=2.5747 (C:1.0659, R:0.0099, T:30.1567(w:0.050)⚠️)
Batch 100/537: Loss=2.5679 (C:1.0447, R:0.0099, T:30.4451(w:0.050)⚠️)
Batch 125/537: Loss=2.5151 (C:0.9790, R:0.0099, T:30.7025(w:0.050)⚠️)
Batch 150/537: Loss=2.5403 (C:0.9957, R:0.0099, T:30.8719(w:0.050)⚠️)
Batch 175/537: Loss=2.5992 (C:1.0867, R:0.0100, T:30.2294(w:0.050)⚠️)
Batch 200/537: Loss=2.5690 (C:1.0227, R:0.0099, T:30.9062(w:0.050)⚠️)
Batch 225/537: Loss=2.5522 (C:1.0179, R:0.0099, T:30.6672(w:0.050)⚠️)
Batch 250/537: Loss=2.6356 (C:1.1238, R:0.0099, T:30.2162(w:0.050)⚠️)
Batch 275/537: Loss=2.6282 (C:1.0796, R:0.0099, T:30.9527(w:0.050)⚠️)
Batch 300/537: Loss=2.5510 (C:0.9952, R:0.0100, T:31.0973(w:0.050)⚠️)
Batch 325/537: Loss=2.6305 (C:1.0624, R:0.0100, T:31.3413(w:0.050)⚠️)
Batch 350/537: Loss=2.5895 (C:0.9973, R:0.0099, T:31.8237(w:0.050)⚠️)
Batch 375/537: Loss=2.6059 (C:1.0368, R:0.0099, T:31.3625(w:0.050)⚠️)
Batch 400/537: Loss=2.5964 (C:1.0294, R:0.0100, T:31.3193(w:0.050)⚠️)
Batch 425/537: Loss=2.5929 (C:1.0845, R:0.0099, T:30.1485(w:0.050)⚠️)
Batch 450/537: Loss=2.6074 (C:1.0747, R:0.0100, T:30.6332(w:0.050)⚠️)
Batch 475/537: Loss=2.5875 (C:1.0147, R:0.0099, T:31.4365(w:0.050)⚠️)
Batch 500/537: Loss=2.5404 (C:1.0135, R:0.0100, T:30.5179(w:0.050)⚠️)
Batch 525/537: Loss=2.6222 (C:1.0861, R:0.0100, T:30.7012(w:0.050)⚠️)

📊 EPOCH 52 TRAINING SUMMARY:
  Total Loss: 2.5884
  Contrastive: 1.0449
  Reconstruction: 0.0100
  Topological: 30.8501 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0988
  Contrastive: 0.7411
  Reconstruction: 0.0100
  Topological: 27.1330 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 52/100 COMPLETE (99.3s)
Train Loss: 2.5884 (C:1.0449, R:0.0100, T:30.8501)
Val Loss:   2.0988 (C:0.7411, R:0.0100, T:27.1330)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 53 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5683 (C:1.0066, R:0.0100, T:31.2135(w:0.050)⚠️)
Batch  25/537: Loss=2.5491 (C:1.0196, R:0.0099, T:30.5701(w:0.050)⚠️)
Batch  50/537: Loss=2.5944 (C:1.0507, R:0.0100, T:30.8547(w:0.050)⚠️)
Batch  75/537: Loss=2.5444 (C:1.0038, R:0.0099, T:30.7921(w:0.050)⚠️)
Batch 100/537: Loss=2.6026 (C:1.0730, R:0.0100, T:30.5722(w:0.050)⚠️)
Batch 125/537: Loss=2.5943 (C:1.0763, R:0.0099, T:30.3394(w:0.050)⚠️)
Batch 150/537: Loss=2.5706 (C:1.0081, R:0.0100, T:31.2310(w:0.050)⚠️)
Batch 175/537: Loss=2.6155 (C:1.0322, R:0.0100, T:31.6465(w:0.050)⚠️)
Batch 200/537: Loss=2.6307 (C:1.0593, R:0.0099, T:31.4080(w:0.050)⚠️)
Batch 225/537: Loss=2.6098 (C:1.0467, R:0.0099, T:31.2425(w:0.050)⚠️)
Batch 250/537: Loss=2.5852 (C:1.0528, R:0.0100, T:30.6287(w:0.050)⚠️)
Batch 275/537: Loss=2.6309 (C:1.0759, R:0.0100, T:31.0804(w:0.050)⚠️)
Batch 300/537: Loss=2.5853 (C:1.0506, R:0.0099, T:30.6745(w:0.050)⚠️)
Batch 325/537: Loss=2.5602 (C:1.0320, R:0.0100, T:30.5438(w:0.050)⚠️)
Batch 350/537: Loss=2.5876 (C:1.0459, R:0.0100, T:30.8126(w:0.050)⚠️)
Batch 375/537: Loss=2.6106 (C:1.0335, R:0.0100, T:31.5222(w:0.050)⚠️)
Batch 400/537: Loss=2.5869 (C:1.0737, R:0.0099, T:30.2429(w:0.050)⚠️)
Batch 425/537: Loss=2.6112 (C:1.0280, R:0.0099, T:31.6458(w:0.050)⚠️)
Batch 450/537: Loss=2.5925 (C:1.0186, R:0.0099, T:31.4585(w:0.050)⚠️)
Batch 475/537: Loss=2.5550 (C:1.0200, R:0.0100, T:30.6808(w:0.050)⚠️)
Batch 500/537: Loss=2.5915 (C:1.0475, R:0.0099, T:30.8615(w:0.050)⚠️)
Batch 525/537: Loss=2.5483 (C:1.0361, R:0.0100, T:30.2245(w:0.050)⚠️)

📊 EPOCH 53 TRAINING SUMMARY:
  Total Loss: 2.5846
  Contrastive: 1.0303
  Reconstruction: 0.0100
  Topological: 31.0663 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1324
  Contrastive: 0.7434
  Reconstruction: 0.0100
  Topological: 27.7590 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 53/100 COMPLETE (99.5s)
Train Loss: 2.5846 (C:1.0303, R:0.0100, T:31.0663)
Val Loss:   2.1324 (C:0.7434, R:0.0100, T:27.7590)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 54 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5544 (C:1.0267, R:0.0099, T:30.5339(w:0.050)⚠️)
Batch  25/537: Loss=2.5345 (C:0.9753, R:0.0099, T:31.1644(w:0.050)⚠️)
Batch  50/537: Loss=2.5855 (C:1.0150, R:0.0099, T:31.3895(w:0.050)⚠️)
Batch  75/537: Loss=2.5367 (C:0.9627, R:0.0099, T:31.4605(w:0.050)⚠️)
Batch 100/537: Loss=2.5112 (C:0.9527, R:0.0099, T:31.1495(w:0.050)⚠️)
Batch 125/537: Loss=2.5564 (C:1.0262, R:0.0100, T:30.5826(w:0.050)⚠️)
Batch 150/537: Loss=2.5876 (C:1.0089, R:0.0100, T:31.5540(w:0.050)⚠️)
Batch 175/537: Loss=2.5472 (C:1.0205, R:0.0099, T:30.5127(w:0.050)⚠️)
Batch 200/537: Loss=2.6133 (C:1.0332, R:0.0100, T:31.5811(w:0.050)⚠️)
Batch 225/537: Loss=2.6161 (C:0.9871, R:0.0099, T:32.5592(w:0.050)⚠️)
Batch 250/537: Loss=2.5937 (C:1.0265, R:0.0100, T:31.3237(w:0.050)⚠️)
Batch 275/537: Loss=2.6076 (C:1.0380, R:0.0100, T:31.3719(w:0.050)⚠️)
Batch 300/537: Loss=2.4723 (C:0.9768, R:0.0099, T:29.8913(w:0.050)⚠️)
Batch 325/537: Loss=2.5713 (C:1.0117, R:0.0100, T:31.1715(w:0.050)⚠️)
Batch 350/537: Loss=2.5918 (C:1.0638, R:0.0099, T:30.5394(w:0.050)⚠️)
Batch 375/537: Loss=2.6260 (C:1.1157, R:0.0100, T:30.1845(w:0.050)⚠️)
Batch 400/537: Loss=2.5384 (C:1.0376, R:0.0099, T:29.9961(w:0.050)⚠️)
Batch 425/537: Loss=2.5718 (C:1.0110, R:0.0100, T:31.1959(w:0.050)⚠️)
Batch 450/537: Loss=2.5826 (C:0.9967, R:0.0099, T:31.6976(w:0.050)⚠️)
Batch 475/537: Loss=2.5596 (C:0.9843, R:0.0099, T:31.4863(w:0.050)⚠️)
Batch 500/537: Loss=2.5750 (C:1.0637, R:0.0099, T:30.2059(w:0.050)⚠️)
Batch 525/537: Loss=2.6665 (C:1.1127, R:0.0100, T:31.0565(w:0.050)⚠️)

📊 EPOCH 54 TRAINING SUMMARY:
  Total Loss: 2.5761
  Contrastive: 1.0246
  Reconstruction: 0.0100
  Topological: 31.0103 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1349
  Contrastive: 0.7513
  Reconstruction: 0.0100
  Topological: 27.6529 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 54/100 COMPLETE (101.3s)
Train Loss: 2.5761 (C:1.0246, R:0.0100, T:31.0103)
Val Loss:   2.1349 (C:0.7513, R:0.0100, T:27.6529)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 55 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6016 (C:1.0372, R:0.0100, T:31.2680(w:0.050)⚠️)
Batch  25/537: Loss=2.6638 (C:1.0953, R:0.0099, T:31.3505(w:0.050)⚠️)
Batch  50/537: Loss=2.4614 (C:0.9169, R:0.0099, T:30.8711(w:0.050)⚠️)
Batch  75/537: Loss=2.5614 (C:1.0347, R:0.0099, T:30.5129(w:0.050)⚠️)
Batch 100/537: Loss=2.5575 (C:0.9564, R:0.0100, T:32.0024(w:0.050)⚠️)
Batch 125/537: Loss=2.6033 (C:1.0117, R:0.0099, T:31.8116(w:0.050)⚠️)
Batch 150/537: Loss=2.6414 (C:1.0453, R:0.0100, T:31.9004(w:0.050)⚠️)
Batch 175/537: Loss=2.5629 (C:0.9805, R:0.0100, T:31.6274(w:0.050)⚠️)
Batch 200/537: Loss=2.5104 (C:0.9734, R:0.0099, T:30.7197(w:0.050)⚠️)
Batch 225/537: Loss=2.5887 (C:0.9808, R:0.0100, T:32.1389(w:0.050)⚠️)
Batch 250/537: Loss=2.6087 (C:1.0117, R:0.0100, T:31.9204(w:0.050)⚠️)
Batch 275/537: Loss=2.6273 (C:1.0263, R:0.0099, T:31.9993(w:0.050)⚠️)
Batch 300/537: Loss=2.6425 (C:1.0686, R:0.0100, T:31.4565(w:0.050)⚠️)
Batch 325/537: Loss=2.5934 (C:1.0518, R:0.0100, T:30.8130(w:0.050)⚠️)
Batch 350/537: Loss=2.5671 (C:1.0090, R:0.0100, T:31.1408(w:0.050)⚠️)
Batch 375/537: Loss=2.5693 (C:1.0514, R:0.0100, T:30.3387(w:0.050)⚠️)
Batch 400/537: Loss=2.5971 (C:1.0236, R:0.0100, T:31.4496(w:0.050)⚠️)
Batch 425/537: Loss=2.5662 (C:1.0191, R:0.0100, T:30.9221(w:0.050)⚠️)
Batch 450/537: Loss=2.6206 (C:1.1058, R:0.0100, T:30.2768(w:0.050)⚠️)
Batch 475/537: Loss=2.6134 (C:1.0059, R:0.0100, T:32.1293(w:0.050)⚠️)
Batch 500/537: Loss=2.5626 (C:1.0162, R:0.0099, T:30.9074(w:0.050)⚠️)
Batch 525/537: Loss=2.6320 (C:1.0885, R:0.0099, T:30.8494(w:0.050)⚠️)

📊 EPOCH 55 TRAINING SUMMARY:
  Total Loss: 2.5766
  Contrastive: 1.0167
  Reconstruction: 0.0100
  Topological: 31.1783 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1177
  Contrastive: 0.7306
  Reconstruction: 0.0100
  Topological: 27.7209 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 55/100 COMPLETE (92.1s)
Train Loss: 2.5766 (C:1.0167, R:0.0100, T:31.1783)
Val Loss:   2.1177 (C:0.7306, R:0.0100, T:27.7209)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 56 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5326 (C:1.0258, R:0.0100, T:30.1168(w:0.050)⚠️)
Batch  25/537: Loss=2.5283 (C:0.9484, R:0.0099, T:31.5788(w:0.050)⚠️)
Batch  50/537: Loss=2.6129 (C:1.0642, R:0.0099, T:30.9551(w:0.050)⚠️)
Batch  75/537: Loss=2.5249 (C:0.9780, R:0.0100, T:30.9189(w:0.050)⚠️)
Batch 100/537: Loss=2.5305 (C:0.9871, R:0.0100, T:30.8477(w:0.050)⚠️)
Batch 125/537: Loss=2.6145 (C:1.0477, R:0.0100, T:31.3164(w:0.050)⚠️)
Batch 150/537: Loss=2.5267 (C:0.9704, R:0.0100, T:31.1069(w:0.050)⚠️)
Batch 175/537: Loss=2.5307 (C:0.9688, R:0.0100, T:31.2172(w:0.050)⚠️)
Batch 200/537: Loss=2.6221 (C:1.0466, R:0.0099, T:31.4891(w:0.050)⚠️)
Batch 225/537: Loss=2.5325 (C:0.9528, R:0.0100, T:31.5745(w:0.050)⚠️)
Batch 250/537: Loss=2.6433 (C:1.0790, R:0.0100, T:31.2653(w:0.050)⚠️)
Batch 275/537: Loss=2.5640 (C:1.0076, R:0.0099, T:31.1087(w:0.050)⚠️)
Batch 300/537: Loss=2.5924 (C:1.0192, R:0.0099, T:31.4457(w:0.050)⚠️)
Batch 325/537: Loss=2.6210 (C:1.0334, R:0.0100, T:31.7313(w:0.050)⚠️)
Batch 350/537: Loss=2.6028 (C:1.0721, R:0.0100, T:30.5948(w:0.050)⚠️)
Batch 375/537: Loss=2.6005 (C:1.0595, R:0.0099, T:30.8008(w:0.050)⚠️)
Batch 400/537: Loss=2.5718 (C:1.0181, R:0.0099, T:31.0546(w:0.050)⚠️)
Batch 425/537: Loss=2.5970 (C:0.9877, R:0.0100, T:32.1666(w:0.050)⚠️)
Batch 450/537: Loss=2.5523 (C:0.9371, R:0.0100, T:32.2842(w:0.050)⚠️)
Batch 475/537: Loss=2.6066 (C:1.0775, R:0.0099, T:30.5608(w:0.050)⚠️)
Batch 500/537: Loss=2.5809 (C:1.0137, R:0.0099, T:31.3240(w:0.050)⚠️)
Batch 525/537: Loss=2.5335 (C:0.9930, R:0.0100, T:30.7897(w:0.050)⚠️)

📊 EPOCH 56 TRAINING SUMMARY:
  Total Loss: 2.5705
  Contrastive: 1.0081
  Reconstruction: 0.0100
  Topological: 31.2277 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0958
  Contrastive: 0.7273
  Reconstruction: 0.0100
  Topological: 27.3496 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 56/100 COMPLETE (88.1s)
Train Loss: 2.5705 (C:1.0081, R:0.0100, T:31.2277)
Val Loss:   2.0958 (C:0.7273, R:0.0100, T:27.3496)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 57 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5173 (C:0.9874, R:0.0099, T:30.5778(w:0.050)⚠️)
Batch  25/537: Loss=2.5777 (C:0.9961, R:0.0099, T:31.6128(w:0.050)⚠️)
Batch  50/537: Loss=2.5539 (C:0.9884, R:0.0100, T:31.2893(w:0.050)⚠️)
Batch  75/537: Loss=2.6031 (C:1.0309, R:0.0099, T:31.4223(w:0.050)⚠️)
Batch 100/537: Loss=2.5317 (C:0.9932, R:0.0099, T:30.7495(w:0.050)⚠️)
Batch 125/537: Loss=2.5901 (C:0.9938, R:0.0100, T:31.9063(w:0.050)⚠️)
Batch 150/537: Loss=2.5675 (C:1.0138, R:0.0099, T:31.0551(w:0.050)⚠️)
Batch 175/537: Loss=2.5505 (C:0.9657, R:0.0100, T:31.6780(w:0.050)⚠️)
Batch 200/537: Loss=2.5597 (C:0.9824, R:0.0099, T:31.5273(w:0.050)⚠️)
Batch 225/537: Loss=2.5653 (C:1.0132, R:0.0100, T:31.0225(w:0.050)⚠️)
Batch 250/537: Loss=2.5307 (C:0.9480, R:0.0099, T:31.6343(w:0.050)⚠️)
Batch 275/537: Loss=2.5651 (C:1.0412, R:0.0099, T:30.4588(w:0.050)⚠️)
Batch 300/537: Loss=2.5815 (C:0.9959, R:0.0100, T:31.6911(w:0.050)⚠️)
Batch 325/537: Loss=2.6046 (C:1.0296, R:0.0099, T:31.4795(w:0.050)⚠️)
Batch 350/537: Loss=2.5364 (C:1.0021, R:0.0099, T:30.6662(w:0.050)⚠️)
Batch 375/537: Loss=2.6099 (C:1.0490, R:0.0100, T:31.1983(w:0.050)⚠️)
Batch 400/537: Loss=2.6029 (C:1.0337, R:0.0100, T:31.3634(w:0.050)⚠️)
Batch 425/537: Loss=2.5492 (C:0.9933, R:0.0100, T:31.0998(w:0.050)⚠️)
Batch 450/537: Loss=2.5961 (C:1.0707, R:0.0099, T:30.4883(w:0.050)⚠️)
Batch 475/537: Loss=2.5995 (C:1.0197, R:0.0100, T:31.5748(w:0.050)⚠️)
Batch 500/537: Loss=2.5802 (C:1.0722, R:0.0099, T:30.1402(w:0.050)⚠️)
Batch 525/537: Loss=2.5655 (C:1.0035, R:0.0099, T:31.2199(w:0.050)⚠️)

📊 EPOCH 57 TRAINING SUMMARY:
  Total Loss: 2.5668
  Contrastive: 1.0025
  Reconstruction: 0.0100
  Topological: 31.2657 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1111
  Contrastive: 0.7089
  Reconstruction: 0.0100
  Topological: 28.0226 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 57/100 COMPLETE (87.6s)
Train Loss: 2.5668 (C:1.0025, R:0.0100, T:31.2657)
Val Loss:   2.1111 (C:0.7089, R:0.0100, T:28.0226)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 58 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6488 (C:1.0668, R:0.0099, T:31.6207(w:0.050)⚠️)
Batch  25/537: Loss=2.6001 (C:1.0202, R:0.0100, T:31.5787(w:0.050)⚠️)
Batch  50/537: Loss=2.5498 (C:0.9513, R:0.0100, T:31.9512(w:0.050)⚠️)
Batch  75/537: Loss=2.5536 (C:1.0182, R:0.0099, T:30.6876(w:0.050)⚠️)
Batch 100/537: Loss=2.4933 (C:0.9609, R:0.0100, T:30.6279(w:0.050)⚠️)
Batch 125/537: Loss=2.5252 (C:0.9644, R:0.0099, T:31.1948(w:0.050)⚠️)
Batch 150/537: Loss=2.5395 (C:0.9682, R:0.0099, T:31.4071(w:0.050)⚠️)
Batch 175/537: Loss=2.5599 (C:0.9589, R:0.0100, T:32.0004(w:0.050)⚠️)
Batch 200/537: Loss=2.5565 (C:0.9845, R:0.0100, T:31.4192(w:0.050)⚠️)
Batch 225/537: Loss=2.5713 (C:0.9613, R:0.0100, T:32.1792(w:0.050)⚠️)
Batch 250/537: Loss=2.5611 (C:1.0411, R:0.0100, T:30.3802(w:0.050)⚠️)
Batch 275/537: Loss=2.5258 (C:0.9612, R:0.0099, T:31.2735(w:0.050)⚠️)
Batch 300/537: Loss=2.4950 (C:0.9288, R:0.0100, T:31.3054(w:0.050)⚠️)
Batch 325/537: Loss=2.5854 (C:1.0243, R:0.0100, T:31.2026(w:0.050)⚠️)
Batch 350/537: Loss=2.5961 (C:1.0408, R:0.0100, T:31.0852(w:0.050)⚠️)
Batch 375/537: Loss=2.5569 (C:0.9570, R:0.0099, T:31.9788(w:0.050)⚠️)
Batch 400/537: Loss=2.6276 (C:1.0807, R:0.0099, T:30.9188(w:0.050)⚠️)
Batch 425/537: Loss=2.5743 (C:1.0185, R:0.0099, T:31.0959(w:0.050)⚠️)
Batch 450/537: Loss=2.5185 (C:0.9808, R:0.0099, T:30.7332(w:0.050)⚠️)
Batch 475/537: Loss=2.5166 (C:0.9495, R:0.0100, T:31.3216(w:0.050)⚠️)
Batch 500/537: Loss=2.5440 (C:1.0100, R:0.0099, T:30.6602(w:0.050)⚠️)
Batch 525/537: Loss=2.5781 (C:1.0440, R:0.0099, T:30.6620(w:0.050)⚠️)

📊 EPOCH 58 TRAINING SUMMARY:
  Total Loss: 2.5616
  Contrastive: 0.9985
  Reconstruction: 0.0100
  Topological: 31.2419 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1184
  Contrastive: 0.7247
  Reconstruction: 0.0100
  Topological: 27.8546 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 58/100 COMPLETE (84.8s)
Train Loss: 2.5616 (C:0.9985, R:0.0100, T:31.2419)
Val Loss:   2.1184 (C:0.7247, R:0.0100, T:27.8546)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 59 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5673 (C:1.0652, R:0.0099, T:30.0214(w:0.050)⚠️)
Batch  25/537: Loss=2.5494 (C:0.9557, R:0.0100, T:31.8559(w:0.050)⚠️)
Batch  50/537: Loss=2.5684 (C:0.9841, R:0.0100, T:31.6663(w:0.050)⚠️)
Batch  75/537: Loss=2.4844 (C:0.9022, R:0.0099, T:31.6239(w:0.050)⚠️)
Batch 100/537: Loss=2.5144 (C:0.9118, R:0.0100, T:32.0320(w:0.050)⚠️)
Batch 125/537: Loss=2.5080 (C:0.9277, R:0.0099, T:31.5859(w:0.050)⚠️)
Batch 150/537: Loss=2.5955 (C:0.9466, R:0.0100, T:32.9590(w:0.050)⚠️)
Batch 175/537: Loss=2.5811 (C:1.0037, R:0.0099, T:31.5282(w:0.050)⚠️)
Batch 200/537: Loss=2.7655 (C:1.0104, R:0.0099, T:35.0831(w:0.050)⚠️)
Batch 225/537: Loss=2.5865 (C:0.9806, R:0.0100, T:32.0971(w:0.050)⚠️)
Batch 250/537: Loss=2.6205 (C:1.0637, R:0.0100, T:31.1146(w:0.050)⚠️)
Batch 275/537: Loss=2.5841 (C:1.0075, R:0.0099, T:31.5114(w:0.050)⚠️)
Batch 300/537: Loss=2.6167 (C:1.0040, R:0.0100, T:32.2345(w:0.050)⚠️)
Batch 325/537: Loss=2.5996 (C:1.0252, R:0.0099, T:31.4681(w:0.050)⚠️)
Batch 350/537: Loss=2.5873 (C:1.0262, R:0.0100, T:31.2002(w:0.050)⚠️)
Batch 375/537: Loss=2.6064 (C:1.0472, R:0.0099, T:31.1629(w:0.050)⚠️)
Batch 400/537: Loss=2.5850 (C:1.0131, R:0.0099, T:31.4178(w:0.050)⚠️)
Batch 425/537: Loss=2.5407 (C:0.9449, R:0.0100, T:31.8963(w:0.050)⚠️)
Batch 450/537: Loss=2.5195 (C:0.9291, R:0.0100, T:31.7877(w:0.050)⚠️)
Batch 475/537: Loss=2.5645 (C:0.9992, R:0.0099, T:31.2867(w:0.050)⚠️)
Batch 500/537: Loss=2.4790 (C:0.9522, R:0.0099, T:30.5160(w:0.050)⚠️)
Batch 525/537: Loss=2.6139 (C:1.0716, R:0.0100, T:30.8259(w:0.050)⚠️)

📊 EPOCH 59 TRAINING SUMMARY:
  Total Loss: 2.5582
  Contrastive: 0.9848
  Reconstruction: 0.0100
  Topological: 31.4478 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.1136
  Contrastive: 0.7233
  Reconstruction: 0.0100
  Topological: 27.7855 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 59/100 COMPLETE (85.2s)
Train Loss: 2.5582 (C:0.9848, R:0.0100, T:31.4478)
Val Loss:   2.1136 (C:0.7233, R:0.0100, T:27.7855)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 60 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.6066 (C:1.0092, R:0.0100, T:31.9276(w:0.050)⚠️)
Batch  25/537: Loss=2.4975 (C:0.9118, R:0.0099, T:31.6933(w:0.050)⚠️)
Batch  50/537: Loss=2.4554 (C:0.9046, R:0.0099, T:30.9970(w:0.050)⚠️)
Batch  75/537: Loss=2.4815 (C:0.9167, R:0.0100, T:31.2763(w:0.050)⚠️)
Batch 100/537: Loss=2.5193 (C:0.9532, R:0.0099, T:31.3033(w:0.050)⚠️)
Batch 125/537: Loss=2.5503 (C:0.9261, R:0.0099, T:32.4654(w:0.050)⚠️)
Batch 150/537: Loss=2.4983 (C:0.8841, R:0.0100, T:32.2631(w:0.050)⚠️)
Batch 175/537: Loss=2.5302 (C:0.9597, R:0.0100, T:31.3894(w:0.050)⚠️)
Batch 200/537: Loss=2.5587 (C:0.9685, R:0.0100, T:31.7847(w:0.050)⚠️)
Batch 225/537: Loss=2.5870 (C:1.0754, R:0.0099, T:30.2120(w:0.050)⚠️)
Batch 250/537: Loss=2.6091 (C:1.0138, R:0.0100, T:31.8859(w:0.050)⚠️)
Batch 275/537: Loss=2.5746 (C:0.9492, R:0.0100, T:32.4880(w:0.050)⚠️)
Batch 300/537: Loss=2.5108 (C:0.9309, R:0.0099, T:31.5776(w:0.050)⚠️)
Batch 325/537: Loss=2.5623 (C:0.9821, R:0.0099, T:31.5855(w:0.050)⚠️)
Batch 350/537: Loss=2.6025 (C:1.0158, R:0.0099, T:31.7158(w:0.050)⚠️)
Batch 375/537: Loss=2.5864 (C:0.9943, R:0.0100, T:31.8224(w:0.050)⚠️)
Batch 400/537: Loss=2.5559 (C:1.0026, R:0.0100, T:31.0460(w:0.050)⚠️)
Batch 425/537: Loss=2.5491 (C:0.9539, R:0.0100, T:31.8846(w:0.050)⚠️)
Batch 450/537: Loss=2.5958 (C:1.0490, R:0.0100, T:30.9154(w:0.050)⚠️)
Batch 475/537: Loss=2.5594 (C:0.9659, R:0.0100, T:31.8496(w:0.050)⚠️)
Batch 500/537: Loss=2.5745 (C:1.0305, R:0.0099, T:30.8615(w:0.050)⚠️)
Batch 525/537: Loss=2.5233 (C:0.9631, R:0.0100, T:31.1832(w:0.050)⚠️)

📊 EPOCH 60 TRAINING SUMMARY:
  Total Loss: 2.5526
  Contrastive: 0.9729
  Reconstruction: 0.0100
  Topological: 31.5732 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0620
  Contrastive: 0.6851
  Reconstruction: 0.0100
  Topological: 27.5191 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 60/100 COMPLETE (85.0s)
Train Loss: 2.5526 (C:0.9729, R:0.0100, T:31.5732)
Val Loss:   2.0620 (C:0.6851, R:0.0100, T:27.5191)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 61 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.4892 (C:0.8933, R:0.0099, T:31.8979(w:0.050)⚠️)
Batch  25/537: Loss=2.5753 (C:0.9739, R:0.0100, T:32.0090(w:0.050)⚠️)
Batch  50/537: Loss=2.5164 (C:0.9749, R:0.0099, T:30.8094(w:0.050)⚠️)
Batch  75/537: Loss=2.5538 (C:0.9412, R:0.0100, T:32.2302(w:0.050)⚠️)
Batch 100/537: Loss=2.4428 (C:0.8758, R:0.0099, T:31.3204(w:0.050)⚠️)
Batch 125/537: Loss=2.5470 (C:0.9823, R:0.0099, T:31.2740(w:0.050)⚠️)
Batch 150/537: Loss=2.5315 (C:0.9778, R:0.0100, T:31.0538(w:0.050)⚠️)
Batch 175/537: Loss=2.5885 (C:0.9879, R:0.0099, T:31.9936(w:0.050)⚠️)
Batch 200/537: Loss=2.4909 (C:0.8940, R:0.0100, T:31.9168(w:0.050)⚠️)
Batch 225/537: Loss=2.5405 (C:0.9671, R:0.0100, T:31.4488(w:0.050)⚠️)
Batch 250/537: Loss=2.6026 (C:0.9941, R:0.0099, T:32.1515(w:0.050)⚠️)
Batch 275/537: Loss=2.5811 (C:0.9775, R:0.0100, T:32.0522(w:0.050)⚠️)
Batch 300/537: Loss=2.5240 (C:0.9352, R:0.0099, T:31.7566(w:0.050)⚠️)
Batch 325/537: Loss=2.5239 (C:0.9659, R:0.0100, T:31.1418(w:0.050)⚠️)
Batch 350/537: Loss=2.5384 (C:0.9882, R:0.0100, T:30.9843(w:0.050)⚠️)
Batch 375/537: Loss=2.5686 (C:0.9758, R:0.0099, T:31.8377(w:0.050)⚠️)
Batch 400/537: Loss=2.4985 (C:0.9415, R:0.0099, T:31.1209(w:0.050)⚠️)
Batch 425/537: Loss=2.5643 (C:0.9914, R:0.0099, T:31.4380(w:0.050)⚠️)
Batch 450/537: Loss=2.5451 (C:0.9803, R:0.0100, T:31.2755(w:0.050)⚠️)
Batch 475/537: Loss=2.6332 (C:1.0483, R:0.0100, T:31.6783(w:0.050)⚠️)
Batch 500/537: Loss=2.5546 (C:0.9309, R:0.0100, T:32.4547(w:0.050)⚠️)
Batch 525/537: Loss=2.5404 (C:0.9252, R:0.0100, T:32.2838(w:0.050)⚠️)

📊 EPOCH 61 TRAINING SUMMARY:
  Total Loss: 2.5444
  Contrastive: 0.9655
  Reconstruction: 0.0100
  Topological: 31.5575 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0686
  Contrastive: 0.6849
  Reconstruction: 0.0100
  Topological: 27.6538 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 61/100 COMPLETE (85.0s)
Train Loss: 2.5444 (C:0.9655, R:0.0100, T:31.5575)
Val Loss:   2.0686 (C:0.6849, R:0.0100, T:27.6538)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 62 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5373 (C:0.9753, R:0.0099, T:31.2202(w:0.050)⚠️)
Batch  25/537: Loss=2.5148 (C:0.9371, R:0.0099, T:31.5345(w:0.050)⚠️)
Batch  50/537: Loss=2.5661 (C:0.9525, R:0.0100, T:32.2525(w:0.050)⚠️)
Batch  75/537: Loss=2.5873 (C:0.9774, R:0.0100, T:32.1778(w:0.050)⚠️)
Batch 100/537: Loss=2.4591 (C:0.8209, R:0.0099, T:32.7456(w:0.050)⚠️)
Batch 125/537: Loss=2.5874 (C:0.9954, R:0.0100, T:31.8191(w:0.050)⚠️)
Batch 150/537: Loss=2.5324 (C:0.9400, R:0.0100, T:31.8287(w:0.050)⚠️)
Batch 175/537: Loss=2.5149 (C:0.9507, R:0.0099, T:31.2645(w:0.050)⚠️)
Batch 200/537: Loss=2.5690 (C:0.9798, R:0.0100, T:31.7654(w:0.050)⚠️)
Batch 225/537: Loss=2.5812 (C:0.9947, R:0.0100, T:31.7113(w:0.050)⚠️)
Batch 250/537: Loss=2.5437 (C:0.9708, R:0.0100, T:31.4384(w:0.050)⚠️)
Batch 275/537: Loss=2.4951 (C:0.9161, R:0.0100, T:31.5606(w:0.050)⚠️)
Batch 300/537: Loss=2.5054 (C:0.9225, R:0.0100, T:31.6377(w:0.050)⚠️)
Batch 325/537: Loss=2.5543 (C:0.9842, R:0.0100, T:31.3812(w:0.050)⚠️)
Batch 350/537: Loss=2.6037 (C:1.0377, R:0.0100, T:31.3012(w:0.050)⚠️)
Batch 375/537: Loss=2.5342 (C:0.9628, R:0.0100, T:31.4084(w:0.050)⚠️)
Batch 400/537: Loss=2.5150 (C:1.0012, R:0.0099, T:30.2554(w:0.050)⚠️)
Batch 425/537: Loss=2.5869 (C:0.9947, R:0.0100, T:31.8242(w:0.050)⚠️)
Batch 450/537: Loss=2.5618 (C:1.0055, R:0.0100, T:31.1070(w:0.050)⚠️)
Batch 475/537: Loss=2.5180 (C:0.9422, R:0.0099, T:31.4959(w:0.050)⚠️)
Batch 500/537: Loss=2.5655 (C:0.9941, R:0.0100, T:31.4082(w:0.050)⚠️)
Batch 525/537: Loss=2.6263 (C:1.0207, R:0.0100, T:32.0928(w:0.050)⚠️)

📊 EPOCH 62 TRAINING SUMMARY:
  Total Loss: 2.5409
  Contrastive: 0.9576
  Reconstruction: 0.0100
  Topological: 31.6470 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0657
  Contrastive: 0.6948
  Reconstruction: 0.0100
  Topological: 27.3975 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 62/100 COMPLETE (76.4s)
Train Loss: 2.5409 (C:0.9576, R:0.0100, T:31.6470)
Val Loss:   2.0657 (C:0.6948, R:0.0100, T:27.3975)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 63 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5072 (C:0.9362, R:0.0100, T:31.3985(w:0.050)⚠️)
Batch  25/537: Loss=2.4992 (C:0.9571, R:0.0100, T:30.8215(w:0.050)⚠️)
Batch  50/537: Loss=2.4873 (C:0.9083, R:0.0099, T:31.5608(w:0.050)⚠️)
Batch  75/537: Loss=2.5470 (C:0.9439, R:0.0100, T:32.0417(w:0.050)⚠️)
Batch 100/537: Loss=2.5112 (C:0.9443, R:0.0100, T:31.3182(w:0.050)⚠️)
Batch 125/537: Loss=2.5615 (C:0.9890, R:0.0100, T:31.4300(w:0.050)⚠️)
Batch 150/537: Loss=2.5739 (C:0.9994, R:0.0100, T:31.4695(w:0.050)⚠️)
Batch 175/537: Loss=2.5029 (C:0.9121, R:0.0099, T:31.7952(w:0.050)⚠️)
Batch 200/537: Loss=2.5300 (C:0.9433, R:0.0100, T:31.7141(w:0.050)⚠️)
Batch 225/537: Loss=2.5523 (C:0.9625, R:0.0099, T:31.7768(w:0.050)⚠️)
Batch 250/537: Loss=2.5471 (C:0.9273, R:0.0100, T:32.3772(w:0.050)⚠️)
Batch 275/537: Loss=2.5912 (C:1.0250, R:0.0099, T:31.3041(w:0.050)⚠️)
Batch 300/537: Loss=2.5076 (C:0.9168, R:0.0100, T:31.7952(w:0.050)⚠️)
Batch 325/537: Loss=2.4978 (C:0.9268, R:0.0100, T:31.4006(w:0.050)⚠️)
Batch 350/537: Loss=2.5745 (C:0.9656, R:0.0100, T:32.1567(w:0.050)⚠️)
Batch 375/537: Loss=2.5328 (C:1.0028, R:0.0100, T:30.5798(w:0.050)⚠️)
Batch 400/537: Loss=2.5682 (C:0.9767, R:0.0100, T:31.8099(w:0.050)⚠️)
Batch 425/537: Loss=2.5340 (C:0.9124, R:0.0099, T:32.4124(w:0.050)⚠️)
Batch 450/537: Loss=2.5357 (C:0.9479, R:0.0100, T:31.7373(w:0.050)⚠️)
Batch 475/537: Loss=2.5388 (C:0.9700, R:0.0099, T:31.3542(w:0.050)⚠️)
Batch 500/537: Loss=2.5309 (C:0.9616, R:0.0099, T:31.3670(w:0.050)⚠️)
Batch 525/537: Loss=2.5385 (C:0.9642, R:0.0100, T:31.4652(w:0.050)⚠️)

📊 EPOCH 63 TRAINING SUMMARY:
  Total Loss: 2.5361
  Contrastive: 0.9525
  Reconstruction: 0.0100
  Topological: 31.6524 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0611
  Contrastive: 0.6941
  Reconstruction: 0.0100
  Topological: 27.3200 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 63/100 COMPLETE (71.6s)
Train Loss: 2.5361 (C:0.9525, R:0.0100, T:31.6524)
Val Loss:   2.0611 (C:0.6941, R:0.0100, T:27.3200)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 64 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5072 (C:0.9436, R:0.0100, T:31.2525(w:0.050)⚠️)
Batch  25/537: Loss=2.5751 (C:0.9616, R:0.0100, T:32.2495(w:0.050)⚠️)
Batch  50/537: Loss=2.5309 (C:0.9048, R:0.0100, T:32.5023(w:0.050)⚠️)
Batch  75/537: Loss=2.5224 (C:0.9411, R:0.0099, T:31.6061(w:0.050)⚠️)
Batch 100/537: Loss=2.4562 (C:0.8644, R:0.0100, T:31.8161(w:0.050)⚠️)
Batch 125/537: Loss=2.5408 (C:0.9912, R:0.0099, T:30.9732(w:0.050)⚠️)
Batch 150/537: Loss=2.5170 (C:0.9305, R:0.0099, T:31.7100(w:0.050)⚠️)
Batch 175/537: Loss=2.5906 (C:0.9943, R:0.0099, T:31.9070(w:0.050)⚠️)
Batch 200/537: Loss=2.4737 (C:0.8843, R:0.0099, T:31.7682(w:0.050)⚠️)
Batch 225/537: Loss=2.5321 (C:0.9689, R:0.0099, T:31.2445(w:0.050)⚠️)
Batch 250/537: Loss=2.5161 (C:0.9375, R:0.0099, T:31.5524(w:0.050)⚠️)
Batch 275/537: Loss=2.5100 (C:0.9544, R:0.0100, T:31.0909(w:0.050)⚠️)
Batch 300/537: Loss=2.5264 (C:0.9753, R:0.0099, T:31.0025(w:0.050)⚠️)
Batch 325/537: Loss=2.5063 (C:0.9039, R:0.0099, T:32.0293(w:0.050)⚠️)
Batch 350/537: Loss=2.5645 (C:0.9450, R:0.0100, T:32.3708(w:0.050)⚠️)
Batch 375/537: Loss=2.5382 (C:0.9650, R:0.0100, T:31.4425(w:0.050)⚠️)
Batch 400/537: Loss=2.5037 (C:0.8623, R:0.0100, T:32.8095(w:0.050)⚠️)
Batch 425/537: Loss=2.5226 (C:0.9159, R:0.0100, T:32.1143(w:0.050)⚠️)
Batch 450/537: Loss=2.5794 (C:0.9779, R:0.0100, T:32.0110(w:0.050)⚠️)
Batch 475/537: Loss=2.5351 (C:0.9262, R:0.0100, T:32.1579(w:0.050)⚠️)
Batch 500/537: Loss=2.5239 (C:0.9279, R:0.0099, T:31.9003(w:0.050)⚠️)
Batch 525/537: Loss=2.5961 (C:1.0146, R:0.0099, T:31.6104(w:0.050)⚠️)

📊 EPOCH 64 TRAINING SUMMARY:
  Total Loss: 2.5312
  Contrastive: 0.9452
  Reconstruction: 0.0100
  Topological: 31.6995 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0666
  Contrastive: 0.6809
  Reconstruction: 0.0100
  Topological: 27.6944 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 64/100 COMPLETE (77.1s)
Train Loss: 2.5312 (C:0.9452, R:0.0100, T:31.6995)
Val Loss:   2.0666 (C:0.6809, R:0.0100, T:27.6944)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 65 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5830 (C:0.9754, R:0.0100, T:32.1334(w:0.050)⚠️)
Batch  25/537: Loss=2.5104 (C:0.9286, R:0.0099, T:31.6161(w:0.050)⚠️)
Batch  50/537: Loss=2.5367 (C:0.9214, R:0.0100, T:32.2854(w:0.050)⚠️)
Batch  75/537: Loss=2.5101 (C:0.9478, R:0.0099, T:31.2261(w:0.050)⚠️)
Batch 100/537: Loss=2.5070 (C:0.9210, R:0.0100, T:31.7001(w:0.050)⚠️)
Batch 125/537: Loss=2.4521 (C:0.8762, R:0.0100, T:31.4996(w:0.050)⚠️)
Batch 150/537: Loss=2.5409 (C:0.9442, R:0.0100, T:31.9143(w:0.050)⚠️)
Batch 175/537: Loss=2.4946 (C:0.8466, R:0.0100, T:32.9416(w:0.050)⚠️)
Batch 200/537: Loss=2.4412 (C:0.8552, R:0.0100, T:31.7000(w:0.050)⚠️)
Batch 225/537: Loss=2.5677 (C:0.9585, R:0.0100, T:32.1639(w:0.050)⚠️)
Batch 250/537: Loss=2.5248 (C:0.9199, R:0.0100, T:32.0790(w:0.050)⚠️)
Batch 275/537: Loss=2.5251 (C:0.9269, R:0.0100, T:31.9444(w:0.050)⚠️)
Batch 300/537: Loss=2.5136 (C:0.9320, R:0.0099, T:31.6130(w:0.050)⚠️)
Batch 325/537: Loss=2.5800 (C:1.0076, R:0.0100, T:31.4278(w:0.050)⚠️)
Batch 350/537: Loss=2.4831 (C:0.8847, R:0.0100, T:31.9469(w:0.050)⚠️)
Batch 375/537: Loss=2.6107 (C:1.0141, R:0.0100, T:31.9132(w:0.050)⚠️)
Batch 400/537: Loss=2.5183 (C:0.9013, R:0.0099, T:32.3204(w:0.050)⚠️)
Batch 425/537: Loss=2.5093 (C:0.9290, R:0.0100, T:31.5850(w:0.050)⚠️)
Batch 450/537: Loss=2.5355 (C:0.9505, R:0.0100, T:31.6806(w:0.050)⚠️)
Batch 475/537: Loss=2.4849 (C:0.9364, R:0.0099, T:30.9515(w:0.050)⚠️)
Batch 500/537: Loss=2.5247 (C:0.9581, R:0.0099, T:31.3128(w:0.050)⚠️)
Batch 525/537: Loss=2.5903 (C:1.0171, R:0.0099, T:31.4436(w:0.050)⚠️)

📊 EPOCH 65 TRAINING SUMMARY:
  Total Loss: 2.5245
  Contrastive: 0.9384
  Reconstruction: 0.0100
  Topological: 31.7025 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0927
  Contrastive: 0.6963
  Reconstruction: 0.0100
  Topological: 27.9084 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 65/100 COMPLETE (79.6s)
Train Loss: 2.5245 (C:0.9384, R:0.0100, T:31.7025)
Val Loss:   2.0927 (C:0.6963, R:0.0100, T:27.9084)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 66 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.5473 (C:0.9769, R:0.0100, T:31.3882(w:0.050)⚠️)
Batch  25/537: Loss=2.5005 (C:0.9083, R:0.0099, T:31.8224(w:0.050)⚠️)
Batch  50/537: Loss=2.5240 (C:0.8970, R:0.0099, T:32.5200(w:0.050)⚠️)
Batch  75/537: Loss=2.6086 (C:1.0178, R:0.0099, T:31.7965(w:0.050)⚠️)
Batch 100/537: Loss=2.5252 (C:0.9185, R:0.0100, T:32.1139(w:0.050)⚠️)
Batch 125/537: Loss=2.5024 (C:0.9305, R:0.0099, T:31.4191(w:0.050)⚠️)
Batch 150/537: Loss=2.5135 (C:0.8766, R:0.0099, T:32.7181(w:0.050)⚠️)
Batch 175/537: Loss=2.5511 (C:0.9425, R:0.0099, T:32.1527(w:0.050)⚠️)
Batch 200/537: Loss=2.4867 (C:0.9101, R:0.0099, T:31.5118(w:0.050)⚠️)
Batch 225/537: Loss=2.5117 (C:0.9146, R:0.0099, T:31.9227(w:0.050)⚠️)
Batch 250/537: Loss=2.5745 (C:0.9689, R:0.0099, T:32.0904(w:0.050)⚠️)
Batch 275/537: Loss=2.4584 (C:0.8658, R:0.0100, T:31.8313(w:0.050)⚠️)
Batch 300/537: Loss=2.5767 (C:1.0402, R:0.0099, T:30.7102(w:0.050)⚠️)
Batch 325/537: Loss=2.5545 (C:0.9717, R:0.0100, T:31.6362(w:0.050)⚠️)
Batch 350/537: Loss=2.5247 (C:0.9434, R:0.0099, T:31.6068(w:0.050)⚠️)
Batch 375/537: Loss=2.5252 (C:0.9109, R:0.0100, T:32.2679(w:0.050)⚠️)
Batch 400/537: Loss=2.5570 (C:0.9270, R:0.0100, T:32.5804(w:0.050)⚠️)
Batch 425/537: Loss=2.4559 (C:0.8791, R:0.0100, T:31.5161(w:0.050)⚠️)
Batch 450/537: Loss=2.5394 (C:0.9720, R:0.0099, T:31.3293(w:0.050)⚠️)
Batch 475/537: Loss=2.5434 (C:0.9822, R:0.0099, T:31.2043(w:0.050)⚠️)
Batch 500/537: Loss=2.4928 (C:0.9422, R:0.0099, T:30.9914(w:0.050)⚠️)
Batch 525/537: Loss=2.5193 (C:0.9793, R:0.0099, T:30.7793(w:0.050)⚠️)

📊 EPOCH 66 TRAINING SUMMARY:
  Total Loss: 2.5226
  Contrastive: 0.9323
  Reconstruction: 0.0100
  Topological: 31.7859 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0581
  Contrastive: 0.6798
  Reconstruction: 0.0100
  Topological: 27.5475 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 66/100 COMPLETE (82.7s)
Train Loss: 2.5226 (C:0.9323, R:0.0100, T:31.7859)
Val Loss:   2.0581 (C:0.6798, R:0.0100, T:27.5475)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 67 | Batches: 537 | Topological Weight: 0.0500
🧠 Full topological learning active
============================================================
Batch   0/537: Loss=2.4967 (C:0.9073, R:0.0099, T:31.7681(w:0.050)⚠️)
Batch  25/537: Loss=2.5325 (C:0.9089, R:0.0100, T:32.4521(w:0.050)⚠️)
Batch  50/537: Loss=2.4922 (C:0.8885, R:0.0099, T:32.0550(w:0.050)⚠️)
Batch  75/537: Loss=2.5616 (C:0.9440, R:0.0099, T:32.3335(w:0.050)⚠️)
Batch 100/537: Loss=2.5000 (C:0.9189, R:0.0099, T:31.6026(w:0.050)⚠️)
Batch 125/537: Loss=2.4819 (C:0.9008, R:0.0100, T:31.6017(w:0.050)⚠️)
Batch 150/537: Loss=2.5202 (C:0.9051, R:0.0099, T:32.2818(w:0.050)⚠️)
Batch 175/537: Loss=2.4744 (C:0.8940, R:0.0099, T:31.5877(w:0.050)⚠️)
Batch 200/537: Loss=2.4638 (C:0.9115, R:0.0100, T:31.0260(w:0.050)⚠️)
Batch 225/537: Loss=2.5052 (C:0.9848, R:0.0099, T:30.3884(w:0.050)⚠️)
Batch 250/537: Loss=2.5720 (C:0.9347, R:0.0100, T:32.7247(w:0.050)⚠️)
Batch 275/537: Loss=2.4941 (C:0.9507, R:0.0099, T:30.8484(w:0.050)⚠️)
Batch 300/537: Loss=2.5270 (C:0.8359, R:0.0100, T:33.8027(w:0.050)⚠️)
Batch 325/537: Loss=2.5091 (C:0.8973, R:0.0099, T:32.2165(w:0.050)⚠️)
Batch 350/537: Loss=2.5176 (C:0.9338, R:0.0099, T:31.6559(w:0.050)⚠️)
Batch 375/537: Loss=2.5594 (C:0.9656, R:0.0099, T:31.8561(w:0.050)⚠️)
Batch 400/537: Loss=2.5250 (C:0.9395, R:0.0099, T:31.6902(w:0.050)⚠️)
Batch 425/537: Loss=2.5288 (C:0.9781, R:0.0100, T:30.9926(w:0.050)⚠️)
Batch 450/537: Loss=2.4941 (C:0.9299, R:0.0100, T:31.2645(w:0.050)⚠️)
Batch 475/537: Loss=2.4753 (C:0.8779, R:0.0100, T:31.9274(w:0.050)⚠️)
Batch 500/537: Loss=2.5417 (C:0.9758, R:0.0100, T:31.2972(w:0.050)⚠️)
Batch 525/537: Loss=2.5756 (C:0.9822, R:0.0099, T:31.8493(w:0.050)⚠️)

📊 EPOCH 67 TRAINING SUMMARY:
  Total Loss: 2.5216
  Contrastive: 0.9277
  Reconstruction: 0.0100
  Topological: 31.8569 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.0541
  Contrastive: 0.6686
  Reconstruction: 0.0100
  Topological: 27.6904 (weight: 0.050)
  Batches with topology: 537/537 (100.0%)

🎯 EPOCH 67/100 COMPLETE (85.6s)
Train Loss: 2.5216 (C:0.9277, R:0.0100, T:31.8569)
Val Loss:   2.0541 (C:0.6686, R:0.0100, T:27.6904)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

🛑 Early stopping triggered after 67 epochs
Best model was at epoch 17 with Val Loss: 1.6761

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 9
Epochs with topology: 59/67
Max consecutive topology epochs: 59
Best topological loss: 17.0738
Final topological loss: 31.8569
✅ SUCCESS: Topological learning achieved!
🚀 EXCELLENT: Very consistent topological learning (>80%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_162459/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/539 batches
  Processed 51/539 batches
  Processed 101/539 batches
  Processed 151/539 batches
  Processed 201/539 batches
  Processed 251/539 batches
  Processed 301/539 batches
  Processed 351/539 batches
  Processed 401/539 batches
  Processed 451/539 batches
  Processed 501/539 batches
Extracted representations: torch.Size([549367, 75])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: 0.1620
  Adjusted Rand Score: 0.3081
  Clustering Accuracy: 0.6103
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.5929
  Per-class F1: [0.6581395348837209, 0.17472030468935967, 0.7726770784035337]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009953
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 0.556 ± 0.851
  Negative distances: 1.234 ± 0.934
  Separation ratio: 2.22x
  Gap: -2.005
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.1620
  Clustering Accuracy: 0.6103
  Adjusted Rand Score: 0.3081

Classification Performance:
  Accuracy: 0.5929

Separation Quality:
  Separation Ratio: 2.22x
  Gap: -2.005
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009953
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_162459/results/evaluation_results_20250721_175036.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_162459/results/evaluation_results_20250721_175036.json

Key Results:
  Separation ratio: 2.22x
  Perfect separation: False
  Classification accuracy: 0.5929

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 67
  Epochs with topological learning: 59
  Current topological loss: 31.8569
  Current topological weight: 0.0500
  ⚠️  Topological loss is increasing (may need tuning)
🚀 EXCELLENT: Consistent topological learning achieved!
Final topological loss: 31.8569
Epochs with topology: 59/67
⚠️  Poor clustering accuracy: 0.610

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_162459/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250721_162459

Analysis completed with exit code: 0
Time: Mon 21 Jul 17:50:38 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
