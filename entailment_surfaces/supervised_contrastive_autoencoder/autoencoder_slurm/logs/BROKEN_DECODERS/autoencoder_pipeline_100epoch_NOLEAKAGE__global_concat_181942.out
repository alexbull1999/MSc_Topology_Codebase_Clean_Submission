Starting Surface Distance Metric Analysis job...
Job ID: 181942
Node: gpuvm18
Time: Sat 12 Jul 18:04:03 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 12 18:04:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   33C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-12 18:04:22.050949
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 100
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_180422
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_180422/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,876,555
Model created with 1,876,555 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,876,555
Starting training...
========================================
Starting Global Dataset Training...
============================================================

üåç Updating global dataset at epoch 1
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.087 ¬± 0.010
    Neg distances: 0.087 ¬± 0.010
    Separation ratio: 1.00x
    Gap: -0.117
    ‚ùå Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9997 (C:1.9997, R:0.0117)
Batch  25/537: Loss=1.9951 (C:1.9951, R:0.0114)
Batch  50/537: Loss=1.9809 (C:1.9809, R:0.0113)
Batch  75/537: Loss=1.9704 (C:1.9704, R:0.0112)
Batch 100/537: Loss=1.9639 (C:1.9639, R:0.0111)
Batch 125/537: Loss=1.9510 (C:1.9510, R:0.0109)
Batch 150/537: Loss=1.9526 (C:1.9526, R:0.0109)
Batch 175/537: Loss=1.9441 (C:1.9441, R:0.0108)
Batch 200/537: Loss=1.9268 (C:1.9268, R:0.0107)
Batch 225/537: Loss=1.9238 (C:1.9238, R:0.0107)
Batch 250/537: Loss=1.9221 (C:1.9221, R:0.0107)
Batch 275/537: Loss=1.9120 (C:1.9120, R:0.0106)
Batch 300/537: Loss=1.9168 (C:1.9168, R:0.0106)
Batch 325/537: Loss=1.9084 (C:1.9084, R:0.0106)
Batch 350/537: Loss=1.9026 (C:1.9026, R:0.0106)
Batch 375/537: Loss=1.8990 (C:1.8990, R:0.0105)
Batch 400/537: Loss=1.9018 (C:1.9018, R:0.0106)
Batch 425/537: Loss=1.9139 (C:1.9139, R:0.0106)
Batch 450/537: Loss=1.9021 (C:1.9021, R:0.0105)
Batch 475/537: Loss=1.8966 (C:1.8966, R:0.0106)
Batch 500/537: Loss=1.8912 (C:1.8912, R:0.0105)
Batch 525/537: Loss=1.8953 (C:1.8953, R:0.0106)

============================================================
Epoch 1/100 completed in 32.2s
Train: Loss=1.9296 (C:1.9296, R:0.0108) Ratio=1.65x
Val:   Loss=1.8899 (C:1.8899, R:0.0105) Ratio=2.13x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8899)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.8907 (C:1.8907, R:0.0105)
Batch  25/537: Loss=1.8895 (C:1.8895, R:0.0105)
Batch  50/537: Loss=1.8934 (C:1.8934, R:0.0105)
Batch  75/537: Loss=1.9090 (C:1.9090, R:0.0105)
Batch 100/537: Loss=1.8887 (C:1.8887, R:0.0105)
Batch 125/537: Loss=1.8810 (C:1.8810, R:0.0105)
Batch 150/537: Loss=1.8813 (C:1.8813, R:0.0105)
Batch 175/537: Loss=1.8942 (C:1.8942, R:0.0105)
Batch 200/537: Loss=1.8858 (C:1.8858, R:0.0105)
Batch 225/537: Loss=1.8842 (C:1.8842, R:0.0105)
Batch 250/537: Loss=1.8814 (C:1.8814, R:0.0105)
Batch 275/537: Loss=1.8721 (C:1.8721, R:0.0105)
Batch 300/537: Loss=1.8779 (C:1.8779, R:0.0105)
Batch 325/537: Loss=1.8854 (C:1.8854, R:0.0105)
Batch 350/537: Loss=1.8780 (C:1.8780, R:0.0106)
Batch 375/537: Loss=1.8830 (C:1.8830, R:0.0105)
Batch 400/537: Loss=1.8821 (C:1.8821, R:0.0105)
Batch 425/537: Loss=1.8854 (C:1.8854, R:0.0105)
Batch 450/537: Loss=1.8822 (C:1.8822, R:0.0105)
Batch 475/537: Loss=1.8856 (C:1.8856, R:0.0106)
Batch 500/537: Loss=1.8853 (C:1.8853, R:0.0105)
Batch 525/537: Loss=1.8731 (C:1.8731, R:0.0105)

============================================================
Epoch 2/100 completed in 21.7s
Train: Loss=1.8867 (C:1.8867, R:0.0105) Ratio=2.18x
Val:   Loss=1.8739 (C:1.8739, R:0.0104) Ratio=2.37x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8739)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.8705 (C:1.8705, R:0.0105)
Batch  25/537: Loss=1.8783 (C:1.8783, R:0.0105)
Batch  50/537: Loss=1.8658 (C:1.8658, R:0.0105)
Batch  75/537: Loss=1.8835 (C:1.8835, R:0.0105)
Batch 100/537: Loss=1.8737 (C:1.8737, R:0.0105)
Batch 125/537: Loss=1.8788 (C:1.8788, R:0.0105)
Batch 150/537: Loss=1.8771 (C:1.8771, R:0.0105)
Batch 175/537: Loss=1.8741 (C:1.8741, R:0.0106)
Batch 200/537: Loss=1.8653 (C:1.8653, R:0.0105)
Batch 225/537: Loss=1.8740 (C:1.8740, R:0.0106)
Batch 250/537: Loss=1.8722 (C:1.8722, R:0.0105)
Batch 275/537: Loss=1.8804 (C:1.8804, R:0.0105)
Batch 300/537: Loss=1.8715 (C:1.8715, R:0.0105)
Batch 325/537: Loss=1.8643 (C:1.8643, R:0.0105)
Batch 350/537: Loss=1.8853 (C:1.8853, R:0.0105)
Batch 375/537: Loss=1.8743 (C:1.8743, R:0.0105)
Batch 400/537: Loss=1.8701 (C:1.8701, R:0.0105)
Batch 425/537: Loss=1.8726 (C:1.8726, R:0.0105)
Batch 450/537: Loss=1.8801 (C:1.8801, R:0.0105)
Batch 475/537: Loss=1.8697 (C:1.8697, R:0.0105)
Batch 500/537: Loss=1.8730 (C:1.8730, R:0.0105)
Batch 525/537: Loss=1.8840 (C:1.8840, R:0.0105)

============================================================
Epoch 3/100 completed in 22.0s
Train: Loss=1.8733 (C:1.8733, R:0.0105) Ratio=2.37x
Val:   Loss=1.8676 (C:1.8676, R:0.0104) Ratio=2.50x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.8676)
============================================================

üåç Updating global dataset at epoch 4
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.615 ¬± 0.583
    Neg distances: 1.676 ¬± 0.916
    Separation ratio: 2.73x
    Gap: -3.732
    ‚úÖ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.2171 (C:1.2171, R:0.0105)
Batch  25/537: Loss=1.1849 (C:1.1849, R:0.0105)
Batch  50/537: Loss=1.2221 (C:1.2221, R:0.0105)
Batch  75/537: Loss=1.1948 (C:1.1948, R:0.0105)
Batch 100/537: Loss=1.2197 (C:1.2197, R:0.0105)
Batch 125/537: Loss=1.2232 (C:1.2232, R:0.0105)
Batch 150/537: Loss=1.2214 (C:1.2214, R:0.0105)
Batch 175/537: Loss=1.2279 (C:1.2279, R:0.0105)
Batch 200/537: Loss=1.2354 (C:1.2354, R:0.0105)
Batch 225/537: Loss=1.2380 (C:1.2380, R:0.0105)
Batch 250/537: Loss=1.2535 (C:1.2535, R:0.0105)
Batch 275/537: Loss=1.2366 (C:1.2366, R:0.0105)
Batch 300/537: Loss=1.2208 (C:1.2208, R:0.0105)
Batch 325/537: Loss=1.1863 (C:1.1863, R:0.0105)
Batch 350/537: Loss=1.2169 (C:1.2169, R:0.0105)
Batch 375/537: Loss=1.2199 (C:1.2199, R:0.0105)
Batch 400/537: Loss=1.2020 (C:1.2020, R:0.0105)
Batch 425/537: Loss=1.1926 (C:1.1926, R:0.0105)
Batch 450/537: Loss=1.2130 (C:1.2130, R:0.0106)
Batch 475/537: Loss=1.2218 (C:1.2218, R:0.0105)
Batch 500/537: Loss=1.2252 (C:1.2252, R:0.0106)
Batch 525/537: Loss=1.1915 (C:1.1915, R:0.0105)

============================================================
Epoch 4/100 completed in 29.1s
Train: Loss=1.2135 (C:1.2135, R:0.0105) Ratio=2.51x
Val:   Loss=1.1971 (C:1.1971, R:0.0104) Ratio=2.59x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1971)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.1621 (C:1.1621, R:0.0105)
Batch  25/537: Loss=1.2101 (C:1.2101, R:0.0105)
Batch  50/537: Loss=1.1988 (C:1.1988, R:0.0105)
Batch  75/537: Loss=1.2397 (C:1.2397, R:0.0105)
Batch 100/537: Loss=1.2189 (C:1.2189, R:0.0105)
Batch 125/537: Loss=1.1786 (C:1.1786, R:0.0106)
Batch 150/537: Loss=1.1755 (C:1.1755, R:0.0105)
Batch 175/537: Loss=1.1903 (C:1.1903, R:0.0105)
Batch 200/537: Loss=1.1928 (C:1.1928, R:0.0105)
Batch 225/537: Loss=1.1623 (C:1.1623, R:0.0106)
Batch 250/537: Loss=1.1497 (C:1.1497, R:0.0105)
Batch 275/537: Loss=1.1998 (C:1.1998, R:0.0105)
Batch 300/537: Loss=1.1685 (C:1.1685, R:0.0105)
Batch 325/537: Loss=1.1626 (C:1.1626, R:0.0105)
Batch 350/537: Loss=1.1587 (C:1.1587, R:0.0105)
Batch 375/537: Loss=1.1989 (C:1.1989, R:0.0105)
Batch 400/537: Loss=1.2117 (C:1.2117, R:0.0105)
Batch 425/537: Loss=1.2261 (C:1.2261, R:0.0105)
Batch 450/537: Loss=1.1792 (C:1.1792, R:0.0105)
Batch 475/537: Loss=1.2006 (C:1.2006, R:0.0105)
Batch 500/537: Loss=1.1785 (C:1.1785, R:0.0106)
Batch 525/537: Loss=1.1857 (C:1.1857, R:0.0105)

============================================================
Epoch 5/100 completed in 22.2s
Train: Loss=1.1917 (C:1.1917, R:0.0105) Ratio=2.71x
Val:   Loss=1.1874 (C:1.1874, R:0.0104) Ratio=2.71x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1874)
Checkpoint saved at epoch 5
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.1785 (C:1.1785, R:0.0105)
Batch  25/537: Loss=1.2106 (C:1.2106, R:0.0105)
Batch  50/537: Loss=1.1887 (C:1.1887, R:0.0105)
Batch  75/537: Loss=1.1325 (C:1.1325, R:0.0105)
Batch 100/537: Loss=1.1790 (C:1.1790, R:0.0105)
Batch 125/537: Loss=1.1820 (C:1.1820, R:0.0105)
Batch 150/537: Loss=1.2084 (C:1.2084, R:0.0105)
Batch 175/537: Loss=1.1835 (C:1.1835, R:0.0105)
Batch 200/537: Loss=1.1886 (C:1.1886, R:0.0105)
Batch 225/537: Loss=1.1888 (C:1.1888, R:0.0105)
Batch 250/537: Loss=1.1928 (C:1.1928, R:0.0105)
Batch 275/537: Loss=1.1720 (C:1.1720, R:0.0105)
Batch 300/537: Loss=1.1850 (C:1.1850, R:0.0105)
Batch 325/537: Loss=1.2125 (C:1.2125, R:0.0105)
Batch 350/537: Loss=1.1905 (C:1.1905, R:0.0105)
Batch 375/537: Loss=1.1844 (C:1.1844, R:0.0105)
Batch 400/537: Loss=1.1645 (C:1.1645, R:0.0105)
Batch 425/537: Loss=1.1701 (C:1.1701, R:0.0105)
Batch 450/537: Loss=1.2173 (C:1.2173, R:0.0105)
Batch 475/537: Loss=1.1837 (C:1.1837, R:0.0105)
Batch 500/537: Loss=1.1979 (C:1.1979, R:0.0105)
Batch 525/537: Loss=1.1651 (C:1.1651, R:0.0105)

============================================================
Epoch 6/100 completed in 22.3s
Train: Loss=1.1772 (C:1.1772, R:0.0105) Ratio=2.82x
Val:   Loss=1.1939 (C:1.1939, R:0.0104) Ratio=2.75x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 7
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.547 ¬± 0.583
    Neg distances: 1.757 ¬± 0.910
    Separation ratio: 3.21x
    Gap: -3.668
    ‚úÖ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.1233 (C:1.1233, R:0.0105)
Batch  25/537: Loss=1.0901 (C:1.0901, R:0.0105)
Batch  50/537: Loss=1.0907 (C:1.0907, R:0.0105)
Batch  75/537: Loss=1.1019 (C:1.1019, R:0.0105)
Batch 100/537: Loss=1.1149 (C:1.1149, R:0.0105)
Batch 125/537: Loss=1.1184 (C:1.1184, R:0.0106)
Batch 150/537: Loss=1.0541 (C:1.0541, R:0.0105)
Batch 175/537: Loss=1.1061 (C:1.1061, R:0.0105)
Batch 200/537: Loss=1.0813 (C:1.0813, R:0.0105)
Batch 225/537: Loss=1.0981 (C:1.0981, R:0.0105)
Batch 250/537: Loss=1.0635 (C:1.0635, R:0.0105)
Batch 275/537: Loss=1.0889 (C:1.0889, R:0.0105)
Batch 300/537: Loss=1.1053 (C:1.1053, R:0.0105)
Batch 325/537: Loss=1.0593 (C:1.0593, R:0.0105)
Batch 350/537: Loss=1.1017 (C:1.1017, R:0.0105)
Batch 375/537: Loss=1.0924 (C:1.0924, R:0.0105)
Batch 400/537: Loss=1.1049 (C:1.1049, R:0.0106)
Batch 425/537: Loss=1.0948 (C:1.0948, R:0.0105)
Batch 450/537: Loss=1.0804 (C:1.0804, R:0.0105)
Batch 475/537: Loss=1.1146 (C:1.1146, R:0.0105)
Batch 500/537: Loss=1.0707 (C:1.0707, R:0.0106)
Batch 525/537: Loss=1.1007 (C:1.1007, R:0.0105)

============================================================
Epoch 7/100 completed in 29.1s
Train: Loss=1.0957 (C:1.0957, R:0.0105) Ratio=2.97x
Val:   Loss=1.1132 (C:1.1132, R:0.0104) Ratio=2.79x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1132)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.0707 (C:1.0707, R:0.0105)
Batch  25/537: Loss=1.0570 (C:1.0570, R:0.0105)
Batch  50/537: Loss=1.1015 (C:1.1015, R:0.0105)
Batch  75/537: Loss=1.1123 (C:1.1123, R:0.0105)
Batch 100/537: Loss=1.0757 (C:1.0757, R:0.0105)
Batch 125/537: Loss=1.1042 (C:1.1042, R:0.0105)
Batch 150/537: Loss=1.0970 (C:1.0970, R:0.0105)
Batch 175/537: Loss=1.0851 (C:1.0851, R:0.0105)
Batch 200/537: Loss=1.0665 (C:1.0665, R:0.0105)
Batch 225/537: Loss=1.0617 (C:1.0617, R:0.0104)
Batch 250/537: Loss=1.1183 (C:1.1183, R:0.0105)
Batch 275/537: Loss=1.0658 (C:1.0658, R:0.0105)
Batch 300/537: Loss=1.1153 (C:1.1153, R:0.0105)
Batch 325/537: Loss=1.1183 (C:1.1183, R:0.0105)
Batch 350/537: Loss=1.1213 (C:1.1213, R:0.0105)
Batch 375/537: Loss=1.0825 (C:1.0825, R:0.0105)
Batch 400/537: Loss=1.0724 (C:1.0724, R:0.0105)
Batch 425/537: Loss=1.0947 (C:1.0947, R:0.0105)
Batch 450/537: Loss=1.0902 (C:1.0902, R:0.0105)
Batch 475/537: Loss=1.0995 (C:1.0995, R:0.0105)
Batch 500/537: Loss=1.0953 (C:1.0953, R:0.0105)
Batch 525/537: Loss=1.1092 (C:1.1092, R:0.0105)

============================================================
Epoch 8/100 completed in 22.3s
Train: Loss=1.0866 (C:1.0866, R:0.0105) Ratio=2.98x
Val:   Loss=1.1039 (C:1.1039, R:0.0104) Ratio=2.84x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1039)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.0619 (C:1.0619, R:0.0105)
Batch  25/537: Loss=1.0497 (C:1.0497, R:0.0105)
Batch  50/537: Loss=1.0447 (C:1.0447, R:0.0105)
Batch  75/537: Loss=1.1105 (C:1.1105, R:0.0105)
Batch 100/537: Loss=1.0798 (C:1.0798, R:0.0105)
Batch 125/537: Loss=1.0863 (C:1.0863, R:0.0105)
Batch 150/537: Loss=1.1012 (C:1.1012, R:0.0106)
Batch 175/537: Loss=1.0886 (C:1.0886, R:0.0105)
Batch 200/537: Loss=1.0928 (C:1.0928, R:0.0105)
Batch 225/537: Loss=1.0534 (C:1.0534, R:0.0105)
Batch 250/537: Loss=1.0736 (C:1.0736, R:0.0105)
Batch 275/537: Loss=1.0797 (C:1.0797, R:0.0105)
Batch 300/537: Loss=1.0752 (C:1.0752, R:0.0105)
Batch 325/537: Loss=1.0781 (C:1.0781, R:0.0105)
Batch 350/537: Loss=1.0856 (C:1.0856, R:0.0105)
Batch 375/537: Loss=1.0517 (C:1.0517, R:0.0105)
Batch 400/537: Loss=1.0783 (C:1.0783, R:0.0105)
Batch 425/537: Loss=1.0787 (C:1.0787, R:0.0105)
Batch 450/537: Loss=1.0985 (C:1.0985, R:0.0105)
Batch 475/537: Loss=1.1208 (C:1.1208, R:0.0105)
Batch 500/537: Loss=1.1136 (C:1.1136, R:0.0105)
Batch 525/537: Loss=1.0651 (C:1.0651, R:0.0105)

============================================================
Epoch 9/100 completed in 22.9s
Train: Loss=1.0775 (C:1.0775, R:0.0105) Ratio=3.10x
Val:   Loss=1.0988 (C:1.0988, R:0.0104) Ratio=2.90x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0988)
============================================================

üåç Updating global dataset at epoch 10
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.498 ¬± 0.581
    Neg distances: 1.863 ¬± 0.926
    Separation ratio: 3.74x
    Gap: -3.452
    ‚úÖ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.0048 (C:1.0048, R:0.0105)
Batch  25/537: Loss=1.0298 (C:1.0298, R:0.0105)
Batch  50/537: Loss=1.0084 (C:1.0084, R:0.0105)
Batch  75/537: Loss=1.0309 (C:1.0309, R:0.0105)
Batch 100/537: Loss=1.0224 (C:1.0224, R:0.0105)
Batch 125/537: Loss=0.9955 (C:0.9955, R:0.0105)
Batch 150/537: Loss=1.0276 (C:1.0276, R:0.0106)
Batch 175/537: Loss=1.0283 (C:1.0283, R:0.0105)
Batch 200/537: Loss=1.0366 (C:1.0366, R:0.0105)
Batch 225/537: Loss=1.0003 (C:1.0003, R:0.0105)
Batch 250/537: Loss=1.0253 (C:1.0253, R:0.0105)
Batch 275/537: Loss=1.0059 (C:1.0059, R:0.0105)
Batch 300/537: Loss=0.9923 (C:0.9923, R:0.0105)
Batch 325/537: Loss=1.0092 (C:1.0092, R:0.0105)
Batch 350/537: Loss=1.0199 (C:1.0199, R:0.0105)
Batch 375/537: Loss=1.0099 (C:1.0099, R:0.0105)
Batch 400/537: Loss=1.0215 (C:1.0215, R:0.0105)
Batch 425/537: Loss=1.0219 (C:1.0219, R:0.0105)
Batch 450/537: Loss=0.9968 (C:0.9968, R:0.0105)
Batch 475/537: Loss=1.0318 (C:1.0318, R:0.0105)
Batch 500/537: Loss=1.0303 (C:1.0303, R:0.0105)
Batch 525/537: Loss=1.0276 (C:1.0276, R:0.0105)

============================================================
Epoch 10/100 completed in 30.5s
Train: Loss=1.0104 (C:1.0104, R:0.0105) Ratio=3.17x
Val:   Loss=1.0312 (C:1.0312, R:0.0104) Ratio=2.93x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.0312)
Checkpoint saved at epoch 10
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.0191 (C:1.0191, R:0.0105)
Batch  25/537: Loss=0.9908 (C:0.9908, R:0.0105)
Batch  50/537: Loss=0.9995 (C:0.9995, R:0.0105)
Batch  75/537: Loss=0.9853 (C:0.9853, R:0.0105)
Batch 100/537: Loss=0.9719 (C:0.9719, R:0.0105)
Batch 125/537: Loss=1.0281 (C:1.0281, R:0.0105)
Batch 150/537: Loss=1.0247 (C:1.0247, R:0.0105)
Batch 175/537: Loss=0.9953 (C:0.9953, R:0.0105)
Batch 200/537: Loss=0.9925 (C:0.9925, R:0.0105)
Batch 225/537: Loss=1.0044 (C:1.0044, R:0.0105)
Batch 250/537: Loss=0.9946 (C:0.9946, R:0.0105)
Batch 275/537: Loss=1.0394 (C:1.0394, R:0.0105)
Batch 300/537: Loss=1.0191 (C:1.0191, R:0.0105)
Batch 325/537: Loss=1.0052 (C:1.0052, R:0.0105)
Batch 350/537: Loss=1.0182 (C:1.0182, R:0.0105)
Batch 375/537: Loss=0.9527 (C:0.9527, R:0.0105)
Batch 400/537: Loss=1.0101 (C:1.0101, R:0.0105)
Batch 425/537: Loss=1.0021 (C:1.0021, R:0.0105)
Batch 450/537: Loss=1.0005 (C:1.0005, R:0.0105)
Batch 475/537: Loss=1.0068 (C:1.0068, R:0.0105)
Batch 500/537: Loss=1.0187 (C:1.0187, R:0.0105)
Batch 525/537: Loss=0.9995 (C:0.9995, R:0.0105)

============================================================
Epoch 11/100 completed in 23.0s
Train: Loss=1.0029 (C:1.0029, R:0.0105) Ratio=3.23x
Val:   Loss=1.0344 (C:1.0344, R:0.0104) Ratio=2.92x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=0.9924 (C:0.9924, R:0.0105)
Batch  25/537: Loss=0.9944 (C:0.9944, R:0.0106)
Batch  50/537: Loss=1.0026 (C:1.0026, R:0.0105)
Batch  75/537: Loss=0.9625 (C:0.9625, R:0.0105)
Batch 100/537: Loss=0.9681 (C:0.9681, R:0.0105)
Batch 125/537: Loss=0.9803 (C:0.9803, R:0.0105)
Batch 150/537: Loss=0.9912 (C:0.9912, R:0.0105)
Batch 175/537: Loss=1.0209 (C:1.0209, R:0.0105)
Batch 200/537: Loss=1.0082 (C:1.0082, R:0.0105)
Batch 225/537: Loss=1.0102 (C:1.0102, R:0.0106)
Batch 250/537: Loss=1.0087 (C:1.0087, R:0.0105)
Batch 275/537: Loss=1.0088 (C:1.0088, R:0.0105)
Batch 300/537: Loss=0.9854 (C:0.9854, R:0.0105)
Batch 325/537: Loss=1.0257 (C:1.0257, R:0.0105)
Batch 350/537: Loss=0.9868 (C:0.9868, R:0.0105)
Batch 375/537: Loss=1.0251 (C:1.0251, R:0.0105)
Batch 400/537: Loss=0.9609 (C:0.9609, R:0.0105)
Batch 425/537: Loss=1.0075 (C:1.0075, R:0.0105)
Batch 450/537: Loss=1.0032 (C:1.0032, R:0.0105)
Batch 475/537: Loss=0.9989 (C:0.9989, R:0.0105)
Batch 500/537: Loss=1.0215 (C:1.0215, R:0.0105)
Batch 525/537: Loss=0.9959 (C:0.9959, R:0.0105)

============================================================
Epoch 12/100 completed in 23.3s
Train: Loss=0.9968 (C:0.9968, R:0.0105) Ratio=3.31x
Val:   Loss=1.0312 (C:1.0312, R:0.0104) Ratio=2.96x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 13
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.461 ¬± 0.556
    Neg distances: 1.934 ¬± 0.926
    Separation ratio: 4.20x
    Gap: -3.420
    ‚úÖ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.9438 (C:0.9438, R:0.0105)
Batch  25/537: Loss=0.9253 (C:0.9253, R:0.0105)
Batch  50/537: Loss=0.9282 (C:0.9282, R:0.0105)
Batch  75/537: Loss=0.9644 (C:0.9644, R:0.0106)
Batch 100/537: Loss=0.9586 (C:0.9586, R:0.0105)
Batch 125/537: Loss=0.9377 (C:0.9377, R:0.0105)
Batch 150/537: Loss=0.9495 (C:0.9495, R:0.0105)
Batch 175/537: Loss=0.9382 (C:0.9382, R:0.0105)
Batch 200/537: Loss=0.9535 (C:0.9535, R:0.0105)
Batch 225/537: Loss=0.9472 (C:0.9472, R:0.0105)
Batch 250/537: Loss=0.9370 (C:0.9370, R:0.0105)
Batch 275/537: Loss=0.9403 (C:0.9403, R:0.0105)
Batch 300/537: Loss=0.9429 (C:0.9429, R:0.0105)
Batch 325/537: Loss=0.9478 (C:0.9478, R:0.0105)
Batch 350/537: Loss=0.9304 (C:0.9304, R:0.0105)
Batch 375/537: Loss=0.9231 (C:0.9231, R:0.0105)
Batch 400/537: Loss=0.9238 (C:0.9238, R:0.0105)
Batch 425/537: Loss=0.9277 (C:0.9277, R:0.0105)
Batch 450/537: Loss=0.9396 (C:0.9396, R:0.0105)
Batch 475/537: Loss=0.9546 (C:0.9546, R:0.0105)
Batch 500/537: Loss=0.9269 (C:0.9269, R:0.0105)
Batch 525/537: Loss=0.9598 (C:0.9598, R:0.0105)

============================================================
Epoch 13/100 completed in 30.4s
Train: Loss=0.9456 (C:0.9456, R:0.0105) Ratio=3.41x
Val:   Loss=0.9910 (C:0.9910, R:0.0104) Ratio=2.95x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9910)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.9575 (C:0.9575, R:0.0105)
Batch  25/537: Loss=0.9094 (C:0.9094, R:0.0105)
Batch  50/537: Loss=0.9730 (C:0.9730, R:0.0105)
Batch  75/537: Loss=0.9270 (C:0.9270, R:0.0105)
Batch 100/537: Loss=0.9428 (C:0.9428, R:0.0105)
Batch 125/537: Loss=0.9175 (C:0.9175, R:0.0105)
Batch 150/537: Loss=0.9326 (C:0.9326, R:0.0105)
Batch 175/537: Loss=0.9561 (C:0.9561, R:0.0105)
Batch 200/537: Loss=0.9336 (C:0.9336, R:0.0105)
Batch 225/537: Loss=0.9508 (C:0.9508, R:0.0105)
Batch 250/537: Loss=0.9080 (C:0.9080, R:0.0105)
Batch 275/537: Loss=0.9339 (C:0.9339, R:0.0105)
Batch 300/537: Loss=0.9257 (C:0.9257, R:0.0106)
Batch 325/537: Loss=0.9361 (C:0.9361, R:0.0105)
Batch 350/537: Loss=0.9253 (C:0.9253, R:0.0105)
Batch 375/537: Loss=0.9250 (C:0.9250, R:0.0105)
Batch 400/537: Loss=0.9347 (C:0.9347, R:0.0105)
Batch 425/537: Loss=0.9486 (C:0.9486, R:0.0105)
Batch 450/537: Loss=0.9480 (C:0.9480, R:0.0106)
Batch 475/537: Loss=0.9250 (C:0.9250, R:0.0105)
Batch 500/537: Loss=0.9117 (C:0.9117, R:0.0105)
Batch 525/537: Loss=0.9597 (C:0.9597, R:0.0105)

============================================================
Epoch 14/100 completed in 23.0s
Train: Loss=0.9383 (C:0.9383, R:0.0105) Ratio=3.46x
Val:   Loss=0.9802 (C:0.9802, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9802)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.8895 (C:0.8895, R:0.0105)
Batch  25/537: Loss=0.9300 (C:0.9300, R:0.0105)
Batch  50/537: Loss=0.9056 (C:0.9056, R:0.0105)
Batch  75/537: Loss=0.9350 (C:0.9350, R:0.0105)
Batch 100/537: Loss=0.9053 (C:0.9053, R:0.0105)
Batch 125/537: Loss=0.9317 (C:0.9317, R:0.0105)
Batch 150/537: Loss=0.9608 (C:0.9608, R:0.0105)
Batch 175/537: Loss=0.9588 (C:0.9588, R:0.0105)
Batch 200/537: Loss=0.9155 (C:0.9155, R:0.0105)
Batch 225/537: Loss=0.9227 (C:0.9227, R:0.0105)
Batch 250/537: Loss=0.9143 (C:0.9143, R:0.0105)
Batch 275/537: Loss=0.9593 (C:0.9593, R:0.0105)
Batch 300/537: Loss=0.9339 (C:0.9339, R:0.0105)
Batch 325/537: Loss=0.9336 (C:0.9336, R:0.0105)
Batch 350/537: Loss=0.9016 (C:0.9016, R:0.0105)
Batch 375/537: Loss=0.9411 (C:0.9411, R:0.0105)
Batch 400/537: Loss=0.9060 (C:0.9060, R:0.0105)
Batch 425/537: Loss=0.9191 (C:0.9191, R:0.0105)
Batch 450/537: Loss=0.9654 (C:0.9654, R:0.0105)
Batch 475/537: Loss=0.9244 (C:0.9244, R:0.0105)
Batch 500/537: Loss=0.9773 (C:0.9773, R:0.0105)
Batch 525/537: Loss=0.9396 (C:0.9396, R:0.0105)

============================================================
Epoch 15/100 completed in 23.7s
Train: Loss=0.9338 (C:0.9338, R:0.0105) Ratio=3.50x
Val:   Loss=0.9928 (C:0.9928, R:0.0104) Ratio=2.97x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 15
============================================================

üåç Updating global dataset at epoch 16
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.451 ¬± 0.567
    Neg distances: 1.994 ¬± 0.936
    Separation ratio: 4.42x
    Gap: -3.604
    ‚úÖ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.8956 (C:0.8956, R:0.0105)
Batch  25/537: Loss=0.9093 (C:0.9093, R:0.0105)
Batch  50/537: Loss=0.8833 (C:0.8833, R:0.0105)
Batch  75/537: Loss=0.9087 (C:0.9087, R:0.0105)
Batch 100/537: Loss=0.9021 (C:0.9021, R:0.0105)
Batch 125/537: Loss=0.9075 (C:0.9075, R:0.0106)
Batch 150/537: Loss=0.8870 (C:0.8870, R:0.0105)
Batch 175/537: Loss=0.8797 (C:0.8797, R:0.0105)
Batch 200/537: Loss=0.8894 (C:0.8894, R:0.0105)
Batch 225/537: Loss=0.9216 (C:0.9216, R:0.0105)
Batch 250/537: Loss=0.9132 (C:0.9132, R:0.0105)
Batch 275/537: Loss=0.9465 (C:0.9465, R:0.0106)
Batch 300/537: Loss=0.9160 (C:0.9160, R:0.0105)
Batch 325/537: Loss=0.8690 (C:0.8690, R:0.0105)
Batch 350/537: Loss=0.9127 (C:0.9127, R:0.0105)
Batch 375/537: Loss=0.8980 (C:0.8980, R:0.0105)
Batch 400/537: Loss=0.9299 (C:0.9299, R:0.0105)
Batch 425/537: Loss=0.8967 (C:0.8967, R:0.0105)
Batch 450/537: Loss=0.9160 (C:0.9160, R:0.0105)
Batch 475/537: Loss=0.9249 (C:0.9249, R:0.0105)
Batch 500/537: Loss=0.9231 (C:0.9231, R:0.0105)
Batch 525/537: Loss=0.9472 (C:0.9472, R:0.0105)

============================================================
Epoch 16/100 completed in 30.1s
Train: Loss=0.9041 (C:0.9041, R:0.0105) Ratio=3.50x
Val:   Loss=0.9545 (C:0.9545, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9545)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.8877 (C:0.8877, R:0.0105)
Batch  25/537: Loss=0.8718 (C:0.8718, R:0.0105)
Batch  50/537: Loss=0.9268 (C:0.9268, R:0.0105)
Batch  75/537: Loss=0.8942 (C:0.8942, R:0.0105)
Batch 100/537: Loss=0.9059 (C:0.9059, R:0.0105)
Batch 125/537: Loss=0.8797 (C:0.8797, R:0.0105)
Batch 150/537: Loss=0.8547 (C:0.8547, R:0.0105)
Batch 175/537: Loss=0.9172 (C:0.9172, R:0.0105)
Batch 200/537: Loss=0.8831 (C:0.8831, R:0.0105)
Batch 225/537: Loss=0.8736 (C:0.8736, R:0.0105)
Batch 250/537: Loss=0.9025 (C:0.9025, R:0.0105)
Batch 275/537: Loss=0.9029 (C:0.9029, R:0.0105)
Batch 300/537: Loss=0.8972 (C:0.8972, R:0.0105)
Batch 325/537: Loss=0.9099 (C:0.9099, R:0.0105)
Batch 350/537: Loss=0.8721 (C:0.8721, R:0.0105)
Batch 375/537: Loss=0.9115 (C:0.9115, R:0.0106)
Batch 400/537: Loss=0.8645 (C:0.8645, R:0.0105)
Batch 425/537: Loss=0.9033 (C:0.9033, R:0.0105)
Batch 450/537: Loss=0.8950 (C:0.8950, R:0.0105)
Batch 475/537: Loss=0.8937 (C:0.8937, R:0.0105)
Batch 500/537: Loss=0.8818 (C:0.8818, R:0.0105)
Batch 525/537: Loss=0.9182 (C:0.9182, R:0.0105)

============================================================
Epoch 17/100 completed in 23.2s
Train: Loss=0.8993 (C:0.8993, R:0.0105) Ratio=3.68x
Val:   Loss=0.9704 (C:0.9704, R:0.0104) Ratio=3.01x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.8762 (C:0.8762, R:0.0105)
Batch  25/537: Loss=0.8767 (C:0.8767, R:0.0105)
Batch  50/537: Loss=0.8936 (C:0.8936, R:0.0105)
Batch  75/537: Loss=0.8925 (C:0.8925, R:0.0105)
Batch 100/537: Loss=0.8902 (C:0.8902, R:0.0105)
Batch 125/537: Loss=0.8719 (C:0.8719, R:0.0105)
Batch 150/537: Loss=0.8494 (C:0.8494, R:0.0106)
Batch 175/537: Loss=0.9071 (C:0.9071, R:0.0106)
Batch 200/537: Loss=0.8697 (C:0.8697, R:0.0105)
Batch 225/537: Loss=0.8838 (C:0.8838, R:0.0105)
Batch 250/537: Loss=0.9242 (C:0.9242, R:0.0105)
Batch 275/537: Loss=0.8870 (C:0.8870, R:0.0105)
Batch 300/537: Loss=0.8784 (C:0.8784, R:0.0105)
Batch 325/537: Loss=0.9519 (C:0.9519, R:0.0105)
Batch 350/537: Loss=0.8577 (C:0.8577, R:0.0105)
Batch 375/537: Loss=0.9134 (C:0.9134, R:0.0105)
Batch 400/537: Loss=0.8749 (C:0.8749, R:0.0106)
Batch 425/537: Loss=0.8883 (C:0.8883, R:0.0105)
Batch 450/537: Loss=0.9031 (C:0.9031, R:0.0105)
Batch 475/537: Loss=0.9034 (C:0.9034, R:0.0106)
Batch 500/537: Loss=0.8568 (C:0.8568, R:0.0105)
Batch 525/537: Loss=0.9299 (C:0.9299, R:0.0105)

============================================================
Epoch 18/100 completed in 23.6s
Train: Loss=0.8943 (C:0.8943, R:0.0105) Ratio=3.64x
Val:   Loss=0.9571 (C:0.9571, R:0.0104) Ratio=3.02x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 19
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.449 ¬± 0.589
    Neg distances: 2.073 ¬± 0.963
    Separation ratio: 4.62x
    Gap: -3.741
    ‚úÖ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.8538 (C:0.8538, R:0.0105)
Batch  25/537: Loss=0.8795 (C:0.8795, R:0.0105)
Batch  50/537: Loss=0.8806 (C:0.8806, R:0.0105)
Batch  75/537: Loss=0.8729 (C:0.8729, R:0.0105)
Batch 100/537: Loss=0.8495 (C:0.8495, R:0.0105)
Batch 125/537: Loss=0.8680 (C:0.8680, R:0.0105)
Batch 150/537: Loss=0.8441 (C:0.8441, R:0.0105)
Batch 175/537: Loss=0.8541 (C:0.8541, R:0.0105)
Batch 200/537: Loss=0.8683 (C:0.8683, R:0.0105)
Batch 225/537: Loss=0.8811 (C:0.8811, R:0.0105)
Batch 250/537: Loss=0.8642 (C:0.8642, R:0.0105)
Batch 275/537: Loss=0.8311 (C:0.8311, R:0.0105)
Batch 300/537: Loss=0.8819 (C:0.8819, R:0.0105)
Batch 325/537: Loss=0.8527 (C:0.8527, R:0.0106)
Batch 350/537: Loss=0.8730 (C:0.8730, R:0.0105)
Batch 375/537: Loss=0.8411 (C:0.8411, R:0.0105)
Batch 400/537: Loss=0.8324 (C:0.8324, R:0.0105)
Batch 425/537: Loss=0.8905 (C:0.8905, R:0.0105)
Batch 450/537: Loss=0.8824 (C:0.8824, R:0.0105)
Batch 475/537: Loss=0.8975 (C:0.8975, R:0.0106)
Batch 500/537: Loss=0.8923 (C:0.8923, R:0.0105)
Batch 525/537: Loss=0.8498 (C:0.8498, R:0.0105)

============================================================
Epoch 19/100 completed in 30.3s
Train: Loss=0.8666 (C:0.8666, R:0.0105) Ratio=3.64x
Val:   Loss=0.9369 (C:0.9369, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9369)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.8585 (C:0.8585, R:0.0105)
Batch  25/537: Loss=0.8294 (C:0.8294, R:0.0105)
Batch  50/537: Loss=0.8620 (C:0.8620, R:0.0105)
Batch  75/537: Loss=0.8583 (C:0.8583, R:0.0105)
Batch 100/537: Loss=0.8623 (C:0.8623, R:0.0105)
Batch 125/537: Loss=0.8513 (C:0.8513, R:0.0105)
Batch 150/537: Loss=0.8610 (C:0.8610, R:0.0105)
Batch 175/537: Loss=0.8709 (C:0.8709, R:0.0105)
Batch 200/537: Loss=0.8389 (C:0.8389, R:0.0105)
Batch 225/537: Loss=0.8310 (C:0.8310, R:0.0105)
Batch 250/537: Loss=0.8523 (C:0.8523, R:0.0105)
Batch 275/537: Loss=0.8584 (C:0.8584, R:0.0105)
Batch 300/537: Loss=0.8684 (C:0.8684, R:0.0105)
Batch 325/537: Loss=0.8909 (C:0.8909, R:0.0105)
Batch 350/537: Loss=0.8459 (C:0.8459, R:0.0105)
Batch 375/537: Loss=0.8646 (C:0.8646, R:0.0105)
Batch 400/537: Loss=0.8552 (C:0.8552, R:0.0105)
Batch 425/537: Loss=0.8564 (C:0.8564, R:0.0105)
Batch 450/537: Loss=0.8592 (C:0.8592, R:0.0105)
Batch 475/537: Loss=0.8614 (C:0.8614, R:0.0106)
Batch 500/537: Loss=0.8114 (C:0.8114, R:0.0105)
Batch 525/537: Loss=0.8919 (C:0.8919, R:0.0105)

============================================================
Epoch 20/100 completed in 23.1s
Train: Loss=0.8636 (C:0.8636, R:0.0105) Ratio=3.76x
Val:   Loss=0.9257 (C:0.9257, R:0.0104) Ratio=3.04x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.9257)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.8479 (C:0.8479, R:0.0105)
Batch  25/537: Loss=0.9024 (C:0.9024, R:0.0105)
Batch  50/537: Loss=0.8301 (C:0.8301, R:0.0105)
Batch  75/537: Loss=0.8338 (C:0.8338, R:0.0105)
Batch 100/537: Loss=0.8733 (C:0.8733, R:0.0105)
Batch 125/537: Loss=0.8769 (C:0.8769, R:0.0105)
Batch 150/537: Loss=0.8748 (C:0.8748, R:0.0105)
Batch 175/537: Loss=0.8382 (C:0.8382, R:0.0105)
Batch 200/537: Loss=0.8380 (C:0.8380, R:0.0105)
Batch 225/537: Loss=0.8492 (C:0.8492, R:0.0105)
Batch 250/537: Loss=0.8604 (C:0.8604, R:0.0105)
Batch 275/537: Loss=0.8373 (C:0.8373, R:0.0105)
Batch 300/537: Loss=0.8478 (C:0.8478, R:0.0105)
Batch 325/537: Loss=0.9022 (C:0.9022, R:0.0105)
Batch 350/537: Loss=0.8931 (C:0.8931, R:0.0105)
Batch 375/537: Loss=0.8559 (C:0.8559, R:0.0105)
Batch 400/537: Loss=0.8642 (C:0.8642, R:0.0105)
Batch 425/537: Loss=0.8362 (C:0.8362, R:0.0105)
Batch 450/537: Loss=0.8415 (C:0.8415, R:0.0105)
Batch 475/537: Loss=0.8221 (C:0.8221, R:0.0105)
Batch 500/537: Loss=0.8342 (C:0.8342, R:0.0105)
Batch 525/537: Loss=0.8761 (C:0.8761, R:0.0105)

============================================================
Epoch 21/100 completed in 22.9s
Train: Loss=0.8574 (C:0.8574, R:0.0105) Ratio=3.80x
Val:   Loss=0.9343 (C:0.9343, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 22
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.431 ¬± 0.581
    Neg distances: 2.145 ¬± 0.976
    Separation ratio: 4.98x
    Gap: -3.880
    ‚úÖ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.8147 (C:0.8147, R:0.0105)
Batch  25/537: Loss=0.8023 (C:0.8023, R:0.0105)
Batch  50/537: Loss=0.7983 (C:0.7983, R:0.0105)
Batch  75/537: Loss=0.8206 (C:0.8206, R:0.0105)
Batch 100/537: Loss=0.8220 (C:0.8220, R:0.0105)
Batch 125/537: Loss=0.7981 (C:0.7981, R:0.0105)
Batch 150/537: Loss=0.8345 (C:0.8345, R:0.0105)
Batch 175/537: Loss=0.7882 (C:0.7882, R:0.0105)
Batch 200/537: Loss=0.8346 (C:0.8346, R:0.0105)
Batch 225/537: Loss=0.8424 (C:0.8424, R:0.0105)
Batch 250/537: Loss=0.8224 (C:0.8224, R:0.0105)
Batch 275/537: Loss=0.7926 (C:0.7926, R:0.0105)
Batch 300/537: Loss=0.7983 (C:0.7983, R:0.0105)
Batch 325/537: Loss=0.8201 (C:0.8201, R:0.0105)
Batch 350/537: Loss=0.8294 (C:0.8294, R:0.0105)
Batch 375/537: Loss=0.7920 (C:0.7920, R:0.0105)
Batch 400/537: Loss=0.8106 (C:0.8106, R:0.0106)
Batch 425/537: Loss=0.8074 (C:0.8074, R:0.0105)
Batch 450/537: Loss=0.8065 (C:0.8065, R:0.0105)
Batch 475/537: Loss=0.8076 (C:0.8076, R:0.0105)
Batch 500/537: Loss=0.8231 (C:0.8231, R:0.0105)
Batch 525/537: Loss=0.7973 (C:0.7973, R:0.0105)

============================================================
Epoch 22/100 completed in 30.0s
Train: Loss=0.8192 (C:0.8192, R:0.0105) Ratio=3.84x
Val:   Loss=0.8886 (C:0.8886, R:0.0104) Ratio=3.08x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8886)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.8179 (C:0.8179, R:0.0105)
Batch  25/537: Loss=0.7965 (C:0.7965, R:0.0105)
Batch  50/537: Loss=0.8114 (C:0.8114, R:0.0105)
Batch  75/537: Loss=0.7846 (C:0.7846, R:0.0105)
Batch 100/537: Loss=0.7998 (C:0.7998, R:0.0105)
Batch 125/537: Loss=0.8037 (C:0.8037, R:0.0105)
Batch 150/537: Loss=0.8165 (C:0.8165, R:0.0105)
Batch 175/537: Loss=0.8205 (C:0.8205, R:0.0106)
Batch 200/537: Loss=0.8151 (C:0.8151, R:0.0105)
Batch 225/537: Loss=0.8247 (C:0.8247, R:0.0105)
Batch 250/537: Loss=0.8580 (C:0.8580, R:0.0105)
Batch 275/537: Loss=0.8096 (C:0.8096, R:0.0106)
Batch 300/537: Loss=0.8312 (C:0.8312, R:0.0105)
Batch 325/537: Loss=0.8769 (C:0.8769, R:0.0105)
Batch 350/537: Loss=0.8005 (C:0.8005, R:0.0105)
Batch 375/537: Loss=0.8424 (C:0.8424, R:0.0105)
Batch 400/537: Loss=0.7993 (C:0.7993, R:0.0105)
Batch 425/537: Loss=0.8056 (C:0.8056, R:0.0105)
Batch 450/537: Loss=0.8249 (C:0.8249, R:0.0105)
Batch 475/537: Loss=0.8232 (C:0.8232, R:0.0105)
Batch 500/537: Loss=0.8038 (C:0.8038, R:0.0105)
Batch 525/537: Loss=0.7995 (C:0.7995, R:0.0105)

============================================================
Epoch 23/100 completed in 22.6s
Train: Loss=0.8151 (C:0.8151, R:0.0105) Ratio=3.83x
Val:   Loss=0.8891 (C:0.8891, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.8083 (C:0.8083, R:0.0105)
Batch  25/537: Loss=0.8022 (C:0.8022, R:0.0105)
Batch  50/537: Loss=0.8136 (C:0.8136, R:0.0105)
Batch  75/537: Loss=0.8250 (C:0.8250, R:0.0105)
Batch 100/537: Loss=0.7743 (C:0.7743, R:0.0105)
Batch 125/537: Loss=0.8200 (C:0.8200, R:0.0106)
Batch 150/537: Loss=0.8235 (C:0.8235, R:0.0105)
Batch 175/537: Loss=0.7736 (C:0.7736, R:0.0105)
Batch 200/537: Loss=0.8231 (C:0.8231, R:0.0105)
Batch 225/537: Loss=0.7879 (C:0.7879, R:0.0105)
Batch 250/537: Loss=0.7948 (C:0.7948, R:0.0105)
Batch 275/537: Loss=0.8302 (C:0.8302, R:0.0105)
Batch 300/537: Loss=0.8299 (C:0.8299, R:0.0105)
Batch 325/537: Loss=0.8248 (C:0.8248, R:0.0106)
Batch 350/537: Loss=0.7772 (C:0.7772, R:0.0105)
Batch 375/537: Loss=0.8005 (C:0.8005, R:0.0105)
Batch 400/537: Loss=0.8071 (C:0.8071, R:0.0106)
Batch 425/537: Loss=0.7926 (C:0.7926, R:0.0105)
Batch 450/537: Loss=0.8299 (C:0.8299, R:0.0105)
Batch 475/537: Loss=0.8225 (C:0.8225, R:0.0105)
Batch 500/537: Loss=0.7983 (C:0.7983, R:0.0105)
Batch 525/537: Loss=0.7967 (C:0.7967, R:0.0105)

============================================================
Epoch 24/100 completed in 22.4s
Train: Loss=0.8117 (C:0.8117, R:0.0105) Ratio=3.93x
Val:   Loss=0.8918 (C:0.8918, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 25
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.420 ¬± 0.597
    Neg distances: 2.297 ¬± 1.026
    Separation ratio: 5.47x
    Gap: -3.972
    ‚úÖ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.7230 (C:0.7230, R:0.0105)
Batch  25/537: Loss=0.7486 (C:0.7486, R:0.0105)
Batch  50/537: Loss=0.7525 (C:0.7525, R:0.0105)
Batch  75/537: Loss=0.7349 (C:0.7349, R:0.0105)
Batch 100/537: Loss=0.7245 (C:0.7245, R:0.0105)
Batch 125/537: Loss=0.7866 (C:0.7866, R:0.0105)
Batch 150/537: Loss=0.7854 (C:0.7854, R:0.0105)
Batch 175/537: Loss=0.7505 (C:0.7505, R:0.0105)
Batch 200/537: Loss=0.7877 (C:0.7877, R:0.0105)
Batch 225/537: Loss=0.7586 (C:0.7586, R:0.0105)
Batch 250/537: Loss=0.7731 (C:0.7731, R:0.0105)
Batch 275/537: Loss=0.7685 (C:0.7685, R:0.0105)
Batch 300/537: Loss=0.7680 (C:0.7680, R:0.0105)
Batch 325/537: Loss=0.7691 (C:0.7691, R:0.0105)
Batch 350/537: Loss=0.7441 (C:0.7441, R:0.0105)
Batch 375/537: Loss=0.7437 (C:0.7437, R:0.0105)
Batch 400/537: Loss=0.7352 (C:0.7352, R:0.0105)
Batch 425/537: Loss=0.7523 (C:0.7523, R:0.0105)
Batch 450/537: Loss=0.7652 (C:0.7652, R:0.0105)
Batch 475/537: Loss=0.7784 (C:0.7784, R:0.0105)
Batch 500/537: Loss=0.7655 (C:0.7655, R:0.0105)
Batch 525/537: Loss=0.7680 (C:0.7680, R:0.0105)

============================================================
Epoch 25/100 completed in 29.0s
Train: Loss=0.7689 (C:0.7689, R:0.0105) Ratio=4.01x
Val:   Loss=0.8465 (C:0.8465, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8465)
Checkpoint saved at epoch 25
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.7608 (C:0.7608, R:0.0105)
Batch  25/537: Loss=0.7103 (C:0.7103, R:0.0105)
Batch  50/537: Loss=0.7858 (C:0.7858, R:0.0105)
Batch  75/537: Loss=0.7391 (C:0.7391, R:0.0105)
Batch 100/537: Loss=0.7917 (C:0.7917, R:0.0105)
Batch 125/537: Loss=0.7189 (C:0.7189, R:0.0105)
Batch 150/537: Loss=0.7597 (C:0.7597, R:0.0105)
Batch 175/537: Loss=0.7588 (C:0.7588, R:0.0105)
Batch 200/537: Loss=0.7420 (C:0.7420, R:0.0105)
Batch 225/537: Loss=0.7728 (C:0.7728, R:0.0105)
Batch 250/537: Loss=0.7688 (C:0.7688, R:0.0105)
Batch 275/537: Loss=0.7705 (C:0.7705, R:0.0105)
Batch 300/537: Loss=0.7615 (C:0.7615, R:0.0105)
Batch 325/537: Loss=0.7621 (C:0.7621, R:0.0105)
Batch 350/537: Loss=0.7901 (C:0.7901, R:0.0105)
Batch 375/537: Loss=0.7701 (C:0.7701, R:0.0105)
Batch 400/537: Loss=0.7634 (C:0.7634, R:0.0105)
Batch 425/537: Loss=0.7515 (C:0.7515, R:0.0105)
Batch 450/537: Loss=0.7528 (C:0.7528, R:0.0105)
Batch 475/537: Loss=0.7565 (C:0.7565, R:0.0105)
Batch 500/537: Loss=0.7584 (C:0.7584, R:0.0105)
Batch 525/537: Loss=0.7805 (C:0.7805, R:0.0105)

============================================================
Epoch 26/100 completed in 22.1s
Train: Loss=0.7653 (C:0.7653, R:0.0105) Ratio=4.02x
Val:   Loss=0.8568 (C:0.8568, R:0.0104) Ratio=3.06x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.8077 (C:0.8077, R:0.0105)
Batch  25/537: Loss=0.7761 (C:0.7761, R:0.0105)
Batch  50/537: Loss=0.7448 (C:0.7448, R:0.0105)
Batch  75/537: Loss=0.7646 (C:0.7646, R:0.0105)
Batch 100/537: Loss=0.7249 (C:0.7249, R:0.0105)
Batch 125/537: Loss=0.7551 (C:0.7551, R:0.0106)
Batch 150/537: Loss=0.7808 (C:0.7808, R:0.0105)
Batch 175/537: Loss=0.7610 (C:0.7610, R:0.0105)
Batch 200/537: Loss=0.7623 (C:0.7623, R:0.0105)
Batch 225/537: Loss=0.7735 (C:0.7735, R:0.0105)
Batch 250/537: Loss=0.7416 (C:0.7416, R:0.0105)
Batch 275/537: Loss=0.7450 (C:0.7450, R:0.0105)
Batch 300/537: Loss=0.7440 (C:0.7440, R:0.0105)
Batch 325/537: Loss=0.7773 (C:0.7773, R:0.0105)
Batch 350/537: Loss=0.7595 (C:0.7595, R:0.0105)
Batch 375/537: Loss=0.7580 (C:0.7580, R:0.0105)
Batch 400/537: Loss=0.7522 (C:0.7522, R:0.0105)
Batch 425/537: Loss=0.7847 (C:0.7847, R:0.0105)
Batch 450/537: Loss=0.7501 (C:0.7501, R:0.0105)
Batch 475/537: Loss=0.7274 (C:0.7274, R:0.0105)
Batch 500/537: Loss=0.7282 (C:0.7282, R:0.0105)
Batch 525/537: Loss=0.7586 (C:0.7586, R:0.0106)

============================================================
Epoch 27/100 completed in 21.9s
Train: Loss=0.7610 (C:0.7610, R:0.0105) Ratio=4.12x
Val:   Loss=0.8409 (C:0.8409, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8409)
============================================================

üåç Updating global dataset at epoch 28
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.405 ¬± 0.604
    Neg distances: 2.329 ¬± 1.033
    Separation ratio: 5.76x
    Gap: -4.015
    ‚úÖ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.7016 (C:0.7016, R:0.0105)
Batch  25/537: Loss=0.7132 (C:0.7132, R:0.0105)
Batch  50/537: Loss=0.7215 (C:0.7215, R:0.0105)
Batch  75/537: Loss=0.7026 (C:0.7026, R:0.0105)
Batch 100/537: Loss=0.7387 (C:0.7387, R:0.0105)
Batch 125/537: Loss=0.7442 (C:0.7442, R:0.0105)
Batch 150/537: Loss=0.7646 (C:0.7646, R:0.0106)
Batch 175/537: Loss=0.7340 (C:0.7340, R:0.0105)
Batch 200/537: Loss=0.7285 (C:0.7285, R:0.0105)
Batch 225/537: Loss=0.7081 (C:0.7081, R:0.0105)
Batch 250/537: Loss=0.7576 (C:0.7576, R:0.0105)
Batch 275/537: Loss=0.7479 (C:0.7479, R:0.0105)
Batch 300/537: Loss=0.7337 (C:0.7337, R:0.0105)
Batch 325/537: Loss=0.7776 (C:0.7776, R:0.0106)
Batch 350/537: Loss=0.7277 (C:0.7277, R:0.0105)
Batch 375/537: Loss=0.7237 (C:0.7237, R:0.0105)
Batch 400/537: Loss=0.7853 (C:0.7853, R:0.0105)
Batch 425/537: Loss=0.7522 (C:0.7522, R:0.0105)
Batch 450/537: Loss=0.7173 (C:0.7173, R:0.0105)
Batch 475/537: Loss=0.7532 (C:0.7532, R:0.0105)
Batch 500/537: Loss=0.7412 (C:0.7412, R:0.0105)
Batch 525/537: Loss=0.7380 (C:0.7380, R:0.0105)

============================================================
Epoch 28/100 completed in 27.7s
Train: Loss=0.7370 (C:0.7370, R:0.0105) Ratio=4.08x
Val:   Loss=0.8248 (C:0.8248, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8248)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.7235 (C:0.7235, R:0.0105)
Batch  25/537: Loss=0.7098 (C:0.7098, R:0.0105)
Batch  50/537: Loss=0.6884 (C:0.6884, R:0.0105)
Batch  75/537: Loss=0.6972 (C:0.6972, R:0.0105)
Batch 100/537: Loss=0.7371 (C:0.7371, R:0.0105)
Batch 125/537: Loss=0.7137 (C:0.7137, R:0.0106)
Batch 150/537: Loss=0.7637 (C:0.7637, R:0.0105)
Batch 175/537: Loss=0.7284 (C:0.7284, R:0.0105)
Batch 200/537: Loss=0.7208 (C:0.7208, R:0.0105)
Batch 225/537: Loss=0.7308 (C:0.7308, R:0.0105)
Batch 250/537: Loss=0.7294 (C:0.7294, R:0.0105)
Batch 275/537: Loss=0.7391 (C:0.7391, R:0.0105)
Batch 300/537: Loss=0.7384 (C:0.7384, R:0.0105)
Batch 325/537: Loss=0.7422 (C:0.7422, R:0.0105)
Batch 350/537: Loss=0.7459 (C:0.7459, R:0.0105)
Batch 375/537: Loss=0.7584 (C:0.7584, R:0.0105)
Batch 400/537: Loss=0.7339 (C:0.7339, R:0.0105)
Batch 425/537: Loss=0.7061 (C:0.7061, R:0.0105)
Batch 450/537: Loss=0.7119 (C:0.7119, R:0.0105)
Batch 475/537: Loss=0.7186 (C:0.7186, R:0.0105)
Batch 500/537: Loss=0.7414 (C:0.7414, R:0.0105)
Batch 525/537: Loss=0.7286 (C:0.7286, R:0.0105)

============================================================
Epoch 29/100 completed in 21.8s
Train: Loss=0.7328 (C:0.7328, R:0.0105) Ratio=4.10x
Val:   Loss=0.8189 (C:0.8189, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 0.8189)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.7176 (C:0.7176, R:0.0105)
Batch  25/537: Loss=0.7262 (C:0.7262, R:0.0105)
Batch  50/537: Loss=0.6995 (C:0.6995, R:0.0105)
Batch  75/537: Loss=0.7252 (C:0.7252, R:0.0105)
Batch 100/537: Loss=0.7514 (C:0.7514, R:0.0105)
Batch 125/537: Loss=0.6905 (C:0.6905, R:0.0105)
Batch 150/537: Loss=0.6745 (C:0.6745, R:0.0105)
Batch 175/537: Loss=0.7548 (C:0.7548, R:0.0105)
Batch 200/537: Loss=0.7540 (C:0.7540, R:0.0105)
Batch 225/537: Loss=0.7192 (C:0.7192, R:0.0105)
Batch 250/537: Loss=0.7294 (C:0.7294, R:0.0105)
Batch 275/537: Loss=0.7468 (C:0.7468, R:0.0105)
Batch 300/537: Loss=0.7240 (C:0.7240, R:0.0105)
Batch 325/537: Loss=0.7518 (C:0.7518, R:0.0105)
Batch 350/537: Loss=0.7244 (C:0.7244, R:0.0105)
Batch 375/537: Loss=0.7292 (C:0.7292, R:0.0105)
Batch 400/537: Loss=0.7237 (C:0.7237, R:0.0105)
Batch 425/537: Loss=0.7411 (C:0.7411, R:0.0105)
Batch 450/537: Loss=0.7327 (C:0.7327, R:0.0105)
Batch 475/537: Loss=0.7498 (C:0.7498, R:0.0105)
Batch 500/537: Loss=0.7519 (C:0.7519, R:0.0105)
Batch 525/537: Loss=0.7549 (C:0.7549, R:0.0105)

============================================================
Epoch 30/100 completed in 21.5s
Train: Loss=0.7309 (C:0.7309, R:0.0105) Ratio=4.18x
Val:   Loss=0.8231 (C:0.8231, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 30
============================================================

üåç Updating global dataset at epoch 31
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.409 ¬± 0.629
    Neg distances: 2.368 ¬± 1.050
    Separation ratio: 5.78x
    Gap: -4.139
    ‚úÖ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.6810 (C:0.6810, R:0.0105)
Batch  25/537: Loss=0.7211 (C:0.7211, R:0.0105)
Batch  50/537: Loss=0.7202 (C:0.7202, R:0.0105)
Batch  75/537: Loss=0.6847 (C:0.6847, R:0.0105)
Batch 100/537: Loss=0.7461 (C:0.7461, R:0.0106)
Batch 125/537: Loss=0.7155 (C:0.7155, R:0.0105)
Batch 150/537: Loss=0.7267 (C:0.7267, R:0.0105)
Batch 175/537: Loss=0.7257 (C:0.7257, R:0.0105)
Batch 200/537: Loss=0.7546 (C:0.7546, R:0.0105)
Batch 225/537: Loss=0.7280 (C:0.7280, R:0.0105)
Batch 250/537: Loss=0.7216 (C:0.7216, R:0.0105)
Batch 275/537: Loss=0.7117 (C:0.7117, R:0.0105)
Batch 300/537: Loss=0.7232 (C:0.7232, R:0.0105)
Batch 325/537: Loss=0.7480 (C:0.7480, R:0.0105)
Batch 350/537: Loss=0.7056 (C:0.7056, R:0.0105)
Batch 375/537: Loss=0.7220 (C:0.7220, R:0.0105)
Batch 400/537: Loss=0.7348 (C:0.7348, R:0.0105)
Batch 425/537: Loss=0.7333 (C:0.7333, R:0.0106)
Batch 450/537: Loss=0.6810 (C:0.6810, R:0.0105)
Batch 475/537: Loss=0.7279 (C:0.7279, R:0.0105)
Batch 500/537: Loss=0.7809 (C:0.7809, R:0.0105)
Batch 525/537: Loss=0.7193 (C:0.7193, R:0.0105)

============================================================
Epoch 31/100 completed in 27.3s
Train: Loss=0.7186 (C:0.7186, R:0.0105) Ratio=4.15x
Val:   Loss=0.8122 (C:0.8122, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.015
‚úÖ New best model saved (Val Loss: 0.8122)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=0.7254 (C:0.7254, R:0.0105)
Batch  25/537: Loss=0.7037 (C:0.7037, R:0.0105)
Batch  50/537: Loss=0.6972 (C:0.6972, R:0.0106)
Batch  75/537: Loss=0.7082 (C:0.7082, R:0.0105)
Batch 100/537: Loss=0.6905 (C:0.6905, R:0.0105)
Batch 125/537: Loss=0.6916 (C:0.6916, R:0.0105)
Batch 150/537: Loss=0.7331 (C:0.7331, R:0.0105)
Batch 175/537: Loss=0.7315 (C:0.7315, R:0.0105)
Batch 200/537: Loss=0.7484 (C:0.7484, R:0.0105)
Batch 225/537: Loss=0.7040 (C:0.7040, R:0.0105)
Batch 250/537: Loss=0.7304 (C:0.7304, R:0.0105)
Batch 275/537: Loss=0.7072 (C:0.7072, R:0.0105)
Batch 300/537: Loss=0.7255 (C:0.7255, R:0.0105)
Batch 325/537: Loss=0.7293 (C:0.7293, R:0.0105)
Batch 350/537: Loss=0.7340 (C:0.7340, R:0.0105)
Batch 375/537: Loss=0.7180 (C:0.7180, R:0.0105)
Batch 400/537: Loss=0.7112 (C:0.7112, R:0.0105)
Batch 425/537: Loss=0.7316 (C:0.7316, R:0.0105)
Batch 450/537: Loss=0.7280 (C:0.7280, R:0.0105)
Batch 475/537: Loss=0.7175 (C:0.7175, R:0.0105)
Batch 500/537: Loss=0.7676 (C:0.7676, R:0.0105)
Batch 525/537: Loss=0.7133 (C:0.7133, R:0.0105)

============================================================
Epoch 32/100 completed in 21.7s
Train: Loss=0.7168 (C:0.7168, R:0.0105) Ratio=4.16x
Val:   Loss=0.8139 (C:0.8139, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.030
No improvement for 1 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=0.7140 (C:0.7140, R:0.0105)
Batch  25/537: Loss=0.6885 (C:0.6885, R:0.0105)
Batch  50/537: Loss=0.7158 (C:0.7158, R:0.0105)
Batch  75/537: Loss=0.7171 (C:0.7171, R:0.0105)
Batch 100/537: Loss=0.7348 (C:0.7348, R:0.0105)
Batch 125/537: Loss=0.7283 (C:0.7283, R:0.0105)
Batch 150/537: Loss=0.7080 (C:0.7080, R:0.0105)
Batch 175/537: Loss=0.7187 (C:0.7187, R:0.0105)
Batch 200/537: Loss=0.7114 (C:0.7114, R:0.0105)
Batch 225/537: Loss=0.6836 (C:0.6836, R:0.0105)
Batch 250/537: Loss=0.7327 (C:0.7327, R:0.0105)
Batch 275/537: Loss=0.6890 (C:0.6890, R:0.0105)
Batch 300/537: Loss=0.7228 (C:0.7228, R:0.0105)
Batch 325/537: Loss=0.7035 (C:0.7035, R:0.0105)
Batch 350/537: Loss=0.7663 (C:0.7663, R:0.0105)
Batch 375/537: Loss=0.7367 (C:0.7367, R:0.0105)
Batch 400/537: Loss=0.7402 (C:0.7402, R:0.0105)
Batch 425/537: Loss=0.7422 (C:0.7422, R:0.0106)
Batch 450/537: Loss=0.7049 (C:0.7049, R:0.0105)
Batch 475/537: Loss=0.7192 (C:0.7192, R:0.0105)
Batch 500/537: Loss=0.7687 (C:0.7687, R:0.0106)
Batch 525/537: Loss=0.7130 (C:0.7130, R:0.0105)

============================================================
Epoch 33/100 completed in 21.5s
Train: Loss=0.7136 (C:0.7136, R:0.0105) Ratio=4.17x
Val:   Loss=0.8089 (C:0.8089, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.045
‚úÖ New best model saved (Val Loss: 0.8089)
============================================================

üåç Updating global dataset at epoch 34
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.403 ¬± 0.639
    Neg distances: 2.425 ¬± 1.066
    Separation ratio: 6.02x
    Gap: -4.217
    ‚úÖ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=0.6704 (C:0.6704, R:0.0105)
Batch  25/537: Loss=0.7101 (C:0.7101, R:0.0105)
Batch  50/537: Loss=0.7183 (C:0.7183, R:0.0105)
Batch  75/537: Loss=0.6981 (C:0.6981, R:0.0105)
Batch 100/537: Loss=0.6968 (C:0.6968, R:0.0105)
Batch 125/537: Loss=0.7043 (C:0.7043, R:0.0105)
Batch 150/537: Loss=0.7121 (C:0.7121, R:0.0105)
Batch 175/537: Loss=0.6924 (C:0.6924, R:0.0105)
Batch 200/537: Loss=0.7048 (C:0.7048, R:0.0105)
Batch 225/537: Loss=0.7301 (C:0.7301, R:0.0105)
Batch 250/537: Loss=0.7370 (C:0.7370, R:0.0105)
Batch 275/537: Loss=0.7497 (C:0.7497, R:0.0105)
Batch 300/537: Loss=0.6762 (C:0.6762, R:0.0105)
Batch 325/537: Loss=0.7062 (C:0.7062, R:0.0105)
Batch 350/537: Loss=0.6776 (C:0.6776, R:0.0105)
Batch 375/537: Loss=0.7128 (C:0.7128, R:0.0105)
Batch 400/537: Loss=0.6871 (C:0.6871, R:0.0105)
Batch 425/537: Loss=0.6958 (C:0.6958, R:0.0105)
Batch 450/537: Loss=0.7291 (C:0.7291, R:0.0105)
Batch 475/537: Loss=0.6902 (C:0.6902, R:0.0105)
Batch 500/537: Loss=0.7205 (C:0.7205, R:0.0105)
Batch 525/537: Loss=0.7128 (C:0.7128, R:0.0105)

============================================================
Epoch 34/100 completed in 27.0s
Train: Loss=0.6943 (C:0.6943, R:0.0105) Ratio=4.18x
Val:   Loss=0.7942 (C:0.7942, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.060
‚úÖ New best model saved (Val Loss: 0.7942)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=0.6537 (C:0.6537, R:0.0105)
Batch  25/537: Loss=0.7019 (C:0.7019, R:0.0106)
Batch  50/537: Loss=0.7044 (C:0.7044, R:0.0105)
Batch  75/537: Loss=0.6531 (C:0.6531, R:0.0105)
Batch 100/537: Loss=0.6798 (C:0.6798, R:0.0106)
Batch 125/537: Loss=0.7211 (C:0.7211, R:0.0105)
Batch 150/537: Loss=0.6756 (C:0.6756, R:0.0105)
Batch 175/537: Loss=0.6884 (C:0.6884, R:0.0105)
Batch 200/537: Loss=0.6830 (C:0.6830, R:0.0105)
Batch 225/537: Loss=0.6955 (C:0.6955, R:0.0105)
Batch 250/537: Loss=0.7227 (C:0.7227, R:0.0105)
Batch 275/537: Loss=0.7074 (C:0.7074, R:0.0105)
Batch 300/537: Loss=0.6670 (C:0.6670, R:0.0105)
Batch 325/537: Loss=0.6900 (C:0.6900, R:0.0105)
Batch 350/537: Loss=0.7252 (C:0.7252, R:0.0105)
Batch 375/537: Loss=0.7137 (C:0.7137, R:0.0105)
Batch 400/537: Loss=0.7116 (C:0.7116, R:0.0105)
Batch 425/537: Loss=0.7050 (C:0.7050, R:0.0105)
Batch 450/537: Loss=0.6744 (C:0.6744, R:0.0105)
Batch 475/537: Loss=0.7229 (C:0.7229, R:0.0105)
Batch 500/537: Loss=0.7071 (C:0.7071, R:0.0105)
Batch 525/537: Loss=0.7093 (C:0.7093, R:0.0105)

============================================================
Epoch 35/100 completed in 21.6s
Train: Loss=0.6933 (C:0.6933, R:0.0105) Ratio=4.37x
Val:   Loss=0.7909 (C:0.7909, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.075
‚úÖ New best model saved (Val Loss: 0.7909)
Checkpoint saved at epoch 35
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=0.6642 (C:0.6642, R:0.0105)
Batch  25/537: Loss=0.7374 (C:0.7374, R:0.0105)
Batch  50/537: Loss=0.6783 (C:0.6783, R:0.0105)
Batch  75/537: Loss=0.7013 (C:0.7013, R:0.0105)
Batch 100/537: Loss=0.6509 (C:0.6509, R:0.0105)
Batch 125/537: Loss=0.6544 (C:0.6544, R:0.0105)
Batch 150/537: Loss=0.7175 (C:0.7175, R:0.0105)
Batch 175/537: Loss=0.7048 (C:0.7048, R:0.0105)
Batch 200/537: Loss=0.6708 (C:0.6708, R:0.0105)
Batch 225/537: Loss=0.7018 (C:0.7018, R:0.0105)
Batch 250/537: Loss=0.6754 (C:0.6754, R:0.0105)
Batch 275/537: Loss=0.7455 (C:0.7455, R:0.0105)
Batch 300/537: Loss=0.7022 (C:0.7022, R:0.0105)
Batch 325/537: Loss=0.7062 (C:0.7062, R:0.0105)
Batch 350/537: Loss=0.6601 (C:0.6601, R:0.0105)
Batch 375/537: Loss=0.7245 (C:0.7245, R:0.0105)
Batch 400/537: Loss=0.6852 (C:0.6852, R:0.0105)
Batch 425/537: Loss=0.6839 (C:0.6839, R:0.0106)
Batch 450/537: Loss=0.6798 (C:0.6798, R:0.0105)
Batch 475/537: Loss=0.6873 (C:0.6873, R:0.0105)
Batch 500/537: Loss=0.6950 (C:0.6950, R:0.0105)
Batch 525/537: Loss=0.6947 (C:0.6947, R:0.0105)

============================================================
Epoch 36/100 completed in 21.6s
Train: Loss=0.6917 (C:0.6917, R:0.0105) Ratio=4.35x
Val:   Loss=0.7992 (C:0.7992, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.090
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 37
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.360 ¬± 0.576
    Neg distances: 2.472 ¬± 1.060
    Separation ratio: 6.87x
    Gap: -4.227
    ‚úÖ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=0.6333 (C:0.6333, R:0.0105)
Batch  25/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch  50/537: Loss=0.6708 (C:0.6708, R:0.0105)
Batch  75/537: Loss=0.6752 (C:0.6752, R:0.0105)
Batch 100/537: Loss=0.6515 (C:0.6515, R:0.0105)
Batch 125/537: Loss=0.6289 (C:0.6289, R:0.0105)
Batch 150/537: Loss=0.6689 (C:0.6689, R:0.0105)
Batch 175/537: Loss=0.6032 (C:0.6032, R:0.0105)
Batch 200/537: Loss=0.6286 (C:0.6286, R:0.0105)
Batch 225/537: Loss=0.6556 (C:0.6556, R:0.0105)
Batch 250/537: Loss=0.6555 (C:0.6555, R:0.0105)
Batch 275/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 300/537: Loss=0.6430 (C:0.6430, R:0.0106)
Batch 325/537: Loss=0.6668 (C:0.6668, R:0.0105)
Batch 350/537: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 375/537: Loss=0.6506 (C:0.6506, R:0.0105)
Batch 400/537: Loss=0.6289 (C:0.6289, R:0.0105)
Batch 425/537: Loss=0.6841 (C:0.6841, R:0.0105)
Batch 450/537: Loss=0.6591 (C:0.6591, R:0.0105)
Batch 475/537: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 500/537: Loss=0.6411 (C:0.6411, R:0.0105)
Batch 525/537: Loss=0.6802 (C:0.6802, R:0.0105)

============================================================
Epoch 37/100 completed in 27.0s
Train: Loss=0.6498 (C:0.6498, R:0.0105) Ratio=4.41x
Val:   Loss=0.7574 (C:0.7574, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.105
‚úÖ New best model saved (Val Loss: 0.7574)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=0.6248 (C:0.6248, R:0.0105)
Batch  25/537: Loss=0.6319 (C:0.6319, R:0.0105)
Batch  50/537: Loss=0.6049 (C:0.6049, R:0.0105)
Batch  75/537: Loss=0.6499 (C:0.6499, R:0.0105)
Batch 100/537: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 125/537: Loss=0.5999 (C:0.5999, R:0.0105)
Batch 150/537: Loss=0.6523 (C:0.6523, R:0.0105)
Batch 175/537: Loss=0.6610 (C:0.6610, R:0.0105)
Batch 200/537: Loss=0.6724 (C:0.6724, R:0.0105)
Batch 225/537: Loss=0.6648 (C:0.6648, R:0.0105)
Batch 250/537: Loss=0.6678 (C:0.6678, R:0.0106)
Batch 275/537: Loss=0.6519 (C:0.6519, R:0.0105)
Batch 300/537: Loss=0.6495 (C:0.6495, R:0.0105)
Batch 325/537: Loss=0.6517 (C:0.6517, R:0.0105)
Batch 350/537: Loss=0.7119 (C:0.7119, R:0.0105)
Batch 375/537: Loss=0.7006 (C:0.7006, R:0.0105)
Batch 400/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 425/537: Loss=0.6597 (C:0.6597, R:0.0105)
Batch 450/537: Loss=0.6602 (C:0.6602, R:0.0105)
Batch 475/537: Loss=0.6598 (C:0.6598, R:0.0105)
Batch 500/537: Loss=0.6705 (C:0.6705, R:0.0106)
Batch 525/537: Loss=0.6438 (C:0.6438, R:0.0105)

============================================================
Epoch 38/100 completed in 21.2s
Train: Loss=0.6484 (C:0.6484, R:0.0105) Ratio=4.39x
Val:   Loss=0.7618 (C:0.7618, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.120
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=0.6463 (C:0.6463, R:0.0105)
Batch  25/537: Loss=0.6042 (C:0.6042, R:0.0105)
Batch  50/537: Loss=0.6303 (C:0.6303, R:0.0105)
Batch  75/537: Loss=0.6661 (C:0.6661, R:0.0105)
Batch 100/537: Loss=0.6355 (C:0.6355, R:0.0105)
Batch 125/537: Loss=0.6712 (C:0.6712, R:0.0105)
Batch 150/537: Loss=0.6234 (C:0.6234, R:0.0105)
Batch 175/537: Loss=0.6635 (C:0.6635, R:0.0105)
Batch 200/537: Loss=0.6719 (C:0.6719, R:0.0105)
Batch 225/537: Loss=0.6558 (C:0.6558, R:0.0105)
Batch 250/537: Loss=0.6532 (C:0.6532, R:0.0105)
Batch 275/537: Loss=0.6332 (C:0.6332, R:0.0105)
Batch 300/537: Loss=0.6476 (C:0.6476, R:0.0105)
Batch 325/537: Loss=0.6867 (C:0.6867, R:0.0105)
Batch 350/537: Loss=0.6305 (C:0.6305, R:0.0105)
Batch 375/537: Loss=0.6685 (C:0.6685, R:0.0105)
Batch 400/537: Loss=0.6440 (C:0.6440, R:0.0105)
Batch 425/537: Loss=0.6069 (C:0.6069, R:0.0105)
Batch 450/537: Loss=0.6308 (C:0.6308, R:0.0105)
Batch 475/537: Loss=0.6585 (C:0.6585, R:0.0105)
Batch 500/537: Loss=0.6539 (C:0.6539, R:0.0105)
Batch 525/537: Loss=0.6291 (C:0.6291, R:0.0105)

============================================================
Epoch 39/100 completed in 21.8s
Train: Loss=0.6445 (C:0.6445, R:0.0105) Ratio=4.47x
Val:   Loss=0.7636 (C:0.7636, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.135
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 40
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.371 ¬± 0.594
    Neg distances: 2.489 ¬± 1.073
    Separation ratio: 6.71x
    Gap: -4.341
    ‚úÖ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=0.6469 (C:0.6469, R:0.0105)
Batch  25/537: Loss=0.5841 (C:0.5841, R:0.0105)
Batch  50/537: Loss=0.6532 (C:0.6532, R:0.0105)
Batch  75/537: Loss=0.6236 (C:0.6236, R:0.0105)
Batch 100/537: Loss=0.6382 (C:0.6382, R:0.0105)
Batch 125/537: Loss=0.6623 (C:0.6623, R:0.0105)
Batch 150/537: Loss=0.6407 (C:0.6407, R:0.0105)
Batch 175/537: Loss=0.6803 (C:0.6803, R:0.0105)
Batch 200/537: Loss=0.6691 (C:0.6691, R:0.0105)
Batch 225/537: Loss=0.6369 (C:0.6369, R:0.0105)
Batch 250/537: Loss=0.6333 (C:0.6333, R:0.0105)
Batch 275/537: Loss=0.6574 (C:0.6574, R:0.0105)
Batch 300/537: Loss=0.6554 (C:0.6554, R:0.0105)
Batch 325/537: Loss=0.6278 (C:0.6278, R:0.0105)
Batch 350/537: Loss=0.6547 (C:0.6547, R:0.0105)
Batch 375/537: Loss=0.6636 (C:0.6636, R:0.0105)
Batch 400/537: Loss=0.6436 (C:0.6436, R:0.0105)
Batch 425/537: Loss=0.6852 (C:0.6852, R:0.0105)
Batch 450/537: Loss=0.6526 (C:0.6526, R:0.0105)
Batch 475/537: Loss=0.6380 (C:0.6380, R:0.0105)
Batch 500/537: Loss=0.6977 (C:0.6977, R:0.0105)
Batch 525/537: Loss=0.6634 (C:0.6634, R:0.0105)

============================================================
Epoch 40/100 completed in 27.7s
Train: Loss=0.6457 (C:0.6457, R:0.0105) Ratio=4.43x
Val:   Loss=0.7721 (C:0.7721, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.150
No improvement for 3 epochs
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=0.6278 (C:0.6278, R:0.0105)
Batch  25/537: Loss=0.6449 (C:0.6449, R:0.0105)
Batch  50/537: Loss=0.6387 (C:0.6387, R:0.0105)
Batch  75/537: Loss=0.6248 (C:0.6248, R:0.0105)
Batch 100/537: Loss=0.6527 (C:0.6527, R:0.0105)
Batch 125/537: Loss=0.6120 (C:0.6120, R:0.0105)
Batch 150/537: Loss=0.6378 (C:0.6378, R:0.0105)
Batch 175/537: Loss=0.6329 (C:0.6329, R:0.0105)
Batch 200/537: Loss=0.6345 (C:0.6345, R:0.0105)
Batch 225/537: Loss=0.6492 (C:0.6492, R:0.0105)
Batch 250/537: Loss=0.6550 (C:0.6550, R:0.0105)
Batch 275/537: Loss=0.6455 (C:0.6455, R:0.0106)
Batch 300/537: Loss=0.6716 (C:0.6716, R:0.0105)
Batch 325/537: Loss=0.6374 (C:0.6374, R:0.0105)
Batch 350/537: Loss=0.6274 (C:0.6274, R:0.0105)
Batch 375/537: Loss=0.6699 (C:0.6699, R:0.0105)
Batch 400/537: Loss=0.5938 (C:0.5938, R:0.0105)
Batch 425/537: Loss=0.6418 (C:0.6418, R:0.0105)
Batch 450/537: Loss=0.6351 (C:0.6351, R:0.0105)
Batch 475/537: Loss=0.6459 (C:0.6459, R:0.0105)
Batch 500/537: Loss=0.6467 (C:0.6467, R:0.0105)
Batch 525/537: Loss=0.6415 (C:0.6415, R:0.0105)

============================================================
Epoch 41/100 completed in 21.6s
Train: Loss=0.6432 (C:0.6432, R:0.0105) Ratio=4.50x
Val:   Loss=0.7608 (C:0.7608, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.165
No improvement for 4 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=0.6481 (C:0.6481, R:0.0105)
Batch  25/537: Loss=0.6489 (C:0.6489, R:0.0105)
Batch  50/537: Loss=0.6215 (C:0.6215, R:0.0105)
Batch  75/537: Loss=0.6481 (C:0.6481, R:0.0105)
Batch 100/537: Loss=0.6090 (C:0.6090, R:0.0105)
Batch 125/537: Loss=0.6473 (C:0.6473, R:0.0105)
Batch 150/537: Loss=0.6430 (C:0.6430, R:0.0105)
Batch 175/537: Loss=0.6206 (C:0.6206, R:0.0105)
Batch 200/537: Loss=0.6505 (C:0.6505, R:0.0105)
Batch 225/537: Loss=0.6416 (C:0.6416, R:0.0105)
Batch 250/537: Loss=0.6690 (C:0.6690, R:0.0105)
Batch 275/537: Loss=0.6719 (C:0.6719, R:0.0106)
Batch 300/537: Loss=0.6627 (C:0.6627, R:0.0105)
Batch 325/537: Loss=0.6416 (C:0.6416, R:0.0105)
Batch 350/537: Loss=0.6339 (C:0.6339, R:0.0105)
Batch 375/537: Loss=0.6275 (C:0.6275, R:0.0105)
Batch 400/537: Loss=0.6475 (C:0.6475, R:0.0105)
Batch 425/537: Loss=0.6421 (C:0.6421, R:0.0105)
Batch 450/537: Loss=0.6367 (C:0.6367, R:0.0105)
Batch 475/537: Loss=0.6693 (C:0.6693, R:0.0106)
Batch 500/537: Loss=0.6020 (C:0.6020, R:0.0105)
Batch 525/537: Loss=0.6280 (C:0.6280, R:0.0105)

============================================================
Epoch 42/100 completed in 21.3s
Train: Loss=0.6409 (C:0.6409, R:0.0105) Ratio=4.52x
Val:   Loss=0.7704 (C:0.7704, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.180
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 43
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.355 ¬± 0.581
    Neg distances: 2.521 ¬± 1.073
    Separation ratio: 7.10x
    Gap: -4.448
    ‚úÖ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=0.6269 (C:0.6269, R:0.0105)
Batch  25/537: Loss=0.6261 (C:0.6261, R:0.0105)
Batch  50/537: Loss=0.6014 (C:0.6014, R:0.0105)
Batch  75/537: Loss=0.6392 (C:0.6392, R:0.0105)
Batch 100/537: Loss=0.5865 (C:0.5865, R:0.0105)
Batch 125/537: Loss=0.6257 (C:0.6257, R:0.0105)
Batch 150/537: Loss=0.6401 (C:0.6401, R:0.0105)
Batch 175/537: Loss=0.6478 (C:0.6478, R:0.0105)
Batch 200/537: Loss=0.6117 (C:0.6117, R:0.0105)
Batch 225/537: Loss=0.6419 (C:0.6419, R:0.0105)
Batch 250/537: Loss=0.6531 (C:0.6531, R:0.0105)
Batch 275/537: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 300/537: Loss=0.6320 (C:0.6320, R:0.0105)
Batch 325/537: Loss=0.6322 (C:0.6322, R:0.0105)
Batch 350/537: Loss=0.6242 (C:0.6242, R:0.0105)
Batch 375/537: Loss=0.6079 (C:0.6079, R:0.0105)
Batch 400/537: Loss=0.6358 (C:0.6358, R:0.0105)
Batch 425/537: Loss=0.6480 (C:0.6480, R:0.0105)
Batch 450/537: Loss=0.6210 (C:0.6210, R:0.0105)
Batch 475/537: Loss=0.6175 (C:0.6175, R:0.0106)
Batch 500/537: Loss=0.6288 (C:0.6288, R:0.0105)
Batch 525/537: Loss=0.6425 (C:0.6425, R:0.0105)

============================================================
Epoch 43/100 completed in 27.0s
Train: Loss=0.6230 (C:0.6230, R:0.0105) Ratio=4.49x
Val:   Loss=0.7445 (C:0.7445, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.195
‚úÖ New best model saved (Val Loss: 0.7445)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=0.5970 (C:0.5970, R:0.0105)
Batch  25/537: Loss=0.6075 (C:0.6075, R:0.0105)
Batch  50/537: Loss=0.6037 (C:0.6037, R:0.0106)
Batch  75/537: Loss=0.6171 (C:0.6171, R:0.0105)
Batch 100/537: Loss=0.6006 (C:0.6006, R:0.0105)
Batch 125/537: Loss=0.6302 (C:0.6302, R:0.0105)
Batch 150/537: Loss=0.6431 (C:0.6431, R:0.0105)
Batch 175/537: Loss=0.6179 (C:0.6179, R:0.0105)
Batch 200/537: Loss=0.6123 (C:0.6123, R:0.0106)
Batch 225/537: Loss=0.6388 (C:0.6388, R:0.0105)
Batch 250/537: Loss=0.5788 (C:0.5788, R:0.0106)
Batch 275/537: Loss=0.6089 (C:0.6089, R:0.0105)
Batch 300/537: Loss=0.6347 (C:0.6347, R:0.0105)
Batch 325/537: Loss=0.6300 (C:0.6300, R:0.0105)
Batch 350/537: Loss=0.6211 (C:0.6211, R:0.0105)
Batch 375/537: Loss=0.6153 (C:0.6153, R:0.0105)
Batch 400/537: Loss=0.6105 (C:0.6105, R:0.0105)
Batch 425/537: Loss=0.6313 (C:0.6313, R:0.0105)
Batch 450/537: Loss=0.5892 (C:0.5892, R:0.0106)
Batch 475/537: Loss=0.6452 (C:0.6452, R:0.0105)
Batch 500/537: Loss=0.6376 (C:0.6376, R:0.0105)
Batch 525/537: Loss=0.6185 (C:0.6185, R:0.0105)

============================================================
Epoch 44/100 completed in 21.1s
Train: Loss=0.6212 (C:0.6212, R:0.0105) Ratio=4.65x
Val:   Loss=0.7532 (C:0.7532, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.210
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=0.5908 (C:0.5908, R:0.0105)
Batch  25/537: Loss=0.6062 (C:0.6062, R:0.0105)
Batch  50/537: Loss=0.5885 (C:0.5885, R:0.0105)
Batch  75/537: Loss=0.6059 (C:0.6059, R:0.0105)
Batch 100/537: Loss=0.6039 (C:0.6039, R:0.0105)
Batch 125/537: Loss=0.6352 (C:0.6352, R:0.0105)
Batch 150/537: Loss=0.5305 (C:0.5305, R:0.0105)
Batch 175/537: Loss=0.5807 (C:0.5807, R:0.0105)
Batch 200/537: Loss=0.6361 (C:0.6361, R:0.0105)
Batch 225/537: Loss=0.6477 (C:0.6477, R:0.0105)
Batch 250/537: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 275/537: Loss=0.5911 (C:0.5911, R:0.0105)
Batch 300/537: Loss=0.5977 (C:0.5977, R:0.0105)
Batch 325/537: Loss=0.6372 (C:0.6372, R:0.0105)
Batch 350/537: Loss=0.6389 (C:0.6389, R:0.0105)
Batch 375/537: Loss=0.6453 (C:0.6453, R:0.0105)
Batch 400/537: Loss=0.5907 (C:0.5907, R:0.0105)
Batch 425/537: Loss=0.6494 (C:0.6494, R:0.0105)
Batch 450/537: Loss=0.6457 (C:0.6457, R:0.0105)
Batch 475/537: Loss=0.6163 (C:0.6163, R:0.0105)
Batch 500/537: Loss=0.6118 (C:0.6118, R:0.0105)
Batch 525/537: Loss=0.6183 (C:0.6183, R:0.0105)

============================================================
Epoch 45/100 completed in 21.4s
Train: Loss=0.6199 (C:0.6199, R:0.0105) Ratio=4.72x
Val:   Loss=0.7467 (C:0.7467, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.225
No improvement for 2 epochs
Checkpoint saved at epoch 45
============================================================

üåç Updating global dataset at epoch 46
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.359 ¬± 0.609
    Neg distances: 2.574 ¬± 1.099
    Separation ratio: 7.18x
    Gap: -4.432
    ‚úÖ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=0.6280 (C:0.6280, R:0.0105)
Batch  25/537: Loss=0.5800 (C:0.5800, R:0.0105)
Batch  50/537: Loss=0.6246 (C:0.6246, R:0.0105)
Batch  75/537: Loss=0.6178 (C:0.6178, R:0.0105)
Batch 100/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 125/537: Loss=0.6047 (C:0.6047, R:0.0106)
Batch 150/537: Loss=0.6331 (C:0.6331, R:0.0105)
Batch 175/537: Loss=0.6473 (C:0.6473, R:0.0105)
Batch 200/537: Loss=0.6152 (C:0.6152, R:0.0105)
Batch 225/537: Loss=0.6383 (C:0.6383, R:0.0105)
Batch 250/537: Loss=0.6579 (C:0.6579, R:0.0105)
Batch 275/537: Loss=0.6233 (C:0.6233, R:0.0105)
Batch 300/537: Loss=0.6575 (C:0.6575, R:0.0105)
Batch 325/537: Loss=0.6270 (C:0.6270, R:0.0105)
Batch 350/537: Loss=0.6126 (C:0.6126, R:0.0105)
Batch 375/537: Loss=0.6005 (C:0.6005, R:0.0105)
Batch 400/537: Loss=0.6279 (C:0.6279, R:0.0105)
Batch 425/537: Loss=0.6041 (C:0.6041, R:0.0105)
Batch 450/537: Loss=0.6298 (C:0.6298, R:0.0105)
Batch 475/537: Loss=0.5839 (C:0.5839, R:0.0105)
Batch 500/537: Loss=0.5791 (C:0.5791, R:0.0105)
Batch 525/537: Loss=0.6620 (C:0.6620, R:0.0105)

============================================================
Epoch 46/100 completed in 27.7s
Train: Loss=0.6158 (C:0.6158, R:0.0105) Ratio=4.58x
Val:   Loss=0.7487 (C:0.7487, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.240
No improvement for 3 epochs
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.5881 (C:0.5881, R:0.0105)
Batch  25/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch  50/537: Loss=0.6179 (C:0.6179, R:0.0105)
Batch  75/537: Loss=0.6292 (C:0.6292, R:0.0105)
Batch 100/537: Loss=0.5950 (C:0.5950, R:0.0105)
Batch 125/537: Loss=0.6123 (C:0.6123, R:0.0105)
Batch 150/537: Loss=0.6241 (C:0.6241, R:0.0105)
Batch 175/537: Loss=0.5859 (C:0.5859, R:0.0105)
Batch 200/537: Loss=0.5991 (C:0.5991, R:0.0105)
Batch 225/537: Loss=0.6269 (C:0.6269, R:0.0105)
Batch 250/537: Loss=0.6225 (C:0.6225, R:0.0105)
Batch 275/537: Loss=0.5933 (C:0.5933, R:0.0105)
Batch 300/537: Loss=0.6129 (C:0.6129, R:0.0105)
Batch 325/537: Loss=0.6302 (C:0.6302, R:0.0105)
Batch 350/537: Loss=0.5868 (C:0.5868, R:0.0105)
Batch 375/537: Loss=0.6065 (C:0.6065, R:0.0105)
Batch 400/537: Loss=0.6344 (C:0.6344, R:0.0105)
Batch 425/537: Loss=0.6243 (C:0.6243, R:0.0105)
Batch 450/537: Loss=0.6201 (C:0.6201, R:0.0105)
Batch 475/537: Loss=0.6200 (C:0.6200, R:0.0105)
Batch 500/537: Loss=0.6376 (C:0.6376, R:0.0105)
Batch 525/537: Loss=0.5800 (C:0.5800, R:0.0105)

============================================================
Epoch 47/100 completed in 21.5s
Train: Loss=0.6141 (C:0.6141, R:0.0105) Ratio=4.75x
Val:   Loss=0.7472 (C:0.7472, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.255
No improvement for 4 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.6260 (C:0.6260, R:0.0105)
Batch  25/537: Loss=0.5810 (C:0.5810, R:0.0105)
Batch  50/537: Loss=0.6155 (C:0.6155, R:0.0105)
Batch  75/537: Loss=0.6106 (C:0.6106, R:0.0105)
Batch 100/537: Loss=0.5780 (C:0.5780, R:0.0105)
Batch 125/537: Loss=0.5971 (C:0.5971, R:0.0105)
Batch 150/537: Loss=0.5999 (C:0.5999, R:0.0105)
Batch 175/537: Loss=0.6038 (C:0.6038, R:0.0105)
Batch 200/537: Loss=0.6195 (C:0.6195, R:0.0105)
Batch 225/537: Loss=0.6353 (C:0.6353, R:0.0105)
Batch 250/537: Loss=0.6166 (C:0.6166, R:0.0105)
Batch 275/537: Loss=0.5839 (C:0.5839, R:0.0105)
Batch 300/537: Loss=0.6549 (C:0.6549, R:0.0105)
Batch 325/537: Loss=0.6157 (C:0.6157, R:0.0105)
Batch 350/537: Loss=0.6679 (C:0.6679, R:0.0105)
Batch 375/537: Loss=0.6439 (C:0.6439, R:0.0105)
Batch 400/537: Loss=0.5931 (C:0.5931, R:0.0105)
Batch 425/537: Loss=0.6252 (C:0.6252, R:0.0105)
Batch 450/537: Loss=0.6146 (C:0.6146, R:0.0105)
Batch 475/537: Loss=0.6263 (C:0.6263, R:0.0105)
Batch 500/537: Loss=0.6495 (C:0.6495, R:0.0105)
Batch 525/537: Loss=0.6320 (C:0.6320, R:0.0105)

============================================================
Epoch 48/100 completed in 21.3s
Train: Loss=0.6126 (C:0.6126, R:0.0105) Ratio=4.62x
Val:   Loss=0.7471 (C:0.7471, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.270
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 49
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.357 ¬± 0.615
    Neg distances: 2.589 ¬± 1.103
    Separation ratio: 7.26x
    Gap: -4.474
    ‚úÖ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.5866 (C:0.5866, R:0.0105)
Batch  25/537: Loss=0.5793 (C:0.5793, R:0.0105)
Batch  50/537: Loss=0.6044 (C:0.6044, R:0.0105)
Batch  75/537: Loss=0.5874 (C:0.5874, R:0.0105)
Batch 100/537: Loss=0.5781 (C:0.5781, R:0.0105)
Batch 125/537: Loss=0.5805 (C:0.5805, R:0.0105)
Batch 150/537: Loss=0.6055 (C:0.6055, R:0.0105)
Batch 175/537: Loss=0.6463 (C:0.6463, R:0.0105)
Batch 200/537: Loss=0.6371 (C:0.6371, R:0.0105)
Batch 225/537: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 250/537: Loss=0.6353 (C:0.6353, R:0.0105)
Batch 275/537: Loss=0.6433 (C:0.6433, R:0.0105)
Batch 300/537: Loss=0.6162 (C:0.6162, R:0.0105)
Batch 325/537: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 350/537: Loss=0.6190 (C:0.6190, R:0.0105)
Batch 375/537: Loss=0.5770 (C:0.5770, R:0.0105)
Batch 400/537: Loss=0.5820 (C:0.5820, R:0.0105)
Batch 425/537: Loss=0.6132 (C:0.6132, R:0.0106)
Batch 450/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 475/537: Loss=0.6546 (C:0.6546, R:0.0105)
Batch 500/537: Loss=0.6145 (C:0.6145, R:0.0105)
Batch 525/537: Loss=0.6289 (C:0.6289, R:0.0105)

============================================================
Epoch 49/100 completed in 27.0s
Train: Loss=0.6063 (C:0.6063, R:0.0105) Ratio=4.79x
Val:   Loss=0.7419 (C:0.7419, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.285
‚úÖ New best model saved (Val Loss: 0.7419)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.5637 (C:0.5637, R:0.0105)
Batch  25/537: Loss=0.6096 (C:0.6096, R:0.0105)
Batch  50/537: Loss=0.5954 (C:0.5954, R:0.0105)
Batch  75/537: Loss=0.6173 (C:0.6173, R:0.0105)
Batch 100/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 125/537: Loss=0.5818 (C:0.5818, R:0.0105)
Batch 150/537: Loss=0.5981 (C:0.5981, R:0.0105)
Batch 175/537: Loss=0.5810 (C:0.5810, R:0.0105)
Batch 200/537: Loss=0.6040 (C:0.6040, R:0.0105)
Batch 225/537: Loss=0.6148 (C:0.6148, R:0.0105)
Batch 250/537: Loss=0.6228 (C:0.6228, R:0.0105)
Batch 275/537: Loss=0.6164 (C:0.6164, R:0.0105)
Batch 300/537: Loss=0.5979 (C:0.5979, R:0.0105)
Batch 325/537: Loss=0.6121 (C:0.6121, R:0.0105)
Batch 350/537: Loss=0.6111 (C:0.6111, R:0.0105)
Batch 375/537: Loss=0.6345 (C:0.6345, R:0.0105)
Batch 400/537: Loss=0.5861 (C:0.5861, R:0.0105)
Batch 425/537: Loss=0.6404 (C:0.6404, R:0.0105)
Batch 450/537: Loss=0.6314 (C:0.6314, R:0.0105)
Batch 475/537: Loss=0.6192 (C:0.6192, R:0.0105)
Batch 500/537: Loss=0.6224 (C:0.6224, R:0.0105)
Batch 525/537: Loss=0.6213 (C:0.6213, R:0.0105)

============================================================
Epoch 50/100 completed in 21.6s
Train: Loss=0.6053 (C:0.6053, R:0.0105) Ratio=4.78x
Val:   Loss=0.7527 (C:0.7527, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 50
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.6063 (C:0.6063, R:0.0105)
Batch  25/537: Loss=0.5986 (C:0.5986, R:0.0105)
Batch  50/537: Loss=0.6110 (C:0.6110, R:0.0105)
Batch  75/537: Loss=0.6100 (C:0.6100, R:0.0105)
Batch 100/537: Loss=0.6346 (C:0.6346, R:0.0105)
Batch 125/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 150/537: Loss=0.6296 (C:0.6296, R:0.0105)
Batch 175/537: Loss=0.5670 (C:0.5670, R:0.0105)
Batch 200/537: Loss=0.5994 (C:0.5994, R:0.0105)
Batch 225/537: Loss=0.6129 (C:0.6129, R:0.0105)
Batch 250/537: Loss=0.5911 (C:0.5911, R:0.0105)
Batch 275/537: Loss=0.5987 (C:0.5987, R:0.0105)
Batch 300/537: Loss=0.6116 (C:0.6116, R:0.0105)
Batch 325/537: Loss=0.5829 (C:0.5829, R:0.0105)
Batch 350/537: Loss=0.6088 (C:0.6088, R:0.0105)
Batch 375/537: Loss=0.5945 (C:0.5945, R:0.0105)
Batch 400/537: Loss=0.6068 (C:0.6068, R:0.0105)
Batch 425/537: Loss=0.6268 (C:0.6268, R:0.0105)
Batch 450/537: Loss=0.6009 (C:0.6009, R:0.0105)
Batch 475/537: Loss=0.6305 (C:0.6305, R:0.0105)
Batch 500/537: Loss=0.6023 (C:0.6023, R:0.0106)
Batch 525/537: Loss=0.6082 (C:0.6082, R:0.0105)

============================================================
Epoch 51/100 completed in 21.3s
Train: Loss=0.6028 (C:0.6028, R:0.0105) Ratio=4.75x
Val:   Loss=0.7423 (C:0.7423, R:0.0104) Ratio=3.11x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 52
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.338 ¬± 0.601
    Neg distances: 2.584 ¬± 1.089
    Separation ratio: 7.65x
    Gap: -4.484
    ‚úÖ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.5939 (C:0.5939, R:0.0105)
Batch  25/537: Loss=0.5732 (C:0.5732, R:0.0105)
Batch  50/537: Loss=0.5622 (C:0.5622, R:0.0105)
Batch  75/537: Loss=0.5725 (C:0.5725, R:0.0105)
Batch 100/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 125/537: Loss=0.5700 (C:0.5700, R:0.0106)
Batch 150/537: Loss=0.5632 (C:0.5632, R:0.0105)
Batch 175/537: Loss=0.6040 (C:0.6040, R:0.0105)
Batch 200/537: Loss=0.5903 (C:0.5903, R:0.0105)
Batch 225/537: Loss=0.5887 (C:0.5887, R:0.0105)
Batch 250/537: Loss=0.5714 (C:0.5714, R:0.0105)
Batch 275/537: Loss=0.5816 (C:0.5816, R:0.0105)
Batch 300/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 325/537: Loss=0.6023 (C:0.6023, R:0.0105)
Batch 350/537: Loss=0.5642 (C:0.5642, R:0.0105)
Batch 375/537: Loss=0.5989 (C:0.5989, R:0.0106)
Batch 400/537: Loss=0.6036 (C:0.6036, R:0.0105)
Batch 425/537: Loss=0.6009 (C:0.6009, R:0.0106)
Batch 450/537: Loss=0.6052 (C:0.6052, R:0.0105)
Batch 475/537: Loss=0.5885 (C:0.5885, R:0.0105)
Batch 500/537: Loss=0.5845 (C:0.5845, R:0.0105)
Batch 525/537: Loss=0.6295 (C:0.6295, R:0.0105)

============================================================
Epoch 52/100 completed in 27.7s
Train: Loss=0.5867 (C:0.5867, R:0.0105) Ratio=4.79x
Val:   Loss=0.7348 (C:0.7348, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.7348)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.5782 (C:0.5782, R:0.0105)
Batch  25/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch  50/537: Loss=0.5650 (C:0.5650, R:0.0105)
Batch  75/537: Loss=0.5806 (C:0.5806, R:0.0105)
Batch 100/537: Loss=0.5854 (C:0.5854, R:0.0105)
Batch 125/537: Loss=0.6081 (C:0.6081, R:0.0105)
Batch 150/537: Loss=0.5766 (C:0.5766, R:0.0105)
Batch 175/537: Loss=0.6114 (C:0.6114, R:0.0105)
Batch 200/537: Loss=0.5683 (C:0.5683, R:0.0105)
Batch 225/537: Loss=0.5896 (C:0.5896, R:0.0105)
Batch 250/537: Loss=0.5830 (C:0.5830, R:0.0106)
Batch 275/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 300/537: Loss=0.5848 (C:0.5848, R:0.0105)
Batch 325/537: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 350/537: Loss=0.5791 (C:0.5791, R:0.0105)
Batch 375/537: Loss=0.5814 (C:0.5814, R:0.0105)
Batch 400/537: Loss=0.5708 (C:0.5708, R:0.0105)
Batch 425/537: Loss=0.6300 (C:0.6300, R:0.0105)
Batch 450/537: Loss=0.5798 (C:0.5798, R:0.0105)
Batch 475/537: Loss=0.6305 (C:0.6305, R:0.0105)
Batch 500/537: Loss=0.5863 (C:0.5863, R:0.0106)
Batch 525/537: Loss=0.5980 (C:0.5980, R:0.0105)

============================================================
Epoch 53/100 completed in 21.3s
Train: Loss=0.5845 (C:0.5845, R:0.0105) Ratio=4.82x
Val:   Loss=0.7257 (C:0.7257, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.7257)
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch  25/537: Loss=0.5850 (C:0.5850, R:0.0105)
Batch  50/537: Loss=0.5256 (C:0.5256, R:0.0106)
Batch  75/537: Loss=0.6001 (C:0.6001, R:0.0106)
Batch 100/537: Loss=0.5891 (C:0.5891, R:0.0105)
Batch 125/537: Loss=0.5958 (C:0.5958, R:0.0105)
Batch 150/537: Loss=0.5756 (C:0.5756, R:0.0105)
Batch 175/537: Loss=0.5811 (C:0.5811, R:0.0105)
Batch 200/537: Loss=0.6005 (C:0.6005, R:0.0105)
Batch 225/537: Loss=0.5610 (C:0.5610, R:0.0105)
Batch 250/537: Loss=0.6014 (C:0.6014, R:0.0105)
Batch 275/537: Loss=0.5944 (C:0.5944, R:0.0105)
Batch 300/537: Loss=0.5586 (C:0.5586, R:0.0105)
Batch 325/537: Loss=0.5832 (C:0.5832, R:0.0105)
Batch 350/537: Loss=0.5876 (C:0.5876, R:0.0105)
Batch 375/537: Loss=0.5962 (C:0.5962, R:0.0105)
Batch 400/537: Loss=0.6238 (C:0.6238, R:0.0105)
Batch 425/537: Loss=0.5748 (C:0.5748, R:0.0105)
Batch 450/537: Loss=0.5764 (C:0.5764, R:0.0105)
Batch 475/537: Loss=0.5867 (C:0.5867, R:0.0105)
Batch 500/537: Loss=0.6276 (C:0.6276, R:0.0105)
Batch 525/537: Loss=0.5820 (C:0.5820, R:0.0105)

============================================================
Epoch 54/100 completed in 21.2s
Train: Loss=0.5828 (C:0.5828, R:0.0105) Ratio=4.85x
Val:   Loss=0.7308 (C:0.7308, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 55
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.352 ¬± 0.641
    Neg distances: 2.635 ¬± 1.120
    Separation ratio: 7.49x
    Gap: -4.465
    ‚úÖ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.5331 (C:0.5331, R:0.0105)
Batch  25/537: Loss=0.5801 (C:0.5801, R:0.0105)
Batch  50/537: Loss=0.6273 (C:0.6273, R:0.0105)
Batch  75/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 100/537: Loss=0.5727 (C:0.5727, R:0.0105)
Batch 125/537: Loss=0.6272 (C:0.6272, R:0.0105)
Batch 150/537: Loss=0.6126 (C:0.6126, R:0.0105)
Batch 175/537: Loss=0.6011 (C:0.6011, R:0.0105)
Batch 200/537: Loss=0.5745 (C:0.5745, R:0.0105)
Batch 225/537: Loss=0.5795 (C:0.5795, R:0.0105)
Batch 250/537: Loss=0.5844 (C:0.5844, R:0.0105)
Batch 275/537: Loss=0.5828 (C:0.5828, R:0.0105)
Batch 300/537: Loss=0.5779 (C:0.5779, R:0.0105)
Batch 325/537: Loss=0.6001 (C:0.6001, R:0.0105)
Batch 350/537: Loss=0.5761 (C:0.5761, R:0.0105)
Batch 375/537: Loss=0.5988 (C:0.5988, R:0.0105)
Batch 400/537: Loss=0.5942 (C:0.5942, R:0.0105)
Batch 425/537: Loss=0.5986 (C:0.5986, R:0.0105)
Batch 450/537: Loss=0.5780 (C:0.5780, R:0.0105)
Batch 475/537: Loss=0.5984 (C:0.5984, R:0.0105)
Batch 500/537: Loss=0.6219 (C:0.6219, R:0.0105)
Batch 525/537: Loss=0.6000 (C:0.6000, R:0.0105)

============================================================
Epoch 55/100 completed in 26.8s
Train: Loss=0.5912 (C:0.5912, R:0.0105) Ratio=4.85x
Val:   Loss=0.7469 (C:0.7469, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 55
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=0.6045 (C:0.6045, R:0.0105)
Batch  25/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch  50/537: Loss=0.5802 (C:0.5802, R:0.0105)
Batch  75/537: Loss=0.5964 (C:0.5964, R:0.0105)
Batch 100/537: Loss=0.5627 (C:0.5627, R:0.0105)
Batch 125/537: Loss=0.5877 (C:0.5877, R:0.0105)
Batch 150/537: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 175/537: Loss=0.5824 (C:0.5824, R:0.0105)
Batch 200/537: Loss=0.6235 (C:0.6235, R:0.0105)
Batch 225/537: Loss=0.6159 (C:0.6159, R:0.0105)
Batch 250/537: Loss=0.5945 (C:0.5945, R:0.0105)
Batch 275/537: Loss=0.5778 (C:0.5778, R:0.0105)
Batch 300/537: Loss=0.5453 (C:0.5453, R:0.0105)
Batch 325/537: Loss=0.5907 (C:0.5907, R:0.0105)
Batch 350/537: Loss=0.6010 (C:0.6010, R:0.0105)
Batch 375/537: Loss=0.6067 (C:0.6067, R:0.0105)
Batch 400/537: Loss=0.5955 (C:0.5955, R:0.0105)
Batch 425/537: Loss=0.5735 (C:0.5735, R:0.0105)
Batch 450/537: Loss=0.5856 (C:0.5856, R:0.0105)
Batch 475/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 500/537: Loss=0.6221 (C:0.6221, R:0.0105)
Batch 525/537: Loss=0.6430 (C:0.6430, R:0.0105)

============================================================
Epoch 56/100 completed in 21.8s
Train: Loss=0.5891 (C:0.5891, R:0.0105) Ratio=4.87x
Val:   Loss=0.7361 (C:0.7361, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.5794 (C:0.5794, R:0.0105)
Batch  25/537: Loss=0.5915 (C:0.5915, R:0.0105)
Batch  50/537: Loss=0.5667 (C:0.5667, R:0.0105)
Batch  75/537: Loss=0.5914 (C:0.5914, R:0.0105)
Batch 100/537: Loss=0.5935 (C:0.5935, R:0.0105)
Batch 125/537: Loss=0.5917 (C:0.5917, R:0.0105)
Batch 150/537: Loss=0.5851 (C:0.5851, R:0.0105)
Batch 175/537: Loss=0.5836 (C:0.5836, R:0.0105)
Batch 200/537: Loss=0.5694 (C:0.5694, R:0.0105)
Batch 225/537: Loss=0.5710 (C:0.5710, R:0.0105)
Batch 250/537: Loss=0.6240 (C:0.6240, R:0.0105)
Batch 275/537: Loss=0.5972 (C:0.5972, R:0.0105)
Batch 300/537: Loss=0.5852 (C:0.5852, R:0.0105)
Batch 325/537: Loss=0.6301 (C:0.6301, R:0.0105)
Batch 350/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 375/537: Loss=0.6002 (C:0.6002, R:0.0105)
Batch 400/537: Loss=0.6220 (C:0.6220, R:0.0106)
Batch 425/537: Loss=0.6364 (C:0.6364, R:0.0105)
Batch 450/537: Loss=0.5754 (C:0.5754, R:0.0105)
Batch 475/537: Loss=0.6281 (C:0.6281, R:0.0105)
Batch 500/537: Loss=0.5903 (C:0.5903, R:0.0105)
Batch 525/537: Loss=0.5423 (C:0.5423, R:0.0105)

============================================================
Epoch 57/100 completed in 21.8s
Train: Loss=0.5874 (C:0.5874, R:0.0105) Ratio=4.96x
Val:   Loss=0.7428 (C:0.7428, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 58
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.316 ¬± 0.563
    Neg distances: 2.659 ¬± 1.110
    Separation ratio: 8.41x
    Gap: -4.428
    ‚úÖ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.5470 (C:0.5470, R:0.0105)
Batch  25/537: Loss=0.5477 (C:0.5477, R:0.0105)
Batch  50/537: Loss=0.5297 (C:0.5297, R:0.0105)
Batch  75/537: Loss=0.5730 (C:0.5730, R:0.0105)
Batch 100/537: Loss=0.5660 (C:0.5660, R:0.0105)
Batch 125/537: Loss=0.5340 (C:0.5340, R:0.0105)
Batch 150/537: Loss=0.5509 (C:0.5509, R:0.0105)
Batch 175/537: Loss=0.5251 (C:0.5251, R:0.0105)
Batch 200/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 225/537: Loss=0.5507 (C:0.5507, R:0.0105)
Batch 250/537: Loss=0.5595 (C:0.5595, R:0.0105)
Batch 275/537: Loss=0.5113 (C:0.5113, R:0.0105)
Batch 300/537: Loss=0.5269 (C:0.5269, R:0.0105)
Batch 325/537: Loss=0.5632 (C:0.5632, R:0.0105)
Batch 350/537: Loss=0.5310 (C:0.5310, R:0.0105)
Batch 375/537: Loss=0.5929 (C:0.5929, R:0.0105)
Batch 400/537: Loss=0.5745 (C:0.5745, R:0.0105)
Batch 425/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 450/537: Loss=0.5717 (C:0.5717, R:0.0105)
Batch 475/537: Loss=0.5572 (C:0.5572, R:0.0105)
Batch 500/537: Loss=0.5628 (C:0.5628, R:0.0106)
Batch 525/537: Loss=0.5973 (C:0.5973, R:0.0105)

============================================================
Epoch 58/100 completed in 26.8s
Train: Loss=0.5577 (C:0.5577, R:0.0105) Ratio=4.96x
Val:   Loss=0.7098 (C:0.7098, R:0.0104) Ratio=3.15x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.7098)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.5392 (C:0.5392, R:0.0105)
Batch  25/537: Loss=0.5539 (C:0.5539, R:0.0105)
Batch  50/537: Loss=0.5704 (C:0.5704, R:0.0105)
Batch  75/537: Loss=0.5932 (C:0.5932, R:0.0105)
Batch 100/537: Loss=0.5447 (C:0.5447, R:0.0105)
Batch 125/537: Loss=0.5816 (C:0.5816, R:0.0105)
Batch 150/537: Loss=0.5427 (C:0.5427, R:0.0106)
Batch 175/537: Loss=0.5119 (C:0.5119, R:0.0106)
Batch 200/537: Loss=0.5536 (C:0.5536, R:0.0105)
Batch 225/537: Loss=0.5644 (C:0.5644, R:0.0105)
Batch 250/537: Loss=0.6025 (C:0.6025, R:0.0105)
Batch 275/537: Loss=0.5460 (C:0.5460, R:0.0105)
Batch 300/537: Loss=0.5582 (C:0.5582, R:0.0105)
Batch 325/537: Loss=0.5541 (C:0.5541, R:0.0105)
Batch 350/537: Loss=0.5937 (C:0.5937, R:0.0105)
Batch 375/537: Loss=0.5734 (C:0.5734, R:0.0105)
Batch 400/537: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 425/537: Loss=0.5568 (C:0.5568, R:0.0105)
Batch 450/537: Loss=0.5922 (C:0.5922, R:0.0105)
Batch 475/537: Loss=0.5381 (C:0.5381, R:0.0105)
Batch 500/537: Loss=0.5634 (C:0.5634, R:0.0105)
Batch 525/537: Loss=0.5653 (C:0.5653, R:0.0106)

============================================================
Epoch 59/100 completed in 21.1s
Train: Loss=0.5557 (C:0.5557, R:0.0105) Ratio=4.89x
Val:   Loss=0.7043 (C:0.7043, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.7043)
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.5459 (C:0.5459, R:0.0105)
Batch  25/537: Loss=0.5007 (C:0.5007, R:0.0105)
Batch  50/537: Loss=0.5349 (C:0.5349, R:0.0105)
Batch  75/537: Loss=0.5386 (C:0.5386, R:0.0105)
Batch 100/537: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 125/537: Loss=0.5630 (C:0.5630, R:0.0105)
Batch 150/537: Loss=0.5287 (C:0.5287, R:0.0105)
Batch 175/537: Loss=0.5666 (C:0.5666, R:0.0105)
Batch 200/537: Loss=0.5599 (C:0.5599, R:0.0105)
Batch 225/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch 250/537: Loss=0.5675 (C:0.5675, R:0.0105)
Batch 275/537: Loss=0.5869 (C:0.5869, R:0.0105)
Batch 300/537: Loss=0.5407 (C:0.5407, R:0.0105)
Batch 325/537: Loss=0.5939 (C:0.5939, R:0.0105)
Batch 350/537: Loss=0.5540 (C:0.5540, R:0.0105)
Batch 375/537: Loss=0.5457 (C:0.5457, R:0.0105)
Batch 400/537: Loss=0.5665 (C:0.5665, R:0.0106)
Batch 425/537: Loss=0.5646 (C:0.5646, R:0.0105)
Batch 450/537: Loss=0.5614 (C:0.5614, R:0.0105)
Batch 475/537: Loss=0.5534 (C:0.5534, R:0.0105)
Batch 500/537: Loss=0.5551 (C:0.5551, R:0.0105)
Batch 525/537: Loss=0.5538 (C:0.5538, R:0.0105)

============================================================
Epoch 60/100 completed in 21.4s
Train: Loss=0.5546 (C:0.5546, R:0.0105) Ratio=5.08x
Val:   Loss=0.7098 (C:0.7098, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 60
============================================================

üåç Updating global dataset at epoch 61
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.328 ¬± 0.615
    Neg distances: 2.660 ¬± 1.119
    Separation ratio: 8.10x
    Gap: -4.519
    ‚úÖ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.5580 (C:0.5580, R:0.0105)
Batch  25/537: Loss=0.5690 (C:0.5690, R:0.0105)
Batch  50/537: Loss=0.5331 (C:0.5331, R:0.0105)
Batch  75/537: Loss=0.5574 (C:0.5574, R:0.0105)
Batch 100/537: Loss=0.5696 (C:0.5696, R:0.0105)
Batch 125/537: Loss=0.5272 (C:0.5272, R:0.0106)
Batch 150/537: Loss=0.5397 (C:0.5397, R:0.0105)
Batch 175/537: Loss=0.5715 (C:0.5715, R:0.0105)
Batch 200/537: Loss=0.6049 (C:0.6049, R:0.0105)
Batch 225/537: Loss=0.5215 (C:0.5215, R:0.0105)
Batch 250/537: Loss=0.5657 (C:0.5657, R:0.0105)
Batch 275/537: Loss=0.5908 (C:0.5908, R:0.0105)
Batch 300/537: Loss=0.5991 (C:0.5991, R:0.0105)
Batch 325/537: Loss=0.5736 (C:0.5736, R:0.0105)
Batch 350/537: Loss=0.5423 (C:0.5423, R:0.0105)
Batch 375/537: Loss=0.5578 (C:0.5578, R:0.0105)
Batch 400/537: Loss=0.5752 (C:0.5752, R:0.0105)
Batch 425/537: Loss=0.5424 (C:0.5424, R:0.0105)
Batch 450/537: Loss=0.5631 (C:0.5631, R:0.0105)
Batch 475/537: Loss=0.5521 (C:0.5521, R:0.0105)
Batch 500/537: Loss=0.5621 (C:0.5621, R:0.0105)
Batch 525/537: Loss=0.5502 (C:0.5502, R:0.0106)

============================================================
Epoch 61/100 completed in 28.6s
Train: Loss=0.5614 (C:0.5614, R:0.0105) Ratio=5.03x
Val:   Loss=0.7170 (C:0.7170, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.5388 (C:0.5388, R:0.0105)
Batch  25/537: Loss=0.5546 (C:0.5546, R:0.0105)
Batch  50/537: Loss=0.5816 (C:0.5816, R:0.0105)
Batch  75/537: Loss=0.5764 (C:0.5764, R:0.0105)
Batch 100/537: Loss=0.5566 (C:0.5566, R:0.0105)
Batch 125/537: Loss=0.5749 (C:0.5749, R:0.0105)
Batch 150/537: Loss=0.5669 (C:0.5669, R:0.0105)
Batch 175/537: Loss=0.5407 (C:0.5407, R:0.0105)
Batch 200/537: Loss=0.5518 (C:0.5518, R:0.0105)
Batch 225/537: Loss=0.5849 (C:0.5849, R:0.0105)
Batch 250/537: Loss=0.5418 (C:0.5418, R:0.0106)
Batch 275/537: Loss=0.5875 (C:0.5875, R:0.0105)
Batch 300/537: Loss=0.5348 (C:0.5348, R:0.0106)
Batch 325/537: Loss=0.5490 (C:0.5490, R:0.0105)
Batch 350/537: Loss=0.5386 (C:0.5386, R:0.0105)
Batch 375/537: Loss=0.5632 (C:0.5632, R:0.0105)
Batch 400/537: Loss=0.5626 (C:0.5626, R:0.0105)
Batch 425/537: Loss=0.6244 (C:0.6244, R:0.0105)
Batch 450/537: Loss=0.5649 (C:0.5649, R:0.0105)
Batch 475/537: Loss=0.5542 (C:0.5542, R:0.0105)
Batch 500/537: Loss=0.5951 (C:0.5951, R:0.0105)
Batch 525/537: Loss=0.5556 (C:0.5556, R:0.0105)

============================================================
Epoch 62/100 completed in 21.7s
Train: Loss=0.5623 (C:0.5623, R:0.0105) Ratio=5.01x
Val:   Loss=0.7188 (C:0.7188, R:0.0104) Ratio=3.14x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.5531 (C:0.5531, R:0.0105)
Batch  25/537: Loss=0.4965 (C:0.4965, R:0.0105)
Batch  50/537: Loss=0.5159 (C:0.5159, R:0.0105)
Batch  75/537: Loss=0.5474 (C:0.5474, R:0.0105)
Batch 100/537: Loss=0.5467 (C:0.5467, R:0.0105)
Batch 125/537: Loss=0.5404 (C:0.5404, R:0.0105)
Batch 150/537: Loss=0.5678 (C:0.5678, R:0.0105)
Batch 175/537: Loss=0.5796 (C:0.5796, R:0.0105)
Batch 200/537: Loss=0.5870 (C:0.5870, R:0.0105)
Batch 225/537: Loss=0.5616 (C:0.5616, R:0.0105)
Batch 250/537: Loss=0.5765 (C:0.5765, R:0.0105)
Batch 275/537: Loss=0.5687 (C:0.5687, R:0.0105)
Batch 300/537: Loss=0.5389 (C:0.5389, R:0.0105)
Batch 325/537: Loss=0.5670 (C:0.5670, R:0.0105)
Batch 350/537: Loss=0.5520 (C:0.5520, R:0.0105)
Batch 375/537: Loss=0.6099 (C:0.6099, R:0.0105)
Batch 400/537: Loss=0.5440 (C:0.5440, R:0.0105)
Batch 425/537: Loss=0.5637 (C:0.5637, R:0.0105)
Batch 450/537: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 475/537: Loss=0.5640 (C:0.5640, R:0.0105)
Batch 500/537: Loss=0.5721 (C:0.5721, R:0.0105)
Batch 525/537: Loss=0.5629 (C:0.5629, R:0.0105)

============================================================
Epoch 63/100 completed in 21.7s
Train: Loss=0.5586 (C:0.5586, R:0.0105) Ratio=5.15x
Val:   Loss=0.7265 (C:0.7265, R:0.0104) Ratio=3.12x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 64
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.285 ¬± 0.561
    Neg distances: 2.688 ¬± 1.099
    Separation ratio: 9.45x
    Gap: -4.553
    ‚úÖ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.4923 (C:0.4923, R:0.0105)
Batch  25/537: Loss=0.4940 (C:0.4940, R:0.0105)
Batch  50/537: Loss=0.5129 (C:0.5129, R:0.0105)
Batch  75/537: Loss=0.5192 (C:0.5192, R:0.0105)
Batch 100/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 125/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch 150/537: Loss=0.4868 (C:0.4868, R:0.0105)
Batch 175/537: Loss=0.5559 (C:0.5559, R:0.0106)
Batch 200/537: Loss=0.5325 (C:0.5325, R:0.0105)
Batch 225/537: Loss=0.5189 (C:0.5189, R:0.0105)
Batch 250/537: Loss=0.5006 (C:0.5006, R:0.0105)
Batch 275/537: Loss=0.5081 (C:0.5081, R:0.0105)
Batch 300/537: Loss=0.5544 (C:0.5544, R:0.0105)
Batch 325/537: Loss=0.5072 (C:0.5072, R:0.0105)
Batch 350/537: Loss=0.5412 (C:0.5412, R:0.0105)
Batch 375/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch 400/537: Loss=0.5015 (C:0.5015, R:0.0105)
Batch 425/537: Loss=0.5175 (C:0.5175, R:0.0105)
Batch 450/537: Loss=0.5624 (C:0.5624, R:0.0105)
Batch 475/537: Loss=0.5070 (C:0.5070, R:0.0105)
Batch 500/537: Loss=0.5341 (C:0.5341, R:0.0105)
Batch 525/537: Loss=0.5161 (C:0.5161, R:0.0105)

============================================================
Epoch 64/100 completed in 27.7s
Train: Loss=0.5223 (C:0.5223, R:0.0105) Ratio=5.15x
Val:   Loss=0.6849 (C:0.6849, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 0.6849)
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.5056 (C:0.5056, R:0.0105)
Batch  25/537: Loss=0.4994 (C:0.4994, R:0.0105)
Batch  50/537: Loss=0.4850 (C:0.4850, R:0.0105)
Batch  75/537: Loss=0.5328 (C:0.5328, R:0.0105)
Batch 100/537: Loss=0.5042 (C:0.5042, R:0.0105)
Batch 125/537: Loss=0.5006 (C:0.5006, R:0.0105)
Batch 150/537: Loss=0.5201 (C:0.5201, R:0.0105)
Batch 175/537: Loss=0.5115 (C:0.5115, R:0.0105)
Batch 200/537: Loss=0.5262 (C:0.5262, R:0.0106)
Batch 225/537: Loss=0.4972 (C:0.4972, R:0.0105)
Batch 250/537: Loss=0.5000 (C:0.5000, R:0.0105)
Batch 275/537: Loss=0.5407 (C:0.5407, R:0.0105)
Batch 300/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch 325/537: Loss=0.5241 (C:0.5241, R:0.0105)
Batch 350/537: Loss=0.5393 (C:0.5393, R:0.0105)
Batch 375/537: Loss=0.5244 (C:0.5244, R:0.0105)
Batch 400/537: Loss=0.5486 (C:0.5486, R:0.0105)
Batch 425/537: Loss=0.5377 (C:0.5377, R:0.0105)
Batch 450/537: Loss=0.5500 (C:0.5500, R:0.0105)
Batch 475/537: Loss=0.5481 (C:0.5481, R:0.0105)
Batch 500/537: Loss=0.5035 (C:0.5035, R:0.0105)
Batch 525/537: Loss=0.5369 (C:0.5369, R:0.0105)

============================================================
Epoch 65/100 completed in 21.5s
Train: Loss=0.5211 (C:0.5211, R:0.0105) Ratio=5.14x
Val:   Loss=0.7007 (C:0.7007, R:0.0104) Ratio=3.09x
Reconstruction weight: 0.300
No improvement for 1 epochs
Checkpoint saved at epoch 65
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.5114 (C:0.5114, R:0.0105)
Batch  25/537: Loss=0.5080 (C:0.5080, R:0.0105)
Batch  50/537: Loss=0.5297 (C:0.5297, R:0.0105)
Batch  75/537: Loss=0.5286 (C:0.5286, R:0.0105)
Batch 100/537: Loss=0.4916 (C:0.4916, R:0.0105)
Batch 125/537: Loss=0.5459 (C:0.5459, R:0.0105)
Batch 150/537: Loss=0.4951 (C:0.4951, R:0.0105)
Batch 175/537: Loss=0.5360 (C:0.5360, R:0.0105)
Batch 200/537: Loss=0.5136 (C:0.5136, R:0.0105)
Batch 225/537: Loss=0.5264 (C:0.5264, R:0.0105)
Batch 250/537: Loss=0.4824 (C:0.4824, R:0.0105)
Batch 275/537: Loss=0.5401 (C:0.5401, R:0.0105)
Batch 300/537: Loss=0.5222 (C:0.5222, R:0.0105)
Batch 325/537: Loss=0.5052 (C:0.5052, R:0.0105)
Batch 350/537: Loss=0.5104 (C:0.5104, R:0.0105)
Batch 375/537: Loss=0.4594 (C:0.4594, R:0.0106)
Batch 400/537: Loss=0.5468 (C:0.5468, R:0.0105)
Batch 425/537: Loss=0.5478 (C:0.5478, R:0.0106)
Batch 450/537: Loss=0.4724 (C:0.4724, R:0.0105)
Batch 475/537: Loss=0.5561 (C:0.5561, R:0.0105)
Batch 500/537: Loss=0.5069 (C:0.5069, R:0.0105)
Batch 525/537: Loss=0.5336 (C:0.5336, R:0.0105)

============================================================
Epoch 66/100 completed in 21.5s
Train: Loss=0.5194 (C:0.5194, R:0.0105) Ratio=5.28x
Val:   Loss=0.6947 (C:0.6947, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 67
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.300 ¬± 0.592
    Neg distances: 2.703 ¬± 1.116
    Separation ratio: 9.02x
    Gap: -4.624
    ‚úÖ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=0.5590 (C:0.5590, R:0.0105)
Batch  25/537: Loss=0.5004 (C:0.5004, R:0.0105)
Batch  50/537: Loss=0.5474 (C:0.5474, R:0.0105)
Batch  75/537: Loss=0.5236 (C:0.5236, R:0.0105)
Batch 100/537: Loss=0.5185 (C:0.5185, R:0.0105)
Batch 125/537: Loss=0.5213 (C:0.5213, R:0.0105)
Batch 150/537: Loss=0.5057 (C:0.5057, R:0.0105)
Batch 175/537: Loss=0.5600 (C:0.5600, R:0.0105)
Batch 200/537: Loss=0.5209 (C:0.5209, R:0.0105)
Batch 225/537: Loss=0.5263 (C:0.5263, R:0.0106)
Batch 250/537: Loss=0.4915 (C:0.4915, R:0.0105)
Batch 275/537: Loss=0.5103 (C:0.5103, R:0.0105)
Batch 300/537: Loss=0.5074 (C:0.5074, R:0.0105)
Batch 325/537: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 350/537: Loss=0.5411 (C:0.5411, R:0.0105)
Batch 375/537: Loss=0.5312 (C:0.5312, R:0.0105)
Batch 400/537: Loss=0.5412 (C:0.5412, R:0.0105)
Batch 425/537: Loss=0.5343 (C:0.5343, R:0.0105)
Batch 450/537: Loss=0.5199 (C:0.5199, R:0.0105)
Batch 475/537: Loss=0.5317 (C:0.5317, R:0.0105)
Batch 500/537: Loss=0.4956 (C:0.4956, R:0.0105)
Batch 525/537: Loss=0.5577 (C:0.5577, R:0.0105)

============================================================
Epoch 67/100 completed in 27.1s
Train: Loss=0.5285 (C:0.5285, R:0.0105) Ratio=5.17x
Val:   Loss=0.6974 (C:0.6974, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=0.4792 (C:0.4792, R:0.0105)
Batch  25/537: Loss=0.5067 (C:0.5067, R:0.0105)
Batch  50/537: Loss=0.5374 (C:0.5374, R:0.0105)
Batch  75/537: Loss=0.5282 (C:0.5282, R:0.0105)
Batch 100/537: Loss=0.5040 (C:0.5040, R:0.0105)
Batch 125/537: Loss=0.4930 (C:0.4930, R:0.0105)
Batch 150/537: Loss=0.5663 (C:0.5663, R:0.0105)
Batch 175/537: Loss=0.5098 (C:0.5098, R:0.0105)
Batch 200/537: Loss=0.5555 (C:0.5555, R:0.0105)
Batch 225/537: Loss=0.5310 (C:0.5310, R:0.0105)
Batch 250/537: Loss=0.5506 (C:0.5506, R:0.0105)
Batch 275/537: Loss=0.5022 (C:0.5022, R:0.0105)
Batch 300/537: Loss=0.4923 (C:0.4923, R:0.0105)
Batch 325/537: Loss=0.5218 (C:0.5218, R:0.0105)
Batch 350/537: Loss=0.5261 (C:0.5261, R:0.0105)
Batch 375/537: Loss=0.5414 (C:0.5414, R:0.0105)
Batch 400/537: Loss=0.5143 (C:0.5143, R:0.0105)
Batch 425/537: Loss=0.5547 (C:0.5547, R:0.0105)
Batch 450/537: Loss=0.5456 (C:0.5456, R:0.0105)
Batch 475/537: Loss=0.5289 (C:0.5289, R:0.0105)
Batch 500/537: Loss=0.5002 (C:0.5002, R:0.0105)
Batch 525/537: Loss=0.5156 (C:0.5156, R:0.0105)

============================================================
Epoch 68/100 completed in 21.3s
Train: Loss=0.5278 (C:0.5278, R:0.0105) Ratio=5.30x
Val:   Loss=0.7123 (C:0.7123, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=0.5488 (C:0.5488, R:0.0105)
Batch  25/537: Loss=0.5349 (C:0.5349, R:0.0105)
Batch  50/537: Loss=0.5326 (C:0.5326, R:0.0105)
Batch  75/537: Loss=0.5078 (C:0.5078, R:0.0106)
Batch 100/537: Loss=0.5038 (C:0.5038, R:0.0105)
Batch 125/537: Loss=0.5477 (C:0.5477, R:0.0105)
Batch 150/537: Loss=0.5158 (C:0.5158, R:0.0105)
Batch 175/537: Loss=0.5431 (C:0.5431, R:0.0105)
Batch 200/537: Loss=0.5284 (C:0.5284, R:0.0105)
Batch 225/537: Loss=0.5454 (C:0.5454, R:0.0105)
Batch 250/537: Loss=0.4875 (C:0.4875, R:0.0105)
Batch 275/537: Loss=0.4944 (C:0.4944, R:0.0105)
Batch 300/537: Loss=0.5279 (C:0.5279, R:0.0105)
Batch 325/537: Loss=0.5240 (C:0.5240, R:0.0105)
Batch 350/537: Loss=0.5211 (C:0.5211, R:0.0105)
Batch 375/537: Loss=0.5193 (C:0.5193, R:0.0105)
Batch 400/537: Loss=0.5684 (C:0.5684, R:0.0105)
Batch 425/537: Loss=0.5234 (C:0.5234, R:0.0106)
Batch 450/537: Loss=0.5359 (C:0.5359, R:0.0105)
Batch 475/537: Loss=0.5566 (C:0.5566, R:0.0105)
Batch 500/537: Loss=0.5143 (C:0.5143, R:0.0105)
Batch 525/537: Loss=0.5774 (C:0.5774, R:0.0105)

============================================================
Epoch 69/100 completed in 21.6s
Train: Loss=0.5258 (C:0.5258, R:0.0105) Ratio=5.15x
Val:   Loss=0.7082 (C:0.7082, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 70
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.291 ¬± 0.569
    Neg distances: 2.697 ¬± 1.115
    Separation ratio: 9.28x
    Gap: -4.609
    ‚úÖ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=0.5430 (C:0.5430, R:0.0105)
Batch  25/537: Loss=0.5486 (C:0.5486, R:0.0105)
Batch  50/537: Loss=0.4959 (C:0.4959, R:0.0105)
Batch  75/537: Loss=0.5476 (C:0.5476, R:0.0105)
Batch 100/537: Loss=0.4741 (C:0.4741, R:0.0105)
Batch 125/537: Loss=0.5116 (C:0.5116, R:0.0105)
Batch 150/537: Loss=0.5384 (C:0.5384, R:0.0105)
Batch 175/537: Loss=0.4979 (C:0.4979, R:0.0105)
Batch 200/537: Loss=0.5400 (C:0.5400, R:0.0105)
Batch 225/537: Loss=0.4995 (C:0.4995, R:0.0105)
Batch 250/537: Loss=0.5128 (C:0.5128, R:0.0105)
Batch 275/537: Loss=0.5815 (C:0.5815, R:0.0105)
Batch 300/537: Loss=0.5350 (C:0.5350, R:0.0105)
Batch 325/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 350/537: Loss=0.5100 (C:0.5100, R:0.0105)
Batch 375/537: Loss=0.4967 (C:0.4967, R:0.0105)
Batch 400/537: Loss=0.5181 (C:0.5181, R:0.0105)
Batch 425/537: Loss=0.5113 (C:0.5113, R:0.0105)
Batch 450/537: Loss=0.5192 (C:0.5192, R:0.0105)
Batch 475/537: Loss=0.5525 (C:0.5525, R:0.0105)
Batch 500/537: Loss=0.5410 (C:0.5410, R:0.0105)
Batch 525/537: Loss=0.5236 (C:0.5236, R:0.0105)

============================================================
Epoch 70/100 completed in 27.5s
Train: Loss=0.5206 (C:0.5206, R:0.0105) Ratio=5.19x
Val:   Loss=0.6930 (C:0.6930, R:0.0104) Ratio=3.13x
Reconstruction weight: 0.300
No improvement for 6 epochs
Checkpoint saved at epoch 70
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=0.5003 (C:0.5003, R:0.0105)
Batch  25/537: Loss=0.5352 (C:0.5352, R:0.0105)
Batch  50/537: Loss=0.5438 (C:0.5438, R:0.0105)
Batch  75/537: Loss=0.5054 (C:0.5054, R:0.0104)
Batch 100/537: Loss=0.5605 (C:0.5605, R:0.0105)
Batch 125/537: Loss=0.5079 (C:0.5079, R:0.0105)
Batch 150/537: Loss=0.5462 (C:0.5462, R:0.0105)
Batch 175/537: Loss=0.5176 (C:0.5176, R:0.0105)
Batch 200/537: Loss=0.5483 (C:0.5483, R:0.0105)
Batch 225/537: Loss=0.5401 (C:0.5401, R:0.0105)
Batch 250/537: Loss=0.5448 (C:0.5448, R:0.0106)
Batch 275/537: Loss=0.5223 (C:0.5223, R:0.0105)
Batch 300/537: Loss=0.5265 (C:0.5265, R:0.0105)
Batch 325/537: Loss=0.5206 (C:0.5206, R:0.0105)
Batch 350/537: Loss=0.5197 (C:0.5197, R:0.0105)
Batch 375/537: Loss=0.5450 (C:0.5450, R:0.0105)
Batch 400/537: Loss=0.5638 (C:0.5638, R:0.0105)
Batch 425/537: Loss=0.4992 (C:0.4992, R:0.0105)
Batch 450/537: Loss=0.5619 (C:0.5619, R:0.0105)
Batch 475/537: Loss=0.5094 (C:0.5094, R:0.0105)
Batch 500/537: Loss=0.5279 (C:0.5279, R:0.0105)
Batch 525/537: Loss=0.5151 (C:0.5151, R:0.0105)

============================================================
Epoch 71/100 completed in 21.8s
Train: Loss=0.5185 (C:0.5185, R:0.0105) Ratio=5.12x
Val:   Loss=0.7007 (C:0.7007, R:0.0104) Ratio=3.10x
Reconstruction weight: 0.300
No improvement for 7 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=0.4977 (C:0.4977, R:0.0105)
Batch  25/537: Loss=0.5098 (C:0.5098, R:0.0105)
Batch  50/537: Loss=0.5232 (C:0.5232, R:0.0105)
Batch  75/537: Loss=0.5203 (C:0.5203, R:0.0105)
Batch 100/537: Loss=0.5126 (C:0.5126, R:0.0105)
Batch 125/537: Loss=0.4894 (C:0.4894, R:0.0105)
Batch 150/537: Loss=0.4941 (C:0.4941, R:0.0105)
Batch 175/537: Loss=0.5444 (C:0.5444, R:0.0105)
Batch 200/537: Loss=0.5422 (C:0.5422, R:0.0105)
Batch 225/537: Loss=0.5144 (C:0.5144, R:0.0105)
Batch 250/537: Loss=0.5428 (C:0.5428, R:0.0105)
Batch 275/537: Loss=0.5084 (C:0.5084, R:0.0105)
Batch 300/537: Loss=0.5854 (C:0.5854, R:0.0105)
Batch 325/537: Loss=0.5441 (C:0.5441, R:0.0105)
Batch 350/537: Loss=0.5071 (C:0.5071, R:0.0105)
Batch 375/537: Loss=0.5451 (C:0.5451, R:0.0105)
Batch 400/537: Loss=0.5139 (C:0.5139, R:0.0105)
Batch 425/537: Loss=0.4781 (C:0.4781, R:0.0105)
Batch 450/537: Loss=0.4844 (C:0.4844, R:0.0105)
Batch 475/537: Loss=0.5178 (C:0.5178, R:0.0105)
Batch 500/537: Loss=0.4876 (C:0.4876, R:0.0105)
Batch 525/537: Loss=0.5102 (C:0.5102, R:0.0105)

============================================================
Epoch 72/100 completed in 21.9s
Train: Loss=0.5179 (C:0.5179, R:0.0105) Ratio=5.33x
Val:   Loss=0.7018 (C:0.7018, R:0.0104) Ratio=3.07x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 72 epochs
Best model was at epoch 64 with Val Loss: 0.6849

Global Dataset Training Completed!
Best epoch: 64
Best validation loss: 0.6849
Final separation ratios: Train=5.33x, Val=3.07x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_180422/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4632
  Adjusted Rand Score: 0.5330
  Clustering Accuracy: 0.8164
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8167
  Per-class F1: [0.839572192513369, 0.7552205997801852, 0.8589978358581655]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.010432
Evaluating separation quality...
Separation Results:
  Positive distances: 0.772 ¬± 0.907
  Negative distances: 2.349 ¬± 1.254
  Separation ratio: 3.04x
  Gap: -4.669
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4632
  Clustering Accuracy: 0.8164
  Adjusted Rand Score: 0.5330

Classification Performance:
  Accuracy: 0.8167

Separation Quality:
  Separation Ratio: 3.04x
  Gap: -4.669
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.010432
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_180422/results/evaluation_results_20250712_183347.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_20250712_180422/results/evaluation_results_20250712_183347.json
Saving final experiment results...

PIPELINE FAILED: Object of type float32 is not JSON serializable

Analysis completed with exit code: 0
Time: Sat 12 Jul 18:33:48 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
