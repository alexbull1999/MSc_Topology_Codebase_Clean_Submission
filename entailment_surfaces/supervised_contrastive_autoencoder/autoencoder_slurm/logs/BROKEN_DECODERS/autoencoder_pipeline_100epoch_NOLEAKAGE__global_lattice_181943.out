Starting Surface Distance Metric Analysis job...
Job ID: 181943
Node: gpuvm19
Time: Sat 12 Jul 18:04:20 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 12 18:04:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   30C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-12 18:04:34.470638
Using device: cuda

Configuration:
  Embedding type: lattice
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 100
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_180434
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_180434/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'lattice'
Output dimension will be: 768
GlobalDataLoader initialized:
  Embedding type: lattice
  Output dimension: 768
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating lattice embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated lattice embeddings: torch.Size([549367, 768])
Generating embeddings for validation...
Generating lattice embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated lattice embeddings: torch.Size([9842, 768])
Generating embeddings for test...
Generating lattice embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated lattice embeddings: torch.Size([9824, 768])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 768])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 768])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 768])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 768
Updated model input_dim to: 768
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 768
  Latent dim: 75
  Hidden dims: [512, 256]
  Dropout rate: 0.2
  Total parameters: 1,089,355
Model created with 1,089,355 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.0
  Scheduled reconstruction: warmup=30 epochs, max_weight=0.3
Optimizer created (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 1,089,355
Starting training...
========================================
Starting Global Dataset Training...
============================================================

üåç Updating global dataset at epoch 1
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.036 ¬± 0.004
    Neg distances: 0.037 ¬± 0.004
    Separation ratio: 1.01x
    Gap: -0.052
    ‚ùå Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=1.9996 (C:1.9996, R:0.0029)
Batch  25/537: Loss=1.9931 (C:1.9931, R:0.0027)
Batch  50/537: Loss=1.9664 (C:1.9664, R:0.0025)
Batch  75/537: Loss=1.9542 (C:1.9542, R:0.0024)
Batch 100/537: Loss=1.9399 (C:1.9399, R:0.0023)
Batch 125/537: Loss=1.9393 (C:1.9393, R:0.0022)
Batch 150/537: Loss=1.9308 (C:1.9308, R:0.0021)
Batch 175/537: Loss=1.9296 (C:1.9296, R:0.0020)
Batch 200/537: Loss=1.9285 (C:1.9285, R:0.0020)
Batch 225/537: Loss=1.9321 (C:1.9321, R:0.0019)
Batch 250/537: Loss=1.9266 (C:1.9266, R:0.0019)
Batch 275/537: Loss=1.9214 (C:1.9214, R:0.0019)
Batch 300/537: Loss=1.9253 (C:1.9253, R:0.0018)
Batch 325/537: Loss=1.9226 (C:1.9226, R:0.0018)
Batch 350/537: Loss=1.9321 (C:1.9321, R:0.0018)
Batch 375/537: Loss=1.9206 (C:1.9206, R:0.0018)
Batch 400/537: Loss=1.9078 (C:1.9078, R:0.0018)
Batch 425/537: Loss=1.9263 (C:1.9263, R:0.0018)
Batch 450/537: Loss=1.9248 (C:1.9248, R:0.0018)
Batch 475/537: Loss=1.9304 (C:1.9304, R:0.0018)
Batch 500/537: Loss=1.9165 (C:1.9165, R:0.0018)
Batch 525/537: Loss=1.9265 (C:1.9265, R:0.0018)

============================================================
Epoch 1/100 completed in 28.5s
Train: Loss=1.9339 (C:1.9339, R:0.0020) Ratio=1.61x
Val:   Loss=1.9120 (C:1.9120, R:0.0017) Ratio=1.87x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.9120)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=1.9293 (C:1.9293, R:0.0018)
Batch  25/537: Loss=1.9297 (C:1.9297, R:0.0018)
Batch  50/537: Loss=1.9213 (C:1.9213, R:0.0018)
Batch  75/537: Loss=1.9238 (C:1.9238, R:0.0018)
Batch 100/537: Loss=1.9104 (C:1.9104, R:0.0018)
Batch 125/537: Loss=1.9122 (C:1.9122, R:0.0017)
Batch 150/537: Loss=1.9144 (C:1.9144, R:0.0018)
Batch 175/537: Loss=1.9150 (C:1.9150, R:0.0018)
Batch 200/537: Loss=1.9175 (C:1.9175, R:0.0018)
Batch 225/537: Loss=1.9199 (C:1.9199, R:0.0018)
Batch 250/537: Loss=1.9154 (C:1.9154, R:0.0018)
Batch 275/537: Loss=1.9152 (C:1.9152, R:0.0018)
Batch 300/537: Loss=1.9113 (C:1.9113, R:0.0018)
Batch 325/537: Loss=1.9150 (C:1.9150, R:0.0018)
Batch 350/537: Loss=1.9176 (C:1.9176, R:0.0018)
Batch 375/537: Loss=1.9133 (C:1.9133, R:0.0018)
Batch 400/537: Loss=1.9102 (C:1.9102, R:0.0018)
Batch 425/537: Loss=1.9133 (C:1.9133, R:0.0018)
Batch 450/537: Loss=1.9017 (C:1.9017, R:0.0018)
Batch 475/537: Loss=1.9073 (C:1.9073, R:0.0018)
Batch 500/537: Loss=1.9221 (C:1.9221, R:0.0018)
Batch 525/537: Loss=1.8976 (C:1.8976, R:0.0018)

============================================================
Epoch 2/100 completed in 20.9s
Train: Loss=1.9142 (C:1.9142, R:0.0018) Ratio=1.81x
Val:   Loss=1.9057 (C:1.9057, R:0.0017) Ratio=1.93x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.9057)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=1.9072 (C:1.9072, R:0.0018)
Batch  25/537: Loss=1.8950 (C:1.8950, R:0.0018)
Batch  50/537: Loss=1.9139 (C:1.9139, R:0.0018)
Batch  75/537: Loss=1.9091 (C:1.9091, R:0.0018)
Batch 100/537: Loss=1.9048 (C:1.9048, R:0.0018)
Batch 125/537: Loss=1.9133 (C:1.9133, R:0.0018)
Batch 150/537: Loss=1.9115 (C:1.9115, R:0.0017)
Batch 175/537: Loss=1.9205 (C:1.9205, R:0.0018)
Batch 200/537: Loss=1.9110 (C:1.9110, R:0.0017)
Batch 225/537: Loss=1.9182 (C:1.9182, R:0.0018)
Batch 250/537: Loss=1.8972 (C:1.8972, R:0.0018)
Batch 275/537: Loss=1.9054 (C:1.9054, R:0.0018)
Batch 300/537: Loss=1.9146 (C:1.9146, R:0.0018)
Batch 325/537: Loss=1.9245 (C:1.9245, R:0.0018)
Batch 350/537: Loss=1.9244 (C:1.9244, R:0.0018)
Batch 375/537: Loss=1.9076 (C:1.9076, R:0.0018)
Batch 400/537: Loss=1.9000 (C:1.9000, R:0.0018)
Batch 425/537: Loss=1.8998 (C:1.8998, R:0.0018)
Batch 450/537: Loss=1.9011 (C:1.9011, R:0.0018)
Batch 475/537: Loss=1.9019 (C:1.9019, R:0.0018)
Batch 500/537: Loss=1.9061 (C:1.9061, R:0.0018)
Batch 525/537: Loss=1.9068 (C:1.9068, R:0.0018)

============================================================
Epoch 3/100 completed in 20.9s
Train: Loss=1.9086 (C:1.9086, R:0.0018) Ratio=1.88x
Val:   Loss=1.9061 (C:1.9061, R:0.0017) Ratio=1.95x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 4
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.610 ¬± 0.694
    Neg distances: 1.229 ¬± 0.824
    Separation ratio: 2.01x
    Gap: -2.468
    ‚úÖ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=1.4315 (C:1.4315, R:0.0018)
Batch  25/537: Loss=1.3970 (C:1.3970, R:0.0018)
Batch  50/537: Loss=1.4225 (C:1.4225, R:0.0018)
Batch  75/537: Loss=1.4635 (C:1.4635, R:0.0017)
Batch 100/537: Loss=1.4014 (C:1.4014, R:0.0018)
Batch 125/537: Loss=1.4472 (C:1.4472, R:0.0018)
Batch 150/537: Loss=1.4131 (C:1.4131, R:0.0017)
Batch 175/537: Loss=1.4299 (C:1.4299, R:0.0018)
Batch 200/537: Loss=1.4356 (C:1.4356, R:0.0018)
Batch 225/537: Loss=1.4283 (C:1.4283, R:0.0018)
Batch 250/537: Loss=1.4974 (C:1.4974, R:0.0017)
Batch 275/537: Loss=1.4018 (C:1.4018, R:0.0018)
Batch 300/537: Loss=1.4460 (C:1.4460, R:0.0018)
Batch 325/537: Loss=1.4212 (C:1.4212, R:0.0018)
Batch 350/537: Loss=1.4154 (C:1.4154, R:0.0018)
Batch 375/537: Loss=1.4616 (C:1.4616, R:0.0018)
Batch 400/537: Loss=1.4632 (C:1.4632, R:0.0018)
Batch 425/537: Loss=1.4305 (C:1.4305, R:0.0018)
Batch 450/537: Loss=1.4429 (C:1.4429, R:0.0018)
Batch 475/537: Loss=1.4707 (C:1.4707, R:0.0018)
Batch 500/537: Loss=1.4720 (C:1.4720, R:0.0018)
Batch 525/537: Loss=1.3970 (C:1.3970, R:0.0018)

============================================================
Epoch 4/100 completed in 26.1s
Train: Loss=1.4320 (C:1.4320, R:0.0018) Ratio=1.94x
Val:   Loss=1.4212 (C:1.4212, R:0.0017) Ratio=2.00x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.4212)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=1.4018 (C:1.4018, R:0.0018)
Batch  25/537: Loss=1.4077 (C:1.4077, R:0.0017)
Batch  50/537: Loss=1.4325 (C:1.4325, R:0.0018)
Batch  75/537: Loss=1.4017 (C:1.4017, R:0.0018)
Batch 100/537: Loss=1.3883 (C:1.3883, R:0.0018)
Batch 125/537: Loss=1.4258 (C:1.4258, R:0.0017)
Batch 150/537: Loss=1.3927 (C:1.3927, R:0.0018)
Batch 175/537: Loss=1.4119 (C:1.4119, R:0.0018)
Batch 200/537: Loss=1.3946 (C:1.3946, R:0.0018)
Batch 225/537: Loss=1.4181 (C:1.4181, R:0.0018)
Batch 250/537: Loss=1.4041 (C:1.4041, R:0.0018)
Batch 275/537: Loss=1.3888 (C:1.3888, R:0.0018)
Batch 300/537: Loss=1.4193 (C:1.4193, R:0.0018)
Batch 325/537: Loss=1.4188 (C:1.4188, R:0.0018)
Batch 350/537: Loss=1.3849 (C:1.3849, R:0.0018)
Batch 375/537: Loss=1.4121 (C:1.4121, R:0.0018)
Batch 400/537: Loss=1.4327 (C:1.4327, R:0.0018)
Batch 425/537: Loss=1.4164 (C:1.4164, R:0.0018)
Batch 450/537: Loss=1.4166 (C:1.4166, R:0.0018)
Batch 475/537: Loss=1.4014 (C:1.4014, R:0.0018)
Batch 500/537: Loss=1.4621 (C:1.4621, R:0.0018)
Batch 525/537: Loss=1.4042 (C:1.4042, R:0.0018)

============================================================
Epoch 5/100 completed in 20.5s
Train: Loss=1.4170 (C:1.4170, R:0.0018) Ratio=2.06x
Val:   Loss=1.4139 (C:1.4139, R:0.0017) Ratio=2.05x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.4139)
Checkpoint saved at epoch 5
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=1.3942 (C:1.3942, R:0.0018)
Batch  25/537: Loss=1.3857 (C:1.3857, R:0.0018)
Batch  50/537: Loss=1.4314 (C:1.4314, R:0.0018)
Batch  75/537: Loss=1.4028 (C:1.4028, R:0.0018)
Batch 100/537: Loss=1.4157 (C:1.4157, R:0.0018)
Batch 125/537: Loss=1.4220 (C:1.4220, R:0.0017)
Batch 150/537: Loss=1.4180 (C:1.4180, R:0.0018)
Batch 175/537: Loss=1.3989 (C:1.3989, R:0.0018)
Batch 200/537: Loss=1.4171 (C:1.4171, R:0.0018)
Batch 225/537: Loss=1.3833 (C:1.3833, R:0.0018)
Batch 250/537: Loss=1.4465 (C:1.4465, R:0.0018)
Batch 275/537: Loss=1.3950 (C:1.3950, R:0.0018)
Batch 300/537: Loss=1.4337 (C:1.4337, R:0.0018)
Batch 325/537: Loss=1.4054 (C:1.4054, R:0.0018)
Batch 350/537: Loss=1.4384 (C:1.4384, R:0.0018)
Batch 375/537: Loss=1.3931 (C:1.3931, R:0.0018)
Batch 400/537: Loss=1.4082 (C:1.4082, R:0.0018)
Batch 425/537: Loss=1.4190 (C:1.4190, R:0.0017)
Batch 450/537: Loss=1.3920 (C:1.3920, R:0.0018)
Batch 475/537: Loss=1.4075 (C:1.4075, R:0.0018)
Batch 500/537: Loss=1.3846 (C:1.3846, R:0.0018)
Batch 525/537: Loss=1.4038 (C:1.4038, R:0.0018)

============================================================
Epoch 6/100 completed in 21.3s
Train: Loss=1.4086 (C:1.4086, R:0.0018) Ratio=2.11x
Val:   Loss=1.4113 (C:1.4113, R:0.0017) Ratio=2.07x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.4113)
============================================================

üåç Updating global dataset at epoch 7
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.580 ¬± 0.694
    Neg distances: 1.271 ¬± 0.839
    Separation ratio: 2.19x
    Gap: -2.590
    ‚úÖ Good global separation

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=1.3549 (C:1.3549, R:0.0018)
Batch  25/537: Loss=1.3372 (C:1.3372, R:0.0018)
Batch  50/537: Loss=1.4044 (C:1.4044, R:0.0018)
Batch  75/537: Loss=1.3642 (C:1.3642, R:0.0018)
Batch 100/537: Loss=1.3474 (C:1.3474, R:0.0018)
Batch 125/537: Loss=1.3975 (C:1.3975, R:0.0018)
Batch 150/537: Loss=1.3889 (C:1.3889, R:0.0018)
Batch 175/537: Loss=1.3517 (C:1.3517, R:0.0018)
Batch 200/537: Loss=1.3716 (C:1.3716, R:0.0017)
Batch 225/537: Loss=1.3645 (C:1.3645, R:0.0018)
Batch 250/537: Loss=1.3507 (C:1.3507, R:0.0018)
Batch 275/537: Loss=1.3335 (C:1.3335, R:0.0017)
Batch 300/537: Loss=1.3405 (C:1.3405, R:0.0018)
Batch 325/537: Loss=1.3698 (C:1.3698, R:0.0018)
Batch 350/537: Loss=1.3270 (C:1.3270, R:0.0018)
Batch 375/537: Loss=1.3365 (C:1.3365, R:0.0018)
Batch 400/537: Loss=1.3892 (C:1.3892, R:0.0018)
Batch 425/537: Loss=1.3565 (C:1.3565, R:0.0018)
Batch 450/537: Loss=1.3320 (C:1.3320, R:0.0018)
Batch 475/537: Loss=1.3933 (C:1.3933, R:0.0018)
Batch 500/537: Loss=1.3619 (C:1.3619, R:0.0018)
Batch 525/537: Loss=1.3421 (C:1.3421, R:0.0018)

============================================================
Epoch 7/100 completed in 26.2s
Train: Loss=1.3624 (C:1.3624, R:0.0018) Ratio=2.21x
Val:   Loss=1.3641 (C:1.3641, R:0.0017) Ratio=2.08x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.3641)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=1.3429 (C:1.3429, R:0.0018)
Batch  25/537: Loss=1.3589 (C:1.3589, R:0.0018)
Batch  50/537: Loss=1.3319 (C:1.3319, R:0.0018)
Batch  75/537: Loss=1.3533 (C:1.3533, R:0.0018)
Batch 100/537: Loss=1.3480 (C:1.3480, R:0.0017)
Batch 125/537: Loss=1.3334 (C:1.3334, R:0.0017)
Batch 150/537: Loss=1.3280 (C:1.3280, R:0.0018)
Batch 175/537: Loss=1.3005 (C:1.3005, R:0.0018)
Batch 200/537: Loss=1.3419 (C:1.3419, R:0.0018)
Batch 225/537: Loss=1.3437 (C:1.3437, R:0.0018)
Batch 250/537: Loss=1.3180 (C:1.3180, R:0.0018)
Batch 275/537: Loss=1.3519 (C:1.3519, R:0.0018)
Batch 300/537: Loss=1.3659 (C:1.3659, R:0.0018)
Batch 325/537: Loss=1.3694 (C:1.3694, R:0.0018)
Batch 350/537: Loss=1.3309 (C:1.3309, R:0.0017)
Batch 375/537: Loss=1.3332 (C:1.3332, R:0.0018)
Batch 400/537: Loss=1.3682 (C:1.3682, R:0.0018)
Batch 425/537: Loss=1.3471 (C:1.3471, R:0.0018)
Batch 450/537: Loss=1.3254 (C:1.3254, R:0.0018)
Batch 475/537: Loss=1.3571 (C:1.3571, R:0.0018)
Batch 500/537: Loss=1.3468 (C:1.3468, R:0.0018)
Batch 525/537: Loss=1.3613 (C:1.3613, R:0.0017)

============================================================
Epoch 8/100 completed in 20.9s
Train: Loss=1.3537 (C:1.3537, R:0.0018) Ratio=2.27x
Val:   Loss=1.3605 (C:1.3605, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.3605)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=1.3253 (C:1.3253, R:0.0018)
Batch  25/537: Loss=1.3583 (C:1.3583, R:0.0018)
Batch  50/537: Loss=1.3527 (C:1.3527, R:0.0017)
Batch  75/537: Loss=1.3551 (C:1.3551, R:0.0017)
Batch 100/537: Loss=1.3311 (C:1.3311, R:0.0018)
Batch 125/537: Loss=1.3520 (C:1.3520, R:0.0018)
Batch 150/537: Loss=1.3187 (C:1.3187, R:0.0018)
Batch 175/537: Loss=1.3681 (C:1.3681, R:0.0018)
Batch 200/537: Loss=1.3556 (C:1.3556, R:0.0018)
Batch 225/537: Loss=1.3669 (C:1.3669, R:0.0018)
Batch 250/537: Loss=1.3509 (C:1.3509, R:0.0018)
Batch 275/537: Loss=1.3495 (C:1.3495, R:0.0018)
Batch 300/537: Loss=1.3520 (C:1.3520, R:0.0018)
Batch 325/537: Loss=1.3386 (C:1.3386, R:0.0018)
Batch 350/537: Loss=1.3660 (C:1.3660, R:0.0018)
Batch 375/537: Loss=1.3595 (C:1.3595, R:0.0018)
Batch 400/537: Loss=1.3337 (C:1.3337, R:0.0018)
Batch 425/537: Loss=1.3417 (C:1.3417, R:0.0018)
Batch 450/537: Loss=1.3407 (C:1.3407, R:0.0018)
Batch 475/537: Loss=1.3629 (C:1.3629, R:0.0018)
Batch 500/537: Loss=1.3295 (C:1.3295, R:0.0018)
Batch 525/537: Loss=1.3338 (C:1.3338, R:0.0018)

============================================================
Epoch 9/100 completed in 20.5s
Train: Loss=1.3471 (C:1.3471, R:0.0018) Ratio=2.31x
Val:   Loss=1.3637 (C:1.3637, R:0.0017) Ratio=2.13x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 10
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.509 ¬± 0.669
    Neg distances: 1.295 ¬± 0.853
    Separation ratio: 2.55x
    Gap: -2.526
    ‚úÖ Good global separation

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.2928 (C:1.2928, R:0.0018)
Batch  25/537: Loss=1.2469 (C:1.2469, R:0.0018)
Batch  50/537: Loss=1.2740 (C:1.2740, R:0.0017)
Batch  75/537: Loss=1.2742 (C:1.2742, R:0.0018)
Batch 100/537: Loss=1.2907 (C:1.2907, R:0.0017)
Batch 125/537: Loss=1.3280 (C:1.3280, R:0.0017)
Batch 150/537: Loss=1.2815 (C:1.2815, R:0.0018)
Batch 175/537: Loss=1.2556 (C:1.2556, R:0.0018)
Batch 200/537: Loss=1.2714 (C:1.2714, R:0.0018)
Batch 225/537: Loss=1.2608 (C:1.2608, R:0.0018)
Batch 250/537: Loss=1.3094 (C:1.3094, R:0.0018)
Batch 275/537: Loss=1.2905 (C:1.2905, R:0.0018)
Batch 300/537: Loss=1.2920 (C:1.2920, R:0.0018)
Batch 325/537: Loss=1.2898 (C:1.2898, R:0.0018)
Batch 350/537: Loss=1.2343 (C:1.2343, R:0.0018)
Batch 375/537: Loss=1.3160 (C:1.3160, R:0.0018)
Batch 400/537: Loss=1.3007 (C:1.3007, R:0.0018)
Batch 425/537: Loss=1.2392 (C:1.2392, R:0.0018)
Batch 450/537: Loss=1.3154 (C:1.3154, R:0.0018)
Batch 475/537: Loss=1.2926 (C:1.2926, R:0.0018)
Batch 500/537: Loss=1.2625 (C:1.2625, R:0.0018)
Batch 525/537: Loss=1.2625 (C:1.2625, R:0.0018)

============================================================
Epoch 10/100 completed in 26.5s
Train: Loss=1.2864 (C:1.2864, R:0.0018) Ratio=2.41x
Val:   Loss=1.3186 (C:1.3186, R:0.0017) Ratio=2.12x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.3186)
Checkpoint saved at epoch 10
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.2807 (C:1.2807, R:0.0018)
Batch  25/537: Loss=1.2827 (C:1.2827, R:0.0017)
Batch  50/537: Loss=1.3066 (C:1.3066, R:0.0018)
Batch  75/537: Loss=1.2793 (C:1.2793, R:0.0018)
Batch 100/537: Loss=1.3206 (C:1.3206, R:0.0018)
Batch 125/537: Loss=1.2841 (C:1.2841, R:0.0018)
Batch 150/537: Loss=1.3005 (C:1.3005, R:0.0018)
Batch 175/537: Loss=1.3026 (C:1.3026, R:0.0018)
Batch 200/537: Loss=1.2909 (C:1.2909, R:0.0018)
Batch 225/537: Loss=1.2766 (C:1.2766, R:0.0018)
Batch 250/537: Loss=1.2701 (C:1.2701, R:0.0018)
Batch 275/537: Loss=1.2815 (C:1.2815, R:0.0018)
Batch 300/537: Loss=1.2796 (C:1.2796, R:0.0017)
Batch 325/537: Loss=1.3183 (C:1.3183, R:0.0018)
Batch 350/537: Loss=1.2690 (C:1.2690, R:0.0018)
Batch 375/537: Loss=1.2434 (C:1.2434, R:0.0018)
Batch 400/537: Loss=1.2360 (C:1.2360, R:0.0017)
Batch 425/537: Loss=1.2943 (C:1.2943, R:0.0018)
Batch 450/537: Loss=1.2998 (C:1.2998, R:0.0018)
Batch 475/537: Loss=1.2945 (C:1.2945, R:0.0018)
Batch 500/537: Loss=1.2979 (C:1.2979, R:0.0018)
Batch 525/537: Loss=1.2821 (C:1.2821, R:0.0018)

============================================================
Epoch 11/100 completed in 20.6s
Train: Loss=1.2802 (C:1.2802, R:0.0018) Ratio=2.40x
Val:   Loss=1.3176 (C:1.3176, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.3176)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.2451 (C:1.2451, R:0.0018)
Batch  25/537: Loss=1.2830 (C:1.2830, R:0.0018)
Batch  50/537: Loss=1.2484 (C:1.2484, R:0.0018)
Batch  75/537: Loss=1.2648 (C:1.2648, R:0.0018)
Batch 100/537: Loss=1.2876 (C:1.2876, R:0.0018)
Batch 125/537: Loss=1.3058 (C:1.3058, R:0.0018)
Batch 150/537: Loss=1.2692 (C:1.2692, R:0.0018)
Batch 175/537: Loss=1.2680 (C:1.2680, R:0.0017)
Batch 200/537: Loss=1.3388 (C:1.3388, R:0.0018)
Batch 225/537: Loss=1.2532 (C:1.2532, R:0.0018)
Batch 250/537: Loss=1.2810 (C:1.2810, R:0.0018)
Batch 275/537: Loss=1.2751 (C:1.2751, R:0.0018)
Batch 300/537: Loss=1.2504 (C:1.2504, R:0.0018)
Batch 325/537: Loss=1.2972 (C:1.2972, R:0.0018)
Batch 350/537: Loss=1.2567 (C:1.2567, R:0.0018)
Batch 375/537: Loss=1.2856 (C:1.2856, R:0.0018)
Batch 400/537: Loss=1.2864 (C:1.2864, R:0.0017)
Batch 425/537: Loss=1.2449 (C:1.2449, R:0.0018)
Batch 450/537: Loss=1.2812 (C:1.2812, R:0.0018)
Batch 475/537: Loss=1.2639 (C:1.2639, R:0.0018)
Batch 500/537: Loss=1.2735 (C:1.2735, R:0.0018)
Batch 525/537: Loss=1.3019 (C:1.3019, R:0.0018)

============================================================
Epoch 12/100 completed in 20.7s
Train: Loss=1.2737 (C:1.2737, R:0.0018) Ratio=2.49x
Val:   Loss=1.3090 (C:1.3090, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.3090)
============================================================

üåç Updating global dataset at epoch 13
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.458 ¬± 0.641
    Neg distances: 1.333 ¬± 0.860
    Separation ratio: 2.91x
    Gap: -2.780
    ‚úÖ Good global separation

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=1.1917 (C:1.1917, R:0.0018)
Batch  25/537: Loss=1.1859 (C:1.1859, R:0.0018)
Batch  50/537: Loss=1.2563 (C:1.2563, R:0.0018)
Batch  75/537: Loss=1.2483 (C:1.2483, R:0.0017)
Batch 100/537: Loss=1.2122 (C:1.2122, R:0.0018)
Batch 125/537: Loss=1.2111 (C:1.2111, R:0.0018)
Batch 150/537: Loss=1.2383 (C:1.2383, R:0.0018)
Batch 175/537: Loss=1.2196 (C:1.2196, R:0.0018)
Batch 200/537: Loss=1.2708 (C:1.2708, R:0.0018)
Batch 225/537: Loss=1.1886 (C:1.1886, R:0.0018)
Batch 250/537: Loss=1.2373 (C:1.2373, R:0.0018)
Batch 275/537: Loss=1.2110 (C:1.2110, R:0.0018)
Batch 300/537: Loss=1.2045 (C:1.2045, R:0.0018)
Batch 325/537: Loss=1.2245 (C:1.2245, R:0.0018)
Batch 350/537: Loss=1.1913 (C:1.1913, R:0.0018)
Batch 375/537: Loss=1.2383 (C:1.2383, R:0.0017)
Batch 400/537: Loss=1.2560 (C:1.2560, R:0.0018)
Batch 425/537: Loss=1.1862 (C:1.1862, R:0.0018)
Batch 450/537: Loss=1.1956 (C:1.1956, R:0.0018)
Batch 475/537: Loss=1.2326 (C:1.2326, R:0.0018)
Batch 500/537: Loss=1.2481 (C:1.2481, R:0.0018)
Batch 525/537: Loss=1.2065 (C:1.2065, R:0.0018)

============================================================
Epoch 13/100 completed in 26.6s
Train: Loss=1.2256 (C:1.2256, R:0.0018) Ratio=2.52x
Val:   Loss=1.2699 (C:1.2699, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2699)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=1.2190 (C:1.2190, R:0.0018)
Batch  25/537: Loss=1.1864 (C:1.1864, R:0.0018)
Batch  50/537: Loss=1.2300 (C:1.2300, R:0.0018)
Batch  75/537: Loss=1.2319 (C:1.2319, R:0.0018)
Batch 100/537: Loss=1.2052 (C:1.2052, R:0.0018)
Batch 125/537: Loss=1.2319 (C:1.2319, R:0.0018)
Batch 150/537: Loss=1.2110 (C:1.2110, R:0.0018)
Batch 175/537: Loss=1.2117 (C:1.2117, R:0.0017)
Batch 200/537: Loss=1.1918 (C:1.1918, R:0.0018)
Batch 225/537: Loss=1.2246 (C:1.2246, R:0.0018)
Batch 250/537: Loss=1.2039 (C:1.2039, R:0.0018)
Batch 275/537: Loss=1.2143 (C:1.2143, R:0.0018)
Batch 300/537: Loss=1.2144 (C:1.2144, R:0.0018)
Batch 325/537: Loss=1.2306 (C:1.2306, R:0.0018)
Batch 350/537: Loss=1.2003 (C:1.2003, R:0.0018)
Batch 375/537: Loss=1.2648 (C:1.2648, R:0.0018)
Batch 400/537: Loss=1.2004 (C:1.2004, R:0.0018)
Batch 425/537: Loss=1.2693 (C:1.2693, R:0.0018)
Batch 450/537: Loss=1.2288 (C:1.2288, R:0.0017)
Batch 475/537: Loss=1.2223 (C:1.2223, R:0.0018)
Batch 500/537: Loss=1.2322 (C:1.2322, R:0.0018)
Batch 525/537: Loss=1.2065 (C:1.2065, R:0.0017)

============================================================
Epoch 14/100 completed in 21.0s
Train: Loss=1.2206 (C:1.2206, R:0.0018) Ratio=2.59x
Val:   Loss=1.2648 (C:1.2648, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2648)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=1.1906 (C:1.1906, R:0.0018)
Batch  25/537: Loss=1.2077 (C:1.2077, R:0.0018)
Batch  50/537: Loss=1.2528 (C:1.2528, R:0.0018)
Batch  75/537: Loss=1.2577 (C:1.2577, R:0.0018)
Batch 100/537: Loss=1.2092 (C:1.2092, R:0.0018)
Batch 125/537: Loss=1.2541 (C:1.2541, R:0.0017)
Batch 150/537: Loss=1.2225 (C:1.2225, R:0.0018)
Batch 175/537: Loss=1.1802 (C:1.1802, R:0.0018)
Batch 200/537: Loss=1.2207 (C:1.2207, R:0.0017)
Batch 225/537: Loss=1.1916 (C:1.1916, R:0.0017)
Batch 250/537: Loss=1.2435 (C:1.2435, R:0.0017)
Batch 275/537: Loss=1.2483 (C:1.2483, R:0.0018)
Batch 300/537: Loss=1.2664 (C:1.2664, R:0.0017)
Batch 325/537: Loss=1.1916 (C:1.1916, R:0.0018)
Batch 350/537: Loss=1.2341 (C:1.2341, R:0.0018)
Batch 375/537: Loss=1.1761 (C:1.1761, R:0.0018)
Batch 400/537: Loss=1.1980 (C:1.1980, R:0.0018)
Batch 425/537: Loss=1.2532 (C:1.2532, R:0.0018)
Batch 450/537: Loss=1.2194 (C:1.2194, R:0.0018)
Batch 475/537: Loss=1.1967 (C:1.1967, R:0.0018)
Batch 500/537: Loss=1.2053 (C:1.2053, R:0.0018)
Batch 525/537: Loss=1.1905 (C:1.1905, R:0.0017)

============================================================
Epoch 15/100 completed in 21.0s
Train: Loss=1.2167 (C:1.2167, R:0.0018) Ratio=2.61x
Val:   Loss=1.2671 (C:1.2671, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.000
No improvement for 1 epochs
Checkpoint saved at epoch 15
============================================================

üåç Updating global dataset at epoch 16
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.439 ¬± 0.635
    Neg distances: 1.358 ¬± 0.873
    Separation ratio: 3.10x
    Gap: -2.748
    ‚úÖ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=1.1773 (C:1.1773, R:0.0017)
Batch  25/537: Loss=1.1572 (C:1.1572, R:0.0018)
Batch  50/537: Loss=1.1788 (C:1.1788, R:0.0018)
Batch  75/537: Loss=1.1940 (C:1.1940, R:0.0018)
Batch 100/537: Loss=1.2132 (C:1.2132, R:0.0018)
Batch 125/537: Loss=1.1440 (C:1.1440, R:0.0018)
Batch 150/537: Loss=1.1488 (C:1.1488, R:0.0017)
Batch 175/537: Loss=1.2025 (C:1.2025, R:0.0018)
Batch 200/537: Loss=1.1807 (C:1.1807, R:0.0018)
Batch 225/537: Loss=1.1773 (C:1.1773, R:0.0018)
Batch 250/537: Loss=1.2269 (C:1.2269, R:0.0018)
Batch 275/537: Loss=1.1996 (C:1.1996, R:0.0018)
Batch 300/537: Loss=1.1538 (C:1.1538, R:0.0018)
Batch 325/537: Loss=1.2332 (C:1.2332, R:0.0018)
Batch 350/537: Loss=1.2050 (C:1.2050, R:0.0018)
Batch 375/537: Loss=1.1744 (C:1.1744, R:0.0018)
Batch 400/537: Loss=1.2094 (C:1.2094, R:0.0018)
Batch 425/537: Loss=1.1993 (C:1.1993, R:0.0018)
Batch 450/537: Loss=1.2125 (C:1.2125, R:0.0018)
Batch 475/537: Loss=1.1735 (C:1.1735, R:0.0017)
Batch 500/537: Loss=1.1838 (C:1.1838, R:0.0018)
Batch 525/537: Loss=1.1709 (C:1.1709, R:0.0018)

============================================================
Epoch 16/100 completed in 26.7s
Train: Loss=1.1913 (C:1.1913, R:0.0018) Ratio=2.69x
Val:   Loss=1.2516 (C:1.2516, R:0.0017) Ratio=2.14x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2516)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=1.1941 (C:1.1941, R:0.0017)
Batch  25/537: Loss=1.1760 (C:1.1760, R:0.0018)
Batch  50/537: Loss=1.1394 (C:1.1394, R:0.0017)
Batch  75/537: Loss=1.1609 (C:1.1609, R:0.0018)
Batch 100/537: Loss=1.1742 (C:1.1742, R:0.0018)
Batch 125/537: Loss=1.1891 (C:1.1891, R:0.0018)
Batch 150/537: Loss=1.1689 (C:1.1689, R:0.0018)
Batch 175/537: Loss=1.2176 (C:1.2176, R:0.0018)
Batch 200/537: Loss=1.1844 (C:1.1844, R:0.0018)
Batch 225/537: Loss=1.1796 (C:1.1796, R:0.0018)
Batch 250/537: Loss=1.1730 (C:1.1730, R:0.0018)
Batch 275/537: Loss=1.2066 (C:1.2066, R:0.0018)
Batch 300/537: Loss=1.1768 (C:1.1768, R:0.0018)
Batch 325/537: Loss=1.1688 (C:1.1688, R:0.0018)
Batch 350/537: Loss=1.1700 (C:1.1700, R:0.0018)
Batch 375/537: Loss=1.2130 (C:1.2130, R:0.0018)
Batch 400/537: Loss=1.1722 (C:1.1722, R:0.0018)
Batch 425/537: Loss=1.1943 (C:1.1943, R:0.0017)
Batch 450/537: Loss=1.2060 (C:1.2060, R:0.0017)
Batch 475/537: Loss=1.1469 (C:1.1469, R:0.0018)
Batch 500/537: Loss=1.1533 (C:1.1533, R:0.0018)
Batch 525/537: Loss=1.1847 (C:1.1847, R:0.0018)

============================================================
Epoch 17/100 completed in 20.9s
Train: Loss=1.1875 (C:1.1875, R:0.0018) Ratio=2.71x
Val:   Loss=1.2497 (C:1.2497, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2497)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=1.1758 (C:1.1758, R:0.0018)
Batch  25/537: Loss=1.1527 (C:1.1527, R:0.0017)
Batch  50/537: Loss=1.1616 (C:1.1616, R:0.0018)
Batch  75/537: Loss=1.1796 (C:1.1796, R:0.0018)
Batch 100/537: Loss=1.1298 (C:1.1298, R:0.0018)
Batch 125/537: Loss=1.1750 (C:1.1750, R:0.0018)
Batch 150/537: Loss=1.1944 (C:1.1944, R:0.0017)
Batch 175/537: Loss=1.2098 (C:1.2098, R:0.0018)
Batch 200/537: Loss=1.1580 (C:1.1580, R:0.0017)
Batch 225/537: Loss=1.1920 (C:1.1920, R:0.0018)
Batch 250/537: Loss=1.1879 (C:1.1879, R:0.0018)
Batch 275/537: Loss=1.1763 (C:1.1763, R:0.0018)
Batch 300/537: Loss=1.1948 (C:1.1948, R:0.0018)
Batch 325/537: Loss=1.1690 (C:1.1690, R:0.0018)
Batch 350/537: Loss=1.1756 (C:1.1756, R:0.0017)
Batch 375/537: Loss=1.1856 (C:1.1856, R:0.0018)
Batch 400/537: Loss=1.1514 (C:1.1514, R:0.0018)
Batch 425/537: Loss=1.1739 (C:1.1739, R:0.0018)
Batch 450/537: Loss=1.1718 (C:1.1718, R:0.0018)
Batch 475/537: Loss=1.2251 (C:1.2251, R:0.0018)
Batch 500/537: Loss=1.1992 (C:1.1992, R:0.0017)
Batch 525/537: Loss=1.1736 (C:1.1736, R:0.0017)

============================================================
Epoch 18/100 completed in 20.9s
Train: Loss=1.1832 (C:1.1832, R:0.0018) Ratio=2.80x
Val:   Loss=1.2535 (C:1.2535, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 19
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.444 ¬± 0.634
    Neg distances: 1.374 ¬± 0.876
    Separation ratio: 3.09x
    Gap: -2.527
    ‚úÖ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=1.1940 (C:1.1940, R:0.0018)
Batch  25/537: Loss=1.1951 (C:1.1951, R:0.0018)
Batch  50/537: Loss=1.1637 (C:1.1637, R:0.0017)
Batch  75/537: Loss=1.1729 (C:1.1729, R:0.0018)
Batch 100/537: Loss=1.1647 (C:1.1647, R:0.0018)
Batch 125/537: Loss=1.1866 (C:1.1866, R:0.0018)
Batch 150/537: Loss=1.1732 (C:1.1732, R:0.0018)
Batch 175/537: Loss=1.2077 (C:1.2077, R:0.0017)
Batch 200/537: Loss=1.1715 (C:1.1715, R:0.0018)
Batch 225/537: Loss=1.1960 (C:1.1960, R:0.0018)
Batch 250/537: Loss=1.2097 (C:1.2097, R:0.0018)
Batch 275/537: Loss=1.2086 (C:1.2086, R:0.0017)
Batch 300/537: Loss=1.2237 (C:1.2237, R:0.0017)
Batch 325/537: Loss=1.1854 (C:1.1854, R:0.0018)
Batch 350/537: Loss=1.1601 (C:1.1601, R:0.0018)
Batch 375/537: Loss=1.1774 (C:1.1774, R:0.0017)
Batch 400/537: Loss=1.2359 (C:1.2359, R:0.0018)
Batch 425/537: Loss=1.1812 (C:1.1812, R:0.0018)
Batch 450/537: Loss=1.1828 (C:1.1828, R:0.0018)
Batch 475/537: Loss=1.1930 (C:1.1930, R:0.0018)
Batch 500/537: Loss=1.1845 (C:1.1845, R:0.0018)
Batch 525/537: Loss=1.1984 (C:1.1984, R:0.0018)

============================================================
Epoch 19/100 completed in 26.5s
Train: Loss=1.1800 (C:1.1800, R:0.0018) Ratio=2.71x
Val:   Loss=1.2513 (C:1.2513, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.000
No improvement for 2 epochs
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=1.1740 (C:1.1740, R:0.0018)
Batch  25/537: Loss=1.1767 (C:1.1767, R:0.0018)
Batch  50/537: Loss=1.1811 (C:1.1811, R:0.0018)
Batch  75/537: Loss=1.1784 (C:1.1784, R:0.0018)
Batch 100/537: Loss=1.1729 (C:1.1729, R:0.0018)
Batch 125/537: Loss=1.1565 (C:1.1565, R:0.0018)
Batch 150/537: Loss=1.1486 (C:1.1486, R:0.0018)
Batch 175/537: Loss=1.1456 (C:1.1456, R:0.0018)
Batch 200/537: Loss=1.1373 (C:1.1373, R:0.0018)
Batch 225/537: Loss=1.1690 (C:1.1690, R:0.0017)
Batch 250/537: Loss=1.2090 (C:1.2090, R:0.0017)
Batch 275/537: Loss=1.1329 (C:1.1329, R:0.0018)
Batch 300/537: Loss=1.1976 (C:1.1976, R:0.0018)
Batch 325/537: Loss=1.1713 (C:1.1713, R:0.0018)
Batch 350/537: Loss=1.2060 (C:1.2060, R:0.0018)
Batch 375/537: Loss=1.1794 (C:1.1794, R:0.0017)
Batch 400/537: Loss=1.2089 (C:1.2089, R:0.0018)
Batch 425/537: Loss=1.1647 (C:1.1647, R:0.0017)
Batch 450/537: Loss=1.1918 (C:1.1918, R:0.0018)
Batch 475/537: Loss=1.1757 (C:1.1757, R:0.0018)
Batch 500/537: Loss=1.1899 (C:1.1899, R:0.0018)
Batch 525/537: Loss=1.2033 (C:1.2033, R:0.0018)

============================================================
Epoch 20/100 completed in 20.8s
Train: Loss=1.1786 (C:1.1786, R:0.0018) Ratio=2.80x
Val:   Loss=1.2463 (C:1.2463, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2463)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=1.1622 (C:1.1622, R:0.0018)
Batch  25/537: Loss=1.1584 (C:1.1584, R:0.0017)
Batch  50/537: Loss=1.2127 (C:1.2127, R:0.0018)
Batch  75/537: Loss=1.1787 (C:1.1787, R:0.0018)
Batch 100/537: Loss=1.1667 (C:1.1667, R:0.0018)
Batch 125/537: Loss=1.1765 (C:1.1765, R:0.0018)
Batch 150/537: Loss=1.1562 (C:1.1562, R:0.0018)
Batch 175/537: Loss=1.1514 (C:1.1514, R:0.0018)
Batch 200/537: Loss=1.1891 (C:1.1891, R:0.0018)
Batch 225/537: Loss=1.1910 (C:1.1910, R:0.0017)
Batch 250/537: Loss=1.1671 (C:1.1671, R:0.0017)
Batch 275/537: Loss=1.1998 (C:1.1998, R:0.0018)
Batch 300/537: Loss=1.1631 (C:1.1631, R:0.0018)
Batch 325/537: Loss=1.1737 (C:1.1737, R:0.0018)
Batch 350/537: Loss=1.1749 (C:1.1749, R:0.0018)
Batch 375/537: Loss=1.1711 (C:1.1711, R:0.0018)
Batch 400/537: Loss=1.1866 (C:1.1866, R:0.0017)
Batch 425/537: Loss=1.1654 (C:1.1654, R:0.0018)
Batch 450/537: Loss=1.1808 (C:1.1808, R:0.0018)
Batch 475/537: Loss=1.1761 (C:1.1761, R:0.0018)
Batch 500/537: Loss=1.1969 (C:1.1969, R:0.0018)
Batch 525/537: Loss=1.1780 (C:1.1780, R:0.0018)

============================================================
Epoch 21/100 completed in 21.2s
Train: Loss=1.1750 (C:1.1750, R:0.0018) Ratio=2.81x
Val:   Loss=1.2491 (C:1.2491, R:0.0017) Ratio=2.14x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 22
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.432 ¬± 0.623
    Neg distances: 1.407 ¬± 0.870
    Separation ratio: 3.26x
    Gap: -2.551
    ‚úÖ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=1.1662 (C:1.1662, R:0.0018)
Batch  25/537: Loss=1.1576 (C:1.1576, R:0.0018)
Batch  50/537: Loss=1.1263 (C:1.1263, R:0.0017)
Batch  75/537: Loss=1.1205 (C:1.1205, R:0.0017)
Batch 100/537: Loss=1.1573 (C:1.1573, R:0.0018)
Batch 125/537: Loss=1.1713 (C:1.1713, R:0.0018)
Batch 150/537: Loss=1.1540 (C:1.1540, R:0.0018)
Batch 175/537: Loss=1.1254 (C:1.1254, R:0.0018)
Batch 200/537: Loss=1.1590 (C:1.1590, R:0.0017)
Batch 225/537: Loss=1.1644 (C:1.1644, R:0.0018)
Batch 250/537: Loss=1.1699 (C:1.1699, R:0.0018)
Batch 275/537: Loss=1.1373 (C:1.1373, R:0.0018)
Batch 300/537: Loss=1.1575 (C:1.1575, R:0.0018)
Batch 325/537: Loss=1.1734 (C:1.1734, R:0.0018)
Batch 350/537: Loss=1.1652 (C:1.1652, R:0.0018)
Batch 375/537: Loss=1.1679 (C:1.1679, R:0.0018)
Batch 400/537: Loss=1.1746 (C:1.1746, R:0.0018)
Batch 425/537: Loss=1.1835 (C:1.1835, R:0.0017)
Batch 450/537: Loss=1.1368 (C:1.1368, R:0.0018)
Batch 475/537: Loss=1.1070 (C:1.1070, R:0.0018)
Batch 500/537: Loss=1.1454 (C:1.1454, R:0.0018)
Batch 525/537: Loss=1.1512 (C:1.1512, R:0.0018)

============================================================
Epoch 22/100 completed in 26.8s
Train: Loss=1.1503 (C:1.1503, R:0.0018) Ratio=2.89x
Val:   Loss=1.2380 (C:1.2380, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2380)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=1.1296 (C:1.1296, R:0.0018)
Batch  25/537: Loss=1.1407 (C:1.1407, R:0.0018)
Batch  50/537: Loss=1.1376 (C:1.1376, R:0.0018)
Batch  75/537: Loss=1.1729 (C:1.1729, R:0.0018)
Batch 100/537: Loss=1.1318 (C:1.1318, R:0.0017)
Batch 125/537: Loss=1.1495 (C:1.1495, R:0.0018)
Batch 150/537: Loss=1.1959 (C:1.1959, R:0.0018)
Batch 175/537: Loss=1.0855 (C:1.0855, R:0.0018)
Batch 200/537: Loss=1.1404 (C:1.1404, R:0.0017)
Batch 225/537: Loss=1.1724 (C:1.1724, R:0.0018)
Batch 250/537: Loss=1.1532 (C:1.1532, R:0.0018)
Batch 275/537: Loss=1.1543 (C:1.1543, R:0.0018)
Batch 300/537: Loss=1.1621 (C:1.1621, R:0.0018)
Batch 325/537: Loss=1.1689 (C:1.1689, R:0.0017)
Batch 350/537: Loss=1.1480 (C:1.1480, R:0.0018)
Batch 375/537: Loss=1.1852 (C:1.1852, R:0.0018)
Batch 400/537: Loss=1.1626 (C:1.1626, R:0.0018)
Batch 425/537: Loss=1.1602 (C:1.1602, R:0.0018)
Batch 450/537: Loss=1.1621 (C:1.1621, R:0.0018)
Batch 475/537: Loss=1.1531 (C:1.1531, R:0.0018)
Batch 500/537: Loss=1.1480 (C:1.1480, R:0.0018)
Batch 525/537: Loss=1.1797 (C:1.1797, R:0.0018)

============================================================
Epoch 23/100 completed in 20.9s
Train: Loss=1.1502 (C:1.1502, R:0.0018) Ratio=2.90x
Val:   Loss=1.2330 (C:1.2330, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2330)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=1.1265 (C:1.1265, R:0.0018)
Batch  25/537: Loss=1.1437 (C:1.1437, R:0.0018)
Batch  50/537: Loss=1.1468 (C:1.1468, R:0.0018)
Batch  75/537: Loss=1.1462 (C:1.1462, R:0.0018)
Batch 100/537: Loss=1.1581 (C:1.1581, R:0.0018)
Batch 125/537: Loss=1.1971 (C:1.1971, R:0.0018)
Batch 150/537: Loss=1.1739 (C:1.1739, R:0.0018)
Batch 175/537: Loss=1.1692 (C:1.1692, R:0.0018)
Batch 200/537: Loss=1.1739 (C:1.1739, R:0.0017)
Batch 225/537: Loss=1.1434 (C:1.1434, R:0.0017)
Batch 250/537: Loss=1.1345 (C:1.1345, R:0.0018)
Batch 275/537: Loss=1.1335 (C:1.1335, R:0.0018)
Batch 300/537: Loss=1.1677 (C:1.1677, R:0.0018)
Batch 325/537: Loss=1.0982 (C:1.0982, R:0.0018)
Batch 350/537: Loss=1.1522 (C:1.1522, R:0.0018)
Batch 375/537: Loss=1.1548 (C:1.1548, R:0.0018)
Batch 400/537: Loss=1.1473 (C:1.1473, R:0.0018)
Batch 425/537: Loss=1.1544 (C:1.1544, R:0.0018)
Batch 450/537: Loss=1.1749 (C:1.1749, R:0.0018)
Batch 475/537: Loss=1.1540 (C:1.1540, R:0.0018)
Batch 500/537: Loss=1.1329 (C:1.1329, R:0.0018)
Batch 525/537: Loss=1.1442 (C:1.1442, R:0.0018)

============================================================
Epoch 24/100 completed in 20.9s
Train: Loss=1.1472 (C:1.1472, R:0.0018) Ratio=2.91x
Val:   Loss=1.2320 (C:1.2320, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2320)
============================================================

üåç Updating global dataset at epoch 25
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.399 ¬± 0.596
    Neg distances: 1.420 ¬± 0.888
    Separation ratio: 3.56x
    Gap: -2.545
    ‚úÖ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=1.1327 (C:1.1327, R:0.0018)
Batch  25/537: Loss=1.1223 (C:1.1223, R:0.0018)
Batch  50/537: Loss=1.1126 (C:1.1126, R:0.0018)
Batch  75/537: Loss=1.0853 (C:1.0853, R:0.0017)
Batch 100/537: Loss=1.1224 (C:1.1224, R:0.0017)
Batch 125/537: Loss=1.0899 (C:1.0899, R:0.0018)
Batch 150/537: Loss=1.1131 (C:1.1131, R:0.0018)
Batch 175/537: Loss=1.1180 (C:1.1180, R:0.0018)
Batch 200/537: Loss=1.1436 (C:1.1436, R:0.0018)
Batch 225/537: Loss=1.1438 (C:1.1438, R:0.0017)
Batch 250/537: Loss=1.0792 (C:1.0792, R:0.0018)
Batch 275/537: Loss=1.1119 (C:1.1119, R:0.0018)
Batch 300/537: Loss=1.1415 (C:1.1415, R:0.0018)
Batch 325/537: Loss=1.1355 (C:1.1355, R:0.0018)
Batch 350/537: Loss=1.1213 (C:1.1213, R:0.0018)
Batch 375/537: Loss=1.1167 (C:1.1167, R:0.0018)
Batch 400/537: Loss=1.1262 (C:1.1262, R:0.0018)
Batch 425/537: Loss=1.1033 (C:1.1033, R:0.0018)
Batch 450/537: Loss=1.1273 (C:1.1273, R:0.0018)
Batch 475/537: Loss=1.1321 (C:1.1321, R:0.0018)
Batch 500/537: Loss=1.1236 (C:1.1236, R:0.0018)
Batch 525/537: Loss=1.1307 (C:1.1307, R:0.0018)

============================================================
Epoch 25/100 completed in 26.5s
Train: Loss=1.1235 (C:1.1235, R:0.0018) Ratio=2.97x
Val:   Loss=1.2199 (C:1.2199, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2199)
Checkpoint saved at epoch 25
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=1.1173 (C:1.1173, R:0.0018)
Batch  25/537: Loss=1.1057 (C:1.1057, R:0.0018)
Batch  50/537: Loss=1.1309 (C:1.1309, R:0.0018)
Batch  75/537: Loss=1.1014 (C:1.1014, R:0.0018)
Batch 100/537: Loss=1.1261 (C:1.1261, R:0.0018)
Batch 125/537: Loss=1.1216 (C:1.1216, R:0.0018)
Batch 150/537: Loss=1.1069 (C:1.1069, R:0.0018)
Batch 175/537: Loss=1.1396 (C:1.1396, R:0.0017)
Batch 200/537: Loss=1.1128 (C:1.1128, R:0.0018)
Batch 225/537: Loss=1.1170 (C:1.1170, R:0.0018)
Batch 250/537: Loss=1.1104 (C:1.1104, R:0.0018)
Batch 275/537: Loss=1.1070 (C:1.1070, R:0.0018)
Batch 300/537: Loss=1.1220 (C:1.1220, R:0.0018)
Batch 325/537: Loss=1.1033 (C:1.1033, R:0.0018)
Batch 350/537: Loss=1.1092 (C:1.1092, R:0.0018)
Batch 375/537: Loss=1.0865 (C:1.0865, R:0.0018)
Batch 400/537: Loss=1.1204 (C:1.1204, R:0.0018)
Batch 425/537: Loss=1.1175 (C:1.1175, R:0.0018)
Batch 450/537: Loss=1.1366 (C:1.1366, R:0.0018)
Batch 475/537: Loss=1.1047 (C:1.1047, R:0.0018)
Batch 500/537: Loss=1.1308 (C:1.1308, R:0.0018)
Batch 525/537: Loss=1.1269 (C:1.1269, R:0.0018)

============================================================
Epoch 26/100 completed in 20.9s
Train: Loss=1.1207 (C:1.1207, R:0.0018) Ratio=3.03x
Val:   Loss=1.2180 (C:1.2180, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2180)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=1.1028 (C:1.1028, R:0.0018)
Batch  25/537: Loss=1.1103 (C:1.1103, R:0.0018)
Batch  50/537: Loss=1.1158 (C:1.1158, R:0.0018)
Batch  75/537: Loss=1.1008 (C:1.1008, R:0.0017)
Batch 100/537: Loss=1.1260 (C:1.1260, R:0.0018)
Batch 125/537: Loss=1.1110 (C:1.1110, R:0.0018)
Batch 150/537: Loss=1.1110 (C:1.1110, R:0.0018)
Batch 175/537: Loss=1.1128 (C:1.1128, R:0.0018)
Batch 200/537: Loss=1.1743 (C:1.1743, R:0.0018)
Batch 225/537: Loss=1.1366 (C:1.1366, R:0.0018)
Batch 250/537: Loss=1.1148 (C:1.1148, R:0.0018)
Batch 275/537: Loss=1.1276 (C:1.1276, R:0.0018)
Batch 300/537: Loss=1.1260 (C:1.1260, R:0.0018)
Batch 325/537: Loss=1.1196 (C:1.1196, R:0.0018)
Batch 350/537: Loss=1.1327 (C:1.1327, R:0.0018)
Batch 375/537: Loss=1.1427 (C:1.1427, R:0.0018)
Batch 400/537: Loss=1.1470 (C:1.1470, R:0.0018)
Batch 425/537: Loss=1.1255 (C:1.1255, R:0.0018)
Batch 450/537: Loss=1.1212 (C:1.1212, R:0.0018)
Batch 475/537: Loss=1.1352 (C:1.1352, R:0.0018)
Batch 500/537: Loss=1.1288 (C:1.1288, R:0.0018)
Batch 525/537: Loss=1.1369 (C:1.1369, R:0.0018)

============================================================
Epoch 27/100 completed in 20.9s
Train: Loss=1.1188 (C:1.1188, R:0.0018) Ratio=3.01x
Val:   Loss=1.2158 (C:1.2158, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.2158)
============================================================

üåç Updating global dataset at epoch 28
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.389 ¬± 0.594
    Neg distances: 1.455 ¬± 0.898
    Separation ratio: 3.74x
    Gap: -2.547
    ‚úÖ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=1.1036 (C:1.1036, R:0.0018)
Batch  25/537: Loss=1.0943 (C:1.0943, R:0.0018)
Batch  50/537: Loss=1.1193 (C:1.1193, R:0.0018)
Batch  75/537: Loss=1.1148 (C:1.1148, R:0.0018)
Batch 100/537: Loss=1.0853 (C:1.0853, R:0.0018)
Batch 125/537: Loss=1.1082 (C:1.1082, R:0.0018)
Batch 150/537: Loss=1.1120 (C:1.1120, R:0.0018)
Batch 175/537: Loss=1.0836 (C:1.0836, R:0.0018)
Batch 200/537: Loss=1.0743 (C:1.0743, R:0.0018)
Batch 225/537: Loss=1.0938 (C:1.0938, R:0.0018)
Batch 250/537: Loss=1.0647 (C:1.0647, R:0.0018)
Batch 275/537: Loss=1.1248 (C:1.1248, R:0.0018)
Batch 300/537: Loss=1.0780 (C:1.0780, R:0.0018)
Batch 325/537: Loss=1.1213 (C:1.1213, R:0.0018)
Batch 350/537: Loss=1.0715 (C:1.0715, R:0.0018)
Batch 375/537: Loss=1.1019 (C:1.1019, R:0.0018)
Batch 400/537: Loss=1.1103 (C:1.1103, R:0.0018)
Batch 425/537: Loss=1.0837 (C:1.0837, R:0.0018)
Batch 450/537: Loss=1.1248 (C:1.1248, R:0.0018)
Batch 475/537: Loss=1.0975 (C:1.0975, R:0.0018)
Batch 500/537: Loss=1.1004 (C:1.1004, R:0.0018)
Batch 525/537: Loss=1.1202 (C:1.1202, R:0.0018)

============================================================
Epoch 28/100 completed in 26.6s
Train: Loss=1.0992 (C:1.0992, R:0.0018) Ratio=3.08x
Val:   Loss=1.1930 (C:1.1930, R:0.0017) Ratio=2.20x
Reconstruction weight: 0.000
‚úÖ New best model saved (Val Loss: 1.1930)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=1.1101 (C:1.1101, R:0.0018)
Batch  25/537: Loss=1.0603 (C:1.0603, R:0.0018)
Batch  50/537: Loss=1.0873 (C:1.0873, R:0.0018)
Batch  75/537: Loss=1.0939 (C:1.0939, R:0.0018)
Batch 100/537: Loss=1.0933 (C:1.0933, R:0.0018)
Batch 125/537: Loss=1.0694 (C:1.0694, R:0.0017)
Batch 150/537: Loss=1.0885 (C:1.0885, R:0.0018)
Batch 175/537: Loss=1.1110 (C:1.1110, R:0.0017)
Batch 200/537: Loss=1.0774 (C:1.0774, R:0.0018)
Batch 225/537: Loss=1.1072 (C:1.1072, R:0.0017)
Batch 250/537: Loss=1.1276 (C:1.1276, R:0.0018)
Batch 275/537: Loss=1.0970 (C:1.0970, R:0.0018)
Batch 300/537: Loss=1.0871 (C:1.0871, R:0.0017)
Batch 325/537: Loss=1.0655 (C:1.0655, R:0.0018)
Batch 350/537: Loss=1.0697 (C:1.0697, R:0.0017)
Batch 375/537: Loss=1.1076 (C:1.1076, R:0.0018)
Batch 400/537: Loss=1.0721 (C:1.0721, R:0.0018)
Batch 425/537: Loss=1.0782 (C:1.0782, R:0.0018)
Batch 450/537: Loss=1.1144 (C:1.1144, R:0.0018)
Batch 475/537: Loss=1.1011 (C:1.1011, R:0.0018)
Batch 500/537: Loss=1.1172 (C:1.1172, R:0.0018)
Batch 525/537: Loss=1.1204 (C:1.1204, R:0.0018)

============================================================
Epoch 29/100 completed in 20.6s
Train: Loss=1.0970 (C:1.0970, R:0.0018) Ratio=3.15x
Val:   Loss=1.1974 (C:1.1974, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.000
No improvement for 1 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=1.0808 (C:1.0808, R:0.0018)
Batch  25/537: Loss=1.0250 (C:1.0250, R:0.0018)
Batch  50/537: Loss=1.1100 (C:1.1100, R:0.0018)
Batch  75/537: Loss=1.0945 (C:1.0945, R:0.0018)
Batch 100/537: Loss=1.1285 (C:1.1285, R:0.0018)
Batch 125/537: Loss=1.0778 (C:1.0778, R:0.0018)
Batch 150/537: Loss=1.1065 (C:1.1065, R:0.0018)
Batch 175/537: Loss=1.0981 (C:1.0981, R:0.0017)
Batch 200/537: Loss=1.0881 (C:1.0881, R:0.0018)
Batch 225/537: Loss=1.0570 (C:1.0570, R:0.0017)
Batch 250/537: Loss=1.0878 (C:1.0878, R:0.0018)
Batch 275/537: Loss=1.1032 (C:1.1032, R:0.0018)
Batch 300/537: Loss=1.1081 (C:1.1081, R:0.0018)
Batch 325/537: Loss=1.0891 (C:1.0891, R:0.0018)
Batch 350/537: Loss=1.1129 (C:1.1129, R:0.0018)
Batch 375/537: Loss=1.0981 (C:1.0981, R:0.0018)
Batch 400/537: Loss=1.0841 (C:1.0841, R:0.0018)
Batch 425/537: Loss=1.0714 (C:1.0714, R:0.0018)
Batch 450/537: Loss=1.1147 (C:1.1147, R:0.0018)
Batch 475/537: Loss=1.0838 (C:1.0838, R:0.0018)
Batch 500/537: Loss=1.1051 (C:1.1051, R:0.0017)
Batch 525/537: Loss=1.1150 (C:1.1150, R:0.0018)

============================================================
Epoch 30/100 completed in 20.4s
Train: Loss=1.0950 (C:1.0950, R:0.0018) Ratio=3.09x
Val:   Loss=1.2024 (C:1.2024, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.000
No improvement for 2 epochs
Checkpoint saved at epoch 30
============================================================

üåç Updating global dataset at epoch 31
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.369 ¬± 0.585
    Neg distances: 1.452 ¬± 0.931
    Separation ratio: 3.93x
    Gap: -2.619
    ‚úÖ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=1.1036 (C:1.1036, R:0.0017)
Batch  25/537: Loss=1.1253 (C:1.1253, R:0.0018)
Batch  50/537: Loss=1.0784 (C:1.0784, R:0.0018)
Batch  75/537: Loss=1.0959 (C:1.0959, R:0.0018)
Batch 100/537: Loss=1.0636 (C:1.0636, R:0.0018)
Batch 125/537: Loss=1.0684 (C:1.0684, R:0.0018)
Batch 150/537: Loss=1.0854 (C:1.0854, R:0.0018)
Batch 175/537: Loss=1.1007 (C:1.1007, R:0.0018)
Batch 200/537: Loss=1.1173 (C:1.1173, R:0.0018)
Batch 225/537: Loss=1.0723 (C:1.0723, R:0.0017)
Batch 250/537: Loss=1.1181 (C:1.1181, R:0.0018)
Batch 275/537: Loss=1.1114 (C:1.1114, R:0.0018)
Batch 300/537: Loss=1.0954 (C:1.0954, R:0.0018)
Batch 325/537: Loss=1.0840 (C:1.0840, R:0.0018)
Batch 350/537: Loss=1.1267 (C:1.1267, R:0.0018)
Batch 375/537: Loss=1.0648 (C:1.0648, R:0.0018)
Batch 400/537: Loss=1.0928 (C:1.0928, R:0.0018)
Batch 425/537: Loss=1.0936 (C:1.0936, R:0.0018)
Batch 450/537: Loss=1.1099 (C:1.1099, R:0.0018)
Batch 475/537: Loss=1.0826 (C:1.0826, R:0.0018)
Batch 500/537: Loss=1.0708 (C:1.0708, R:0.0018)
Batch 525/537: Loss=1.0991 (C:1.0991, R:0.0018)

============================================================
Epoch 31/100 completed in 25.6s
Train: Loss=1.0899 (C:1.0899, R:0.0018) Ratio=3.14x
Val:   Loss=1.1969 (C:1.1969, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.015
No improvement for 3 epochs
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=1.0731 (C:1.0731, R:0.0018)
Batch  25/537: Loss=1.0512 (C:1.0512, R:0.0018)
Batch  50/537: Loss=1.1014 (C:1.1014, R:0.0017)
Batch  75/537: Loss=1.0877 (C:1.0877, R:0.0017)
Batch 100/537: Loss=1.0749 (C:1.0749, R:0.0018)
Batch 125/537: Loss=1.0976 (C:1.0976, R:0.0018)
Batch 150/537: Loss=1.0770 (C:1.0770, R:0.0018)
Batch 175/537: Loss=1.0889 (C:1.0889, R:0.0018)
Batch 200/537: Loss=1.0651 (C:1.0651, R:0.0018)
Batch 225/537: Loss=1.0954 (C:1.0954, R:0.0018)
Batch 250/537: Loss=1.0711 (C:1.0711, R:0.0018)
Batch 275/537: Loss=1.0826 (C:1.0826, R:0.0018)
Batch 300/537: Loss=1.0741 (C:1.0741, R:0.0018)
Batch 325/537: Loss=1.0935 (C:1.0935, R:0.0018)
Batch 350/537: Loss=1.0788 (C:1.0788, R:0.0018)
Batch 375/537: Loss=1.1016 (C:1.1016, R:0.0017)
Batch 400/537: Loss=1.0990 (C:1.0990, R:0.0018)
Batch 425/537: Loss=1.0765 (C:1.0765, R:0.0018)
Batch 450/537: Loss=1.0704 (C:1.0704, R:0.0018)
Batch 475/537: Loss=1.0682 (C:1.0682, R:0.0018)
Batch 500/537: Loss=1.0552 (C:1.0552, R:0.0018)
Batch 525/537: Loss=1.0552 (C:1.0552, R:0.0018)

============================================================
Epoch 32/100 completed in 20.3s
Train: Loss=1.0886 (C:1.0886, R:0.0018) Ratio=3.28x
Val:   Loss=1.2004 (C:1.2004, R:0.0017) Ratio=2.21x
Reconstruction weight: 0.030
No improvement for 4 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=1.0475 (C:1.0475, R:0.0018)
Batch  25/537: Loss=1.0714 (C:1.0714, R:0.0018)
Batch  50/537: Loss=1.0684 (C:1.0684, R:0.0017)
Batch  75/537: Loss=1.0603 (C:1.0603, R:0.0018)
Batch 100/537: Loss=1.0968 (C:1.0968, R:0.0018)
Batch 125/537: Loss=1.0868 (C:1.0868, R:0.0018)
Batch 150/537: Loss=1.0701 (C:1.0701, R:0.0018)
Batch 175/537: Loss=1.0763 (C:1.0763, R:0.0018)
Batch 200/537: Loss=1.0606 (C:1.0606, R:0.0018)
Batch 225/537: Loss=1.0752 (C:1.0752, R:0.0018)
Batch 250/537: Loss=1.0925 (C:1.0925, R:0.0018)
Batch 275/537: Loss=1.0369 (C:1.0369, R:0.0018)
Batch 300/537: Loss=1.0824 (C:1.0824, R:0.0018)
Batch 325/537: Loss=1.0656 (C:1.0656, R:0.0018)
Batch 350/537: Loss=1.1097 (C:1.1097, R:0.0018)
Batch 375/537: Loss=1.0597 (C:1.0597, R:0.0018)
Batch 400/537: Loss=1.0866 (C:1.0866, R:0.0018)
Batch 425/537: Loss=1.0688 (C:1.0688, R:0.0017)
Batch 450/537: Loss=1.1009 (C:1.1009, R:0.0018)
Batch 475/537: Loss=1.0428 (C:1.0428, R:0.0018)
Batch 500/537: Loss=1.0728 (C:1.0728, R:0.0018)
Batch 525/537: Loss=1.0783 (C:1.0783, R:0.0018)

============================================================
Epoch 33/100 completed in 20.0s
Train: Loss=1.0869 (C:1.0869, R:0.0018) Ratio=3.29x
Val:   Loss=1.1902 (C:1.1902, R:0.0017) Ratio=2.20x
Reconstruction weight: 0.045
‚úÖ New best model saved (Val Loss: 1.1902)
============================================================

üåç Updating global dataset at epoch 34
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.334 ¬± 0.547
    Neg distances: 1.493 ¬± 0.937
    Separation ratio: 4.47x
    Gap: -2.593
    ‚úÖ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=1.0392 (C:1.0392, R:0.0018)
Batch  25/537: Loss=1.0099 (C:1.0099, R:0.0018)
Batch  50/537: Loss=1.0171 (C:1.0171, R:0.0018)
Batch  75/537: Loss=1.0735 (C:1.0735, R:0.0018)
Batch 100/537: Loss=1.0170 (C:1.0170, R:0.0018)
Batch 125/537: Loss=1.0571 (C:1.0571, R:0.0018)
Batch 150/537: Loss=1.0336 (C:1.0336, R:0.0018)
Batch 175/537: Loss=1.0518 (C:1.0518, R:0.0017)
Batch 200/537: Loss=1.0447 (C:1.0447, R:0.0017)
Batch 225/537: Loss=1.0685 (C:1.0685, R:0.0017)
Batch 250/537: Loss=1.0503 (C:1.0503, R:0.0018)
Batch 275/537: Loss=1.0737 (C:1.0737, R:0.0018)
Batch 300/537: Loss=1.0607 (C:1.0607, R:0.0018)
Batch 325/537: Loss=1.0332 (C:1.0332, R:0.0018)
Batch 350/537: Loss=1.0679 (C:1.0679, R:0.0018)
Batch 375/537: Loss=1.0544 (C:1.0544, R:0.0018)
Batch 400/537: Loss=1.0242 (C:1.0242, R:0.0018)
Batch 425/537: Loss=1.0463 (C:1.0463, R:0.0017)
Batch 450/537: Loss=1.0638 (C:1.0638, R:0.0018)
Batch 475/537: Loss=1.0422 (C:1.0422, R:0.0018)
Batch 500/537: Loss=1.0748 (C:1.0748, R:0.0018)
Batch 525/537: Loss=1.0387 (C:1.0387, R:0.0018)

============================================================
Epoch 34/100 completed in 25.4s
Train: Loss=1.0482 (C:1.0482, R:0.0018) Ratio=3.25x
Val:   Loss=1.1641 (C:1.1641, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.060
‚úÖ New best model saved (Val Loss: 1.1641)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=1.0572 (C:1.0572, R:0.0017)
Batch  25/537: Loss=1.0411 (C:1.0411, R:0.0017)
Batch  50/537: Loss=1.0373 (C:1.0373, R:0.0018)
Batch  75/537: Loss=1.0323 (C:1.0323, R:0.0018)
Batch 100/537: Loss=1.0248 (C:1.0248, R:0.0018)
Batch 125/537: Loss=1.0838 (C:1.0838, R:0.0018)
Batch 150/537: Loss=1.0106 (C:1.0106, R:0.0018)
Batch 175/537: Loss=1.0348 (C:1.0348, R:0.0018)
Batch 200/537: Loss=1.0137 (C:1.0137, R:0.0018)
Batch 225/537: Loss=1.0703 (C:1.0703, R:0.0018)
Batch 250/537: Loss=1.0247 (C:1.0247, R:0.0018)
Batch 275/537: Loss=1.0675 (C:1.0675, R:0.0018)
Batch 300/537: Loss=1.0395 (C:1.0395, R:0.0018)
Batch 325/537: Loss=1.0349 (C:1.0349, R:0.0018)
Batch 350/537: Loss=1.0416 (C:1.0416, R:0.0018)
Batch 375/537: Loss=1.0539 (C:1.0539, R:0.0018)
Batch 400/537: Loss=1.0574 (C:1.0574, R:0.0018)
Batch 425/537: Loss=1.0509 (C:1.0509, R:0.0017)
Batch 450/537: Loss=1.0506 (C:1.0506, R:0.0018)
Batch 475/537: Loss=1.0206 (C:1.0206, R:0.0018)
Batch 500/537: Loss=1.0872 (C:1.0872, R:0.0017)
Batch 525/537: Loss=1.0557 (C:1.0557, R:0.0018)

============================================================
Epoch 35/100 completed in 20.4s
Train: Loss=1.0477 (C:1.0477, R:0.0018) Ratio=3.25x
Val:   Loss=1.1670 (C:1.1670, R:0.0017) Ratio=2.19x
Reconstruction weight: 0.075
No improvement for 1 epochs
Checkpoint saved at epoch 35
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=1.0539 (C:1.0539, R:0.0018)
Batch  25/537: Loss=1.0469 (C:1.0469, R:0.0018)
Batch  50/537: Loss=1.0576 (C:1.0576, R:0.0018)
Batch  75/537: Loss=1.0108 (C:1.0108, R:0.0018)
Batch 100/537: Loss=1.0578 (C:1.0578, R:0.0018)
Batch 125/537: Loss=1.0686 (C:1.0686, R:0.0018)
Batch 150/537: Loss=1.0344 (C:1.0344, R:0.0018)
Batch 175/537: Loss=1.0228 (C:1.0228, R:0.0018)
Batch 200/537: Loss=1.0437 (C:1.0437, R:0.0018)
Batch 225/537: Loss=0.9988 (C:0.9988, R:0.0018)
Batch 250/537: Loss=1.0516 (C:1.0516, R:0.0018)
Batch 275/537: Loss=1.0530 (C:1.0530, R:0.0018)
Batch 300/537: Loss=1.0213 (C:1.0213, R:0.0018)
Batch 325/537: Loss=1.0447 (C:1.0447, R:0.0018)
Batch 350/537: Loss=1.0561 (C:1.0561, R:0.0018)
Batch 375/537: Loss=1.0166 (C:1.0166, R:0.0018)
Batch 400/537: Loss=1.0419 (C:1.0419, R:0.0018)
Batch 425/537: Loss=1.0305 (C:1.0305, R:0.0018)
Batch 450/537: Loss=1.0414 (C:1.0414, R:0.0018)
Batch 475/537: Loss=1.0391 (C:1.0391, R:0.0018)
Batch 500/537: Loss=1.0221 (C:1.0221, R:0.0018)
Batch 525/537: Loss=1.0670 (C:1.0670, R:0.0018)

============================================================
Epoch 36/100 completed in 20.5s
Train: Loss=1.0449 (C:1.0449, R:0.0018) Ratio=3.39x
Val:   Loss=1.1733 (C:1.1733, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.090
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 37
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.346 ¬± 0.553
    Neg distances: 1.506 ¬± 0.933
    Separation ratio: 4.35x
    Gap: -2.583
    ‚úÖ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=1.0394 (C:1.0394, R:0.0018)
Batch  25/537: Loss=1.0411 (C:1.0411, R:0.0017)
Batch  50/537: Loss=1.0241 (C:1.0241, R:0.0018)
Batch  75/537: Loss=1.0324 (C:1.0324, R:0.0018)
Batch 100/537: Loss=1.0629 (C:1.0629, R:0.0018)
Batch 125/537: Loss=1.0597 (C:1.0597, R:0.0018)
Batch 150/537: Loss=1.0391 (C:1.0391, R:0.0018)
Batch 175/537: Loss=1.0328 (C:1.0328, R:0.0018)
Batch 200/537: Loss=1.0710 (C:1.0710, R:0.0018)
Batch 225/537: Loss=1.0202 (C:1.0202, R:0.0018)
Batch 250/537: Loss=1.0744 (C:1.0744, R:0.0018)
Batch 275/537: Loss=1.0372 (C:1.0372, R:0.0018)
Batch 300/537: Loss=1.0133 (C:1.0133, R:0.0018)
Batch 325/537: Loss=1.0683 (C:1.0683, R:0.0018)
Batch 350/537: Loss=1.0248 (C:1.0248, R:0.0018)
Batch 375/537: Loss=1.0245 (C:1.0245, R:0.0018)
Batch 400/537: Loss=1.0590 (C:1.0590, R:0.0018)
Batch 425/537: Loss=1.0312 (C:1.0312, R:0.0018)
Batch 450/537: Loss=1.0247 (C:1.0247, R:0.0018)
Batch 475/537: Loss=1.0228 (C:1.0228, R:0.0018)
Batch 500/537: Loss=1.0427 (C:1.0427, R:0.0017)
Batch 525/537: Loss=1.0540 (C:1.0540, R:0.0018)

============================================================
Epoch 37/100 completed in 25.3s
Train: Loss=1.0475 (C:1.0475, R:0.0018) Ratio=3.37x
Val:   Loss=1.1666 (C:1.1666, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.105
No improvement for 3 epochs
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=1.0412 (C:1.0412, R:0.0018)
Batch  25/537: Loss=1.0330 (C:1.0330, R:0.0018)
Batch  50/537: Loss=1.0342 (C:1.0342, R:0.0018)
Batch  75/537: Loss=1.0644 (C:1.0644, R:0.0018)
Batch 100/537: Loss=1.0304 (C:1.0304, R:0.0018)
Batch 125/537: Loss=1.0469 (C:1.0469, R:0.0018)
Batch 150/537: Loss=1.0445 (C:1.0445, R:0.0017)
Batch 175/537: Loss=1.0739 (C:1.0739, R:0.0018)
Batch 200/537: Loss=1.0432 (C:1.0432, R:0.0018)
Batch 225/537: Loss=1.0552 (C:1.0552, R:0.0018)
Batch 250/537: Loss=1.0276 (C:1.0276, R:0.0018)
Batch 275/537: Loss=1.0209 (C:1.0209, R:0.0018)
Batch 300/537: Loss=1.0366 (C:1.0366, R:0.0018)
Batch 325/537: Loss=1.0667 (C:1.0667, R:0.0018)
Batch 350/537: Loss=1.0644 (C:1.0644, R:0.0017)
Batch 375/537: Loss=1.0317 (C:1.0317, R:0.0018)
Batch 400/537: Loss=1.0137 (C:1.0137, R:0.0018)
Batch 425/537: Loss=1.0304 (C:1.0304, R:0.0017)
Batch 450/537: Loss=1.0285 (C:1.0285, R:0.0018)
Batch 475/537: Loss=1.0642 (C:1.0642, R:0.0018)
Batch 500/537: Loss=1.0804 (C:1.0804, R:0.0018)
Batch 525/537: Loss=1.0514 (C:1.0514, R:0.0018)

============================================================
Epoch 38/100 completed in 20.4s
Train: Loss=1.0448 (C:1.0448, R:0.0018) Ratio=3.37x
Val:   Loss=1.1752 (C:1.1752, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.120
No improvement for 4 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=1.0421 (C:1.0421, R:0.0018)
Batch  25/537: Loss=1.0170 (C:1.0170, R:0.0018)
Batch  50/537: Loss=1.0549 (C:1.0549, R:0.0018)
Batch  75/537: Loss=1.0341 (C:1.0341, R:0.0018)
Batch 100/537: Loss=1.0660 (C:1.0660, R:0.0018)
Batch 125/537: Loss=1.0372 (C:1.0372, R:0.0018)
Batch 150/537: Loss=1.0111 (C:1.0111, R:0.0018)
Batch 175/537: Loss=1.0239 (C:1.0239, R:0.0018)
Batch 200/537: Loss=1.0323 (C:1.0323, R:0.0018)
Batch 225/537: Loss=1.0436 (C:1.0436, R:0.0018)
Batch 250/537: Loss=1.0358 (C:1.0358, R:0.0018)
Batch 275/537: Loss=1.0402 (C:1.0402, R:0.0018)
Batch 300/537: Loss=0.9946 (C:0.9946, R:0.0018)
Batch 325/537: Loss=1.0203 (C:1.0203, R:0.0018)
Batch 350/537: Loss=1.0382 (C:1.0382, R:0.0018)
Batch 375/537: Loss=1.0101 (C:1.0101, R:0.0018)
Batch 400/537: Loss=1.0210 (C:1.0210, R:0.0018)
Batch 425/537: Loss=1.0626 (C:1.0626, R:0.0018)
Batch 450/537: Loss=1.0313 (C:1.0313, R:0.0018)
Batch 475/537: Loss=1.0486 (C:1.0486, R:0.0018)
Batch 500/537: Loss=1.0527 (C:1.0527, R:0.0018)
Batch 525/537: Loss=1.0753 (C:1.0753, R:0.0018)

============================================================
Epoch 39/100 completed in 20.3s
Train: Loss=1.0431 (C:1.0431, R:0.0018) Ratio=3.48x
Val:   Loss=1.1663 (C:1.1663, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.135
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 40
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.318 ¬± 0.545
    Neg distances: 1.508 ¬± 0.951
    Separation ratio: 4.74x
    Gap: -2.711
    ‚úÖ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=1.0288 (C:1.0288, R:0.0018)
Batch  25/537: Loss=1.0127 (C:1.0127, R:0.0018)
Batch  50/537: Loss=1.0193 (C:1.0193, R:0.0018)
Batch  75/537: Loss=1.0207 (C:1.0207, R:0.0018)
Batch 100/537: Loss=1.0395 (C:1.0395, R:0.0018)
Batch 125/537: Loss=1.0460 (C:1.0460, R:0.0017)
Batch 150/537: Loss=1.0301 (C:1.0301, R:0.0018)
Batch 175/537: Loss=1.0076 (C:1.0076, R:0.0017)
Batch 200/537: Loss=1.0053 (C:1.0053, R:0.0018)
Batch 225/537: Loss=1.0387 (C:1.0387, R:0.0018)
Batch 250/537: Loss=1.0173 (C:1.0173, R:0.0018)
Batch 275/537: Loss=1.0073 (C:1.0073, R:0.0018)
Batch 300/537: Loss=1.0429 (C:1.0429, R:0.0018)
Batch 325/537: Loss=1.0699 (C:1.0699, R:0.0018)
Batch 350/537: Loss=1.0074 (C:1.0074, R:0.0018)
Batch 375/537: Loss=1.0210 (C:1.0210, R:0.0018)
Batch 400/537: Loss=1.0087 (C:1.0087, R:0.0017)
Batch 425/537: Loss=1.0306 (C:1.0306, R:0.0018)
Batch 450/537: Loss=1.0430 (C:1.0430, R:0.0018)
Batch 475/537: Loss=1.0389 (C:1.0389, R:0.0018)
Batch 500/537: Loss=1.0242 (C:1.0242, R:0.0018)
Batch 525/537: Loss=1.0382 (C:1.0382, R:0.0018)

============================================================
Epoch 40/100 completed in 25.2s
Train: Loss=1.0271 (C:1.0271, R:0.0018) Ratio=3.41x
Val:   Loss=1.1627 (C:1.1627, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.150
‚úÖ New best model saved (Val Loss: 1.1627)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=1.0141 (C:1.0141, R:0.0018)
Batch  25/537: Loss=1.0316 (C:1.0316, R:0.0018)
Batch  50/537: Loss=1.0368 (C:1.0368, R:0.0018)
Batch  75/537: Loss=1.0040 (C:1.0040, R:0.0018)
Batch 100/537: Loss=1.0373 (C:1.0373, R:0.0018)
Batch 125/537: Loss=1.0140 (C:1.0140, R:0.0018)
Batch 150/537: Loss=1.0337 (C:1.0337, R:0.0018)
Batch 175/537: Loss=1.0170 (C:1.0170, R:0.0018)
Batch 200/537: Loss=1.0349 (C:1.0349, R:0.0017)
Batch 225/537: Loss=1.0105 (C:1.0105, R:0.0018)
Batch 250/537: Loss=1.0201 (C:1.0201, R:0.0017)
Batch 275/537: Loss=1.0616 (C:1.0616, R:0.0018)
Batch 300/537: Loss=1.0260 (C:1.0260, R:0.0017)
Batch 325/537: Loss=1.0405 (C:1.0405, R:0.0018)
Batch 350/537: Loss=1.0392 (C:1.0392, R:0.0018)
Batch 375/537: Loss=1.0283 (C:1.0283, R:0.0018)
Batch 400/537: Loss=1.0273 (C:1.0273, R:0.0018)
Batch 425/537: Loss=0.9994 (C:0.9994, R:0.0018)
Batch 450/537: Loss=0.9702 (C:0.9702, R:0.0018)
Batch 475/537: Loss=1.0061 (C:1.0061, R:0.0018)
Batch 500/537: Loss=1.0580 (C:1.0580, R:0.0018)
Batch 525/537: Loss=1.0366 (C:1.0366, R:0.0018)

============================================================
Epoch 41/100 completed in 20.3s
Train: Loss=1.0267 (C:1.0267, R:0.0018) Ratio=3.43x
Val:   Loss=1.1604 (C:1.1604, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.165
‚úÖ New best model saved (Val Loss: 1.1604)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=1.0006 (C:1.0006, R:0.0018)
Batch  25/537: Loss=1.0334 (C:1.0334, R:0.0018)
Batch  50/537: Loss=1.0196 (C:1.0196, R:0.0018)
Batch  75/537: Loss=1.0206 (C:1.0206, R:0.0018)
Batch 100/537: Loss=0.9877 (C:0.9877, R:0.0018)
Batch 125/537: Loss=0.9962 (C:0.9962, R:0.0018)
Batch 150/537: Loss=1.0161 (C:1.0161, R:0.0018)
Batch 175/537: Loss=1.0145 (C:1.0145, R:0.0018)
Batch 200/537: Loss=1.0148 (C:1.0148, R:0.0018)
Batch 225/537: Loss=1.0282 (C:1.0282, R:0.0017)
Batch 250/537: Loss=1.0316 (C:1.0316, R:0.0018)
Batch 275/537: Loss=1.0318 (C:1.0318, R:0.0018)
Batch 300/537: Loss=1.0037 (C:1.0037, R:0.0018)
Batch 325/537: Loss=0.9945 (C:0.9945, R:0.0018)
Batch 350/537: Loss=0.9799 (C:0.9799, R:0.0017)
Batch 375/537: Loss=1.0465 (C:1.0465, R:0.0018)
Batch 400/537: Loss=1.0314 (C:1.0314, R:0.0018)
Batch 425/537: Loss=0.9786 (C:0.9786, R:0.0018)
Batch 450/537: Loss=1.0152 (C:1.0152, R:0.0017)
Batch 475/537: Loss=1.0153 (C:1.0153, R:0.0018)
Batch 500/537: Loss=1.0169 (C:1.0169, R:0.0017)
Batch 525/537: Loss=1.0317 (C:1.0317, R:0.0018)

============================================================
Epoch 42/100 completed in 20.5s
Train: Loss=1.0252 (C:1.0252, R:0.0018) Ratio=3.48x
Val:   Loss=1.1499 (C:1.1499, R:0.0017) Ratio=2.14x
Reconstruction weight: 0.180
‚úÖ New best model saved (Val Loss: 1.1499)
============================================================

üåç Updating global dataset at epoch 43
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.341 ¬± 0.557
    Neg distances: 1.478 ¬± 0.942
    Separation ratio: 4.34x
    Gap: -2.659
    ‚úÖ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=1.0487 (C:1.0487, R:0.0018)
Batch  25/537: Loss=1.0361 (C:1.0361, R:0.0018)
Batch  50/537: Loss=1.0316 (C:1.0316, R:0.0018)
Batch  75/537: Loss=1.0663 (C:1.0663, R:0.0018)
Batch 100/537: Loss=1.0655 (C:1.0655, R:0.0018)
Batch 125/537: Loss=1.0165 (C:1.0165, R:0.0018)
Batch 150/537: Loss=1.0642 (C:1.0642, R:0.0018)
Batch 175/537: Loss=1.0396 (C:1.0396, R:0.0018)
Batch 200/537: Loss=1.0616 (C:1.0616, R:0.0018)
Batch 225/537: Loss=1.0455 (C:1.0455, R:0.0018)
Batch 250/537: Loss=1.0591 (C:1.0591, R:0.0018)
Batch 275/537: Loss=1.0443 (C:1.0443, R:0.0018)
Batch 300/537: Loss=1.0688 (C:1.0688, R:0.0018)
Batch 325/537: Loss=1.0362 (C:1.0362, R:0.0018)
Batch 350/537: Loss=1.0457 (C:1.0457, R:0.0018)
Batch 375/537: Loss=1.0422 (C:1.0422, R:0.0018)
Batch 400/537: Loss=1.0367 (C:1.0367, R:0.0018)
Batch 425/537: Loss=1.0715 (C:1.0715, R:0.0017)
Batch 450/537: Loss=1.0775 (C:1.0775, R:0.0018)
Batch 475/537: Loss=1.0693 (C:1.0693, R:0.0018)
Batch 500/537: Loss=1.0355 (C:1.0355, R:0.0018)
Batch 525/537: Loss=1.0028 (C:1.0028, R:0.0018)

============================================================
Epoch 43/100 completed in 25.7s
Train: Loss=1.0484 (C:1.0484, R:0.0018) Ratio=3.49x
Val:   Loss=1.1860 (C:1.1860, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.195
No improvement for 1 epochs
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=1.0184 (C:1.0184, R:0.0018)
Batch  25/537: Loss=1.0354 (C:1.0354, R:0.0018)
Batch  50/537: Loss=1.0309 (C:1.0309, R:0.0017)
Batch  75/537: Loss=1.0274 (C:1.0274, R:0.0018)
Batch 100/537: Loss=1.0439 (C:1.0439, R:0.0018)
Batch 125/537: Loss=1.0329 (C:1.0329, R:0.0018)
Batch 150/537: Loss=1.0461 (C:1.0461, R:0.0018)
Batch 175/537: Loss=1.0249 (C:1.0249, R:0.0017)
Batch 200/537: Loss=1.0616 (C:1.0616, R:0.0017)
Batch 225/537: Loss=1.0454 (C:1.0454, R:0.0018)
Batch 250/537: Loss=1.0447 (C:1.0447, R:0.0018)
Batch 275/537: Loss=1.0463 (C:1.0463, R:0.0018)
Batch 300/537: Loss=1.0410 (C:1.0410, R:0.0018)
Batch 325/537: Loss=1.0492 (C:1.0492, R:0.0017)
Batch 350/537: Loss=1.0672 (C:1.0672, R:0.0018)
Batch 375/537: Loss=1.0482 (C:1.0482, R:0.0018)
Batch 400/537: Loss=1.0512 (C:1.0512, R:0.0018)
Batch 425/537: Loss=1.0428 (C:1.0428, R:0.0018)
Batch 450/537: Loss=1.0675 (C:1.0675, R:0.0018)
Batch 475/537: Loss=1.0864 (C:1.0864, R:0.0018)
Batch 500/537: Loss=1.0403 (C:1.0403, R:0.0018)
Batch 525/537: Loss=1.0353 (C:1.0353, R:0.0018)

============================================================
Epoch 44/100 completed in 20.3s
Train: Loss=1.0463 (C:1.0463, R:0.0018) Ratio=3.51x
Val:   Loss=1.1727 (C:1.1727, R:0.0017) Ratio=2.14x
Reconstruction weight: 0.210
No improvement for 2 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=1.0477 (C:1.0477, R:0.0018)
Batch  25/537: Loss=1.0258 (C:1.0258, R:0.0018)
Batch  50/537: Loss=1.0338 (C:1.0338, R:0.0018)
Batch  75/537: Loss=1.0528 (C:1.0528, R:0.0018)
Batch 100/537: Loss=1.0416 (C:1.0416, R:0.0018)
Batch 125/537: Loss=1.0606 (C:1.0606, R:0.0018)
Batch 150/537: Loss=1.0815 (C:1.0815, R:0.0018)
Batch 175/537: Loss=1.0171 (C:1.0171, R:0.0018)
Batch 200/537: Loss=1.0356 (C:1.0356, R:0.0017)
Batch 225/537: Loss=1.0545 (C:1.0545, R:0.0018)
Batch 250/537: Loss=1.0193 (C:1.0193, R:0.0018)
Batch 275/537: Loss=1.0333 (C:1.0333, R:0.0017)
Batch 300/537: Loss=1.0475 (C:1.0475, R:0.0017)
Batch 325/537: Loss=1.0205 (C:1.0205, R:0.0018)
Batch 350/537: Loss=1.0553 (C:1.0553, R:0.0018)
Batch 375/537: Loss=1.0727 (C:1.0727, R:0.0018)
Batch 400/537: Loss=1.0411 (C:1.0411, R:0.0018)
Batch 425/537: Loss=1.0015 (C:1.0015, R:0.0018)
Batch 450/537: Loss=1.0542 (C:1.0542, R:0.0017)
Batch 475/537: Loss=1.0794 (C:1.0794, R:0.0018)
Batch 500/537: Loss=1.0200 (C:1.0200, R:0.0017)
Batch 525/537: Loss=1.0471 (C:1.0471, R:0.0018)

============================================================
Epoch 45/100 completed in 20.0s
Train: Loss=1.0453 (C:1.0453, R:0.0018) Ratio=3.50x
Val:   Loss=1.1919 (C:1.1919, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.225
No improvement for 3 epochs
Checkpoint saved at epoch 45
============================================================

üåç Updating global dataset at epoch 46
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.278 ¬± 0.501
    Neg distances: 1.514 ¬± 0.963
    Separation ratio: 5.45x
    Gap: -2.665
    ‚úÖ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=1.0106 (C:1.0106, R:0.0018)
Batch  25/537: Loss=0.9772 (C:0.9772, R:0.0018)
Batch  50/537: Loss=1.0088 (C:1.0088, R:0.0018)
Batch  75/537: Loss=0.9711 (C:0.9711, R:0.0018)
Batch 100/537: Loss=0.9632 (C:0.9632, R:0.0018)
Batch 125/537: Loss=0.9693 (C:0.9693, R:0.0018)
Batch 150/537: Loss=0.9814 (C:0.9814, R:0.0018)
Batch 175/537: Loss=1.0300 (C:1.0300, R:0.0018)
Batch 200/537: Loss=0.9814 (C:0.9814, R:0.0018)
Batch 225/537: Loss=1.0157 (C:1.0157, R:0.0018)
Batch 250/537: Loss=0.9928 (C:0.9928, R:0.0018)
Batch 275/537: Loss=0.9806 (C:0.9806, R:0.0018)
Batch 300/537: Loss=1.0143 (C:1.0143, R:0.0017)
Batch 325/537: Loss=0.9833 (C:0.9833, R:0.0018)
Batch 350/537: Loss=1.0356 (C:1.0356, R:0.0018)
Batch 375/537: Loss=0.9851 (C:0.9851, R:0.0017)
Batch 400/537: Loss=1.0118 (C:1.0118, R:0.0018)
Batch 425/537: Loss=1.0019 (C:1.0019, R:0.0018)
Batch 450/537: Loss=0.9714 (C:0.9714, R:0.0018)
Batch 475/537: Loss=1.0414 (C:1.0414, R:0.0017)
Batch 500/537: Loss=0.9837 (C:0.9837, R:0.0018)
Batch 525/537: Loss=0.9892 (C:0.9892, R:0.0018)

============================================================
Epoch 46/100 completed in 24.7s
Train: Loss=0.9971 (C:0.9971, R:0.0018) Ratio=3.55x
Val:   Loss=1.1466 (C:1.1466, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.240
‚úÖ New best model saved (Val Loss: 1.1466)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=0.9507 (C:0.9507, R:0.0018)
Batch  25/537: Loss=0.9953 (C:0.9953, R:0.0018)
Batch  50/537: Loss=0.9768 (C:0.9768, R:0.0018)
Batch  75/537: Loss=0.9838 (C:0.9838, R:0.0018)
Batch 100/537: Loss=0.9901 (C:0.9901, R:0.0018)
Batch 125/537: Loss=1.0109 (C:1.0109, R:0.0018)
Batch 150/537: Loss=1.0024 (C:1.0024, R:0.0018)
Batch 175/537: Loss=0.9484 (C:0.9484, R:0.0018)
Batch 200/537: Loss=0.9928 (C:0.9928, R:0.0018)
Batch 225/537: Loss=1.0111 (C:1.0111, R:0.0017)
Batch 250/537: Loss=0.9938 (C:0.9938, R:0.0018)
Batch 275/537: Loss=1.0028 (C:1.0028, R:0.0018)
Batch 300/537: Loss=1.0064 (C:1.0064, R:0.0018)
Batch 325/537: Loss=1.0153 (C:1.0153, R:0.0017)
Batch 350/537: Loss=0.9675 (C:0.9675, R:0.0018)
Batch 375/537: Loss=0.9888 (C:0.9888, R:0.0018)
Batch 400/537: Loss=1.0012 (C:1.0012, R:0.0018)
Batch 425/537: Loss=0.9839 (C:0.9839, R:0.0017)
Batch 450/537: Loss=0.9952 (C:0.9952, R:0.0018)
Batch 475/537: Loss=0.9882 (C:0.9882, R:0.0018)
Batch 500/537: Loss=0.9779 (C:0.9779, R:0.0018)
Batch 525/537: Loss=1.0155 (C:1.0155, R:0.0018)

============================================================
Epoch 47/100 completed in 19.8s
Train: Loss=0.9954 (C:0.9954, R:0.0018) Ratio=3.58x
Val:   Loss=1.1446 (C:1.1446, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.255
‚úÖ New best model saved (Val Loss: 1.1446)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=0.9962 (C:0.9962, R:0.0017)
Batch  25/537: Loss=1.0215 (C:1.0215, R:0.0017)
Batch  50/537: Loss=0.9695 (C:0.9695, R:0.0018)
Batch  75/537: Loss=0.9498 (C:0.9498, R:0.0018)
Batch 100/537: Loss=0.9339 (C:0.9339, R:0.0018)
Batch 125/537: Loss=0.9673 (C:0.9673, R:0.0018)
Batch 150/537: Loss=0.9986 (C:0.9986, R:0.0018)
Batch 175/537: Loss=0.9828 (C:0.9828, R:0.0018)
Batch 200/537: Loss=1.0112 (C:1.0112, R:0.0018)
Batch 225/537: Loss=1.0105 (C:1.0105, R:0.0018)
Batch 250/537: Loss=0.9831 (C:0.9831, R:0.0018)
Batch 275/537: Loss=0.9716 (C:0.9716, R:0.0018)
Batch 300/537: Loss=0.9463 (C:0.9463, R:0.0018)
Batch 325/537: Loss=0.9993 (C:0.9993, R:0.0018)
Batch 350/537: Loss=1.0030 (C:1.0030, R:0.0018)
Batch 375/537: Loss=0.9957 (C:0.9957, R:0.0018)
Batch 400/537: Loss=1.0002 (C:1.0002, R:0.0018)
Batch 425/537: Loss=1.0114 (C:1.0114, R:0.0018)
Batch 450/537: Loss=1.0131 (C:1.0131, R:0.0018)
Batch 475/537: Loss=0.9952 (C:0.9952, R:0.0018)
Batch 500/537: Loss=0.9688 (C:0.9688, R:0.0017)
Batch 525/537: Loss=1.0272 (C:1.0272, R:0.0018)

============================================================
Epoch 48/100 completed in 19.8s
Train: Loss=0.9923 (C:0.9923, R:0.0018) Ratio=3.62x
Val:   Loss=1.1407 (C:1.1407, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.270
‚úÖ New best model saved (Val Loss: 1.1407)
============================================================

üåç Updating global dataset at epoch 49
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.300 ¬± 0.534
    Neg distances: 1.510 ¬± 0.969
    Separation ratio: 5.03x
    Gap: -2.642
    ‚úÖ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=0.9832 (C:0.9832, R:0.0018)
Batch  25/537: Loss=0.9897 (C:0.9897, R:0.0018)
Batch  50/537: Loss=0.9961 (C:0.9961, R:0.0018)
Batch  75/537: Loss=0.9621 (C:0.9621, R:0.0018)
Batch 100/537: Loss=1.0112 (C:1.0112, R:0.0018)
Batch 125/537: Loss=1.0475 (C:1.0475, R:0.0018)
Batch 150/537: Loss=1.0135 (C:1.0135, R:0.0017)
Batch 175/537: Loss=0.9896 (C:0.9896, R:0.0018)
Batch 200/537: Loss=1.0211 (C:1.0211, R:0.0018)
Batch 225/537: Loss=1.0148 (C:1.0148, R:0.0018)
Batch 250/537: Loss=1.0278 (C:1.0278, R:0.0018)
Batch 275/537: Loss=1.0208 (C:1.0208, R:0.0017)
Batch 300/537: Loss=0.9890 (C:0.9890, R:0.0018)
Batch 325/537: Loss=1.0037 (C:1.0037, R:0.0018)
Batch 350/537: Loss=1.0149 (C:1.0149, R:0.0018)
Batch 375/537: Loss=1.0292 (C:1.0292, R:0.0018)
Batch 400/537: Loss=1.0046 (C:1.0046, R:0.0018)
Batch 425/537: Loss=1.0391 (C:1.0391, R:0.0018)
Batch 450/537: Loss=1.0101 (C:1.0101, R:0.0018)
Batch 475/537: Loss=1.0132 (C:1.0132, R:0.0017)
Batch 500/537: Loss=0.9883 (C:0.9883, R:0.0018)
Batch 525/537: Loss=1.0556 (C:1.0556, R:0.0018)

============================================================
Epoch 49/100 completed in 25.5s
Train: Loss=1.0095 (C:1.0095, R:0.0018) Ratio=3.60x
Val:   Loss=1.1511 (C:1.1511, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.285
No improvement for 1 epochs
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=0.9594 (C:0.9594, R:0.0018)
Batch  25/537: Loss=0.9760 (C:0.9760, R:0.0018)
Batch  50/537: Loss=1.0207 (C:1.0207, R:0.0018)
Batch  75/537: Loss=0.9816 (C:0.9816, R:0.0018)
Batch 100/537: Loss=1.0078 (C:1.0078, R:0.0018)
Batch 125/537: Loss=1.0034 (C:1.0034, R:0.0018)
Batch 150/537: Loss=0.9910 (C:0.9910, R:0.0018)
Batch 175/537: Loss=1.0015 (C:1.0015, R:0.0018)
Batch 200/537: Loss=1.0142 (C:1.0142, R:0.0018)
Batch 225/537: Loss=1.0186 (C:1.0186, R:0.0018)
Batch 250/537: Loss=1.0151 (C:1.0151, R:0.0018)
Batch 275/537: Loss=0.9952 (C:0.9952, R:0.0018)
Batch 300/537: Loss=1.0044 (C:1.0044, R:0.0018)
Batch 325/537: Loss=0.9811 (C:0.9811, R:0.0018)
Batch 350/537: Loss=1.0442 (C:1.0442, R:0.0018)
Batch 375/537: Loss=0.9763 (C:0.9763, R:0.0018)
Batch 400/537: Loss=0.9999 (C:0.9999, R:0.0018)
Batch 425/537: Loss=1.0279 (C:1.0279, R:0.0018)
Batch 450/537: Loss=1.0274 (C:1.0274, R:0.0018)
Batch 475/537: Loss=1.0188 (C:1.0188, R:0.0018)
Batch 500/537: Loss=1.0215 (C:1.0215, R:0.0018)
Batch 525/537: Loss=1.0080 (C:1.0080, R:0.0018)

============================================================
Epoch 50/100 completed in 20.4s
Train: Loss=1.0080 (C:1.0080, R:0.0018) Ratio=3.65x
Val:   Loss=1.1469 (C:1.1469, R:0.0017) Ratio=2.16x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 50
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=0.9973 (C:0.9973, R:0.0018)
Batch  25/537: Loss=0.9973 (C:0.9973, R:0.0018)
Batch  50/537: Loss=1.0139 (C:1.0139, R:0.0018)
Batch  75/537: Loss=0.9743 (C:0.9743, R:0.0018)
Batch 100/537: Loss=1.0046 (C:1.0046, R:0.0017)
Batch 125/537: Loss=0.9963 (C:0.9963, R:0.0018)
Batch 150/537: Loss=0.9876 (C:0.9876, R:0.0018)
Batch 175/537: Loss=0.9685 (C:0.9685, R:0.0018)
Batch 200/537: Loss=1.0017 (C:1.0017, R:0.0018)
Batch 225/537: Loss=1.0072 (C:1.0072, R:0.0018)
Batch 250/537: Loss=0.9974 (C:0.9974, R:0.0018)
Batch 275/537: Loss=0.9814 (C:0.9814, R:0.0018)
Batch 300/537: Loss=0.9847 (C:0.9847, R:0.0018)
Batch 325/537: Loss=1.0082 (C:1.0082, R:0.0018)
Batch 350/537: Loss=1.0092 (C:1.0092, R:0.0018)
Batch 375/537: Loss=0.9999 (C:0.9999, R:0.0018)
Batch 400/537: Loss=1.0280 (C:1.0280, R:0.0018)
Batch 425/537: Loss=1.0027 (C:1.0027, R:0.0017)
Batch 450/537: Loss=1.0259 (C:1.0259, R:0.0017)
Batch 475/537: Loss=1.0404 (C:1.0404, R:0.0018)
Batch 500/537: Loss=1.0267 (C:1.0267, R:0.0018)
Batch 525/537: Loss=1.0055 (C:1.0055, R:0.0017)

============================================================
Epoch 51/100 completed in 20.0s
Train: Loss=1.0055 (C:1.0055, R:0.0018) Ratio=3.66x
Val:   Loss=1.1559 (C:1.1559, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

üåç Updating global dataset at epoch 52
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.288 ¬± 0.511
    Neg distances: 1.519 ¬± 0.967
    Separation ratio: 5.28x
    Gap: -2.593
    ‚úÖ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=0.9927 (C:0.9927, R:0.0018)
Batch  25/537: Loss=0.9674 (C:0.9674, R:0.0018)
Batch  50/537: Loss=0.9699 (C:0.9699, R:0.0018)
Batch  75/537: Loss=0.9900 (C:0.9900, R:0.0017)
Batch 100/537: Loss=0.9679 (C:0.9679, R:0.0018)
Batch 125/537: Loss=0.9877 (C:0.9877, R:0.0018)
Batch 150/537: Loss=1.0048 (C:1.0048, R:0.0018)
Batch 175/537: Loss=0.9786 (C:0.9786, R:0.0018)
Batch 200/537: Loss=0.9803 (C:0.9803, R:0.0018)
Batch 225/537: Loss=0.9674 (C:0.9674, R:0.0018)
Batch 250/537: Loss=0.9716 (C:0.9716, R:0.0018)
Batch 275/537: Loss=0.9988 (C:0.9988, R:0.0017)
Batch 300/537: Loss=1.0110 (C:1.0110, R:0.0018)
Batch 325/537: Loss=0.9807 (C:0.9807, R:0.0018)
Batch 350/537: Loss=1.0241 (C:1.0241, R:0.0018)
Batch 375/537: Loss=1.0016 (C:1.0016, R:0.0018)
Batch 400/537: Loss=0.9672 (C:0.9672, R:0.0018)
Batch 425/537: Loss=1.0273 (C:1.0273, R:0.0018)
Batch 450/537: Loss=0.9830 (C:0.9830, R:0.0018)
Batch 475/537: Loss=0.9919 (C:0.9919, R:0.0018)
Batch 500/537: Loss=0.9711 (C:0.9711, R:0.0018)
Batch 525/537: Loss=0.9956 (C:0.9956, R:0.0018)

============================================================
Epoch 52/100 completed in 24.9s
Train: Loss=0.9947 (C:0.9947, R:0.0018) Ratio=3.72x
Val:   Loss=1.1487 (C:1.1487, R:0.0017) Ratio=2.14x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=0.9725 (C:0.9725, R:0.0018)
Batch  25/537: Loss=0.9926 (C:0.9926, R:0.0018)
Batch  50/537: Loss=0.9944 (C:0.9944, R:0.0018)
Batch  75/537: Loss=0.9689 (C:0.9689, R:0.0018)
Batch 100/537: Loss=0.9762 (C:0.9762, R:0.0018)
Batch 125/537: Loss=0.9636 (C:0.9636, R:0.0017)
Batch 150/537: Loss=0.9945 (C:0.9945, R:0.0017)
Batch 175/537: Loss=0.9758 (C:0.9758, R:0.0018)
Batch 200/537: Loss=1.0074 (C:1.0074, R:0.0018)
Batch 225/537: Loss=1.0215 (C:1.0215, R:0.0018)
Batch 250/537: Loss=1.0083 (C:1.0083, R:0.0018)
Batch 275/537: Loss=1.0068 (C:1.0068, R:0.0018)
Batch 300/537: Loss=0.9870 (C:0.9870, R:0.0018)
Batch 325/537: Loss=0.9967 (C:0.9967, R:0.0018)
Batch 350/537: Loss=0.9858 (C:0.9858, R:0.0018)
Batch 375/537: Loss=0.9718 (C:0.9718, R:0.0018)
Batch 400/537: Loss=0.9958 (C:0.9958, R:0.0018)
Batch 425/537: Loss=0.9898 (C:0.9898, R:0.0017)
Batch 450/537: Loss=1.0041 (C:1.0041, R:0.0017)
Batch 475/537: Loss=1.0289 (C:1.0289, R:0.0018)
Batch 500/537: Loss=1.0144 (C:1.0144, R:0.0018)
Batch 525/537: Loss=1.0048 (C:1.0048, R:0.0018)

============================================================
Epoch 53/100 completed in 19.8s
Train: Loss=0.9925 (C:0.9925, R:0.0018) Ratio=3.65x
Val:   Loss=1.1407 (C:1.1407, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 1.1407)
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=0.9907 (C:0.9907, R:0.0018)
Batch  25/537: Loss=1.0067 (C:1.0067, R:0.0018)
Batch  50/537: Loss=0.9773 (C:0.9773, R:0.0018)
Batch  75/537: Loss=0.9808 (C:0.9808, R:0.0017)
Batch 100/537: Loss=0.9796 (C:0.9796, R:0.0018)
Batch 125/537: Loss=0.9842 (C:0.9842, R:0.0018)
Batch 150/537: Loss=1.0018 (C:1.0018, R:0.0018)
Batch 175/537: Loss=0.9975 (C:0.9975, R:0.0018)
Batch 200/537: Loss=1.0018 (C:1.0018, R:0.0018)
Batch 225/537: Loss=1.0120 (C:1.0120, R:0.0017)
Batch 250/537: Loss=1.0014 (C:1.0014, R:0.0018)
Batch 275/537: Loss=0.9937 (C:0.9937, R:0.0017)
Batch 300/537: Loss=1.0034 (C:1.0034, R:0.0018)
Batch 325/537: Loss=0.9858 (C:0.9858, R:0.0017)
Batch 350/537: Loss=0.9884 (C:0.9884, R:0.0018)
Batch 375/537: Loss=1.0094 (C:1.0094, R:0.0017)
Batch 400/537: Loss=0.9810 (C:0.9810, R:0.0018)
Batch 425/537: Loss=0.9932 (C:0.9932, R:0.0018)
Batch 450/537: Loss=0.9984 (C:0.9984, R:0.0018)
Batch 475/537: Loss=1.0081 (C:1.0081, R:0.0018)
Batch 500/537: Loss=1.0142 (C:1.0142, R:0.0017)
Batch 525/537: Loss=1.0099 (C:1.0099, R:0.0018)

============================================================
Epoch 54/100 completed in 20.4s
Train: Loss=0.9916 (C:0.9916, R:0.0018) Ratio=3.67x
Val:   Loss=1.1469 (C:1.1469, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 55
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.281 ¬± 0.517
    Neg distances: 1.548 ¬± 0.977
    Separation ratio: 5.50x
    Gap: -2.665
    ‚úÖ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=0.9991 (C:0.9991, R:0.0018)
Batch  25/537: Loss=0.9645 (C:0.9645, R:0.0018)
Batch  50/537: Loss=0.9441 (C:0.9441, R:0.0018)
Batch  75/537: Loss=0.9864 (C:0.9864, R:0.0018)
Batch 100/537: Loss=0.9913 (C:0.9913, R:0.0018)
Batch 125/537: Loss=0.9764 (C:0.9764, R:0.0018)
Batch 150/537: Loss=0.9889 (C:0.9889, R:0.0018)
Batch 175/537: Loss=1.0049 (C:1.0049, R:0.0018)
Batch 200/537: Loss=0.9723 (C:0.9723, R:0.0018)
Batch 225/537: Loss=1.0037 (C:1.0037, R:0.0018)
Batch 250/537: Loss=0.9474 (C:0.9474, R:0.0018)
Batch 275/537: Loss=0.9858 (C:0.9858, R:0.0018)
Batch 300/537: Loss=1.0048 (C:1.0048, R:0.0018)
Batch 325/537: Loss=0.9841 (C:0.9841, R:0.0018)
Batch 350/537: Loss=1.0023 (C:1.0023, R:0.0018)
Batch 375/537: Loss=0.9761 (C:0.9761, R:0.0018)
Batch 400/537: Loss=0.9905 (C:0.9905, R:0.0017)
Batch 425/537: Loss=0.9712 (C:0.9712, R:0.0017)
Batch 450/537: Loss=0.9709 (C:0.9709, R:0.0017)
Batch 475/537: Loss=1.0007 (C:1.0007, R:0.0018)
Batch 500/537: Loss=0.9546 (C:0.9546, R:0.0018)
Batch 525/537: Loss=0.9730 (C:0.9730, R:0.0017)

============================================================
Epoch 55/100 completed in 25.1s
Train: Loss=0.9793 (C:0.9793, R:0.0018) Ratio=3.72x
Val:   Loss=1.1498 (C:1.1498, R:0.0017) Ratio=2.12x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 55
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=1.0090 (C:1.0090, R:0.0018)
Batch  25/537: Loss=0.9896 (C:0.9896, R:0.0018)
Batch  50/537: Loss=1.0251 (C:1.0251, R:0.0018)
Batch  75/537: Loss=0.9887 (C:0.9887, R:0.0018)
Batch 100/537: Loss=0.9824 (C:0.9824, R:0.0018)
Batch 125/537: Loss=0.9556 (C:0.9556, R:0.0017)
Batch 150/537: Loss=0.9687 (C:0.9687, R:0.0018)
Batch 175/537: Loss=0.9544 (C:0.9544, R:0.0018)
Batch 200/537: Loss=0.9296 (C:0.9296, R:0.0017)
Batch 225/537: Loss=0.9466 (C:0.9466, R:0.0018)
Batch 250/537: Loss=0.9671 (C:0.9671, R:0.0018)
Batch 275/537: Loss=0.9818 (C:0.9818, R:0.0018)
Batch 300/537: Loss=0.9684 (C:0.9684, R:0.0018)
Batch 325/537: Loss=0.9636 (C:0.9636, R:0.0018)
Batch 350/537: Loss=0.9832 (C:0.9832, R:0.0018)
Batch 375/537: Loss=0.9586 (C:0.9586, R:0.0018)
Batch 400/537: Loss=0.9821 (C:0.9821, R:0.0018)
Batch 425/537: Loss=0.9786 (C:0.9786, R:0.0018)
Batch 450/537: Loss=1.0021 (C:1.0021, R:0.0018)
Batch 475/537: Loss=0.9693 (C:0.9693, R:0.0018)
Batch 500/537: Loss=0.9713 (C:0.9713, R:0.0017)
Batch 525/537: Loss=0.9949 (C:0.9949, R:0.0017)

============================================================
Epoch 56/100 completed in 20.5s
Train: Loss=0.9769 (C:0.9769, R:0.0018) Ratio=3.78x
Val:   Loss=1.1354 (C:1.1354, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 1.1354)
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=0.9240 (C:0.9240, R:0.0018)
Batch  25/537: Loss=0.9694 (C:0.9694, R:0.0018)
Batch  50/537: Loss=0.9948 (C:0.9948, R:0.0018)
Batch  75/537: Loss=0.9668 (C:0.9668, R:0.0018)
Batch 100/537: Loss=1.0077 (C:1.0077, R:0.0018)
Batch 125/537: Loss=0.9807 (C:0.9807, R:0.0018)
Batch 150/537: Loss=0.9420 (C:0.9420, R:0.0018)
Batch 175/537: Loss=0.9918 (C:0.9918, R:0.0018)
Batch 200/537: Loss=0.9733 (C:0.9733, R:0.0018)
Batch 225/537: Loss=0.9673 (C:0.9673, R:0.0018)
Batch 250/537: Loss=0.9677 (C:0.9677, R:0.0018)
Batch 275/537: Loss=0.9719 (C:0.9719, R:0.0018)
Batch 300/537: Loss=0.9509 (C:0.9509, R:0.0018)
Batch 325/537: Loss=0.9768 (C:0.9768, R:0.0017)
Batch 350/537: Loss=0.9936 (C:0.9936, R:0.0018)
Batch 375/537: Loss=0.9788 (C:0.9788, R:0.0018)
Batch 400/537: Loss=0.9767 (C:0.9767, R:0.0017)
Batch 425/537: Loss=0.9592 (C:0.9592, R:0.0017)
Batch 450/537: Loss=0.9605 (C:0.9605, R:0.0018)
Batch 475/537: Loss=0.9945 (C:0.9945, R:0.0018)
Batch 500/537: Loss=1.0052 (C:1.0052, R:0.0018)
Batch 525/537: Loss=0.9667 (C:0.9667, R:0.0017)

============================================================
Epoch 57/100 completed in 20.2s
Train: Loss=0.9777 (C:0.9777, R:0.0018) Ratio=3.75x
Val:   Loss=1.1343 (C:1.1343, R:0.0017) Ratio=2.14x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 1.1343)
============================================================

üåç Updating global dataset at epoch 58
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.259 ¬± 0.473
    Neg distances: 1.520 ¬± 0.976
    Separation ratio: 5.87x
    Gap: -2.601
    ‚úÖ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=0.9717 (C:0.9717, R:0.0018)
Batch  25/537: Loss=0.9543 (C:0.9543, R:0.0018)
Batch  50/537: Loss=0.9568 (C:0.9568, R:0.0017)
Batch  75/537: Loss=0.9581 (C:0.9581, R:0.0018)
Batch 100/537: Loss=0.9713 (C:0.9713, R:0.0018)
Batch 125/537: Loss=0.9463 (C:0.9463, R:0.0017)
Batch 150/537: Loss=0.9602 (C:0.9602, R:0.0018)
Batch 175/537: Loss=0.9837 (C:0.9837, R:0.0018)
Batch 200/537: Loss=0.9719 (C:0.9719, R:0.0018)
Batch 225/537: Loss=1.0314 (C:1.0314, R:0.0017)
Batch 250/537: Loss=0.9523 (C:0.9523, R:0.0018)
Batch 275/537: Loss=1.0008 (C:1.0008, R:0.0018)
Batch 300/537: Loss=0.9787 (C:0.9787, R:0.0017)
Batch 325/537: Loss=0.9541 (C:0.9541, R:0.0018)
Batch 350/537: Loss=0.9876 (C:0.9876, R:0.0018)
Batch 375/537: Loss=0.9858 (C:0.9858, R:0.0018)
Batch 400/537: Loss=0.9855 (C:0.9855, R:0.0018)
Batch 425/537: Loss=1.0069 (C:1.0069, R:0.0018)
Batch 450/537: Loss=0.9615 (C:0.9615, R:0.0018)
Batch 475/537: Loss=0.9418 (C:0.9418, R:0.0018)
Batch 500/537: Loss=0.9780 (C:0.9780, R:0.0018)
Batch 525/537: Loss=0.9888 (C:0.9888, R:0.0017)

============================================================
Epoch 58/100 completed in 24.7s
Train: Loss=0.9721 (C:0.9721, R:0.0018) Ratio=3.73x
Val:   Loss=1.1298 (C:1.1298, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.300
‚úÖ New best model saved (Val Loss: 1.1298)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=0.9440 (C:0.9440, R:0.0018)
Batch  25/537: Loss=1.0014 (C:1.0014, R:0.0018)
Batch  50/537: Loss=0.9419 (C:0.9419, R:0.0018)
Batch  75/537: Loss=0.9686 (C:0.9686, R:0.0018)
Batch 100/537: Loss=0.9647 (C:0.9647, R:0.0018)
Batch 125/537: Loss=0.9487 (C:0.9487, R:0.0018)
Batch 150/537: Loss=0.9770 (C:0.9770, R:0.0017)
Batch 175/537: Loss=0.9867 (C:0.9867, R:0.0018)
Batch 200/537: Loss=0.9814 (C:0.9814, R:0.0018)
Batch 225/537: Loss=0.9848 (C:0.9848, R:0.0018)
Batch 250/537: Loss=0.9850 (C:0.9850, R:0.0018)
Batch 275/537: Loss=0.9568 (C:0.9568, R:0.0018)
Batch 300/537: Loss=0.9753 (C:0.9753, R:0.0018)
Batch 325/537: Loss=0.9528 (C:0.9528, R:0.0018)
Batch 350/537: Loss=0.9690 (C:0.9690, R:0.0018)
Batch 375/537: Loss=0.9788 (C:0.9788, R:0.0018)
Batch 400/537: Loss=0.9595 (C:0.9595, R:0.0018)
Batch 425/537: Loss=0.9577 (C:0.9577, R:0.0017)
Batch 450/537: Loss=0.9551 (C:0.9551, R:0.0018)
Batch 475/537: Loss=0.9580 (C:0.9580, R:0.0018)
Batch 500/537: Loss=0.9733 (C:0.9733, R:0.0018)
Batch 525/537: Loss=0.9616 (C:0.9616, R:0.0018)

============================================================
Epoch 59/100 completed in 19.8s
Train: Loss=0.9723 (C:0.9723, R:0.0018) Ratio=3.91x
Val:   Loss=1.1308 (C:1.1308, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.300
No improvement for 1 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=0.9578 (C:0.9578, R:0.0018)
Batch  25/537: Loss=0.9362 (C:0.9362, R:0.0018)
Batch  50/537: Loss=0.9573 (C:0.9573, R:0.0018)
Batch  75/537: Loss=0.9740 (C:0.9740, R:0.0018)
Batch 100/537: Loss=0.9636 (C:0.9636, R:0.0018)
Batch 125/537: Loss=0.9499 (C:0.9499, R:0.0018)
Batch 150/537: Loss=0.9663 (C:0.9663, R:0.0018)
Batch 175/537: Loss=0.9689 (C:0.9689, R:0.0018)
Batch 200/537: Loss=0.9513 (C:0.9513, R:0.0017)
Batch 225/537: Loss=0.9539 (C:0.9539, R:0.0018)
Batch 250/537: Loss=0.9417 (C:0.9417, R:0.0018)
Batch 275/537: Loss=0.9716 (C:0.9716, R:0.0017)
Batch 300/537: Loss=0.9497 (C:0.9497, R:0.0018)
Batch 325/537: Loss=0.9771 (C:0.9771, R:0.0018)
Batch 350/537: Loss=0.9920 (C:0.9920, R:0.0018)
Batch 375/537: Loss=0.9400 (C:0.9400, R:0.0018)
Batch 400/537: Loss=0.9939 (C:0.9939, R:0.0018)
Batch 425/537: Loss=0.9876 (C:0.9876, R:0.0018)
Batch 450/537: Loss=0.9475 (C:0.9475, R:0.0018)
Batch 475/537: Loss=0.9774 (C:0.9774, R:0.0018)
Batch 500/537: Loss=0.9798 (C:0.9798, R:0.0018)
Batch 525/537: Loss=0.9475 (C:0.9475, R:0.0018)

============================================================
Epoch 60/100 completed in 20.1s
Train: Loss=0.9707 (C:0.9707, R:0.0018) Ratio=3.85x
Val:   Loss=1.1357 (C:1.1357, R:0.0017) Ratio=2.17x
Reconstruction weight: 0.300
No improvement for 2 epochs
Checkpoint saved at epoch 60
============================================================

üåç Updating global dataset at epoch 61
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.256 ¬± 0.486
    Neg distances: 1.521 ¬± 0.981
    Separation ratio: 5.93x
    Gap: -2.626
    ‚úÖ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=0.9322 (C:0.9322, R:0.0018)
Batch  25/537: Loss=0.9766 (C:0.9766, R:0.0018)
Batch  50/537: Loss=0.9176 (C:0.9176, R:0.0017)
Batch  75/537: Loss=1.0015 (C:1.0015, R:0.0018)
Batch 100/537: Loss=0.9874 (C:0.9874, R:0.0018)
Batch 125/537: Loss=1.0107 (C:1.0107, R:0.0018)
Batch 150/537: Loss=0.9950 (C:0.9950, R:0.0018)
Batch 175/537: Loss=0.9688 (C:0.9688, R:0.0018)
Batch 200/537: Loss=0.9582 (C:0.9582, R:0.0018)
Batch 225/537: Loss=0.9593 (C:0.9593, R:0.0018)
Batch 250/537: Loss=0.9308 (C:0.9308, R:0.0018)
Batch 275/537: Loss=0.9505 (C:0.9505, R:0.0018)
Batch 300/537: Loss=0.9708 (C:0.9708, R:0.0018)
Batch 325/537: Loss=0.9558 (C:0.9558, R:0.0018)
Batch 350/537: Loss=0.9927 (C:0.9927, R:0.0018)
Batch 375/537: Loss=0.9834 (C:0.9834, R:0.0018)
Batch 400/537: Loss=0.9512 (C:0.9512, R:0.0018)
Batch 425/537: Loss=0.9605 (C:0.9605, R:0.0018)
Batch 450/537: Loss=0.9741 (C:0.9741, R:0.0018)
Batch 475/537: Loss=0.9576 (C:0.9576, R:0.0018)
Batch 500/537: Loss=0.9848 (C:0.9848, R:0.0018)
Batch 525/537: Loss=0.9501 (C:0.9501, R:0.0017)

============================================================
Epoch 61/100 completed in 26.0s
Train: Loss=0.9684 (C:0.9684, R:0.0018) Ratio=3.82x
Val:   Loss=1.1351 (C:1.1351, R:0.0017) Ratio=2.18x
Reconstruction weight: 0.300
No improvement for 3 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=0.9775 (C:0.9775, R:0.0018)
Batch  25/537: Loss=0.9570 (C:0.9570, R:0.0018)
Batch  50/537: Loss=0.9586 (C:0.9586, R:0.0018)
Batch  75/537: Loss=0.9832 (C:0.9832, R:0.0018)
Batch 100/537: Loss=0.9452 (C:0.9452, R:0.0018)
Batch 125/537: Loss=0.9629 (C:0.9629, R:0.0018)
Batch 150/537: Loss=0.9617 (C:0.9617, R:0.0018)
Batch 175/537: Loss=0.9671 (C:0.9671, R:0.0018)
Batch 200/537: Loss=0.9903 (C:0.9903, R:0.0018)
Batch 225/537: Loss=0.9544 (C:0.9544, R:0.0018)
Batch 250/537: Loss=0.9691 (C:0.9691, R:0.0018)
Batch 275/537: Loss=0.9788 (C:0.9788, R:0.0018)
Batch 300/537: Loss=0.9870 (C:0.9870, R:0.0018)
Batch 325/537: Loss=0.9701 (C:0.9701, R:0.0018)
Batch 350/537: Loss=0.9991 (C:0.9991, R:0.0018)
Batch 375/537: Loss=0.9957 (C:0.9957, R:0.0018)
Batch 400/537: Loss=0.9734 (C:0.9734, R:0.0017)
Batch 425/537: Loss=0.9653 (C:0.9653, R:0.0018)
Batch 450/537: Loss=0.9153 (C:0.9153, R:0.0018)
Batch 475/537: Loss=0.9612 (C:0.9612, R:0.0018)
Batch 500/537: Loss=0.9819 (C:0.9819, R:0.0018)
Batch 525/537: Loss=0.9545 (C:0.9545, R:0.0017)

============================================================
Epoch 62/100 completed in 20.0s
Train: Loss=0.9684 (C:0.9684, R:0.0018) Ratio=3.83x
Val:   Loss=1.1344 (C:1.1344, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.300
No improvement for 4 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=0.9349 (C:0.9349, R:0.0018)
Batch  25/537: Loss=0.9694 (C:0.9694, R:0.0018)
Batch  50/537: Loss=0.9618 (C:0.9618, R:0.0018)
Batch  75/537: Loss=0.9944 (C:0.9944, R:0.0017)
Batch 100/537: Loss=0.9884 (C:0.9884, R:0.0018)
Batch 125/537: Loss=0.9746 (C:0.9746, R:0.0017)
Batch 150/537: Loss=0.9468 (C:0.9468, R:0.0018)
Batch 175/537: Loss=0.9743 (C:0.9743, R:0.0018)
Batch 200/537: Loss=0.9691 (C:0.9691, R:0.0018)
Batch 225/537: Loss=0.9370 (C:0.9370, R:0.0018)
Batch 250/537: Loss=0.9969 (C:0.9969, R:0.0018)
Batch 275/537: Loss=0.9747 (C:0.9747, R:0.0018)
Batch 300/537: Loss=0.9973 (C:0.9973, R:0.0017)
Batch 325/537: Loss=0.9818 (C:0.9818, R:0.0018)
Batch 350/537: Loss=0.9444 (C:0.9444, R:0.0018)
Batch 375/537: Loss=0.9924 (C:0.9924, R:0.0018)
Batch 400/537: Loss=0.9462 (C:0.9462, R:0.0018)
Batch 425/537: Loss=0.9904 (C:0.9904, R:0.0018)
Batch 450/537: Loss=0.9929 (C:0.9929, R:0.0017)
Batch 475/537: Loss=0.9522 (C:0.9522, R:0.0017)
Batch 500/537: Loss=0.9512 (C:0.9512, R:0.0018)
Batch 525/537: Loss=0.9787 (C:0.9787, R:0.0018)

============================================================
Epoch 63/100 completed in 19.8s
Train: Loss=0.9669 (C:0.9669, R:0.0018) Ratio=3.87x
Val:   Loss=1.1309 (C:1.1309, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.300
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 64
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.261 ¬± 0.491
    Neg distances: 1.511 ¬± 0.997
    Separation ratio: 5.80x
    Gap: -2.629
    ‚úÖ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=0.9713 (C:0.9713, R:0.0018)
Batch  25/537: Loss=0.9831 (C:0.9831, R:0.0018)
Batch  50/537: Loss=0.9718 (C:0.9718, R:0.0018)
Batch  75/537: Loss=0.9491 (C:0.9491, R:0.0017)
Batch 100/537: Loss=0.9661 (C:0.9661, R:0.0017)
Batch 125/537: Loss=1.0014 (C:1.0014, R:0.0018)
Batch 150/537: Loss=1.0054 (C:1.0054, R:0.0018)
Batch 175/537: Loss=0.9806 (C:0.9806, R:0.0018)
Batch 200/537: Loss=0.9869 (C:0.9869, R:0.0018)
Batch 225/537: Loss=0.9842 (C:0.9842, R:0.0018)
Batch 250/537: Loss=0.9574 (C:0.9574, R:0.0018)
Batch 275/537: Loss=0.9707 (C:0.9707, R:0.0018)
Batch 300/537: Loss=0.9650 (C:0.9650, R:0.0018)
Batch 325/537: Loss=0.9718 (C:0.9718, R:0.0018)
Batch 350/537: Loss=0.9839 (C:0.9839, R:0.0018)
Batch 375/537: Loss=0.9391 (C:0.9391, R:0.0018)
Batch 400/537: Loss=0.9778 (C:0.9778, R:0.0018)
Batch 425/537: Loss=1.0114 (C:1.0114, R:0.0017)
Batch 450/537: Loss=0.9999 (C:0.9999, R:0.0018)
Batch 475/537: Loss=1.0086 (C:1.0086, R:0.0018)
Batch 500/537: Loss=1.0033 (C:1.0033, R:0.0018)
Batch 525/537: Loss=0.9753 (C:0.9753, R:0.0018)

============================================================
Epoch 64/100 completed in 24.6s
Train: Loss=0.9771 (C:0.9771, R:0.0018) Ratio=3.89x
Val:   Loss=1.1490 (C:1.1490, R:0.0017) Ratio=2.14x
Reconstruction weight: 0.300
No improvement for 6 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=0.9913 (C:0.9913, R:0.0018)
Batch  25/537: Loss=0.9686 (C:0.9686, R:0.0018)
Batch  50/537: Loss=0.9646 (C:0.9646, R:0.0017)
Batch  75/537: Loss=0.9874 (C:0.9874, R:0.0017)
Batch 100/537: Loss=0.9812 (C:0.9812, R:0.0018)
Batch 125/537: Loss=0.9373 (C:0.9373, R:0.0018)
Batch 150/537: Loss=0.9600 (C:0.9600, R:0.0018)
Batch 175/537: Loss=0.9732 (C:0.9732, R:0.0018)
Batch 200/537: Loss=0.9467 (C:0.9467, R:0.0018)
Batch 225/537: Loss=0.9949 (C:0.9949, R:0.0018)
Batch 250/537: Loss=0.9760 (C:0.9760, R:0.0018)
Batch 275/537: Loss=0.9527 (C:0.9527, R:0.0018)
Batch 300/537: Loss=0.9363 (C:0.9363, R:0.0018)
Batch 325/537: Loss=0.9922 (C:0.9922, R:0.0018)
Batch 350/537: Loss=0.9958 (C:0.9958, R:0.0018)
Batch 375/537: Loss=0.9661 (C:0.9661, R:0.0018)
Batch 400/537: Loss=1.0069 (C:1.0069, R:0.0018)
Batch 425/537: Loss=0.9963 (C:0.9963, R:0.0018)
Batch 450/537: Loss=0.9732 (C:0.9732, R:0.0017)
Batch 475/537: Loss=1.0076 (C:1.0076, R:0.0018)
Batch 500/537: Loss=0.9855 (C:0.9855, R:0.0018)
Batch 525/537: Loss=0.9599 (C:0.9599, R:0.0018)

============================================================
Epoch 65/100 completed in 20.6s
Train: Loss=0.9766 (C:0.9766, R:0.0018) Ratio=3.97x
Val:   Loss=1.1509 (C:1.1509, R:0.0017) Ratio=2.15x
Reconstruction weight: 0.300
No improvement for 7 epochs
Checkpoint saved at epoch 65
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=0.9970 (C:0.9970, R:0.0018)
Batch  25/537: Loss=0.9837 (C:0.9837, R:0.0018)
Batch  50/537: Loss=0.9502 (C:0.9502, R:0.0018)
Batch  75/537: Loss=0.9776 (C:0.9776, R:0.0018)
Batch 100/537: Loss=0.9802 (C:0.9802, R:0.0018)
Batch 125/537: Loss=0.9933 (C:0.9933, R:0.0018)
Batch 150/537: Loss=1.0047 (C:1.0047, R:0.0018)
Batch 175/537: Loss=0.9691 (C:0.9691, R:0.0017)
Batch 200/537: Loss=0.9822 (C:0.9822, R:0.0018)
Batch 225/537: Loss=0.9610 (C:0.9610, R:0.0017)
Batch 250/537: Loss=0.9637 (C:0.9637, R:0.0018)
Batch 275/537: Loss=0.9546 (C:0.9546, R:0.0018)
Batch 300/537: Loss=0.9728 (C:0.9728, R:0.0018)
Batch 325/537: Loss=0.9617 (C:0.9617, R:0.0018)
Batch 350/537: Loss=0.9784 (C:0.9784, R:0.0018)
Batch 375/537: Loss=0.9960 (C:0.9960, R:0.0018)
Batch 400/537: Loss=0.9651 (C:0.9651, R:0.0018)
Batch 425/537: Loss=0.9782 (C:0.9782, R:0.0018)
Batch 450/537: Loss=1.0084 (C:1.0084, R:0.0018)
Batch 475/537: Loss=0.9739 (C:0.9739, R:0.0017)
Batch 500/537: Loss=0.9384 (C:0.9384, R:0.0018)
Batch 525/537: Loss=0.9622 (C:0.9622, R:0.0018)

============================================================
Epoch 66/100 completed in 20.8s
Train: Loss=0.9762 (C:0.9762, R:0.0018) Ratio=3.95x
Val:   Loss=1.1422 (C:1.1422, R:0.0017) Ratio=2.19x
Reconstruction weight: 0.300
No improvement for 8 epochs

Early stopping triggered after 66 epochs
Best model was at epoch 58 with Val Loss: 1.1298

Global Dataset Training Completed!
Best epoch: 58
Best validation loss: 1.1298
Final separation ratios: Train=3.95x, Val=2.19x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_180434/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.1627
  Adjusted Rand Score: 0.2938
  Clustering Accuracy: 0.6176
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.6563
  Per-class F1: [0.6545334215751158, 0.543797311911015, 0.7828170460379942]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.001732
Evaluating separation quality...
Separation Results:
  Positive distances: 0.628 ¬± 0.740
  Negative distances: 1.324 ¬± 0.921
  Separation ratio: 2.11x
  Gap: -2.698
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.1627
  Clustering Accuracy: 0.6176
  Adjusted Rand Score: 0.2938

Classification Performance:
  Accuracy: 0.6563

Separation Quality:
  Separation Ratio: 2.11x
  Gap: -2.698
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.001732
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_180434/results/evaluation_results_20250712_182933.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_lattice_test_20250712_180434/results/evaluation_results_20250712_182933.json
Saving final experiment results...

PIPELINE FAILED: Object of type float32 is not JSON serializable

Analysis completed with exit code: 0
Time: Sat 12 Jul 18:29:34 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
