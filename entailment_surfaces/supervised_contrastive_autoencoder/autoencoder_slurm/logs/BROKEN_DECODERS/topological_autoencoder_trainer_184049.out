Starting Surface Distance Metric Analysis job...
Job ID: 184049
Node: gpuvm19
Time: Sat 19 Jul 16:47:36 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 16:47:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   30C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164751
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164751/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,852,466
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,852,466
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 5.0
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=34660.8438 (C:2.0000, R:0.0110, T:34658.8438(w:5.000)⚠️)
Batch  25/365: Loss=2453.5361 (C:13.8746, R:0.0099, T:2439.6604(w:5.000)⚠️)
Batch  50/365: Loss=2258.7554 (C:24.9198, R:0.0098, T:2233.8345(w:5.000)⚠️)
Batch  75/365: Loss=2370.8462 (C:24.6955, R:0.0097, T:2346.1497(w:5.000)⚠️)
Batch 100/365: Loss=1174.9373 (C:23.2677, R:0.0097, T:1151.6686(w:5.000)⚠️)
Batch 125/365: Loss=974.1382 (C:29.4638, R:0.0097, T:944.6735(w:5.000)⚠️)
Batch 150/365: Loss=266.6350 (C:23.7245, R:0.0097, T:242.9095(w:5.000)⚠️)
Batch 175/365: Loss=917.6504 (C:26.7621, R:0.0098, T:890.8873(w:5.000)⚠️)
Batch 200/365: Loss=774.9136 (C:27.7933, R:0.0098, T:747.1194(w:5.000)⚠️)
Batch 225/365: Loss=910.6430 (C:27.4930, R:0.0097, T:883.1490(w:5.000)⚠️)
Batch 250/365: Loss=543.1484 (C:31.6116, R:0.0098, T:511.5358(w:5.000)⚠️)
Batch 275/365: Loss=834.5988 (C:27.9848, R:0.0099, T:806.6130(w:5.000)⚠️)
Batch 300/365: Loss=140.7137 (C:32.1501, R:0.0098, T:108.5626(w:5.000)⚠️)
Batch 325/365: Loss=460.3926 (C:29.0107, R:0.0098, T:431.3809(w:5.000)⚠️)
Batch 350/365: Loss=97.2497 (C:28.2220, R:0.0098, T:69.0267(w:5.000)⚠️)
🎉 MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 2314.5891
📈 New best topological loss: 2314.5891

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 2340.6608
  Contrastive: 26.0707
  Reconstruction: 0.0098
  Topological: 2314.5891 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 35231.3124
  Contrastive: 22.2251
  Reconstruction: 0.0098
  Topological: 35209.0864 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (122.5s)
Train Loss: 2340.6608 (C:26.0707, R:0.0098, T:2314.5891)
Val Loss:   35231.3124 (C:22.2251, R:0.0098, T:35209.0864)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=505.8723 (C:31.4664, R:0.0098, T:474.4049(w:5.000)⚠️)
Batch  25/365: Loss=958.1122 (C:31.6341, R:0.0097, T:926.4772(w:5.000)⚠️)
Batch  50/365: Loss=482.9664 (C:28.9571, R:0.0098, T:454.0083(w:5.000)⚠️)
Batch  75/365: Loss=109.0397 (C:26.4622, R:0.0098, T:82.5764(w:5.000)⚠️)
Batch 100/365: Loss=1142.6246 (C:28.1617, R:0.0098, T:1114.4619(w:5.000)⚠️)
Batch 125/365: Loss=678.1707 (C:27.1704, R:0.0098, T:650.9993(w:5.000)⚠️)
Batch 150/365: Loss=266.0795 (C:30.3178, R:0.0098, T:235.7607(w:5.000)⚠️)
Batch 175/365: Loss=486.1231 (C:33.6282, R:0.0098, T:452.4940(w:5.000)⚠️)
Batch 200/365: Loss=2068.7710 (C:28.0898, R:0.0098, T:2040.6802(w:5.000)⚠️)
Batch 225/365: Loss=361.1066 (C:27.9867, R:0.0098, T:333.1189(w:5.000)⚠️)
Batch 250/365: Loss=253.9492 (C:28.9004, R:0.0098, T:225.0478(w:5.000)⚠️)
Batch 275/365: Loss=250.3142 (C:30.3988, R:0.0098, T:219.9144(w:5.000)⚠️)
Batch 300/365: Loss=2389.2944 (C:28.5525, R:0.0098, T:2360.7410(w:5.000)⚠️)
Batch 325/365: Loss=113.0199 (C:29.6841, R:0.0098, T:83.3349(w:5.000)⚠️)
Batch 350/365: Loss=135.4802 (C:28.0192, R:0.0098, T:107.4600(w:5.000)⚠️)
📈 New best topological loss: 950.6742

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 979.8262
  Contrastive: 29.1511
  Reconstruction: 0.0098
  Topological: 950.6742 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 26164.9310
  Contrastive: 20.1865
  Reconstruction: 0.0098
  Topological: 26144.7435 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (126.3s)
Train Loss: 979.8262 (C:29.1511, R:0.0098, T:950.6742)
Val Loss:   26164.9310 (C:20.1865, R:0.0098, T:26144.7435)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=284.2956 (C:31.6248, R:0.0098, T:252.6698(w:5.000)⚠️)
Batch  25/365: Loss=1004.4346 (C:32.1956, R:0.0098, T:972.2381(w:5.000)⚠️)
Batch  50/365: Loss=643.0153 (C:32.2436, R:0.0098, T:610.7707(w:5.000)⚠️)
Batch  75/365: Loss=1916.6757 (C:31.9532, R:0.0098, T:1884.7214(w:5.000)⚠️)
Batch 100/365: Loss=459.7347 (C:34.7901, R:0.0098, T:424.9436(w:5.000)⚠️)
Batch 125/365: Loss=2539.2510 (C:34.6669, R:0.0098, T:2504.5830(w:5.000)⚠️)
Batch 150/365: Loss=976.8338 (C:37.2112, R:0.0098, T:939.6216(w:5.000)⚠️)
Batch 175/365: Loss=931.1019 (C:29.1089, R:0.0098, T:901.9921(w:5.000)⚠️)
Batch 200/365: Loss=183.5897 (C:22.3526, R:0.0099, T:161.2361(w:5.000)⚠️)
Batch 225/365: Loss=1593.9886 (C:26.5167, R:0.0099, T:1567.4709(w:5.000)⚠️)
Batch 250/365: Loss=118.1669 (C:26.1259, R:0.0098, T:92.0400(w:5.000)⚠️)
Batch 275/365: Loss=248.5570 (C:25.8918, R:0.0098, T:222.6642(w:5.000)⚠️)
Batch 300/365: Loss=729.6303 (C:23.9800, R:0.0098, T:705.6494(w:5.000)⚠️)
Batch 325/365: Loss=352.1416 (C:26.0363, R:0.0098, T:326.1043(w:5.000)⚠️)
Batch 350/365: Loss=51.0924 (C:26.2996, R:0.0098, T:24.7918(w:5.000)⚠️)
📈 New best topological loss: 947.2341

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 975.9836
  Contrastive: 28.7485
  Reconstruction: 0.0098
  Topological: 947.2341 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 25161.5496
  Contrastive: 20.9417
  Reconstruction: 0.0098
  Topological: 25140.6070 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (126.7s)
Train Loss: 975.9836 (C:28.7485, R:0.0098, T:947.2341)
Val Loss:   25161.5496 (C:20.9417, R:0.0098, T:25140.6070)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=276.8071 (C:27.0779, R:0.0098, T:249.7282(w:5.000)⚠️)
Batch  25/365: Loss=513.2740 (C:30.7319, R:0.0098, T:482.5411(w:5.000)⚠️)
Batch  50/365: Loss=1310.3986 (C:27.9464, R:0.0098, T:1282.4512(w:5.000)⚠️)
Batch  75/365: Loss=106.0867 (C:24.2995, R:0.0098, T:81.7862(w:5.000)⚠️)
Batch 100/365: Loss=357.0020 (C:25.7257, R:0.0098, T:331.2754(w:5.000)⚠️)
Batch 125/365: Loss=852.5198 (C:22.2557, R:0.0098, T:830.2631(w:5.000)⚠️)
Batch 150/365: Loss=1167.4194 (C:23.0597, R:0.0098, T:1144.3588(w:5.000)⚠️)
Batch 175/365: Loss=607.1538 (C:22.3877, R:0.0099, T:584.7651(w:5.000)⚠️)
Batch 200/365: Loss=502.5535 (C:22.1872, R:0.0098, T:480.3653(w:5.000)⚠️)
Batch 225/365: Loss=334.5626 (C:24.1315, R:0.0098, T:310.4301(w:5.000)⚠️)
Batch 250/365: Loss=1126.5098 (C:24.8081, R:0.0097, T:1101.7007(w:5.000)⚠️)
Batch 275/365: Loss=641.0492 (C:24.9021, R:0.0097, T:616.1461(w:5.000)⚠️)
Batch 300/365: Loss=775.7372 (C:24.0887, R:0.0098, T:751.6476(w:5.000)⚠️)
Batch 325/365: Loss=2736.9858 (C:26.0470, R:0.0098, T:2710.9377(w:5.000)⚠️)
Batch 350/365: Loss=594.7615 (C:23.6957, R:0.0097, T:571.0648(w:5.000)⚠️)
📈 New best topological loss: 778.0743

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 802.8427
  Contrastive: 24.7675
  Reconstruction: 0.0098
  Topological: 778.0743 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15379.3794
  Contrastive: 19.7050
  Reconstruction: 0.0097
  Topological: 15359.6734 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (132.9s)
Train Loss: 802.8427 (C:24.7675, R:0.0098, T:778.0743)
Val Loss:   15379.3794 (C:19.7050, R:0.0097, T:15359.6734)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=375.8224 (C:26.6190, R:0.0098, T:349.2024(w:5.000)⚠️)
Batch  25/365: Loss=616.3989 (C:29.3312, R:0.0097, T:587.0667(w:5.000)⚠️)
Batch  50/365: Loss=440.3785 (C:25.7145, R:0.0097, T:414.6630(w:5.000)⚠️)
Batch  75/365: Loss=2234.8872 (C:26.6804, R:0.0098, T:2208.2058(w:5.000)⚠️)
Batch 100/365: Loss=1347.1283 (C:24.0518, R:0.0099, T:1323.0756(w:5.000)⚠️)
Batch 125/365: Loss=298.9696 (C:22.6125, R:0.0097, T:276.3562(w:5.000)⚠️)
Batch 150/365: Loss=302.0896 (C:24.8047, R:0.0098, T:277.2840(w:5.000)⚠️)
Batch 175/365: Loss=833.3038 (C:22.3067, R:0.0098, T:810.9961(w:5.000)⚠️)
Batch 200/365: Loss=529.1144 (C:22.7295, R:0.0098, T:506.3839(w:5.000)⚠️)
Batch 225/365: Loss=1069.0718 (C:23.2212, R:0.0098, T:1045.8496(w:5.000)⚠️)
Batch 250/365: Loss=656.0596 (C:22.2705, R:0.0098, T:633.7881(w:5.000)⚠️)
Batch 275/365: Loss=1497.1414 (C:22.2193, R:0.0098, T:1474.9211(w:5.000)⚠️)
Batch 300/365: Loss=591.2978 (C:23.1373, R:0.0098, T:568.1595(w:5.000)⚠️)
Batch 325/365: Loss=456.7401 (C:26.6492, R:0.0098, T:430.0899(w:5.000)⚠️)
Batch 350/365: Loss=458.3486 (C:28.9210, R:0.0098, T:429.4266(w:5.000)⚠️)
📈 New best topological loss: 548.7739

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 573.6507
  Contrastive: 24.8758
  Reconstruction: 0.0098
  Topological: 548.7739 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 23570.7835
  Contrastive: 21.9848
  Reconstruction: 0.0098
  Topological: 23548.7978 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 5/50 COMPLETE (127.4s)
Train Loss: 573.6507 (C:24.8758, R:0.0098, T:548.7739)
Val Loss:   23570.7835 (C:21.9848, R:0.0098, T:23548.7978)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=86.1693 (C:28.3472, R:0.0098, T:57.8211(w:5.000)⚠️)
Batch  25/365: Loss=601.3525 (C:34.7314, R:0.0097, T:566.6201(w:5.000)⚠️)
Batch  50/365: Loss=249.0560 (C:28.2844, R:0.0097, T:220.7706(w:5.000)⚠️)
Batch  75/365: Loss=712.7289 (C:25.1292, R:0.0097, T:687.5988(w:5.000)⚠️)
Batch 100/365: Loss=202.5516 (C:29.9581, R:0.0097, T:172.5926(w:5.000)⚠️)
Batch 125/365: Loss=647.7555 (C:29.0629, R:0.0098, T:618.6917(w:5.000)⚠️)
Batch 150/365: Loss=355.5203 (C:24.8112, R:0.0099, T:330.7082(w:5.000)⚠️)
Batch 175/365: Loss=829.9653 (C:23.7205, R:0.0099, T:806.2439(w:5.000)⚠️)
Batch 200/365: Loss=131.8195 (C:24.5431, R:0.0099, T:107.2755(w:5.000)⚠️)
Batch 225/365: Loss=75.1321 (C:26.6853, R:0.0098, T:48.4458(w:5.000)⚠️)
Batch 250/365: Loss=51.5524 (C:26.7083, R:0.0099, T:24.8431(w:5.000)⚠️)
Batch 275/365: Loss=978.9209 (C:24.2545, R:0.0099, T:954.6654(w:5.000)⚠️)
Batch 300/365: Loss=197.5985 (C:26.4554, R:0.0099, T:171.1421(w:5.000)⚠️)
Batch 325/365: Loss=138.0319 (C:23.4790, R:0.0099, T:114.5520(w:5.000)⚠️)
Batch 350/365: Loss=942.2220 (C:21.0553, R:0.0099, T:921.1656(w:5.000)⚠️)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 613.6912
  Contrastive: 26.4191
  Reconstruction: 0.0098
  Topological: 587.2712 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 11465.7607
  Contrastive: 16.1119
  Reconstruction: 0.0099
  Topological: 11449.6478 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (134.8s)
Train Loss: 613.6912 (C:26.4191, R:0.0098, T:587.2712)
Val Loss:   11465.7607 (C:16.1119, R:0.0099, T:11449.6478)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=510.3706 (C:20.6257, R:0.0099, T:489.7439(w:5.000)⚠️)
Batch  25/365: Loss=636.4622 (C:21.8914, R:0.0099, T:614.5698(w:5.000)⚠️)
Batch  50/365: Loss=721.6696 (C:25.0860, R:0.0099, T:696.5825(w:5.000)⚠️)
Batch  75/365: Loss=790.8016 (C:24.7820, R:0.0098, T:766.0187(w:5.000)⚠️)
Batch 100/365: Loss=206.3506 (C:26.7221, R:0.0098, T:179.6276(w:5.000)⚠️)
Batch 125/365: Loss=62.2424 (C:25.2873, R:0.0098, T:36.9541(w:5.000)⚠️)
Batch 150/365: Loss=431.2756 (C:22.3192, R:0.0099, T:408.9554(w:5.000)⚠️)
Batch 175/365: Loss=546.6154 (C:19.8908, R:0.0099, T:526.7236(w:5.000)⚠️)
Batch 200/365: Loss=800.3770 (C:21.0297, R:0.0099, T:779.3462(w:5.000)⚠️)
Batch 225/365: Loss=409.1899 (C:21.1957, R:0.0098, T:387.9931(w:5.000)⚠️)
Batch 250/365: Loss=242.0829 (C:21.3295, R:0.0099, T:220.7524(w:5.000)⚠️)
Batch 275/365: Loss=361.4414 (C:23.8232, R:0.0098, T:337.6172(w:5.000)⚠️)
Batch 300/365: Loss=650.9800 (C:19.0734, R:0.0099, T:631.9056(w:5.000)⚠️)
Batch 325/365: Loss=947.0479 (C:20.4493, R:0.0098, T:926.5975(w:5.000)⚠️)
Batch 350/365: Loss=179.9286 (C:17.8023, R:0.0099, T:162.1253(w:5.000)⚠️)
📈 New best topological loss: 514.9134

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 537.0704
  Contrastive: 22.1560
  Reconstruction: 0.0099
  Topological: 514.9134 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14470.1190
  Contrastive: 10.9327
  Reconstruction: 0.0098
  Topological: 14459.1853 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 7/50 COMPLETE (138.6s)
Train Loss: 537.0704 (C:22.1560, R:0.0099, T:514.9134)
Val Loss:   14470.1190 (C:10.9327, R:0.0098, T:14459.1853)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=173.5439 (C:18.0171, R:0.0099, T:155.5258(w:5.000)⚠️)
Batch  25/365: Loss=470.4271 (C:18.9856, R:0.0098, T:451.4406(w:5.000)⚠️)
Batch  50/365: Loss=666.2689 (C:18.1596, R:0.0099, T:648.1082(w:5.000)⚠️)
Batch  75/365: Loss=1041.3217 (C:18.3392, R:0.0098, T:1022.9816(w:5.000)⚠️)
Batch 100/365: Loss=871.8892 (C:17.5182, R:0.0099, T:854.3701(w:5.000)⚠️)
Batch 125/365: Loss=37.6090 (C:17.3303, R:0.0099, T:20.2777(w:5.000)⚠️)
Batch 150/365: Loss=859.1993 (C:18.7899, R:0.0099, T:840.4084(w:5.000)⚠️)
Batch 175/365: Loss=241.3320 (C:18.2291, R:0.0099, T:223.1019(w:5.000)⚠️)
Batch 200/365: Loss=773.5576 (C:18.7170, R:0.0099, T:754.8395(w:5.000)⚠️)
Batch 225/365: Loss=982.6696 (C:20.3765, R:0.0099, T:962.2921(w:5.000)⚠️)
Batch 250/365: Loss=242.5111 (C:20.3960, R:0.0098, T:222.1141(w:5.000)⚠️)
Batch 275/365: Loss=46.6705 (C:21.1143, R:0.0098, T:25.5553(w:5.000)⚠️)
Batch 300/365: Loss=439.2053 (C:23.2687, R:0.0099, T:415.9356(w:5.000)⚠️)
Batch 325/365: Loss=76.6777 (C:21.5909, R:0.0098, T:55.0858(w:5.000)⚠️)
Batch 350/365: Loss=680.2994 (C:19.4739, R:0.0098, T:660.8246(w:5.000)⚠️)
📈 New best topological loss: 418.4534

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 438.1750
  Contrastive: 19.7206
  Reconstruction: 0.0099
  Topological: 418.4534 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14361.3417
  Contrastive: 16.5778
  Reconstruction: 0.0098
  Topological: 14344.7630 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 8/50 COMPLETE (137.3s)
Train Loss: 438.1750 (C:19.7206, R:0.0099, T:418.4534)
Val Loss:   14361.3417 (C:16.5778, R:0.0098, T:14344.7630)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=119.8297 (C:21.0646, R:0.0099, T:98.7642(w:5.000)⚠️)
Batch  25/365: Loss=1350.2233 (C:24.1042, R:0.0098, T:1326.1182(w:5.000)⚠️)
Batch  50/365: Loss=337.6436 (C:19.8926, R:0.0099, T:317.7501(w:5.000)⚠️)
Batch  75/365: Loss=292.6290 (C:22.5003, R:0.0098, T:270.1277(w:5.000)⚠️)
Batch 100/365: Loss=414.0212 (C:21.1195, R:0.0099, T:392.9007(w:5.000)⚠️)
Batch 125/365: Loss=967.1497 (C:18.1927, R:0.0099, T:948.9559(w:5.000)⚠️)
Batch 150/365: Loss=139.2823 (C:17.4876, R:0.0099, T:121.7937(w:5.000)⚠️)
Batch 175/365: Loss=52.4126 (C:18.5789, R:0.0098, T:33.8327(w:5.000)⚠️)
Batch 200/365: Loss=39.2834 (C:17.9849, R:0.0099, T:21.2975(w:5.000)⚠️)
Batch 225/365: Loss=196.7326 (C:16.1208, R:0.0099, T:180.6108(w:5.000)⚠️)
Batch 250/365: Loss=105.2414 (C:16.9998, R:0.0099, T:88.2405(w:5.000)⚠️)
Batch 275/365: Loss=948.9688 (C:17.9270, R:0.0098, T:931.0409(w:5.000)⚠️)
Batch 300/365: Loss=808.4777 (C:18.0257, R:0.0098, T:790.4510(w:5.000)⚠️)
Batch 325/365: Loss=961.1591 (C:18.3517, R:0.0099, T:942.8064(w:5.000)⚠️)
Batch 350/365: Loss=348.7199 (C:16.6646, R:0.0099, T:332.0544(w:5.000)⚠️)
📈 New best topological loss: 405.3208

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 423.9758
  Contrastive: 18.6541
  Reconstruction: 0.0099
  Topological: 405.3208 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 10535.9308
  Contrastive: 12.3037
  Reconstruction: 0.0099
  Topological: 10523.6261 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 9/50 COMPLETE (140.2s)
Train Loss: 423.9758 (C:18.6541, R:0.0099, T:405.3208)
Val Loss:   10535.9308 (C:12.3037, R:0.0099, T:10523.6261)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1482.8237 (C:17.9512, R:0.0099, T:1464.8716(w:5.000)⚠️)
Batch  25/365: Loss=279.5946 (C:17.7112, R:0.0099, T:261.8824(w:5.000)⚠️)
Batch  50/365: Loss=702.7336 (C:18.4043, R:0.0099, T:684.3282(w:5.000)⚠️)
Batch  75/365: Loss=174.4794 (C:18.7805, R:0.0099, T:155.6980(w:5.000)⚠️)
Batch 100/365: Loss=512.6097 (C:17.7983, R:0.0098, T:494.8104(w:5.000)⚠️)
Batch 125/365: Loss=840.9962 (C:18.6497, R:0.0098, T:822.3455(w:5.000)⚠️)
Batch 150/365: Loss=585.5576 (C:23.0084, R:0.0099, T:562.5482(w:5.000)⚠️)
Batch 175/365: Loss=2426.7896 (C:21.3601, R:0.0098, T:2405.4285(w:5.000)⚠️)
Batch 200/365: Loss=304.4436 (C:20.9456, R:0.0099, T:283.4970(w:5.000)⚠️)
Batch 225/365: Loss=614.3379 (C:19.8548, R:0.0098, T:594.4821(w:5.000)⚠️)
Batch 250/365: Loss=605.1687 (C:23.0003, R:0.0098, T:582.1674(w:5.000)⚠️)
Batch 275/365: Loss=281.0352 (C:21.7399, R:0.0099, T:259.2943(w:5.000)⚠️)
Batch 300/365: Loss=202.4152 (C:22.3411, R:0.0099, T:180.0731(w:5.000)⚠️)
Batch 325/365: Loss=1019.9914 (C:25.9669, R:0.0099, T:994.0236(w:5.000)⚠️)
Batch 350/365: Loss=1656.0120 (C:24.3197, R:0.0098, T:1631.6913(w:5.000)⚠️)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 528.9030
  Contrastive: 20.9491
  Reconstruction: 0.0099
  Topological: 507.9529 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15885.3590
  Contrastive: 17.7713
  Reconstruction: 0.0098
  Topological: 15867.5868 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 10/50 COMPLETE (133.9s)
Train Loss: 528.9030 (C:20.9491, R:0.0099, T:507.9529)
Val Loss:   15885.3590 (C:17.7713, R:0.0098, T:15867.5868)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=909.7523 (C:24.3190, R:0.0098, T:885.4323(w:5.000)⚠️)
Batch  25/365: Loss=1955.5941 (C:23.2723, R:0.0099, T:1932.3208(w:5.000)⚠️)
Batch  50/365: Loss=404.8120 (C:22.7069, R:0.0098, T:382.1041(w:5.000)⚠️)
Batch  75/365: Loss=361.9915 (C:25.7658, R:0.0098, T:336.2247(w:5.000)⚠️)
Batch 100/365: Loss=1409.7163 (C:25.5444, R:0.0098, T:1384.1710(w:5.000)⚠️)
Batch 125/365: Loss=258.8476 (C:22.7856, R:0.0099, T:236.0610(w:5.000)⚠️)
Batch 150/365: Loss=293.7415 (C:23.4872, R:0.0098, T:270.2533(w:5.000)⚠️)
Batch 175/365: Loss=336.5638 (C:24.2427, R:0.0098, T:312.3201(w:5.000)⚠️)
Batch 200/365: Loss=1571.8188 (C:24.1962, R:0.0099, T:1547.6216(w:5.000)⚠️)
Batch 225/365: Loss=496.9316 (C:28.0842, R:0.0098, T:468.8464(w:5.000)⚠️)
Batch 250/365: Loss=228.0988 (C:26.2180, R:0.0098, T:201.8799(w:5.000)⚠️)
Batch 275/365: Loss=626.2952 (C:28.5591, R:0.0098, T:597.7350(w:5.000)⚠️)
Batch 300/365: Loss=1494.7976 (C:29.1560, R:0.0098, T:1465.6406(w:5.000)⚠️)
Batch 325/365: Loss=273.0191 (C:26.8172, R:0.0098, T:246.2009(w:5.000)⚠️)
Batch 350/365: Loss=567.4065 (C:27.9855, R:0.0098, T:539.4200(w:5.000)⚠️)

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 667.5739
  Contrastive: 25.6266
  Reconstruction: 0.0098
  Topological: 641.9463 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 10727.7909
  Contrastive: 24.3979
  Reconstruction: 0.0098
  Topological: 10703.3921 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 11/50 COMPLETE (130.4s)
Train Loss: 667.5739 (C:25.6266, R:0.0098, T:641.9463)
Val Loss:   10727.7909 (C:24.3979, R:0.0098, T:10703.3921)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=467.6937 (C:27.2060, R:0.0099, T:440.4867(w:5.000)⚠️)
Batch  25/365: Loss=124.1350 (C:25.6015, R:0.0098, T:98.5325(w:5.000)⚠️)
Batch  50/365: Loss=1403.6031 (C:22.7597, R:0.0099, T:1380.8425(w:5.000)⚠️)
Batch  75/365: Loss=275.6379 (C:22.7485, R:0.0099, T:252.8885(w:5.000)⚠️)
Batch 100/365: Loss=785.6353 (C:22.1685, R:0.0099, T:763.4658(w:5.000)⚠️)
Batch 125/365: Loss=186.7806 (C:20.5344, R:0.0098, T:166.2453(w:5.000)⚠️)
Batch 150/365: Loss=328.6103 (C:22.9494, R:0.0099, T:305.6599(w:5.000)⚠️)
Batch 175/365: Loss=384.3682 (C:23.7988, R:0.0099, T:360.5684(w:5.000)⚠️)
Batch 200/365: Loss=709.5647 (C:23.6422, R:0.0099, T:685.9216(w:5.000)⚠️)
Batch 225/365: Loss=834.6830 (C:23.9106, R:0.0099, T:810.7715(w:5.000)⚠️)
Batch 250/365: Loss=2179.9976 (C:28.0719, R:0.0098, T:2151.9248(w:5.000)⚠️)
Batch 275/365: Loss=1067.8479 (C:24.3722, R:0.0098, T:1043.4747(w:5.000)⚠️)
Batch 300/365: Loss=63.5920 (C:24.2231, R:0.0099, T:39.3679(w:5.000)⚠️)
Batch 325/365: Loss=866.8587 (C:23.7721, R:0.0098, T:843.0856(w:5.000)⚠️)
Batch 350/365: Loss=1019.2874 (C:25.4186, R:0.0098, T:993.8678(w:5.000)⚠️)

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 635.3098
  Contrastive: 24.5008
  Reconstruction: 0.0098
  Topological: 610.8081 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 11947.7125
  Contrastive: 21.2950
  Reconstruction: 0.0097
  Topological: 11926.4165 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (129.5s)
Train Loss: 635.3098 (C:24.5008, R:0.0098, T:610.8081)
Val Loss:   11947.7125 (C:21.2950, R:0.0097, T:11926.4165)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2193.4104 (C:28.8142, R:0.0098, T:2164.5952(w:5.000)⚠️)
Batch  25/365: Loss=1143.7307 (C:34.0801, R:0.0097, T:1109.6497(w:5.000)⚠️)
Batch  50/365: Loss=1860.2583 (C:33.8205, R:0.0098, T:1826.4368(w:5.000)⚠️)
Batch  75/365: Loss=475.9831 (C:33.4442, R:0.0098, T:442.5380(w:5.000)⚠️)
Batch 100/365: Loss=99.2076 (C:37.2073, R:0.0098, T:61.9993(w:5.000)⚠️)
Batch 125/365: Loss=353.5081 (C:33.1269, R:0.0098, T:320.3802(w:5.000)⚠️)
Batch 150/365: Loss=754.7731 (C:33.0443, R:0.0098, T:721.7278(w:5.000)⚠️)
Batch 175/365: Loss=77.0741 (C:35.1330, R:0.0098, T:41.9401(w:5.000)⚠️)
Batch 200/365: Loss=376.2365 (C:37.1690, R:0.0098, T:339.0665(w:5.000)⚠️)
Batch 225/365: Loss=980.0277 (C:34.5949, R:0.0098, T:945.4318(w:5.000)⚠️)
Batch 250/365: Loss=456.0739 (C:31.8410, R:0.0098, T:424.2319(w:5.000)⚠️)
Batch 275/365: Loss=1154.9774 (C:35.3632, R:0.0099, T:1119.6132(w:5.000)⚠️)
Batch 300/365: Loss=547.2147 (C:37.0176, R:0.0098, T:510.1962(w:5.000)⚠️)
Batch 325/365: Loss=871.9008 (C:35.1031, R:0.0098, T:836.7968(w:5.000)⚠️)
Batch 350/365: Loss=351.6252 (C:41.4934, R:0.0098, T:310.1307(w:5.000)⚠️)

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 869.5487
  Contrastive: 35.4515
  Reconstruction: 0.0098
  Topological: 834.0962 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6255.8173
  Contrastive: 35.0893
  Reconstruction: 0.0098
  Topological: 6220.7270 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 13/50 COMPLETE (128.8s)
Train Loss: 869.5487 (C:35.4515, R:0.0098, T:834.0962)
Val Loss:   6255.8173 (C:35.0893, R:0.0098, T:6220.7270)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=91.4531 (C:41.4216, R:0.0098, T:50.0306(w:5.000)⚠️)
Batch  25/365: Loss=987.7446 (C:43.6282, R:0.0098, T:944.1155(w:5.000)⚠️)
Batch  50/365: Loss=839.5699 (C:35.5456, R:0.0098, T:804.0233(w:5.000)⚠️)
Batch  75/365: Loss=165.1211 (C:38.0662, R:0.0098, T:127.0539(w:5.000)⚠️)
Batch 100/365: Loss=561.5848 (C:38.2314, R:0.0098, T:523.3524(w:5.000)⚠️)
Batch 125/365: Loss=104.0189 (C:40.0566, R:0.0098, T:63.9613(w:5.000)⚠️)
Batch 150/365: Loss=391.4061 (C:40.2107, R:0.0098, T:351.1944(w:5.000)⚠️)
Batch 175/365: Loss=365.2646 (C:39.9928, R:0.0098, T:325.2708(w:5.000)⚠️)
Batch 200/365: Loss=281.1598 (C:38.5209, R:0.0098, T:242.6379(w:5.000)⚠️)
Batch 225/365: Loss=137.3388 (C:39.8071, R:0.0098, T:97.5307(w:5.000)⚠️)
Batch 250/365: Loss=50.5017 (C:34.0886, R:0.0098, T:16.4121(w:5.000)⚠️)
Batch 275/365: Loss=2032.3030 (C:29.5298, R:0.0099, T:2002.7722(w:5.000)⚠️)
Batch 300/365: Loss=335.7879 (C:22.0180, R:0.0098, T:313.7689(w:5.000)⚠️)
Batch 325/365: Loss=191.3984 (C:21.1961, R:0.0099, T:170.2012(w:5.000)⚠️)
Batch 350/365: Loss=166.7140 (C:22.8790, R:0.0098, T:143.8340(w:5.000)⚠️)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 664.3631
  Contrastive: 34.5310
  Reconstruction: 0.0098
  Topological: 629.8311 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 8311.6999
  Contrastive: 16.6439
  Reconstruction: 0.0099
  Topological: 8295.0551 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 14/50 COMPLETE (131.7s)
Train Loss: 664.3631 (C:34.5310, R:0.0098, T:629.8311)
Val Loss:   8311.6999 (C:16.6439, R:0.0099, T:8295.0551)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=37.9295 (C:22.0073, R:0.0099, T:15.9213(w:5.000)⚠️)
Batch  25/365: Loss=752.8740 (C:24.5668, R:0.0099, T:728.3062(w:5.000)⚠️)
Batch  50/365: Loss=138.4300 (C:22.8955, R:0.0099, T:115.5336(w:5.000)⚠️)
Batch  75/365: Loss=626.4338 (C:24.6313, R:0.0099, T:601.8016(w:5.000)⚠️)
Batch 100/365: Loss=158.9398 (C:23.4293, R:0.0099, T:135.5096(w:5.000)⚠️)
Batch 125/365: Loss=181.3937 (C:20.4223, R:0.0098, T:160.9704(w:5.000)⚠️)
Batch 150/365: Loss=736.6712 (C:21.0658, R:0.0098, T:715.6044(w:5.000)⚠️)
Batch 175/365: Loss=484.7934 (C:23.5244, R:0.0098, T:461.2680(w:5.000)⚠️)
Batch 200/365: Loss=731.2330 (C:23.7867, R:0.0098, T:707.4453(w:5.000)⚠️)
Batch 225/365: Loss=919.8725 (C:22.7521, R:0.0099, T:897.1194(w:5.000)⚠️)
Batch 250/365: Loss=50.5256 (C:22.8904, R:0.0098, T:27.6343(w:5.000)⚠️)
Batch 275/365: Loss=58.0609 (C:25.1662, R:0.0099, T:32.8937(w:5.000)⚠️)
Batch 300/365: Loss=305.9712 (C:27.8676, R:0.0099, T:278.1026(w:5.000)⚠️)
Batch 325/365: Loss=274.9457 (C:26.6275, R:0.0098, T:248.3171(w:5.000)⚠️)
Batch 350/365: Loss=1062.8716 (C:25.3672, R:0.0098, T:1037.5034(w:5.000)⚠️)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 552.8178
  Contrastive: 24.0775
  Reconstruction: 0.0098
  Topological: 528.7393 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 3049.0009
  Contrastive: 20.7709
  Reconstruction: 0.0098
  Topological: 3028.2290 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 15/50 COMPLETE (132.4s)
Train Loss: 552.8178 (C:24.0775, R:0.0098, T:528.7393)
Val Loss:   3049.0009 (C:20.7709, R:0.0098, T:3028.2290)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=397.4181 (C:25.0728, R:0.0098, T:372.3443(w:5.000)⚠️)
Batch  25/365: Loss=182.6814 (C:29.5707, R:0.0098, T:153.1097(w:5.000)⚠️)
Batch  50/365: Loss=641.2477 (C:32.2858, R:0.0098, T:608.9609(w:5.000)⚠️)
Batch  75/365: Loss=319.3509 (C:30.2625, R:0.0098, T:289.0873(w:5.000)⚠️)
Batch 100/365: Loss=603.2855 (C:28.0359, R:0.0098, T:575.2485(w:5.000)⚠️)
Batch 125/365: Loss=414.1750 (C:28.5413, R:0.0097, T:385.6328(w:5.000)⚠️)
Batch 150/365: Loss=65.1554 (C:28.8797, R:0.0098, T:36.2747(w:5.000)⚠️)
Batch 175/365: Loss=332.2980 (C:27.1429, R:0.0098, T:305.1542(w:5.000)⚠️)
Batch 200/365: Loss=400.5549 (C:29.1428, R:0.0098, T:371.4111(w:5.000)⚠️)
Batch 225/365: Loss=659.8656 (C:32.2258, R:0.0098, T:627.6388(w:5.000)⚠️)
Batch 250/365: Loss=1019.3461 (C:32.8906, R:0.0098, T:986.4545(w:5.000)⚠️)
Batch 275/365: Loss=148.2809 (C:32.0871, R:0.0098, T:116.1928(w:5.000)⚠️)
Batch 300/365: Loss=830.8757 (C:26.8257, R:0.0098, T:804.0491(w:5.000)⚠️)
Batch 325/365: Loss=441.2823 (C:27.9800, R:0.0098, T:413.3014(w:5.000)⚠️)
Batch 350/365: Loss=559.6681 (C:24.8298, R:0.0098, T:534.8373(w:5.000)⚠️)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 544.4718
  Contrastive: 28.9722
  Reconstruction: 0.0098
  Topological: 515.4986 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 10179.0928
  Contrastive: 20.5265
  Reconstruction: 0.0098
  Topological: 10158.5654 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 16/50 COMPLETE (129.6s)
Train Loss: 544.4718 (C:28.9722, R:0.0098, T:515.4986)
Val Loss:   10179.0928 (C:20.5265, R:0.0098, T:10158.5654)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=471.8310 (C:26.4294, R:0.0098, T:445.4006(w:5.000)⚠️)
Batch  25/365: Loss=582.7518 (C:31.6333, R:0.0099, T:551.1174(w:5.000)⚠️)
Batch  50/365: Loss=317.3415 (C:29.9246, R:0.0099, T:287.4160(w:5.000)⚠️)
Batch  75/365: Loss=1723.2771 (C:27.0100, R:0.0099, T:1696.2661(w:5.000)⚠️)
Batch 100/365: Loss=933.7103 (C:35.2090, R:0.0099, T:898.5003(w:5.000)⚠️)
Batch 125/365: Loss=1079.0364 (C:35.4435, R:0.0098, T:1043.5919(w:5.000)⚠️)
Batch 150/365: Loss=1422.4199 (C:37.9962, R:0.0099, T:1384.4227(w:5.000)⚠️)
Batch 175/365: Loss=1067.1615 (C:35.0364, R:0.0099, T:1032.1241(w:5.000)⚠️)
Batch 200/365: Loss=170.7012 (C:39.0555, R:0.0099, T:131.6447(w:5.000)⚠️)
Batch 225/365: Loss=318.6209 (C:47.8800, R:0.0099, T:270.7399(w:5.000)⚠️)
Batch 250/365: Loss=564.3998 (C:43.7204, R:0.0099, T:520.6784(w:5.000)⚠️)
Batch 275/365: Loss=622.8738 (C:39.7005, R:0.0099, T:583.1722(w:5.000)⚠️)
Batch 300/365: Loss=84.9000 (C:40.3684, R:0.0099, T:44.5306(w:5.000)⚠️)
Batch 325/365: Loss=272.2620 (C:42.1323, R:0.0099, T:230.1288(w:5.000)⚠️)
Batch 350/365: Loss=1210.9822 (C:38.1819, R:0.0099, T:1172.7992(w:5.000)⚠️)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 636.2160
  Contrastive: 36.7226
  Reconstruction: 0.0099
  Topological: 599.4924 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 8768.5228
  Contrastive: 35.8034
  Reconstruction: 0.0099
  Topological: 8732.7184 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 17/50 COMPLETE (129.2s)
Train Loss: 636.2160 (C:36.7226, R:0.0099, T:599.4924)
Val Loss:   8768.5228 (C:35.8034, R:0.0099, T:8732.7184)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=133.9303 (C:38.4191, R:0.0099, T:95.5102(w:5.000)⚠️)
Batch  25/365: Loss=671.5198 (C:36.2280, R:0.0099, T:635.2908(w:5.000)⚠️)
Batch  50/365: Loss=371.2271 (C:39.4617, R:0.0099, T:331.7645(w:5.000)⚠️)
Batch  75/365: Loss=710.1783 (C:45.5057, R:0.0099, T:664.6716(w:5.000)⚠️)
Batch 100/365: Loss=1511.3506 (C:43.7870, R:0.0099, T:1467.5625(w:5.000)⚠️)
Batch 125/365: Loss=888.9654 (C:38.0245, R:0.0098, T:850.9399(w:5.000)⚠️)
Batch 150/365: Loss=1158.2131 (C:40.4030, R:0.0098, T:1117.8091(w:5.000)⚠️)
Batch 175/365: Loss=312.3949 (C:37.3932, R:0.0098, T:275.0007(w:5.000)⚠️)
Batch 200/365: Loss=1418.9912 (C:43.1998, R:0.0099, T:1375.7904(w:5.000)⚠️)
Batch 225/365: Loss=124.3937 (C:37.6225, R:0.0099, T:86.7703(w:5.000)⚠️)
Batch 250/365: Loss=872.5215 (C:31.8762, R:0.0099, T:840.6444(w:5.000)⚠️)
Batch 275/365: Loss=950.6930 (C:31.0449, R:0.0099, T:919.6472(w:5.000)⚠️)
Batch 300/365: Loss=448.5188 (C:33.1069, R:0.0099, T:415.4109(w:5.000)⚠️)
Batch 325/365: Loss=135.8855 (C:36.3229, R:0.0099, T:99.5616(w:5.000)⚠️)
Batch 350/365: Loss=237.2581 (C:33.4288, R:0.0100, T:203.8283(w:5.000)⚠️)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 699.5452
  Contrastive: 38.0462
  Reconstruction: 0.0099
  Topological: 661.4980 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12579.7397
  Contrastive: 25.4716
  Reconstruction: 0.0100
  Topological: 12554.2671 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 18/50 COMPLETE (126.8s)
Train Loss: 699.5452 (C:38.0462, R:0.0099, T:661.4980)
Val Loss:   12579.7397 (C:25.4716, R:0.0100, T:12554.2671)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=656.3218 (C:30.7162, R:0.0100, T:625.6046(w:5.000)⚠️)
Batch  25/365: Loss=481.7032 (C:31.7184, R:0.0100, T:449.9838(w:5.000)⚠️)
Batch  50/365: Loss=774.0449 (C:28.6269, R:0.0100, T:745.4170(w:5.000)⚠️)
Batch  75/365: Loss=336.8351 (C:26.6076, R:0.0100, T:310.2265(w:5.000)⚠️)
Batch 100/365: Loss=446.0190 (C:31.8231, R:0.0100, T:414.1949(w:5.000)⚠️)
Batch 125/365: Loss=103.0405 (C:34.7916, R:0.0100, T:68.2480(w:5.000)⚠️)
Batch 150/365: Loss=1075.0057 (C:34.2394, R:0.0099, T:1040.7654(w:5.000)⚠️)
Batch 175/365: Loss=373.1797 (C:34.7055, R:0.0099, T:338.4732(w:5.000)⚠️)
Batch 200/365: Loss=532.9227 (C:30.8456, R:0.0100, T:502.0761(w:5.000)⚠️)
Batch 225/365: Loss=118.9679 (C:36.9460, R:0.0099, T:82.0209(w:5.000)⚠️)
Batch 250/365: Loss=518.4393 (C:32.5887, R:0.0099, T:485.8496(w:5.000)⚠️)
Batch 275/365: Loss=255.1218 (C:40.6470, R:0.0100, T:214.4738(w:5.000)⚠️)
Batch 300/365: Loss=2209.4817 (C:47.9500, R:0.0099, T:2161.5308(w:5.000)⚠️)
Batch 325/365: Loss=1173.6199 (C:54.7823, R:0.0100, T:1118.8365(w:5.000)⚠️)
Batch 350/365: Loss=595.5872 (C:58.5178, R:0.0100, T:537.0685(w:5.000)⚠️)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 687.4264
  Contrastive: 37.5415
  Reconstruction: 0.0100
  Topological: 649.8839 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 15518.9229
  Contrastive: 48.2501
  Reconstruction: 0.0100
  Topological: 15470.6717 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 19/50 COMPLETE (127.3s)
Train Loss: 687.4264 (C:37.5415, R:0.0100, T:649.8839)
Val Loss:   15518.9229 (C:48.2501, R:0.0100, T:15470.6717)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=65.5475 (C:51.4920, R:0.0099, T:14.0545(w:5.000)⚠️)
Batch  25/365: Loss=1942.6398 (C:61.9296, R:0.0100, T:1880.7092(w:5.000)⚠️)
Batch  50/365: Loss=3816.5156 (C:68.3655, R:0.0100, T:3748.1492(w:5.000)⚠️)
Batch  75/365: Loss=806.8879 (C:49.7709, R:0.0099, T:757.1160(w:5.000)⚠️)
Batch 100/365: Loss=817.2106 (C:42.8255, R:0.0099, T:774.3840(w:5.000)⚠️)
Batch 125/365: Loss=460.1895 (C:33.4799, R:0.0099, T:426.7086(w:5.000)⚠️)
Batch 150/365: Loss=73.6856 (C:25.2923, R:0.0100, T:48.3923(w:5.000)⚠️)
Batch 175/365: Loss=135.5902 (C:22.1454, R:0.0100, T:113.4437(w:5.000)⚠️)
Batch 200/365: Loss=294.8589 (C:20.4968, R:0.0100, T:274.3611(w:5.000)⚠️)
Batch 225/365: Loss=281.0428 (C:24.1120, R:0.0099, T:256.9298(w:5.000)⚠️)
Batch 250/365: Loss=879.4521 (C:25.5755, R:0.0100, T:853.8756(w:5.000)⚠️)
Batch 275/365: Loss=342.9246 (C:25.1725, R:0.0100, T:317.7511(w:5.000)⚠️)
Batch 300/365: Loss=963.3510 (C:26.8698, R:0.0100, T:936.4802(w:5.000)⚠️)
Batch 325/365: Loss=221.1688 (C:26.8629, R:0.0100, T:194.3049(w:5.000)⚠️)
Batch 350/365: Loss=506.8226 (C:24.2469, R:0.0099, T:482.5747(w:5.000)⚠️)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 651.2239
  Contrastive: 34.5693
  Reconstruction: 0.0100
  Topological: 616.6535 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 10890.4201
  Contrastive: 21.0760
  Reconstruction: 0.0100
  Topological: 10869.3431 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 20/50 COMPLETE (139.9s)
Train Loss: 651.2239 (C:34.5693, R:0.0100, T:616.6535)
Val Loss:   10890.4201 (C:21.0760, R:0.0100, T:10869.3431)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=830.9978 (C:26.2674, R:0.0100, T:804.7294(w:5.000)⚠️)
Batch  25/365: Loss=167.8945 (C:31.6715, R:0.0099, T:136.2220(w:5.000)⚠️)
Batch  50/365: Loss=1005.0303 (C:26.8809, R:0.0100, T:978.1483(w:5.000)⚠️)
Batch  75/365: Loss=663.9104 (C:29.9197, R:0.0099, T:633.9897(w:5.000)⚠️)
Batch 100/365: Loss=252.6710 (C:30.5037, R:0.0100, T:222.1663(w:5.000)⚠️)
Batch 125/365: Loss=425.4860 (C:31.0498, R:0.0100, T:394.4352(w:5.000)⚠️)
Batch 150/365: Loss=210.1891 (C:31.2377, R:0.0099, T:178.9505(w:5.000)⚠️)
Batch 175/365: Loss=737.4615 (C:29.1112, R:0.0100, T:708.3494(w:5.000)⚠️)
Batch 200/365: Loss=216.2080 (C:34.7551, R:0.0099, T:181.4519(w:5.000)⚠️)
Batch 225/365: Loss=1031.8030 (C:35.8142, R:0.0100, T:995.9879(w:5.000)⚠️)
Batch 250/365: Loss=160.8285 (C:32.3972, R:0.0100, T:128.4304(w:5.000)⚠️)
Batch 275/365: Loss=2013.7410 (C:34.6948, R:0.0100, T:1979.0452(w:5.000)⚠️)
Batch 300/365: Loss=517.4007 (C:31.1743, R:0.0100, T:486.2254(w:5.000)⚠️)
Batch 325/365: Loss=141.7610 (C:32.5757, R:0.0100, T:109.1843(w:5.000)⚠️)
Batch 350/365: Loss=127.5075 (C:32.1241, R:0.0099, T:95.3824(w:5.000)⚠️)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 517.1441
  Contrastive: 31.4783
  Reconstruction: 0.0100
  Topological: 485.6648 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 8519.0961
  Contrastive: 30.7852
  Reconstruction: 0.0100
  Topological: 8488.3100 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 21/50 COMPLETE (134.2s)
Train Loss: 517.1441 (C:31.4783, R:0.0100, T:485.6648)
Val Loss:   8519.0961 (C:30.7852, R:0.0100, T:8488.3100)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=100.3162 (C:35.6447, R:0.0099, T:64.6705(w:5.000)⚠️)
Batch  25/365: Loss=311.4014 (C:39.3981, R:0.0100, T:272.0023(w:5.000)⚠️)
Batch  50/365: Loss=118.5015 (C:39.3498, R:0.0100, T:79.1506(w:5.000)⚠️)
Batch  75/365: Loss=650.6288 (C:38.3801, R:0.0100, T:612.2477(w:5.000)⚠️)
Batch 100/365: Loss=315.6196 (C:40.1833, R:0.0099, T:275.4353(w:5.000)⚠️)
Batch 125/365: Loss=287.3394 (C:39.6190, R:0.0100, T:247.7194(w:5.000)⚠️)
Batch 150/365: Loss=483.0607 (C:38.9374, R:0.0099, T:444.1223(w:5.000)⚠️)
Batch 175/365: Loss=1471.6620 (C:37.6483, R:0.0099, T:1434.0127(w:5.000)⚠️)
Batch 200/365: Loss=1168.1310 (C:45.7618, R:0.0099, T:1122.3682(w:5.000)⚠️)
Batch 225/365: Loss=440.3531 (C:48.1653, R:0.0099, T:392.1868(w:5.000)⚠️)
Batch 250/365: Loss=1061.2332 (C:43.8150, R:0.0100, T:1017.4171(w:5.000)⚠️)
Batch 275/365: Loss=528.4391 (C:45.9781, R:0.0100, T:482.4600(w:5.000)⚠️)
Batch 300/365: Loss=434.7688 (C:50.6705, R:0.0099, T:384.0973(w:5.000)⚠️)
Batch 325/365: Loss=544.8553 (C:46.3039, R:0.0099, T:498.5504(w:5.000)⚠️)
Batch 350/365: Loss=1907.5352 (C:46.7421, R:0.0100, T:1860.7920(w:5.000)⚠️)

📊 EPOCH 22 TRAINING SUMMARY:
  Total Loss: 609.0898
  Contrastive: 43.0947
  Reconstruction: 0.0100
  Topological: 565.9942 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 14963.5259
  Contrastive: 45.0591
  Reconstruction: 0.0100
  Topological: 14918.4658 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 22/50 COMPLETE (125.5s)
Train Loss: 609.0898 (C:43.0947, R:0.0100, T:565.9942)
Val Loss:   14963.5259 (C:45.0591, R:0.0100, T:14918.4658)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 23 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2071.3154 (C:52.4853, R:0.0099, T:2018.8291(w:5.000)⚠️)
Batch  25/365: Loss=1445.6520 (C:48.2540, R:0.0100, T:1397.3970(w:5.000)⚠️)
Batch  50/365: Loss=193.3394 (C:45.3806, R:0.0100, T:147.9579(w:5.000)⚠️)
Batch  75/365: Loss=345.0017 (C:47.3360, R:0.0099, T:297.6647(w:5.000)⚠️)
Batch 100/365: Loss=181.3310 (C:53.0652, R:0.0099, T:128.2648(w:5.000)⚠️)
Batch 125/365: Loss=285.1390 (C:48.8843, R:0.0100, T:236.2537(w:5.000)⚠️)
Batch 150/365: Loss=1147.9347 (C:59.4945, R:0.0100, T:1088.4392(w:5.000)⚠️)
Batch 175/365: Loss=628.7187 (C:62.0829, R:0.0100, T:566.6348(w:5.000)⚠️)
Batch 200/365: Loss=448.1289 (C:71.8528, R:0.0099, T:376.2751(w:5.000)⚠️)
Batch 225/365: Loss=238.5399 (C:78.6176, R:0.0100, T:159.9213(w:5.000)⚠️)
Batch 250/365: Loss=1250.9026 (C:83.2366, R:0.0100, T:1167.6650(w:5.000)⚠️)
Batch 275/365: Loss=302.5808 (C:101.4659, R:0.0099, T:201.1139(w:5.000)⚠️)
Batch 300/365: Loss=675.8858 (C:108.9664, R:0.0099, T:566.9184(w:5.000)⚠️)
Batch 325/365: Loss=880.6530 (C:123.9122, R:0.0100, T:756.7398(w:5.000)⚠️)
Batch 350/365: Loss=408.6386 (C:112.2078, R:0.0100, T:296.4298(w:5.000)⚠️)

📊 EPOCH 23 TRAINING SUMMARY:
  Total Loss: 722.5849
  Contrastive: 72.9584
  Reconstruction: 0.0100
  Topological: 649.6255 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 13596.6024
  Contrastive: 90.6152
  Reconstruction: 0.0100
  Topological: 13505.9862 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 23/50 COMPLETE (122.1s)
Train Loss: 722.5849 (C:72.9584, R:0.0100, T:649.6255)
Val Loss:   13596.6024 (C:90.6152, R:0.0100, T:13505.9862)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 24 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=536.1156 (C:100.5964, R:0.0100, T:435.5182(w:5.000)⚠️)
Batch  25/365: Loss=1336.1713 (C:103.8663, R:0.0099, T:1232.3040(w:5.000)⚠️)
Batch  50/365: Loss=1418.2189 (C:110.5272, R:0.0099, T:1307.6907(w:5.000)⚠️)
Batch  75/365: Loss=972.3331 (C:120.1716, R:0.0100, T:852.1605(w:5.000)⚠️)
Batch 100/365: Loss=584.3494 (C:109.6417, R:0.0099, T:474.7067(w:5.000)⚠️)
Batch 125/365: Loss=721.0609 (C:91.9301, R:0.0099, T:629.1299(w:5.000)⚠️)
Batch 150/365: Loss=226.2692 (C:75.3250, R:0.0100, T:150.9432(w:5.000)⚠️)
Batch 175/365: Loss=345.1611 (C:78.4042, R:0.0100, T:266.7559(w:5.000)⚠️)
Batch 200/365: Loss=214.1595 (C:81.5833, R:0.0099, T:132.5752(w:5.000)⚠️)
Batch 225/365: Loss=751.9922 (C:88.9073, R:0.0099, T:663.0839(w:5.000)⚠️)
Batch 250/365: Loss=1174.7605 (C:85.4156, R:0.0100, T:1089.3439(w:5.000)⚠️)
Batch 275/365: Loss=753.2656 (C:106.5864, R:0.0100, T:646.6782(w:5.000)⚠️)
Batch 300/365: Loss=3258.7922 (C:92.6256, R:0.0099, T:3166.1658(w:5.000)⚠️)
Batch 325/365: Loss=847.6006 (C:102.4429, R:0.0099, T:745.1568(w:5.000)⚠️)
Batch 350/365: Loss=312.2679 (C:109.7792, R:0.0100, T:202.4877(w:5.000)⚠️)

📊 EPOCH 24 TRAINING SUMMARY:
  Total Loss: 929.9473
  Contrastive: 96.9183
  Reconstruction: 0.0100
  Topological: 833.0280 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 13542.3354
  Contrastive: 93.8051
  Reconstruction: 0.0100
  Topological: 13448.5293 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 24/50 COMPLETE (120.3s)
Train Loss: 929.9473 (C:96.9183, R:0.0100, T:833.0280)
Val Loss:   13542.3354 (C:93.8051, R:0.0100, T:13448.5293)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 25 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=215.8077 (C:104.6460, R:0.0100, T:111.1607(w:5.000)⚠️)
Batch  25/365: Loss=714.3604 (C:104.0126, R:0.0099, T:610.3468(w:5.000)⚠️)
Batch  50/365: Loss=463.9750 (C:112.1876, R:0.0100, T:351.7863(w:5.000)⚠️)
Batch  75/365: Loss=606.7362 (C:128.5880, R:0.0100, T:478.1472(w:5.000)⚠️)
Batch 100/365: Loss=264.1473 (C:63.7803, R:0.0100, T:200.3659(w:5.000)⚠️)
Batch 125/365: Loss=1753.6036 (C:66.1017, R:0.0100, T:1687.5010(w:5.000)⚠️)
Batch 150/365: Loss=309.5988 (C:64.8284, R:0.0100, T:244.7694(w:5.000)⚠️)
Batch 175/365: Loss=1465.7739 (C:57.8713, R:0.0100, T:1407.9016(w:5.000)⚠️)
Batch 200/365: Loss=1152.7555 (C:93.5560, R:0.0100, T:1059.1985(w:5.000)⚠️)
Batch 225/365: Loss=2553.5420 (C:86.4747, R:0.0100, T:2467.0664(w:5.000)⚠️)
Batch 250/365: Loss=409.3546 (C:109.9280, R:0.0099, T:299.4257(w:5.000)⚠️)
Batch 275/365: Loss=919.9359 (C:123.1269, R:0.0099, T:796.8080(w:5.000)⚠️)
Batch 300/365: Loss=1508.0660 (C:102.8564, R:0.0100, T:1405.2086(w:5.000)⚠️)
Batch 325/365: Loss=1071.6486 (C:103.3685, R:0.0100, T:968.2791(w:5.000)⚠️)
Batch 350/365: Loss=833.0770 (C:82.0153, R:0.0100, T:751.0607(w:5.000)⚠️)

📊 EPOCH 25 TRAINING SUMMARY:
  Total Loss: 1094.5345
  Contrastive: 92.6278
  Reconstruction: 0.0100
  Topological: 1001.9056 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 13596.1363
  Contrastive: 54.5915
  Reconstruction: 0.0100
  Topological: 13541.5438 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 25/50 COMPLETE (123.6s)
Train Loss: 1094.5345 (C:92.6278, R:0.0100, T:1001.9056)
Val Loss:   13596.1363 (C:54.5915, R:0.0100, T:13541.5438)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

🛑 Early stopping triggered after 25 epochs
Best model was at epoch 15 with Val Loss: 3049.0009

======================================================================
📈 FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 1
Epochs with topology: 25/25
Max consecutive topology epochs: 25
Best topological loss: 405.3208
Final topological loss: 1001.9056
✅ SUCCESS: Topological learning achieved!
🚀 EXCELLENT: Very consistent topological learning (>80%)
📈 Topological learning appears stable

======================================================================
🎯 TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
✅ Topological training completed successfully!
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164751/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/367 batches
  Processed 51/367 batches
  Processed 101/367 batches
  Processed 151/367 batches
  Processed 201/367 batches
  Processed 251/367 batches
  Processed 301/367 batches
  Processed 351/367 batches
Extracted representations: torch.Size([549367, 50])
Evaluating clustering performance...
Subsampled to 50000 points for clustering evaluation
Clustering Results:
  Silhouette Score: -0.0634
  Adjusted Rand Score: 0.0073
  Clustering Accuracy: 0.3788
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 50])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/365 batches
  Processed 51/365 batches
  Processed 101/365 batches
  Processed 151/365 batches
  Processed 201/365 batches
  Processed 251/365 batches
  Processed 301/365 batches
  Processed 351/365 batches
Extracted representations: torch.Size([547500, 50])
Subsampled training to 50000 samples
Subsampled validation to 10000 samples
  Training on 50000 samples, evaluating on 10000 samples
Classification Results:
  Accuracy: 0.4254
  Per-class F1: [0.4598986091251787, 0.38100943537516846, 0.43090586145648313]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.009777
Evaluating separation quality...
Subsampled to 20000 points for separation evaluation
Separation Results:
  Positive distances: 20.478 ± 16.624
  Negative distances: 21.040 ± 17.202
  Separation ratio: 1.03x
  Gap: -191.182
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: -0.0634
  Clustering Accuracy: 0.3788
  Adjusted Rand Score: 0.0073

Classification Performance:
  Accuracy: 0.4254

Separation Quality:
  Separation Ratio: 1.03x
  Gap: -191.182
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.009777
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164751/results/evaluation_results_20250719_174345.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164751/results/evaluation_results_20250719_174345.json

Key Results:
  Separation ratio: 1.03x
  Perfect separation: False
  Classification accuracy: 0.4254

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

📈 TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 25
  Epochs with topological learning: 25
  Current topological loss: 1001.9056
  Current topological weight: 5.0000
  ⚠️  Topological loss is increasing (may need tuning)
🚀 EXCELLENT: Consistent topological learning achieved!
Final topological loss: 1001.9056
Epochs with topology: 25/25
⚠️  Poor clustering accuracy: 0.379

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164751/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_164751

Analysis completed with exit code: 0
Time: Sat 19 Jul 17:43:46 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
