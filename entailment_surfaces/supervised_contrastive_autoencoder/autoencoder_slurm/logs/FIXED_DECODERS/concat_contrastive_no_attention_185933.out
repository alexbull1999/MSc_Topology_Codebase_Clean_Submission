Starting Surface Distance Metric Analysis job...
Job ID: 185933
Node: gpuvm17
Time: Thu 24 Jul 20:37:28 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Thu Jul 24 20:37:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   31C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-24 20:37:40.959107
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 200
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 3,045,451
Model created with 3,045,451 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 100.0
Optimizer created: Adam (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 3,045,451
Starting training...
========================================
Starting Global Dataset Training...
============================================================

üåç Updating global dataset at epoch 1
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.006 ¬± 0.001
    Neg distances: 0.006 ¬± 0.001
    Separation ratio: 1.00x
    Gap: -0.010
    ‚ùå Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=3.6232 (C:2.0000, R:0.0162)
Batch  25/537: Loss=3.4600 (C:2.0001, R:0.0146)
Batch  50/537: Loss=3.3549 (C:1.9999, R:0.0135)
Batch  75/537: Loss=3.2513 (C:1.9993, R:0.0125)
Batch 100/537: Loss=3.1546 (C:1.9998, R:0.0115)
Batch 125/537: Loss=3.0775 (C:1.9996, R:0.0108)
Batch 150/537: Loss=3.0319 (C:2.0000, R:0.0103)
Batch 175/537: Loss=3.0050 (C:1.9999, R:0.0101)
Batch 200/537: Loss=2.9901 (C:1.9994, R:0.0099)
Batch 225/537: Loss=2.9795 (C:1.9994, R:0.0098)
Batch 250/537: Loss=2.9776 (C:1.9997, R:0.0098)
Batch 275/537: Loss=2.9620 (C:1.9993, R:0.0096)
Batch 300/537: Loss=2.9529 (C:1.9995, R:0.0095)
Batch 325/537: Loss=2.9442 (C:1.9988, R:0.0095)
Batch 350/537: Loss=2.9307 (C:1.9994, R:0.0093)
Batch 375/537: Loss=2.9295 (C:1.9989, R:0.0093)
Batch 400/537: Loss=2.9221 (C:1.9996, R:0.0092)
Batch 425/537: Loss=2.9025 (C:1.9991, R:0.0090)
Batch 450/537: Loss=2.8991 (C:1.9991, R:0.0090)
Batch 475/537: Loss=2.8930 (C:1.9983, R:0.0089)
Batch 500/537: Loss=2.8837 (C:1.9998, R:0.0088)
Batch 525/537: Loss=2.8721 (C:1.9989, R:0.0087)

============================================================
Epoch 1/200 completed in 33.5s
Train: Loss=3.0320 (C:1.9993, R:0.0103) Ratio=1.01x
Val:   Loss=2.8512 (C:1.9983, R:0.0085) Ratio=1.02x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.8512)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=2.8730 (C:1.9986, R:0.0087)
Batch  25/537: Loss=2.8667 (C:1.9989, R:0.0087)
Batch  50/537: Loss=2.8569 (C:1.9966, R:0.0086)
Batch  75/537: Loss=2.8584 (C:1.9979, R:0.0086)
Batch 100/537: Loss=2.8479 (C:1.9971, R:0.0085)
Batch 125/537: Loss=2.8435 (C:1.9978, R:0.0085)
Batch 150/537: Loss=2.8415 (C:1.9969, R:0.0084)
Batch 175/537: Loss=2.8296 (C:1.9977, R:0.0083)
Batch 200/537: Loss=2.8343 (C:1.9983, R:0.0084)
Batch 225/537: Loss=2.8283 (C:1.9978, R:0.0083)
Batch 250/537: Loss=2.8263 (C:1.9983, R:0.0083)
Batch 275/537: Loss=2.8186 (C:1.9976, R:0.0082)
Batch 300/537: Loss=2.8166 (C:1.9979, R:0.0082)
Batch 325/537: Loss=2.8050 (C:1.9967, R:0.0081)
Batch 350/537: Loss=2.8103 (C:1.9977, R:0.0081)
Batch 375/537: Loss=2.8034 (C:1.9961, R:0.0081)
Batch 400/537: Loss=2.7946 (C:1.9964, R:0.0080)
Batch 425/537: Loss=2.7899 (C:1.9945, R:0.0080)
Batch 450/537: Loss=2.7881 (C:1.9956, R:0.0079)
Batch 475/537: Loss=2.7808 (C:1.9953, R:0.0079)
Batch 500/537: Loss=2.7826 (C:1.9955, R:0.0079)
Batch 525/537: Loss=2.7824 (C:1.9904, R:0.0079)

============================================================
Epoch 2/200 completed in 24.1s
Train: Loss=2.8206 (C:1.9968, R:0.0082) Ratio=1.02x
Val:   Loss=2.7483 (C:1.9919, R:0.0076) Ratio=1.05x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.7483)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=2.7838 (C:1.9939, R:0.0079)
Batch  25/537: Loss=2.7719 (C:1.9924, R:0.0078)
Batch  50/537: Loss=2.7718 (C:1.9864, R:0.0079)
Batch  75/537: Loss=2.7649 (C:1.9818, R:0.0078)
Batch 100/537: Loss=2.7639 (C:1.9819, R:0.0078)
Batch 125/537: Loss=2.7535 (C:1.9653, R:0.0079)
Batch 150/537: Loss=2.7463 (C:1.9671, R:0.0078)
Batch 175/537: Loss=2.7340 (C:1.9591, R:0.0077)
Batch 200/537: Loss=2.7323 (C:1.9522, R:0.0078)
Batch 225/537: Loss=2.7294 (C:1.9528, R:0.0078)
Batch 250/537: Loss=2.7262 (C:1.9444, R:0.0078)
Batch 275/537: Loss=2.7302 (C:1.9542, R:0.0078)
Batch 300/537: Loss=2.7234 (C:1.9503, R:0.0077)
Batch 325/537: Loss=2.7193 (C:1.9461, R:0.0077)
Batch 350/537: Loss=2.7113 (C:1.9445, R:0.0077)
Batch 375/537: Loss=2.7173 (C:1.9478, R:0.0077)
Batch 400/537: Loss=2.7108 (C:1.9404, R:0.0077)
Batch 425/537: Loss=2.7073 (C:1.9448, R:0.0076)
Batch 450/537: Loss=2.6955 (C:1.9329, R:0.0076)
Batch 475/537: Loss=2.6985 (C:1.9381, R:0.0076)
Batch 500/537: Loss=2.7053 (C:1.9399, R:0.0077)
Batch 525/537: Loss=2.6874 (C:1.9272, R:0.0076)

============================================================
Epoch 3/200 completed in 26.1s
Train: Loss=2.7292 (C:1.9558, R:0.0077) Ratio=1.29x
Val:   Loss=2.6547 (C:1.9245, R:0.0073) Ratio=1.53x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.6547)
============================================================

üåç Updating global dataset at epoch 4
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.998 ¬± 0.431
    Neg distances: 1.533 ¬± 0.608
    Separation ratio: 1.54x
    Gap: -2.703
    ‚ö†Ô∏è  Moderate global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=2.3166 (C:1.5651, R:0.0075)
Batch  25/537: Loss=2.2998 (C:1.5311, R:0.0077)
Batch  50/537: Loss=2.3171 (C:1.5441, R:0.0077)
Batch  75/537: Loss=2.3143 (C:1.5445, R:0.0077)
Batch 100/537: Loss=2.2992 (C:1.5316, R:0.0077)
Batch 125/537: Loss=2.3045 (C:1.5334, R:0.0077)
Batch 150/537: Loss=2.3027 (C:1.5341, R:0.0077)
Batch 175/537: Loss=2.3155 (C:1.5466, R:0.0077)
Batch 200/537: Loss=2.3061 (C:1.5380, R:0.0077)
Batch 225/537: Loss=2.2694 (C:1.5068, R:0.0076)
Batch 250/537: Loss=2.3087 (C:1.5409, R:0.0077)
Batch 275/537: Loss=2.2797 (C:1.5144, R:0.0077)
Batch 300/537: Loss=2.2817 (C:1.5149, R:0.0077)
Batch 325/537: Loss=2.2875 (C:1.5303, R:0.0076)
Batch 350/537: Loss=2.3074 (C:1.5436, R:0.0076)
Batch 375/537: Loss=2.2735 (C:1.5166, R:0.0076)
Batch 400/537: Loss=2.2785 (C:1.5185, R:0.0076)
Batch 425/537: Loss=2.2807 (C:1.5229, R:0.0076)
Batch 450/537: Loss=2.2723 (C:1.5113, R:0.0076)
Batch 475/537: Loss=2.2787 (C:1.5230, R:0.0076)
Batch 500/537: Loss=2.2879 (C:1.5297, R:0.0076)
Batch 525/537: Loss=2.2609 (C:1.5085, R:0.0075)

============================================================
Epoch 4/200 completed in 33.7s
Train: Loss=2.2950 (C:1.5308, R:0.0076) Ratio=1.65x
Val:   Loss=2.2295 (C:1.5010, R:0.0073) Ratio=1.81x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.2295)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=2.2886 (C:1.5344, R:0.0075)
Batch  25/537: Loss=2.2913 (C:1.5319, R:0.0076)
Batch  50/537: Loss=2.2309 (C:1.4758, R:0.0076)
Batch  75/537: Loss=2.2722 (C:1.5168, R:0.0076)
Batch 100/537: Loss=2.2446 (C:1.4925, R:0.0075)
Batch 125/537: Loss=2.2759 (C:1.5241, R:0.0075)
Batch 150/537: Loss=2.2676 (C:1.5093, R:0.0076)
Batch 175/537: Loss=2.2597 (C:1.5105, R:0.0075)
Batch 200/537: Loss=2.2302 (C:1.4828, R:0.0075)
Batch 225/537: Loss=2.2428 (C:1.4926, R:0.0075)
Batch 250/537: Loss=2.2459 (C:1.4959, R:0.0075)
Batch 275/537: Loss=2.2714 (C:1.5259, R:0.0075)
Batch 300/537: Loss=2.2326 (C:1.4883, R:0.0074)
Batch 325/537: Loss=2.2594 (C:1.5078, R:0.0075)
Batch 350/537: Loss=2.2489 (C:1.5030, R:0.0075)
Batch 375/537: Loss=2.2796 (C:1.5284, R:0.0075)
Batch 400/537: Loss=2.2643 (C:1.5137, R:0.0075)
Batch 425/537: Loss=2.2532 (C:1.5070, R:0.0075)
Batch 450/537: Loss=2.2510 (C:1.5048, R:0.0075)
Batch 475/537: Loss=2.2278 (C:1.4806, R:0.0075)
Batch 500/537: Loss=2.2434 (C:1.5098, R:0.0073)
Batch 525/537: Loss=2.2499 (C:1.5070, R:0.0074)

============================================================
Epoch 5/200 completed in 23.5s
Train: Loss=2.2518 (C:1.5019, R:0.0075) Ratio=1.79x
Val:   Loss=2.1954 (C:1.4788, R:0.0072) Ratio=1.89x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1954)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=2.2294 (C:1.4944, R:0.0074)
Batch  25/537: Loss=2.2199 (C:1.4781, R:0.0074)
Batch  50/537: Loss=2.2144 (C:1.4755, R:0.0074)
Batch  75/537: Loss=2.2408 (C:1.4895, R:0.0075)
Batch 100/537: Loss=2.2478 (C:1.5043, R:0.0074)
Batch 125/537: Loss=2.2231 (C:1.4854, R:0.0074)
Batch 150/537: Loss=2.2210 (C:1.4785, R:0.0074)
Batch 175/537: Loss=2.2000 (C:1.4581, R:0.0074)
Batch 200/537: Loss=2.2167 (C:1.4767, R:0.0074)
Batch 225/537: Loss=2.2120 (C:1.4659, R:0.0075)
Batch 250/537: Loss=2.2081 (C:1.4676, R:0.0074)
Batch 275/537: Loss=2.2169 (C:1.4817, R:0.0074)
Batch 300/537: Loss=2.2230 (C:1.4862, R:0.0074)
Batch 325/537: Loss=2.2196 (C:1.4801, R:0.0074)
Batch 350/537: Loss=2.2635 (C:1.5271, R:0.0074)
Batch 375/537: Loss=2.2397 (C:1.5017, R:0.0074)
Batch 400/537: Loss=2.2108 (C:1.4799, R:0.0073)
Batch 425/537: Loss=2.2144 (C:1.4811, R:0.0073)
Batch 450/537: Loss=2.2509 (C:1.5089, R:0.0074)
Batch 475/537: Loss=2.2428 (C:1.5057, R:0.0074)
Batch 500/537: Loss=2.2161 (C:1.4819, R:0.0073)
Batch 525/537: Loss=2.2369 (C:1.4997, R:0.0074)

============================================================
Epoch 6/200 completed in 23.6s
Train: Loss=2.2280 (C:1.4881, R:0.0074) Ratio=1.89x
Val:   Loss=2.1949 (C:1.4879, R:0.0071) Ratio=1.94x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1949)
============================================================

üåç Updating global dataset at epoch 7
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.702 ¬± 0.518
    Neg distances: 1.522 ¬± 0.747
    Separation ratio: 2.17x
    Gap: -2.578
    ‚úÖ Good global separation

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=2.0606 (C:1.3204, R:0.0074)
Batch  25/537: Loss=2.0269 (C:1.2915, R:0.0074)
Batch  50/537: Loss=2.0356 (C:1.2997, R:0.0074)
Batch  75/537: Loss=2.0411 (C:1.3008, R:0.0074)
Batch 100/537: Loss=2.0451 (C:1.3039, R:0.0074)
Batch 125/537: Loss=2.0445 (C:1.3133, R:0.0073)
Batch 150/537: Loss=2.0558 (C:1.3210, R:0.0073)
Batch 175/537: Loss=2.0593 (C:1.3169, R:0.0074)
Batch 200/537: Loss=2.0275 (C:1.2904, R:0.0074)
Batch 225/537: Loss=2.0653 (C:1.3281, R:0.0074)
Batch 250/537: Loss=2.0409 (C:1.3027, R:0.0074)
Batch 275/537: Loss=2.0588 (C:1.3203, R:0.0074)
Batch 300/537: Loss=2.0596 (C:1.3232, R:0.0074)
Batch 325/537: Loss=2.0310 (C:1.2954, R:0.0074)
Batch 350/537: Loss=2.0572 (C:1.3188, R:0.0074)
Batch 375/537: Loss=2.0722 (C:1.3305, R:0.0074)
Batch 400/537: Loss=2.0675 (C:1.3371, R:0.0073)
Batch 425/537: Loss=2.0168 (C:1.2770, R:0.0074)
Batch 450/537: Loss=2.0556 (C:1.3170, R:0.0074)
Batch 475/537: Loss=2.0438 (C:1.3146, R:0.0073)
Batch 500/537: Loss=2.0494 (C:1.3125, R:0.0074)
Batch 525/537: Loss=2.0346 (C:1.2979, R:0.0074)

============================================================
Epoch 7/200 completed in 31.9s
Train: Loss=2.0427 (C:1.3057, R:0.0074) Ratio=1.99x
Val:   Loss=2.0048 (C:1.2981, R:0.0071) Ratio=2.02x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.0048)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=2.0318 (C:1.3007, R:0.0073)
Batch  25/537: Loss=2.0354 (C:1.3012, R:0.0073)
Batch  50/537: Loss=2.0825 (C:1.3526, R:0.0073)
Batch  75/537: Loss=2.0381 (C:1.3079, R:0.0073)
Batch 100/537: Loss=2.0032 (C:1.2639, R:0.0074)
Batch 125/537: Loss=1.9988 (C:1.2654, R:0.0073)
Batch 150/537: Loss=2.0259 (C:1.2968, R:0.0073)
Batch 175/537: Loss=2.0285 (C:1.3000, R:0.0073)
Batch 200/537: Loss=2.0079 (C:1.2774, R:0.0073)
Batch 225/537: Loss=2.0181 (C:1.2837, R:0.0073)
Batch 250/537: Loss=2.0067 (C:1.2760, R:0.0073)
Batch 275/537: Loss=2.0582 (C:1.3239, R:0.0073)
Batch 300/537: Loss=2.0253 (C:1.2996, R:0.0073)
Batch 325/537: Loss=1.9937 (C:1.2697, R:0.0072)
Batch 350/537: Loss=1.9996 (C:1.2745, R:0.0073)
Batch 375/537: Loss=2.0243 (C:1.2890, R:0.0074)
Batch 400/537: Loss=2.0299 (C:1.2913, R:0.0074)
Batch 425/537: Loss=2.0397 (C:1.3129, R:0.0073)
Batch 450/537: Loss=2.0027 (C:1.2705, R:0.0073)
Batch 475/537: Loss=2.0133 (C:1.2856, R:0.0073)
Batch 500/537: Loss=2.0126 (C:1.2787, R:0.0073)
Batch 525/537: Loss=1.9982 (C:1.2684, R:0.0073)

============================================================
Epoch 8/200 completed in 23.8s
Train: Loss=2.0255 (C:1.2938, R:0.0073) Ratio=2.10x
Val:   Loss=1.9836 (C:1.2825, R:0.0070) Ratio=2.10x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9836)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=2.0139 (C:1.2896, R:0.0072)
Batch  25/537: Loss=1.9938 (C:1.2638, R:0.0073)
Batch  50/537: Loss=2.0124 (C:1.2797, R:0.0073)
Batch  75/537: Loss=2.0305 (C:1.2962, R:0.0073)
Batch 100/537: Loss=2.0032 (C:1.2704, R:0.0073)
Batch 125/537: Loss=2.0105 (C:1.2823, R:0.0073)
Batch 150/537: Loss=2.0322 (C:1.3047, R:0.0073)
Batch 175/537: Loss=2.0144 (C:1.2777, R:0.0074)
Batch 200/537: Loss=2.0181 (C:1.2877, R:0.0073)
Batch 225/537: Loss=2.0377 (C:1.3089, R:0.0073)
Batch 250/537: Loss=1.9582 (C:1.2339, R:0.0072)
Batch 275/537: Loss=2.0104 (C:1.2902, R:0.0072)
Batch 300/537: Loss=2.0108 (C:1.2781, R:0.0073)
Batch 325/537: Loss=2.0065 (C:1.2824, R:0.0072)
Batch 350/537: Loss=1.9989 (C:1.2679, R:0.0073)
Batch 375/537: Loss=2.0041 (C:1.2747, R:0.0073)
Batch 400/537: Loss=2.0100 (C:1.2755, R:0.0073)
Batch 425/537: Loss=1.9871 (C:1.2641, R:0.0072)
Batch 450/537: Loss=2.0226 (C:1.2966, R:0.0073)
Batch 475/537: Loss=2.0302 (C:1.3056, R:0.0072)
Batch 500/537: Loss=2.0163 (C:1.2913, R:0.0073)
Batch 525/537: Loss=2.0325 (C:1.3090, R:0.0072)

============================================================
Epoch 9/200 completed in 23.6s
Train: Loss=2.0135 (C:1.2858, R:0.0073) Ratio=2.15x
Val:   Loss=1.9863 (C:1.2892, R:0.0070) Ratio=2.09x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 10
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.618 ¬± 0.546
    Neg distances: 1.470 ¬± 0.795
    Separation ratio: 2.38x
    Gap: -2.466
    ‚úÖ Good global separation

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.9791 (C:1.2607, R:0.0072)
Batch  25/537: Loss=1.9715 (C:1.2415, R:0.0073)
Batch  50/537: Loss=1.9734 (C:1.2447, R:0.0073)
Batch  75/537: Loss=1.9813 (C:1.2533, R:0.0073)
Batch 100/537: Loss=1.9665 (C:1.2381, R:0.0073)
Batch 125/537: Loss=1.9750 (C:1.2489, R:0.0073)
Batch 150/537: Loss=1.9933 (C:1.2608, R:0.0073)
Batch 175/537: Loss=1.9692 (C:1.2415, R:0.0073)
Batch 200/537: Loss=1.9780 (C:1.2598, R:0.0072)
Batch 225/537: Loss=1.9680 (C:1.2453, R:0.0072)
Batch 250/537: Loss=1.9725 (C:1.2452, R:0.0073)
Batch 275/537: Loss=1.9308 (C:1.2021, R:0.0073)
Batch 300/537: Loss=2.0117 (C:1.2834, R:0.0073)
Batch 325/537: Loss=1.9505 (C:1.2279, R:0.0072)
Batch 350/537: Loss=2.0130 (C:1.2859, R:0.0073)
Batch 375/537: Loss=2.0035 (C:1.2890, R:0.0071)
Batch 400/537: Loss=1.9689 (C:1.2443, R:0.0072)
Batch 425/537: Loss=1.9923 (C:1.2630, R:0.0073)
Batch 450/537: Loss=1.9709 (C:1.2432, R:0.0073)
Batch 475/537: Loss=1.9691 (C:1.2395, R:0.0073)
Batch 500/537: Loss=1.9760 (C:1.2537, R:0.0072)
Batch 525/537: Loss=1.9763 (C:1.2510, R:0.0073)

============================================================
Epoch 10/200 completed in 31.7s
Train: Loss=1.9773 (C:1.2520, R:0.0073) Ratio=2.21x
Val:   Loss=1.9527 (C:1.2576, R:0.0070) Ratio=2.14x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9527)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.9509 (C:1.2271, R:0.0072)
Batch  25/537: Loss=1.9864 (C:1.2579, R:0.0073)
Batch  50/537: Loss=1.9617 (C:1.2389, R:0.0072)
Batch  75/537: Loss=1.9752 (C:1.2513, R:0.0072)
Batch 100/537: Loss=1.9829 (C:1.2595, R:0.0072)
Batch 125/537: Loss=1.9485 (C:1.2276, R:0.0072)
Batch 150/537: Loss=1.9825 (C:1.2639, R:0.0072)
Batch 175/537: Loss=1.9694 (C:1.2472, R:0.0072)
Batch 200/537: Loss=1.9671 (C:1.2510, R:0.0072)
Batch 225/537: Loss=1.9808 (C:1.2614, R:0.0072)
Batch 250/537: Loss=1.9859 (C:1.2653, R:0.0072)
Batch 275/537: Loss=2.0162 (C:1.2907, R:0.0073)
Batch 300/537: Loss=1.9536 (C:1.2281, R:0.0073)
Batch 325/537: Loss=1.9671 (C:1.2363, R:0.0073)
Batch 350/537: Loss=1.9587 (C:1.2410, R:0.0072)
Batch 375/537: Loss=1.9524 (C:1.2291, R:0.0072)
Batch 400/537: Loss=1.9795 (C:1.2590, R:0.0072)
Batch 425/537: Loss=1.9662 (C:1.2461, R:0.0072)
Batch 450/537: Loss=1.9818 (C:1.2598, R:0.0072)
Batch 475/537: Loss=1.9658 (C:1.2496, R:0.0072)
Batch 500/537: Loss=1.9452 (C:1.2218, R:0.0072)
Batch 525/537: Loss=1.9744 (C:1.2527, R:0.0072)

============================================================
Epoch 11/200 completed in 23.6s
Train: Loss=1.9692 (C:1.2466, R:0.0072) Ratio=2.25x
Val:   Loss=1.9494 (C:1.2579, R:0.0069) Ratio=2.21x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9494)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.9414 (C:1.2191, R:0.0072)
Batch  25/537: Loss=1.9487 (C:1.2282, R:0.0072)
Batch  50/537: Loss=1.9532 (C:1.2364, R:0.0072)
Batch  75/537: Loss=1.9966 (C:1.2669, R:0.0073)
Batch 100/537: Loss=1.9539 (C:1.2390, R:0.0071)
Batch 125/537: Loss=2.0015 (C:1.2789, R:0.0072)
Batch 150/537: Loss=1.9666 (C:1.2554, R:0.0071)
Batch 175/537: Loss=1.9759 (C:1.2481, R:0.0073)
Batch 200/537: Loss=1.8982 (C:1.1821, R:0.0072)
Batch 225/537: Loss=1.9947 (C:1.2711, R:0.0072)
Batch 250/537: Loss=1.9010 (C:1.1872, R:0.0071)
Batch 275/537: Loss=1.9559 (C:1.2361, R:0.0072)
Batch 300/537: Loss=1.9655 (C:1.2528, R:0.0071)
Batch 325/537: Loss=1.9769 (C:1.2640, R:0.0071)
Batch 350/537: Loss=1.9408 (C:1.2159, R:0.0072)
Batch 375/537: Loss=1.9512 (C:1.2343, R:0.0072)
Batch 400/537: Loss=1.9565 (C:1.2380, R:0.0072)
Batch 425/537: Loss=1.9831 (C:1.2568, R:0.0073)
Batch 450/537: Loss=2.0070 (C:1.2837, R:0.0072)
Batch 475/537: Loss=1.9677 (C:1.2504, R:0.0072)
Batch 500/537: Loss=1.9703 (C:1.2549, R:0.0072)
Batch 525/537: Loss=1.9548 (C:1.2332, R:0.0072)

============================================================
Epoch 12/200 completed in 23.9s
Train: Loss=1.9602 (C:1.2401, R:0.0072) Ratio=2.31x
Val:   Loss=1.9409 (C:1.2508, R:0.0069) Ratio=2.19x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9409)
============================================================

üåç Updating global dataset at epoch 13
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.544 ¬± 0.541
    Neg distances: 1.488 ¬± 0.821
    Separation ratio: 2.73x
    Gap: -2.417
    ‚úÖ Good global separation

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=1.8918 (C:1.1724, R:0.0072)
Batch  25/537: Loss=1.8575 (C:1.1390, R:0.0072)
Batch  50/537: Loss=1.8936 (C:1.1756, R:0.0072)
Batch  75/537: Loss=1.9074 (C:1.1917, R:0.0072)
Batch 100/537: Loss=1.8651 (C:1.1430, R:0.0072)
Batch 125/537: Loss=1.8917 (C:1.1755, R:0.0072)
Batch 150/537: Loss=1.9413 (C:1.2249, R:0.0072)
Batch 175/537: Loss=1.8757 (C:1.1529, R:0.0072)
Batch 200/537: Loss=1.9033 (C:1.1818, R:0.0072)
Batch 225/537: Loss=1.9041 (C:1.1908, R:0.0071)
Batch 250/537: Loss=1.9133 (C:1.1922, R:0.0072)
Batch 275/537: Loss=1.8860 (C:1.1692, R:0.0072)
Batch 300/537: Loss=1.9354 (C:1.2094, R:0.0073)
Batch 325/537: Loss=1.9198 (C:1.2001, R:0.0072)
Batch 350/537: Loss=1.9096 (C:1.1905, R:0.0072)
Batch 375/537: Loss=1.8734 (C:1.1520, R:0.0072)
Batch 400/537: Loss=1.8940 (C:1.1722, R:0.0072)
Batch 425/537: Loss=1.9372 (C:1.2215, R:0.0072)
Batch 450/537: Loss=1.8964 (C:1.1786, R:0.0072)
Batch 475/537: Loss=1.8722 (C:1.1587, R:0.0071)
Batch 500/537: Loss=1.8969 (C:1.1835, R:0.0071)
Batch 525/537: Loss=1.8701 (C:1.1526, R:0.0072)

============================================================
Epoch 13/200 completed in 31.5s
Train: Loss=1.9040 (C:1.1852, R:0.0072) Ratio=2.38x
Val:   Loss=1.8929 (C:1.2055, R:0.0069) Ratio=2.25x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8929)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=1.9313 (C:1.2080, R:0.0072)
Batch  25/537: Loss=1.8889 (C:1.1687, R:0.0072)
Batch  50/537: Loss=1.8920 (C:1.1797, R:0.0071)
Batch  75/537: Loss=1.8900 (C:1.1697, R:0.0072)
Batch 100/537: Loss=1.8808 (C:1.1666, R:0.0071)
Batch 125/537: Loss=1.8914 (C:1.1727, R:0.0072)
Batch 150/537: Loss=1.9012 (C:1.1856, R:0.0072)
Batch 175/537: Loss=1.9251 (C:1.2086, R:0.0072)
Batch 200/537: Loss=1.8905 (C:1.1760, R:0.0071)
Batch 225/537: Loss=1.8702 (C:1.1582, R:0.0071)
Batch 250/537: Loss=1.9058 (C:1.1910, R:0.0071)
Batch 275/537: Loss=1.9035 (C:1.1871, R:0.0072)
Batch 300/537: Loss=1.9112 (C:1.2016, R:0.0071)
Batch 325/537: Loss=1.8819 (C:1.1623, R:0.0072)
Batch 350/537: Loss=1.9473 (C:1.2324, R:0.0071)
Batch 375/537: Loss=1.8910 (C:1.1767, R:0.0071)
Batch 400/537: Loss=1.8919 (C:1.1823, R:0.0071)
Batch 425/537: Loss=1.8642 (C:1.1508, R:0.0071)
Batch 450/537: Loss=1.8955 (C:1.1787, R:0.0072)
Batch 475/537: Loss=1.8920 (C:1.1795, R:0.0071)
Batch 500/537: Loss=1.9122 (C:1.1877, R:0.0072)
Batch 525/537: Loss=1.8678 (C:1.1510, R:0.0072)

============================================================
Epoch 14/200 completed in 23.6s
Train: Loss=1.8989 (C:1.1822, R:0.0072) Ratio=2.39x
Val:   Loss=1.8853 (C:1.1986, R:0.0069) Ratio=2.25x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8853)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=1.8914 (C:1.1768, R:0.0071)
Batch  25/537: Loss=1.9005 (C:1.1813, R:0.0072)
Batch  50/537: Loss=1.9145 (C:1.1934, R:0.0072)
Batch  75/537: Loss=1.8968 (C:1.1828, R:0.0071)
Batch 100/537: Loss=1.8756 (C:1.1606, R:0.0072)
Batch 125/537: Loss=1.9488 (C:1.2341, R:0.0071)
Batch 150/537: Loss=1.8911 (C:1.1760, R:0.0072)
Batch 175/537: Loss=1.8587 (C:1.1404, R:0.0072)
Batch 200/537: Loss=1.8854 (C:1.1691, R:0.0072)
Batch 225/537: Loss=1.8733 (C:1.1603, R:0.0071)
Batch 250/537: Loss=1.8442 (C:1.1268, R:0.0072)
Batch 275/537: Loss=1.8891 (C:1.1748, R:0.0071)
Batch 300/537: Loss=1.8807 (C:1.1662, R:0.0071)
Batch 325/537: Loss=1.9188 (C:1.2017, R:0.0072)
Batch 350/537: Loss=1.8808 (C:1.1685, R:0.0071)
Batch 375/537: Loss=1.9197 (C:1.2041, R:0.0072)
Batch 400/537: Loss=1.9163 (C:1.1955, R:0.0072)
Batch 425/537: Loss=1.9096 (C:1.1914, R:0.0072)
Batch 450/537: Loss=1.8828 (C:1.1641, R:0.0072)
Batch 475/537: Loss=1.8662 (C:1.1524, R:0.0071)
Batch 500/537: Loss=1.8598 (C:1.1434, R:0.0072)
Batch 525/537: Loss=1.8494 (C:1.1396, R:0.0071)

============================================================
Epoch 15/200 completed in 24.0s
Train: Loss=1.8921 (C:1.1772, R:0.0071) Ratio=2.46x
Val:   Loss=1.8847 (C:1.2011, R:0.0068) Ratio=2.27x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8847)
============================================================

üåç Updating global dataset at epoch 16
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.530 ¬± 0.564
    Neg distances: 1.465 ¬± 0.851
    Separation ratio: 2.77x
    Gap: -2.404
    ‚úÖ Good global separation

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=1.9024 (C:1.1871, R:0.0072)
Batch  25/537: Loss=1.9092 (C:1.1951, R:0.0071)
Batch  50/537: Loss=1.9450 (C:1.2274, R:0.0072)
Batch  75/537: Loss=1.8873 (C:1.1693, R:0.0072)
Batch 100/537: Loss=1.8741 (C:1.1599, R:0.0071)
Batch 125/537: Loss=1.8896 (C:1.1763, R:0.0071)
Batch 150/537: Loss=1.9092 (C:1.2020, R:0.0071)
Batch 175/537: Loss=1.8686 (C:1.1577, R:0.0071)
Batch 200/537: Loss=1.9193 (C:1.2054, R:0.0071)
Batch 225/537: Loss=1.8849 (C:1.1747, R:0.0071)
Batch 250/537: Loss=1.8877 (C:1.1813, R:0.0071)
Batch 275/537: Loss=1.8995 (C:1.1868, R:0.0071)
Batch 300/537: Loss=1.9015 (C:1.1864, R:0.0072)
Batch 325/537: Loss=1.8988 (C:1.1859, R:0.0071)
Batch 350/537: Loss=1.8947 (C:1.1813, R:0.0071)
Batch 375/537: Loss=1.9155 (C:1.2053, R:0.0071)
Batch 400/537: Loss=1.8975 (C:1.1828, R:0.0071)
Batch 425/537: Loss=1.8961 (C:1.1814, R:0.0071)
Batch 450/537: Loss=1.8781 (C:1.1625, R:0.0072)
Batch 475/537: Loss=1.8907 (C:1.1795, R:0.0071)
Batch 500/537: Loss=1.8959 (C:1.1782, R:0.0072)
Batch 525/537: Loss=1.8603 (C:1.1506, R:0.0071)

============================================================
Epoch 16/200 completed in 33.3s
Train: Loss=1.8960 (C:1.1827, R:0.0071) Ratio=2.46x
Val:   Loss=1.8990 (C:1.2168, R:0.0068) Ratio=2.25x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=1.8656 (C:1.1509, R:0.0071)
Batch  25/537: Loss=1.8776 (C:1.1629, R:0.0071)
Batch  50/537: Loss=1.8648 (C:1.1554, R:0.0071)
Batch  75/537: Loss=1.9106 (C:1.1995, R:0.0071)
Batch 100/537: Loss=1.8924 (C:1.1765, R:0.0072)
Batch 125/537: Loss=1.9112 (C:1.1980, R:0.0071)
Batch 150/537: Loss=1.9292 (C:1.2186, R:0.0071)
Batch 175/537: Loss=1.9258 (C:1.2158, R:0.0071)
Batch 200/537: Loss=1.8768 (C:1.1633, R:0.0071)
Batch 225/537: Loss=1.9123 (C:1.1921, R:0.0072)
Batch 250/537: Loss=1.9017 (C:1.1945, R:0.0071)
Batch 275/537: Loss=1.8885 (C:1.1811, R:0.0071)
Batch 300/537: Loss=1.8868 (C:1.1708, R:0.0072)
Batch 325/537: Loss=1.9161 (C:1.2069, R:0.0071)
Batch 350/537: Loss=1.8746 (C:1.1686, R:0.0071)
Batch 375/537: Loss=1.8817 (C:1.1716, R:0.0071)
Batch 400/537: Loss=1.8882 (C:1.1825, R:0.0071)
Batch 425/537: Loss=1.9106 (C:1.1979, R:0.0071)
Batch 450/537: Loss=1.8926 (C:1.1714, R:0.0072)
Batch 475/537: Loss=1.8996 (C:1.1893, R:0.0071)
Batch 500/537: Loss=1.8814 (C:1.1681, R:0.0071)
Batch 525/537: Loss=1.8725 (C:1.1604, R:0.0071)

============================================================
Epoch 17/200 completed in 24.8s
Train: Loss=1.8910 (C:1.1794, R:0.0071) Ratio=2.49x
Val:   Loss=1.8834 (C:1.2042, R:0.0068) Ratio=2.31x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8834)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=1.8643 (C:1.1556, R:0.0071)
Batch  25/537: Loss=1.8445 (C:1.1419, R:0.0070)
Batch  50/537: Loss=1.8544 (C:1.1449, R:0.0071)
Batch  75/537: Loss=1.8817 (C:1.1769, R:0.0070)
Batch 100/537: Loss=1.8789 (C:1.1666, R:0.0071)
Batch 125/537: Loss=1.9049 (C:1.1956, R:0.0071)
Batch 150/537: Loss=1.8834 (C:1.1819, R:0.0070)
Batch 175/537: Loss=1.9024 (C:1.1912, R:0.0071)
Batch 200/537: Loss=1.8775 (C:1.1627, R:0.0071)
Batch 225/537: Loss=1.8718 (C:1.1603, R:0.0071)
Batch 250/537: Loss=1.8708 (C:1.1621, R:0.0071)
Batch 275/537: Loss=1.8772 (C:1.1628, R:0.0071)
Batch 300/537: Loss=1.9081 (C:1.2001, R:0.0071)
Batch 325/537: Loss=1.8960 (C:1.1774, R:0.0072)
Batch 350/537: Loss=1.8640 (C:1.1525, R:0.0071)
Batch 375/537: Loss=1.9098 (C:1.2023, R:0.0071)
Batch 400/537: Loss=1.8717 (C:1.1608, R:0.0071)
Batch 425/537: Loss=1.8530 (C:1.1463, R:0.0071)
Batch 450/537: Loss=1.9184 (C:1.2077, R:0.0071)
Batch 475/537: Loss=1.8866 (C:1.1775, R:0.0071)
Batch 500/537: Loss=1.8551 (C:1.1483, R:0.0071)
Batch 525/537: Loss=1.8686 (C:1.1613, R:0.0071)

============================================================
Epoch 18/200 completed in 24.0s
Train: Loss=1.8852 (C:1.1753, R:0.0071) Ratio=2.60x
Val:   Loss=1.8932 (C:1.2155, R:0.0068) Ratio=2.30x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 19
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.487 ¬± 0.536
    Neg distances: 1.472 ¬± 0.853
    Separation ratio: 3.02x
    Gap: -2.381
    ‚úÖ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=1.8760 (C:1.1671, R:0.0071)
Batch  25/537: Loss=1.8497 (C:1.1355, R:0.0071)
Batch  50/537: Loss=1.8833 (C:1.1763, R:0.0071)
Batch  75/537: Loss=1.8130 (C:1.1066, R:0.0071)
Batch 100/537: Loss=1.8317 (C:1.1207, R:0.0071)
Batch 125/537: Loss=1.8337 (C:1.1309, R:0.0070)
Batch 150/537: Loss=1.8992 (C:1.1847, R:0.0071)
Batch 175/537: Loss=1.8519 (C:1.1432, R:0.0071)
Batch 200/537: Loss=1.8434 (C:1.1388, R:0.0070)
Batch 225/537: Loss=1.8442 (C:1.1359, R:0.0071)
Batch 250/537: Loss=1.8075 (C:1.0994, R:0.0071)
Batch 275/537: Loss=1.8515 (C:1.1423, R:0.0071)
Batch 300/537: Loss=1.8574 (C:1.1468, R:0.0071)
Batch 325/537: Loss=1.8824 (C:1.1764, R:0.0071)
Batch 350/537: Loss=1.8496 (C:1.1442, R:0.0071)
Batch 375/537: Loss=1.8416 (C:1.1328, R:0.0071)
Batch 400/537: Loss=1.8529 (C:1.1408, R:0.0071)
Batch 425/537: Loss=1.8553 (C:1.1440, R:0.0071)
Batch 450/537: Loss=1.8294 (C:1.1160, R:0.0071)
Batch 475/537: Loss=1.8400 (C:1.1278, R:0.0071)
Batch 500/537: Loss=1.8828 (C:1.1771, R:0.0071)
Batch 525/537: Loss=1.8563 (C:1.1504, R:0.0071)

============================================================
Epoch 19/200 completed in 33.3s
Train: Loss=1.8508 (C:1.1422, R:0.0071) Ratio=2.59x
Val:   Loss=1.8511 (C:1.1747, R:0.0068) Ratio=2.35x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8511)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=1.8216 (C:1.1082, R:0.0071)
Batch  25/537: Loss=1.8405 (C:1.1278, R:0.0071)
Batch  50/537: Loss=1.8377 (C:1.1326, R:0.0071)
Batch  75/537: Loss=1.8635 (C:1.1564, R:0.0071)
Batch 100/537: Loss=1.8697 (C:1.1621, R:0.0071)
Batch 125/537: Loss=1.8158 (C:1.1041, R:0.0071)
Batch 150/537: Loss=1.8513 (C:1.1454, R:0.0071)
Batch 175/537: Loss=1.8502 (C:1.1408, R:0.0071)
Batch 200/537: Loss=1.8406 (C:1.1312, R:0.0071)
Batch 225/537: Loss=1.8201 (C:1.1143, R:0.0071)
Batch 250/537: Loss=1.8442 (C:1.1400, R:0.0070)
Batch 275/537: Loss=1.8554 (C:1.1465, R:0.0071)
Batch 300/537: Loss=1.8660 (C:1.1571, R:0.0071)
Batch 325/537: Loss=1.8446 (C:1.1397, R:0.0070)
Batch 350/537: Loss=1.8482 (C:1.1398, R:0.0071)
Batch 375/537: Loss=1.8342 (C:1.1293, R:0.0070)
Batch 400/537: Loss=1.8607 (C:1.1539, R:0.0071)
Batch 425/537: Loss=1.8331 (C:1.1304, R:0.0070)
Batch 450/537: Loss=1.8640 (C:1.1579, R:0.0071)
Batch 475/537: Loss=1.8519 (C:1.1499, R:0.0070)
Batch 500/537: Loss=1.8637 (C:1.1632, R:0.0070)
Batch 525/537: Loss=1.8560 (C:1.1506, R:0.0071)

============================================================
Epoch 20/200 completed in 25.1s
Train: Loss=1.8472 (C:1.1402, R:0.0071) Ratio=2.59x
Val:   Loss=1.8472 (C:1.1723, R:0.0067) Ratio=2.33x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8472)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=1.8533 (C:1.1409, R:0.0071)
Batch  25/537: Loss=1.8514 (C:1.1414, R:0.0071)
Batch  50/537: Loss=1.8289 (C:1.1239, R:0.0070)
Batch  75/537: Loss=1.8296 (C:1.1250, R:0.0070)
Batch 100/537: Loss=1.8520 (C:1.1458, R:0.0071)
Batch 125/537: Loss=1.8449 (C:1.1348, R:0.0071)
Batch 150/537: Loss=1.9021 (C:1.1934, R:0.0071)
Batch 175/537: Loss=1.8890 (C:1.1814, R:0.0071)
Batch 200/537: Loss=1.8435 (C:1.1412, R:0.0070)
Batch 225/537: Loss=1.8474 (C:1.1446, R:0.0070)
Batch 250/537: Loss=1.8287 (C:1.1246, R:0.0070)
Batch 275/537: Loss=1.8155 (C:1.1132, R:0.0070)
Batch 300/537: Loss=1.8368 (C:1.1335, R:0.0070)
Batch 325/537: Loss=1.8509 (C:1.1470, R:0.0070)
Batch 350/537: Loss=1.8551 (C:1.1458, R:0.0071)
Batch 375/537: Loss=1.8457 (C:1.1364, R:0.0071)
Batch 400/537: Loss=1.8171 (C:1.1061, R:0.0071)
Batch 425/537: Loss=1.8537 (C:1.1524, R:0.0070)
Batch 450/537: Loss=1.8567 (C:1.1508, R:0.0071)
Batch 475/537: Loss=1.8360 (C:1.1314, R:0.0070)
Batch 500/537: Loss=1.8631 (C:1.1590, R:0.0070)
Batch 525/537: Loss=1.8333 (C:1.1302, R:0.0070)

============================================================
Epoch 21/200 completed in 24.1s
Train: Loss=1.8431 (C:1.1375, R:0.0071) Ratio=2.64x
Val:   Loss=1.8373 (C:1.1648, R:0.0067) Ratio=2.37x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8373)
============================================================

üåç Updating global dataset at epoch 22
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.470 ¬± 0.534
    Neg distances: 1.491 ¬± 0.867
    Separation ratio: 3.17x
    Gap: -2.399
    ‚úÖ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=1.8342 (C:1.1297, R:0.0070)
Batch  25/537: Loss=1.8255 (C:1.1176, R:0.0071)
Batch  50/537: Loss=1.8346 (C:1.1294, R:0.0071)
Batch  75/537: Loss=1.8292 (C:1.1180, R:0.0071)
Batch 100/537: Loss=1.8307 (C:1.1277, R:0.0070)
Batch 125/537: Loss=1.8312 (C:1.1263, R:0.0070)
Batch 150/537: Loss=1.8377 (C:1.1354, R:0.0070)
Batch 175/537: Loss=1.8157 (C:1.1081, R:0.0071)
Batch 200/537: Loss=1.8212 (C:1.1190, R:0.0070)
Batch 225/537: Loss=1.8739 (C:1.1748, R:0.0070)
Batch 250/537: Loss=1.8325 (C:1.1248, R:0.0071)
Batch 275/537: Loss=1.8417 (C:1.1388, R:0.0070)
Batch 300/537: Loss=1.8558 (C:1.1567, R:0.0070)
Batch 325/537: Loss=1.8162 (C:1.1127, R:0.0070)
Batch 350/537: Loss=1.8453 (C:1.1417, R:0.0070)
Batch 375/537: Loss=1.8226 (C:1.1149, R:0.0071)
Batch 400/537: Loss=1.8126 (C:1.1121, R:0.0070)
Batch 425/537: Loss=1.7897 (C:1.0897, R:0.0070)
Batch 450/537: Loss=1.8283 (C:1.1244, R:0.0070)
Batch 475/537: Loss=1.8255 (C:1.1203, R:0.0071)
Batch 500/537: Loss=1.8384 (C:1.1391, R:0.0070)
Batch 525/537: Loss=1.8275 (C:1.1300, R:0.0070)

============================================================
Epoch 22/200 completed in 32.4s
Train: Loss=1.8270 (C:1.1227, R:0.0070) Ratio=2.69x
Val:   Loss=1.8281 (C:1.1575, R:0.0067) Ratio=2.36x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8281)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=1.8443 (C:1.1324, R:0.0071)
Batch  25/537: Loss=1.8171 (C:1.1142, R:0.0070)
Batch  50/537: Loss=1.8354 (C:1.1327, R:0.0070)
Batch  75/537: Loss=1.8266 (C:1.1228, R:0.0070)
Batch 100/537: Loss=1.7792 (C:1.0708, R:0.0071)
Batch 125/537: Loss=1.7971 (C:1.0974, R:0.0070)
Batch 150/537: Loss=1.8297 (C:1.1234, R:0.0071)
Batch 175/537: Loss=1.8145 (C:1.1081, R:0.0071)
Batch 200/537: Loss=1.7908 (C:1.0911, R:0.0070)
Batch 225/537: Loss=1.8273 (C:1.1242, R:0.0070)
Batch 250/537: Loss=1.8223 (C:1.1233, R:0.0070)
Batch 275/537: Loss=1.8544 (C:1.1488, R:0.0071)
Batch 300/537: Loss=1.8349 (C:1.1275, R:0.0071)
Batch 325/537: Loss=1.8718 (C:1.1672, R:0.0070)
Batch 350/537: Loss=1.8088 (C:1.1098, R:0.0070)
Batch 375/537: Loss=1.8205 (C:1.1169, R:0.0070)
Batch 400/537: Loss=1.8212 (C:1.1141, R:0.0071)
Batch 425/537: Loss=1.8479 (C:1.1445, R:0.0070)
Batch 450/537: Loss=1.8144 (C:1.1131, R:0.0070)
Batch 475/537: Loss=1.8233 (C:1.1229, R:0.0070)
Batch 500/537: Loss=1.8382 (C:1.1360, R:0.0070)
Batch 525/537: Loss=1.8066 (C:1.1016, R:0.0070)

============================================================
Epoch 23/200 completed in 23.9s
Train: Loss=1.8225 (C:1.1195, R:0.0070) Ratio=2.71x
Val:   Loss=1.8296 (C:1.1596, R:0.0067) Ratio=2.42x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=1.8130 (C:1.1117, R:0.0070)
Batch  25/537: Loss=1.8321 (C:1.1299, R:0.0070)
Batch  50/537: Loss=1.8269 (C:1.1306, R:0.0070)
Batch  75/537: Loss=1.8354 (C:1.1333, R:0.0070)
Batch 100/537: Loss=1.8389 (C:1.1338, R:0.0071)
Batch 125/537: Loss=1.8225 (C:1.1247, R:0.0070)
Batch 150/537: Loss=1.8095 (C:1.1088, R:0.0070)
Batch 175/537: Loss=1.8527 (C:1.1449, R:0.0071)
Batch 200/537: Loss=1.7905 (C:1.0899, R:0.0070)
Batch 225/537: Loss=1.8580 (C:1.1533, R:0.0070)
Batch 250/537: Loss=1.8290 (C:1.1262, R:0.0070)
Batch 275/537: Loss=1.8689 (C:1.1709, R:0.0070)
Batch 300/537: Loss=1.8197 (C:1.1143, R:0.0071)
Batch 325/537: Loss=1.8419 (C:1.1343, R:0.0071)
Batch 350/537: Loss=1.8019 (C:1.0976, R:0.0070)
Batch 375/537: Loss=1.8298 (C:1.1269, R:0.0070)
Batch 400/537: Loss=1.8073 (C:1.1085, R:0.0070)
Batch 425/537: Loss=1.8092 (C:1.1052, R:0.0070)
Batch 450/537: Loss=1.7943 (C:1.0985, R:0.0070)
Batch 475/537: Loss=1.7709 (C:1.0655, R:0.0071)
Batch 500/537: Loss=1.8176 (C:1.1231, R:0.0069)
Batch 525/537: Loss=1.8247 (C:1.1313, R:0.0069)

============================================================
Epoch 24/200 completed in 23.9s
Train: Loss=1.8209 (C:1.1193, R:0.0070) Ratio=2.74x
Val:   Loss=1.8320 (C:1.1644, R:0.0067) Ratio=2.34x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 25
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.451 ¬± 0.517
    Neg distances: 1.484 ¬± 0.857
    Separation ratio: 3.29x
    Gap: -2.390
    ‚úÖ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=1.8171 (C:1.1130, R:0.0070)
Batch  25/537: Loss=1.8070 (C:1.1051, R:0.0070)
Batch  50/537: Loss=1.8378 (C:1.1353, R:0.0070)
Batch  75/537: Loss=1.7959 (C:1.0973, R:0.0070)
Batch 100/537: Loss=1.8100 (C:1.1095, R:0.0070)
Batch 125/537: Loss=1.8337 (C:1.1376, R:0.0070)
Batch 150/537: Loss=1.7932 (C:1.0967, R:0.0070)
Batch 175/537: Loss=1.8176 (C:1.1124, R:0.0071)
Batch 200/537: Loss=1.8246 (C:1.1159, R:0.0071)
Batch 225/537: Loss=1.8123 (C:1.1104, R:0.0070)
Batch 250/537: Loss=1.7745 (C:1.0754, R:0.0070)
Batch 275/537: Loss=1.7975 (C:1.0932, R:0.0070)
Batch 300/537: Loss=1.7892 (C:1.0902, R:0.0070)
Batch 325/537: Loss=1.7691 (C:1.0702, R:0.0070)
Batch 350/537: Loss=1.8426 (C:1.1423, R:0.0070)
Batch 375/537: Loss=1.7943 (C:1.0917, R:0.0070)
Batch 400/537: Loss=1.7694 (C:1.0679, R:0.0070)
Batch 425/537: Loss=1.7823 (C:1.0810, R:0.0070)
Batch 450/537: Loss=1.8073 (C:1.1091, R:0.0070)
Batch 475/537: Loss=1.8258 (C:1.1316, R:0.0069)
Batch 500/537: Loss=1.8037 (C:1.1054, R:0.0070)
Batch 525/537: Loss=1.7974 (C:1.0979, R:0.0070)

============================================================
Epoch 25/200 completed in 31.8s
Train: Loss=1.8053 (C:1.1048, R:0.0070) Ratio=2.75x
Val:   Loss=1.8071 (C:1.1410, R:0.0067) Ratio=2.36x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8071)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=1.8165 (C:1.1213, R:0.0070)
Batch  25/537: Loss=1.8043 (C:1.1018, R:0.0070)
Batch  50/537: Loss=1.8091 (C:1.1050, R:0.0070)
Batch  75/537: Loss=1.8028 (C:1.1005, R:0.0070)
Batch 100/537: Loss=1.8251 (C:1.1207, R:0.0070)
Batch 125/537: Loss=1.8084 (C:1.1123, R:0.0070)
Batch 150/537: Loss=1.7949 (C:1.0973, R:0.0070)
Batch 175/537: Loss=1.7932 (C:1.0915, R:0.0070)
Batch 200/537: Loss=1.7948 (C:1.1020, R:0.0069)
Batch 225/537: Loss=1.8318 (C:1.1298, R:0.0070)
Batch 250/537: Loss=1.8075 (C:1.1069, R:0.0070)
Batch 275/537: Loss=1.8296 (C:1.1269, R:0.0070)
Batch 300/537: Loss=1.8303 (C:1.1297, R:0.0070)
Batch 325/537: Loss=1.8265 (C:1.1238, R:0.0070)
Batch 350/537: Loss=1.7767 (C:1.0692, R:0.0071)
Batch 375/537: Loss=1.7927 (C:1.0979, R:0.0069)
Batch 400/537: Loss=1.8169 (C:1.1172, R:0.0070)
Batch 425/537: Loss=1.8006 (C:1.0959, R:0.0070)
Batch 450/537: Loss=1.8112 (C:1.1103, R:0.0070)
Batch 475/537: Loss=1.7865 (C:1.0852, R:0.0070)
Batch 500/537: Loss=1.8258 (C:1.1259, R:0.0070)
Batch 525/537: Loss=1.8099 (C:1.1127, R:0.0070)

============================================================
Epoch 26/200 completed in 24.1s
Train: Loss=1.8017 (C:1.1024, R:0.0070) Ratio=2.74x
Val:   Loss=1.8046 (C:1.1402, R:0.0066) Ratio=2.39x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8046)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=1.7861 (C:1.0938, R:0.0069)
Batch  25/537: Loss=1.7951 (C:1.0891, R:0.0071)
Batch  50/537: Loss=1.7725 (C:1.0762, R:0.0070)
Batch  75/537: Loss=1.7979 (C:1.0965, R:0.0070)
Batch 100/537: Loss=1.8053 (C:1.1028, R:0.0070)
Batch 125/537: Loss=1.8125 (C:1.1083, R:0.0070)
Batch 150/537: Loss=1.8229 (C:1.1253, R:0.0070)
Batch 175/537: Loss=1.8164 (C:1.1180, R:0.0070)
Batch 200/537: Loss=1.7846 (C:1.0842, R:0.0070)
Batch 225/537: Loss=1.8312 (C:1.1323, R:0.0070)
Batch 250/537: Loss=1.7772 (C:1.0762, R:0.0070)
Batch 275/537: Loss=1.7833 (C:1.0899, R:0.0069)
Batch 300/537: Loss=1.8283 (C:1.1297, R:0.0070)
Batch 325/537: Loss=1.7951 (C:1.0941, R:0.0070)
Batch 350/537: Loss=1.7845 (C:1.0860, R:0.0070)
Batch 375/537: Loss=1.7801 (C:1.0831, R:0.0070)
Batch 400/537: Loss=1.7853 (C:1.0899, R:0.0070)
Batch 425/537: Loss=1.7688 (C:1.0722, R:0.0070)
Batch 450/537: Loss=1.8372 (C:1.1420, R:0.0070)
Batch 475/537: Loss=1.8401 (C:1.1418, R:0.0070)
Batch 500/537: Loss=1.8231 (C:1.1287, R:0.0069)
Batch 525/537: Loss=1.7968 (C:1.1056, R:0.0069)

============================================================
Epoch 27/200 completed in 24.1s
Train: Loss=1.7984 (C:1.1007, R:0.0070) Ratio=2.79x
Val:   Loss=1.8032 (C:1.1407, R:0.0066) Ratio=2.40x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8032)
============================================================

üåç Updating global dataset at epoch 28
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.445 ¬± 0.526
    Neg distances: 1.494 ¬± 0.871
    Separation ratio: 3.36x
    Gap: -2.353
    ‚úÖ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=1.7858 (C:1.0847, R:0.0070)
Batch  25/537: Loss=1.7944 (C:1.1026, R:0.0069)
Batch  50/537: Loss=1.8103 (C:1.1134, R:0.0070)
Batch  75/537: Loss=1.7903 (C:1.0930, R:0.0070)
Batch 100/537: Loss=1.7700 (C:1.0717, R:0.0070)
Batch 125/537: Loss=1.8222 (C:1.1209, R:0.0070)
Batch 150/537: Loss=1.7968 (C:1.1018, R:0.0070)
Batch 175/537: Loss=1.7993 (C:1.1024, R:0.0070)
Batch 200/537: Loss=1.7665 (C:1.0653, R:0.0070)
Batch 225/537: Loss=1.7699 (C:1.0730, R:0.0070)
Batch 250/537: Loss=1.7944 (C:1.0927, R:0.0070)
Batch 275/537: Loss=1.7816 (C:1.0855, R:0.0070)
Batch 300/537: Loss=1.7894 (C:1.0883, R:0.0070)
Batch 325/537: Loss=1.7818 (C:1.0830, R:0.0070)
Batch 350/537: Loss=1.7799 (C:1.0897, R:0.0069)
Batch 375/537: Loss=1.7657 (C:1.0660, R:0.0070)
Batch 400/537: Loss=1.8018 (C:1.1043, R:0.0070)
Batch 425/537: Loss=1.8018 (C:1.0995, R:0.0070)
Batch 450/537: Loss=1.7866 (C:1.0876, R:0.0070)
Batch 475/537: Loss=1.7890 (C:1.0950, R:0.0069)
Batch 500/537: Loss=1.8043 (C:1.1047, R:0.0070)
Batch 525/537: Loss=1.7954 (C:1.0982, R:0.0070)

============================================================
Epoch 28/200 completed in 31.5s
Train: Loss=1.7919 (C:1.0953, R:0.0070) Ratio=2.83x
Val:   Loss=1.7990 (C:1.1372, R:0.0066) Ratio=2.41x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7990)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=1.8339 (C:1.1380, R:0.0070)
Batch  25/537: Loss=1.7692 (C:1.0731, R:0.0070)
Batch  50/537: Loss=1.8124 (C:1.1223, R:0.0069)
Batch  75/537: Loss=1.7497 (C:1.0573, R:0.0069)
Batch 100/537: Loss=1.7664 (C:1.0745, R:0.0069)
Batch 125/537: Loss=1.8223 (C:1.1258, R:0.0070)
Batch 150/537: Loss=1.7909 (C:1.0908, R:0.0070)
Batch 175/537: Loss=1.7891 (C:1.0914, R:0.0070)
Batch 200/537: Loss=1.7689 (C:1.0758, R:0.0069)
Batch 225/537: Loss=1.7759 (C:1.0809, R:0.0069)
Batch 250/537: Loss=1.7715 (C:1.0794, R:0.0069)
Batch 275/537: Loss=1.7660 (C:1.0744, R:0.0069)
Batch 300/537: Loss=1.7922 (C:1.0917, R:0.0070)
Batch 325/537: Loss=1.7929 (C:1.0899, R:0.0070)
Batch 350/537: Loss=1.8036 (C:1.1130, R:0.0069)
Batch 375/537: Loss=1.8000 (C:1.1039, R:0.0070)
Batch 400/537: Loss=1.8283 (C:1.1302, R:0.0070)
Batch 425/537: Loss=1.8162 (C:1.1191, R:0.0070)
Batch 450/537: Loss=1.7863 (C:1.0956, R:0.0069)
Batch 475/537: Loss=1.7431 (C:1.0496, R:0.0069)
Batch 500/537: Loss=1.8102 (C:1.1129, R:0.0070)
Batch 525/537: Loss=1.8165 (C:1.1217, R:0.0069)

============================================================
Epoch 29/200 completed in 23.4s
Train: Loss=1.7880 (C:1.0925, R:0.0070) Ratio=2.81x
Val:   Loss=1.7943 (C:1.1336, R:0.0066) Ratio=2.44x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7943)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=1.7653 (C:1.0728, R:0.0069)
Batch  25/537: Loss=1.8058 (C:1.1167, R:0.0069)
Batch  50/537: Loss=1.7885 (C:1.0956, R:0.0069)
Batch  75/537: Loss=1.8188 (C:1.1224, R:0.0070)
Batch 100/537: Loss=1.7877 (C:1.0894, R:0.0070)
Batch 125/537: Loss=1.7487 (C:1.0583, R:0.0069)
Batch 150/537: Loss=1.7753 (C:1.0771, R:0.0070)
Batch 175/537: Loss=1.8200 (C:1.1283, R:0.0069)
Batch 200/537: Loss=1.7846 (C:1.0911, R:0.0069)
Batch 225/537: Loss=1.7971 (C:1.1001, R:0.0070)
Batch 250/537: Loss=1.7892 (C:1.1018, R:0.0069)
Batch 275/537: Loss=1.7960 (C:1.1005, R:0.0070)
Batch 300/537: Loss=1.8140 (C:1.1195, R:0.0069)
Batch 325/537: Loss=1.7869 (C:1.0872, R:0.0070)
Batch 350/537: Loss=1.7859 (C:1.0942, R:0.0069)
Batch 375/537: Loss=1.7795 (C:1.0894, R:0.0069)
Batch 400/537: Loss=1.8152 (C:1.1173, R:0.0070)
Batch 425/537: Loss=1.7938 (C:1.0973, R:0.0070)
Batch 450/537: Loss=1.8215 (C:1.1231, R:0.0070)
Batch 475/537: Loss=1.7917 (C:1.0988, R:0.0069)
Batch 500/537: Loss=1.7828 (C:1.0788, R:0.0070)
Batch 525/537: Loss=1.7685 (C:1.0705, R:0.0070)

============================================================
Epoch 30/200 completed in 22.8s
Train: Loss=1.7869 (C:1.0927, R:0.0069) Ratio=2.81x
Val:   Loss=1.8013 (C:1.1431, R:0.0066) Ratio=2.40x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 31
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.459 ¬± 0.545
    Neg distances: 1.474 ¬± 0.876
    Separation ratio: 3.21x
    Gap: -2.346
    ‚úÖ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=1.7984 (C:1.0995, R:0.0070)
Batch  25/537: Loss=1.7795 (C:1.0863, R:0.0069)
Batch  50/537: Loss=1.7974 (C:1.1085, R:0.0069)
Batch  75/537: Loss=1.7945 (C:1.1051, R:0.0069)
Batch 100/537: Loss=1.7937 (C:1.1045, R:0.0069)
Batch 125/537: Loss=1.7411 (C:1.0531, R:0.0069)
Batch 150/537: Loss=1.8219 (C:1.1241, R:0.0070)
Batch 175/537: Loss=1.8156 (C:1.1229, R:0.0069)
Batch 200/537: Loss=1.7963 (C:1.0956, R:0.0070)
Batch 225/537: Loss=1.8183 (C:1.1246, R:0.0069)
Batch 250/537: Loss=1.7995 (C:1.1093, R:0.0069)
Batch 275/537: Loss=1.8216 (C:1.1276, R:0.0069)
Batch 300/537: Loss=1.7906 (C:1.0963, R:0.0069)
Batch 325/537: Loss=1.8232 (C:1.1324, R:0.0069)
Batch 350/537: Loss=1.7862 (C:1.0928, R:0.0069)
Batch 375/537: Loss=1.7965 (C:1.1054, R:0.0069)
Batch 400/537: Loss=1.8001 (C:1.1082, R:0.0069)
Batch 425/537: Loss=1.7982 (C:1.1051, R:0.0069)
Batch 450/537: Loss=1.8211 (C:1.1313, R:0.0069)
Batch 475/537: Loss=1.8177 (C:1.1231, R:0.0069)
Batch 500/537: Loss=1.7680 (C:1.0805, R:0.0069)
Batch 525/537: Loss=1.8286 (C:1.1419, R:0.0069)

============================================================
Epoch 31/200 completed in 29.3s
Train: Loss=1.8025 (C:1.1095, R:0.0069) Ratio=2.83x
Val:   Loss=1.8063 (C:1.1490, R:0.0066) Ratio=2.42x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=1.7975 (C:1.1057, R:0.0069)
Batch  25/537: Loss=1.8079 (C:1.1119, R:0.0070)
Batch  50/537: Loss=1.8421 (C:1.1511, R:0.0069)
Batch  75/537: Loss=1.8159 (C:1.1201, R:0.0070)
Batch 100/537: Loss=1.8248 (C:1.1353, R:0.0069)
Batch 125/537: Loss=1.8382 (C:1.1444, R:0.0069)
Batch 150/537: Loss=1.8141 (C:1.1219, R:0.0069)
Batch 175/537: Loss=1.7757 (C:1.0935, R:0.0068)
Batch 200/537: Loss=1.8138 (C:1.1252, R:0.0069)
Batch 225/537: Loss=1.8183 (C:1.1267, R:0.0069)
Batch 250/537: Loss=1.7807 (C:1.0873, R:0.0069)
Batch 275/537: Loss=1.7626 (C:1.0724, R:0.0069)
Batch 300/537: Loss=1.8078 (C:1.1180, R:0.0069)
Batch 325/537: Loss=1.7711 (C:1.0795, R:0.0069)
Batch 350/537: Loss=1.8238 (C:1.1317, R:0.0069)
Batch 375/537: Loss=1.8018 (C:1.1120, R:0.0069)
Batch 400/537: Loss=1.7944 (C:1.1016, R:0.0069)
Batch 425/537: Loss=1.8283 (C:1.1371, R:0.0069)
Batch 450/537: Loss=1.7805 (C:1.0926, R:0.0069)
Batch 475/537: Loss=1.7975 (C:1.1050, R:0.0069)
Batch 500/537: Loss=1.8054 (C:1.1153, R:0.0069)
Batch 525/537: Loss=1.7792 (C:1.0829, R:0.0070)

============================================================
Epoch 32/200 completed in 23.0s
Train: Loss=1.8009 (C:1.1090, R:0.0069) Ratio=2.87x
Val:   Loss=1.8124 (C:1.1571, R:0.0066) Ratio=2.46x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=1.8235 (C:1.1336, R:0.0069)
Batch  25/537: Loss=1.7745 (C:1.0830, R:0.0069)
Batch  50/537: Loss=1.8045 (C:1.1117, R:0.0069)
Batch  75/537: Loss=1.7825 (C:1.0914, R:0.0069)
Batch 100/537: Loss=1.7844 (C:1.0920, R:0.0069)
Batch 125/537: Loss=1.7984 (C:1.1107, R:0.0069)
Batch 150/537: Loss=1.7929 (C:1.0989, R:0.0069)
Batch 175/537: Loss=1.8173 (C:1.1284, R:0.0069)
Batch 200/537: Loss=1.7424 (C:1.0544, R:0.0069)
Batch 225/537: Loss=1.8170 (C:1.1287, R:0.0069)
Batch 250/537: Loss=1.8010 (C:1.1071, R:0.0069)
Batch 275/537: Loss=1.7898 (C:1.0973, R:0.0069)
Batch 300/537: Loss=1.8034 (C:1.1078, R:0.0070)
Batch 325/537: Loss=1.7936 (C:1.1012, R:0.0069)
Batch 350/537: Loss=1.7924 (C:1.1016, R:0.0069)
Batch 375/537: Loss=1.7867 (C:1.0985, R:0.0069)
Batch 400/537: Loss=1.7948 (C:1.1052, R:0.0069)
Batch 425/537: Loss=1.8060 (C:1.1131, R:0.0069)
Batch 450/537: Loss=1.8241 (C:1.1301, R:0.0069)
Batch 475/537: Loss=1.7958 (C:1.1023, R:0.0069)
Batch 500/537: Loss=1.8121 (C:1.1223, R:0.0069)
Batch 525/537: Loss=1.7858 (C:1.0908, R:0.0069)

============================================================
Epoch 33/200 completed in 22.9s
Train: Loss=1.7964 (C:1.1054, R:0.0069) Ratio=2.93x
Val:   Loss=1.8080 (C:1.1530, R:0.0065) Ratio=2.42x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 34
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.448 ¬± 0.530
    Neg distances: 1.461 ¬± 0.878
    Separation ratio: 3.26x
    Gap: -2.443
    ‚úÖ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=1.7914 (C:1.0940, R:0.0070)
Batch  25/537: Loss=1.8033 (C:1.1067, R:0.0070)
Batch  50/537: Loss=1.8028 (C:1.1140, R:0.0069)
Batch  75/537: Loss=1.7830 (C:1.0920, R:0.0069)
Batch 100/537: Loss=1.8076 (C:1.1160, R:0.0069)
Batch 125/537: Loss=1.7958 (C:1.1084, R:0.0069)
Batch 150/537: Loss=1.7536 (C:1.0643, R:0.0069)
Batch 175/537: Loss=1.7898 (C:1.1095, R:0.0068)
Batch 200/537: Loss=1.7777 (C:1.0826, R:0.0070)
Batch 225/537: Loss=1.7872 (C:1.0917, R:0.0070)
Batch 250/537: Loss=1.8044 (C:1.1126, R:0.0069)
Batch 275/537: Loss=1.8118 (C:1.1293, R:0.0068)
Batch 300/537: Loss=1.7636 (C:1.0750, R:0.0069)
Batch 325/537: Loss=1.7943 (C:1.1077, R:0.0069)
Batch 350/537: Loss=1.7845 (C:1.0975, R:0.0069)
Batch 375/537: Loss=1.7821 (C:1.0915, R:0.0069)
Batch 400/537: Loss=1.7712 (C:1.0808, R:0.0069)
Batch 425/537: Loss=1.7973 (C:1.1097, R:0.0069)
Batch 450/537: Loss=1.7799 (C:1.0919, R:0.0069)
Batch 475/537: Loss=1.7672 (C:1.0764, R:0.0069)
Batch 500/537: Loss=1.7696 (C:1.0814, R:0.0069)
Batch 525/537: Loss=1.8107 (C:1.1244, R:0.0069)

============================================================
Epoch 34/200 completed in 29.8s
Train: Loss=1.7952 (C:1.1052, R:0.0069) Ratio=2.94x
Val:   Loss=1.7926 (C:1.1400, R:0.0065) Ratio=2.45x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7926)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=1.7787 (C:1.0885, R:0.0069)
Batch  25/537: Loss=1.7773 (C:1.0844, R:0.0069)
Batch  50/537: Loss=1.7848 (C:1.0956, R:0.0069)
Batch  75/537: Loss=1.7882 (C:1.0959, R:0.0069)
Batch 100/537: Loss=1.7703 (C:1.0837, R:0.0069)
Batch 125/537: Loss=1.7723 (C:1.0849, R:0.0069)
Batch 150/537: Loss=1.7966 (C:1.1076, R:0.0069)
Batch 175/537: Loss=1.8125 (C:1.1231, R:0.0069)
Batch 200/537: Loss=1.8066 (C:1.1141, R:0.0069)
Batch 225/537: Loss=1.7767 (C:1.0822, R:0.0069)
Batch 250/537: Loss=1.8014 (C:1.1111, R:0.0069)
Batch 275/537: Loss=1.7695 (C:1.0757, R:0.0069)
Batch 300/537: Loss=1.8043 (C:1.1121, R:0.0069)
Batch 325/537: Loss=1.7731 (C:1.0893, R:0.0068)
Batch 350/537: Loss=1.7809 (C:1.0908, R:0.0069)
Batch 375/537: Loss=1.7948 (C:1.1012, R:0.0069)
Batch 400/537: Loss=1.8034 (C:1.1164, R:0.0069)
Batch 425/537: Loss=1.7927 (C:1.1042, R:0.0069)
Batch 450/537: Loss=1.8023 (C:1.1132, R:0.0069)
Batch 475/537: Loss=1.7885 (C:1.1037, R:0.0068)
Batch 500/537: Loss=1.8029 (C:1.1166, R:0.0069)
Batch 525/537: Loss=1.8346 (C:1.1456, R:0.0069)

============================================================
Epoch 35/200 completed in 22.9s
Train: Loss=1.7922 (C:1.1033, R:0.0069) Ratio=2.95x
Val:   Loss=1.8096 (C:1.1579, R:0.0065) Ratio=2.45x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=1.8082 (C:1.1142, R:0.0069)
Batch  25/537: Loss=1.7879 (C:1.1009, R:0.0069)
Batch  50/537: Loss=1.7739 (C:1.0865, R:0.0069)
Batch  75/537: Loss=1.7644 (C:1.0789, R:0.0069)
Batch 100/537: Loss=1.7784 (C:1.0906, R:0.0069)
Batch 125/537: Loss=1.7726 (C:1.0901, R:0.0068)
Batch 150/537: Loss=1.7906 (C:1.0974, R:0.0069)
Batch 175/537: Loss=1.8072 (C:1.1225, R:0.0068)
Batch 200/537: Loss=1.7580 (C:1.0776, R:0.0068)
Batch 225/537: Loss=1.8121 (C:1.1230, R:0.0069)
Batch 250/537: Loss=1.8048 (C:1.1143, R:0.0069)
Batch 275/537: Loss=1.7414 (C:1.0586, R:0.0068)
Batch 300/537: Loss=1.8321 (C:1.1400, R:0.0069)
Batch 325/537: Loss=1.7706 (C:1.0872, R:0.0068)
Batch 350/537: Loss=1.7643 (C:1.0748, R:0.0069)
Batch 375/537: Loss=1.7835 (C:1.0958, R:0.0069)
Batch 400/537: Loss=1.7963 (C:1.1104, R:0.0069)
Batch 425/537: Loss=1.7778 (C:1.0940, R:0.0068)
Batch 450/537: Loss=1.7799 (C:1.0915, R:0.0069)
Batch 475/537: Loss=1.8338 (C:1.1444, R:0.0069)
Batch 500/537: Loss=1.7809 (C:1.0874, R:0.0069)
Batch 525/537: Loss=1.7842 (C:1.0898, R:0.0069)

============================================================
Epoch 36/200 completed in 22.8s
Train: Loss=1.7906 (C:1.1026, R:0.0069) Ratio=2.95x
Val:   Loss=1.8073 (C:1.1570, R:0.0065) Ratio=2.41x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 37
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.410 ¬± 0.489
    Neg distances: 1.481 ¬± 0.869
    Separation ratio: 3.61x
    Gap: -2.386
    ‚úÖ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=1.7277 (C:1.0325, R:0.0070)
Batch  25/537: Loss=1.7314 (C:1.0405, R:0.0069)
Batch  50/537: Loss=1.7578 (C:1.0711, R:0.0069)
Batch  75/537: Loss=1.7111 (C:1.0193, R:0.0069)
Batch 100/537: Loss=1.7246 (C:1.0416, R:0.0068)
Batch 125/537: Loss=1.7450 (C:1.0624, R:0.0068)
Batch 150/537: Loss=1.7628 (C:1.0745, R:0.0069)
Batch 175/537: Loss=1.7794 (C:1.0945, R:0.0068)
Batch 200/537: Loss=1.7620 (C:1.0709, R:0.0069)
Batch 225/537: Loss=1.7623 (C:1.0792, R:0.0068)
Batch 250/537: Loss=1.7706 (C:1.0868, R:0.0068)
Batch 275/537: Loss=1.7139 (C:1.0248, R:0.0069)
Batch 300/537: Loss=1.7569 (C:1.0688, R:0.0069)
Batch 325/537: Loss=1.7661 (C:1.0785, R:0.0069)
Batch 350/537: Loss=1.7478 (C:1.0597, R:0.0069)
Batch 375/537: Loss=1.7366 (C:1.0553, R:0.0068)
Batch 400/537: Loss=1.7540 (C:1.0698, R:0.0068)
Batch 425/537: Loss=1.7898 (C:1.1046, R:0.0069)
Batch 450/537: Loss=1.7273 (C:1.0349, R:0.0069)
Batch 475/537: Loss=1.7582 (C:1.0732, R:0.0069)
Batch 500/537: Loss=1.7651 (C:1.0779, R:0.0069)
Batch 525/537: Loss=1.7244 (C:1.0391, R:0.0069)

============================================================
Epoch 37/200 completed in 29.8s
Train: Loss=1.7545 (C:1.0674, R:0.0069) Ratio=2.97x
Val:   Loss=1.7689 (C:1.1193, R:0.0065) Ratio=2.43x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7689)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=1.7468 (C:1.0603, R:0.0069)
Batch  25/537: Loss=1.7502 (C:1.0717, R:0.0068)
Batch  50/537: Loss=1.7887 (C:1.1064, R:0.0068)
Batch  75/537: Loss=1.7832 (C:1.0883, R:0.0069)
Batch 100/537: Loss=1.7612 (C:1.0751, R:0.0069)
Batch 125/537: Loss=1.7343 (C:1.0479, R:0.0069)
Batch 150/537: Loss=1.7133 (C:1.0290, R:0.0068)
Batch 175/537: Loss=1.7379 (C:1.0529, R:0.0068)
Batch 200/537: Loss=1.7579 (C:1.0720, R:0.0069)
Batch 225/537: Loss=1.7678 (C:1.0756, R:0.0069)
Batch 250/537: Loss=1.7926 (C:1.1016, R:0.0069)
Batch 275/537: Loss=1.7633 (C:1.0786, R:0.0068)
Batch 300/537: Loss=1.7432 (C:1.0621, R:0.0068)
Batch 325/537: Loss=1.7622 (C:1.0780, R:0.0068)
Batch 350/537: Loss=1.7675 (C:1.0842, R:0.0068)
Batch 375/537: Loss=1.7084 (C:1.0215, R:0.0069)
Batch 400/537: Loss=1.7754 (C:1.0880, R:0.0069)
Batch 425/537: Loss=1.7447 (C:1.0589, R:0.0069)
Batch 450/537: Loss=1.7452 (C:1.0564, R:0.0069)
Batch 475/537: Loss=1.7199 (C:1.0310, R:0.0069)
Batch 500/537: Loss=1.7634 (C:1.0730, R:0.0069)
Batch 525/537: Loss=1.7565 (C:1.0733, R:0.0068)

============================================================
Epoch 38/200 completed in 22.9s
Train: Loss=1.7529 (C:1.0665, R:0.0069) Ratio=2.95x
Val:   Loss=1.7621 (C:1.1136, R:0.0065) Ratio=2.48x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7621)
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=1.7764 (C:1.0894, R:0.0069)
Batch  25/537: Loss=1.7667 (C:1.0840, R:0.0068)
Batch  50/537: Loss=1.7675 (C:1.0821, R:0.0069)
Batch  75/537: Loss=1.7544 (C:1.0647, R:0.0069)
Batch 100/537: Loss=1.7399 (C:1.0544, R:0.0069)
Batch 125/537: Loss=1.7647 (C:1.0712, R:0.0069)
Batch 150/537: Loss=1.7428 (C:1.0588, R:0.0068)
Batch 175/537: Loss=1.7748 (C:1.0882, R:0.0069)
Batch 200/537: Loss=1.7326 (C:1.0435, R:0.0069)
Batch 225/537: Loss=1.7404 (C:1.0536, R:0.0069)
Batch 250/537: Loss=1.7577 (C:1.0686, R:0.0069)
Batch 275/537: Loss=1.7405 (C:1.0512, R:0.0069)
Batch 300/537: Loss=1.7442 (C:1.0632, R:0.0068)
Batch 325/537: Loss=1.7696 (C:1.0837, R:0.0069)
Batch 350/537: Loss=1.7627 (C:1.0772, R:0.0069)
Batch 375/537: Loss=1.7437 (C:1.0599, R:0.0068)
Batch 400/537: Loss=1.7571 (C:1.0742, R:0.0068)
Batch 425/537: Loss=1.7600 (C:1.0738, R:0.0069)
Batch 450/537: Loss=1.7665 (C:1.0762, R:0.0069)
Batch 475/537: Loss=1.7125 (C:1.0284, R:0.0068)
Batch 500/537: Loss=1.7333 (C:1.0448, R:0.0069)
Batch 525/537: Loss=1.7391 (C:1.0508, R:0.0069)

============================================================
Epoch 39/200 completed in 23.2s
Train: Loss=1.7512 (C:1.0658, R:0.0069) Ratio=2.98x
Val:   Loss=1.7676 (C:1.1194, R:0.0065) Ratio=2.46x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 40
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.422 ¬± 0.501
    Neg distances: 1.474 ¬± 0.876
    Separation ratio: 3.50x
    Gap: -2.362
    ‚úÖ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=1.7655 (C:1.0823, R:0.0068)
Batch  25/537: Loss=1.7609 (C:1.0716, R:0.0069)
Batch  50/537: Loss=1.7738 (C:1.0843, R:0.0069)
Batch  75/537: Loss=1.7946 (C:1.1085, R:0.0069)
Batch 100/537: Loss=1.7216 (C:1.0393, R:0.0068)
Batch 125/537: Loss=1.7521 (C:1.0635, R:0.0069)
Batch 150/537: Loss=1.7633 (C:1.0763, R:0.0069)
Batch 175/537: Loss=1.7482 (C:1.0645, R:0.0068)
Batch 200/537: Loss=1.7323 (C:1.0533, R:0.0068)
Batch 225/537: Loss=1.7820 (C:1.0972, R:0.0068)
Batch 250/537: Loss=1.7722 (C:1.0869, R:0.0069)
Batch 275/537: Loss=1.7785 (C:1.0897, R:0.0069)
Batch 300/537: Loss=1.7588 (C:1.0794, R:0.0068)
Batch 325/537: Loss=1.7491 (C:1.0610, R:0.0069)
Batch 350/537: Loss=1.7461 (C:1.0685, R:0.0068)
Batch 375/537: Loss=1.7782 (C:1.0896, R:0.0069)
Batch 400/537: Loss=1.7813 (C:1.0978, R:0.0068)
Batch 425/537: Loss=1.7777 (C:1.0874, R:0.0069)
Batch 450/537: Loss=1.7639 (C:1.0782, R:0.0069)
Batch 475/537: Loss=1.7717 (C:1.0831, R:0.0069)
Batch 500/537: Loss=1.7804 (C:1.0930, R:0.0069)
Batch 525/537: Loss=1.7631 (C:1.0821, R:0.0068)

============================================================
Epoch 40/200 completed in 29.4s
Train: Loss=1.7635 (C:1.0790, R:0.0068) Ratio=3.00x
Val:   Loss=1.7743 (C:1.1289, R:0.0065) Ratio=2.50x
Reconstruction weight: 100.000
No improvement for 2 epochs
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=1.7220 (C:1.0366, R:0.0069)
Batch  25/537: Loss=1.7366 (C:1.0521, R:0.0068)
Batch  50/537: Loss=1.7652 (C:1.0841, R:0.0068)
Batch  75/537: Loss=1.7432 (C:1.0579, R:0.0069)
Batch 100/537: Loss=1.7781 (C:1.0927, R:0.0069)
Batch 125/537: Loss=1.7523 (C:1.0661, R:0.0069)
Batch 150/537: Loss=1.7525 (C:1.0709, R:0.0068)
Batch 175/537: Loss=1.7714 (C:1.0899, R:0.0068)
Batch 200/537: Loss=1.7508 (C:1.0672, R:0.0068)
Batch 225/537: Loss=1.7567 (C:1.0709, R:0.0069)
Batch 250/537: Loss=1.7382 (C:1.0599, R:0.0068)
Batch 275/537: Loss=1.7678 (C:1.0852, R:0.0068)
Batch 300/537: Loss=1.7704 (C:1.0904, R:0.0068)
Batch 325/537: Loss=1.7397 (C:1.0555, R:0.0068)
Batch 350/537: Loss=1.7391 (C:1.0550, R:0.0068)
Batch 375/537: Loss=1.7545 (C:1.0751, R:0.0068)
Batch 400/537: Loss=1.7630 (C:1.0775, R:0.0069)
Batch 425/537: Loss=1.7752 (C:1.0931, R:0.0068)
Batch 450/537: Loss=1.7801 (C:1.0935, R:0.0069)
Batch 475/537: Loss=1.7493 (C:1.0654, R:0.0068)
Batch 500/537: Loss=1.7659 (C:1.0834, R:0.0068)
Batch 525/537: Loss=1.7564 (C:1.0712, R:0.0069)

============================================================
Epoch 41/200 completed in 22.7s
Train: Loss=1.7602 (C:1.0765, R:0.0068) Ratio=3.04x
Val:   Loss=1.7812 (C:1.1362, R:0.0065) Ratio=2.48x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=1.7535 (C:1.0705, R:0.0068)
Batch  25/537: Loss=1.7671 (C:1.0818, R:0.0069)
Batch  50/537: Loss=1.7683 (C:1.0790, R:0.0069)
Batch  75/537: Loss=1.7719 (C:1.0796, R:0.0069)
Batch 100/537: Loss=1.7290 (C:1.0404, R:0.0069)
Batch 125/537: Loss=1.7404 (C:1.0647, R:0.0068)
Batch 150/537: Loss=1.7401 (C:1.0585, R:0.0068)
Batch 175/537: Loss=1.7657 (C:1.0839, R:0.0068)
Batch 200/537: Loss=1.7742 (C:1.0950, R:0.0068)
Batch 225/537: Loss=1.7755 (C:1.0933, R:0.0068)
Batch 250/537: Loss=1.7632 (C:1.0763, R:0.0069)
Batch 275/537: Loss=1.7535 (C:1.0734, R:0.0068)
Batch 300/537: Loss=1.7450 (C:1.0623, R:0.0068)
Batch 325/537: Loss=1.7826 (C:1.0980, R:0.0068)
Batch 350/537: Loss=1.7455 (C:1.0592, R:0.0069)
Batch 375/537: Loss=1.7859 (C:1.1057, R:0.0068)
Batch 400/537: Loss=1.7516 (C:1.0706, R:0.0068)
Batch 425/537: Loss=1.7481 (C:1.0670, R:0.0068)
Batch 450/537: Loss=1.7467 (C:1.0668, R:0.0068)
Batch 475/537: Loss=1.7882 (C:1.1076, R:0.0068)
Batch 500/537: Loss=1.7349 (C:1.0556, R:0.0068)
Batch 525/537: Loss=1.8075 (C:1.1261, R:0.0068)

============================================================
Epoch 42/200 completed in 23.0s
Train: Loss=1.7598 (C:1.0768, R:0.0068) Ratio=3.04x
Val:   Loss=1.7747 (C:1.1302, R:0.0064) Ratio=2.44x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 43
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.407 ¬± 0.490
    Neg distances: 1.500 ¬± 0.885
    Separation ratio: 3.69x
    Gap: -2.380
    ‚úÖ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=1.7349 (C:1.0553, R:0.0068)
Batch  25/537: Loss=1.7246 (C:1.0415, R:0.0068)
Batch  50/537: Loss=1.7395 (C:1.0575, R:0.0068)
Batch  75/537: Loss=1.7189 (C:1.0339, R:0.0068)
Batch 100/537: Loss=1.7414 (C:1.0688, R:0.0067)
Batch 125/537: Loss=1.7196 (C:1.0335, R:0.0069)
Batch 150/537: Loss=1.7262 (C:1.0466, R:0.0068)
Batch 175/537: Loss=1.7427 (C:1.0609, R:0.0068)
Batch 200/537: Loss=1.7436 (C:1.0587, R:0.0068)
Batch 225/537: Loss=1.7401 (C:1.0639, R:0.0068)
Batch 250/537: Loss=1.7265 (C:1.0492, R:0.0068)
Batch 275/537: Loss=1.7395 (C:1.0564, R:0.0068)
Batch 300/537: Loss=1.7247 (C:1.0385, R:0.0069)
Batch 325/537: Loss=1.7135 (C:1.0316, R:0.0068)
Batch 350/537: Loss=1.7475 (C:1.0682, R:0.0068)
Batch 375/537: Loss=1.7429 (C:1.0586, R:0.0068)
Batch 400/537: Loss=1.7455 (C:1.0646, R:0.0068)
Batch 425/537: Loss=1.7509 (C:1.0688, R:0.0068)
Batch 450/537: Loss=1.7414 (C:1.0661, R:0.0068)
Batch 475/537: Loss=1.7410 (C:1.0533, R:0.0069)
Batch 500/537: Loss=1.7360 (C:1.0531, R:0.0068)
Batch 525/537: Loss=1.7166 (C:1.0441, R:0.0067)

============================================================
Epoch 43/200 completed in 30.3s
Train: Loss=1.7405 (C:1.0581, R:0.0068) Ratio=3.06x
Val:   Loss=1.7559 (C:1.1118, R:0.0064) Ratio=2.48x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7559)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=1.7229 (C:1.0444, R:0.0068)
Batch  25/537: Loss=1.7432 (C:1.0597, R:0.0068)
Batch  50/537: Loss=1.7654 (C:1.0803, R:0.0069)
Batch  75/537: Loss=1.7152 (C:1.0396, R:0.0068)
Batch 100/537: Loss=1.7512 (C:1.0701, R:0.0068)
Batch 125/537: Loss=1.7394 (C:1.0562, R:0.0068)
Batch 150/537: Loss=1.7529 (C:1.0702, R:0.0068)
Batch 175/537: Loss=1.7206 (C:1.0430, R:0.0068)
Batch 200/537: Loss=1.7270 (C:1.0426, R:0.0068)
Batch 225/537: Loss=1.7504 (C:1.0736, R:0.0068)
Batch 250/537: Loss=1.7884 (C:1.0984, R:0.0069)
Batch 275/537: Loss=1.7359 (C:1.0617, R:0.0067)
Batch 300/537: Loss=1.7604 (C:1.0825, R:0.0068)
Batch 325/537: Loss=1.7272 (C:1.0428, R:0.0068)
Batch 350/537: Loss=1.7305 (C:1.0493, R:0.0068)
Batch 375/537: Loss=1.7519 (C:1.0667, R:0.0069)
Batch 400/537: Loss=1.7422 (C:1.0576, R:0.0068)
Batch 425/537: Loss=1.7889 (C:1.1098, R:0.0068)
Batch 450/537: Loss=1.7392 (C:1.0605, R:0.0068)
Batch 475/537: Loss=1.7240 (C:1.0377, R:0.0069)
Batch 500/537: Loss=1.7332 (C:1.0569, R:0.0068)
Batch 525/537: Loss=1.7653 (C:1.0889, R:0.0068)

============================================================
Epoch 44/200 completed in 22.8s
Train: Loss=1.7381 (C:1.0564, R:0.0068) Ratio=3.00x
Val:   Loss=1.7594 (C:1.1168, R:0.0064) Ratio=2.49x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=1.7473 (C:1.0614, R:0.0069)
Batch  25/537: Loss=1.7566 (C:1.0681, R:0.0069)
Batch  50/537: Loss=1.7020 (C:1.0239, R:0.0068)
Batch  75/537: Loss=1.7286 (C:1.0463, R:0.0068)
Batch 100/537: Loss=1.7353 (C:1.0533, R:0.0068)
Batch 125/537: Loss=1.7419 (C:1.0613, R:0.0068)
Batch 150/537: Loss=1.7446 (C:1.0647, R:0.0068)
Batch 175/537: Loss=1.7297 (C:1.0558, R:0.0067)
Batch 200/537: Loss=1.7263 (C:1.0376, R:0.0069)
Batch 225/537: Loss=1.7301 (C:1.0461, R:0.0068)
Batch 250/537: Loss=1.7182 (C:1.0377, R:0.0068)
Batch 275/537: Loss=1.7337 (C:1.0534, R:0.0068)
Batch 300/537: Loss=1.7456 (C:1.0713, R:0.0067)
Batch 325/537: Loss=1.7377 (C:1.0569, R:0.0068)
Batch 350/537: Loss=1.7430 (C:1.0615, R:0.0068)
Batch 375/537: Loss=1.7674 (C:1.0840, R:0.0068)
Batch 400/537: Loss=1.6957 (C:1.0175, R:0.0068)
Batch 425/537: Loss=1.7326 (C:1.0447, R:0.0069)
Batch 450/537: Loss=1.7542 (C:1.0743, R:0.0068)
Batch 475/537: Loss=1.7206 (C:1.0366, R:0.0068)
Batch 500/537: Loss=1.7632 (C:1.0868, R:0.0068)
Batch 525/537: Loss=1.7338 (C:1.0523, R:0.0068)

============================================================
Epoch 45/200 completed in 22.7s
Train: Loss=1.7361 (C:1.0550, R:0.0068) Ratio=3.07x
Val:   Loss=1.7610 (C:1.1188, R:0.0064) Ratio=2.45x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 46
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.397 ¬± 0.478
    Neg distances: 1.506 ¬± 0.886
    Separation ratio: 3.79x
    Gap: -2.441
    ‚úÖ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=1.7370 (C:1.0528, R:0.0068)
Batch  25/537: Loss=1.7470 (C:1.0666, R:0.0068)
Batch  50/537: Loss=1.7297 (C:1.0510, R:0.0068)
Batch  75/537: Loss=1.7412 (C:1.0608, R:0.0068)
Batch 100/537: Loss=1.7089 (C:1.0288, R:0.0068)
Batch 125/537: Loss=1.7501 (C:1.0708, R:0.0068)
Batch 150/537: Loss=1.7332 (C:1.0481, R:0.0069)
Batch 175/537: Loss=1.7289 (C:1.0524, R:0.0068)
Batch 200/537: Loss=1.7363 (C:1.0549, R:0.0068)
Batch 225/537: Loss=1.6917 (C:1.0173, R:0.0067)
Batch 250/537: Loss=1.7390 (C:1.0583, R:0.0068)
Batch 275/537: Loss=1.7431 (C:1.0569, R:0.0069)
Batch 300/537: Loss=1.7216 (C:1.0413, R:0.0068)
Batch 325/537: Loss=1.7287 (C:1.0486, R:0.0068)
Batch 350/537: Loss=1.7342 (C:1.0585, R:0.0068)
Batch 375/537: Loss=1.7213 (C:1.0409, R:0.0068)
Batch 400/537: Loss=1.7240 (C:1.0458, R:0.0068)
Batch 425/537: Loss=1.7267 (C:1.0482, R:0.0068)
Batch 450/537: Loss=1.7544 (C:1.0710, R:0.0068)
Batch 475/537: Loss=1.7246 (C:1.0432, R:0.0068)
Batch 500/537: Loss=1.7321 (C:1.0578, R:0.0067)
Batch 525/537: Loss=1.7472 (C:1.0662, R:0.0068)

============================================================
Epoch 46/200 completed in 29.3s
Train: Loss=1.7285 (C:1.0481, R:0.0068) Ratio=3.08x
Val:   Loss=1.7419 (C:1.1010, R:0.0064) Ratio=2.44x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7419)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=1.7259 (C:1.0559, R:0.0067)
Batch  25/537: Loss=1.7297 (C:1.0504, R:0.0068)
Batch  50/537: Loss=1.7018 (C:1.0196, R:0.0068)
Batch  75/537: Loss=1.7591 (C:1.0797, R:0.0068)
Batch 100/537: Loss=1.7351 (C:1.0582, R:0.0068)
Batch 125/537: Loss=1.7236 (C:1.0437, R:0.0068)
Batch 150/537: Loss=1.7292 (C:1.0531, R:0.0068)
Batch 175/537: Loss=1.7355 (C:1.0576, R:0.0068)
Batch 200/537: Loss=1.7203 (C:1.0384, R:0.0068)
Batch 225/537: Loss=1.7163 (C:1.0311, R:0.0069)
Batch 250/537: Loss=1.7325 (C:1.0508, R:0.0068)
Batch 275/537: Loss=1.7120 (C:1.0295, R:0.0068)
Batch 300/537: Loss=1.7147 (C:1.0355, R:0.0068)
Batch 325/537: Loss=1.7637 (C:1.0763, R:0.0069)
Batch 350/537: Loss=1.7368 (C:1.0542, R:0.0068)
Batch 375/537: Loss=1.7710 (C:1.0883, R:0.0068)
Batch 400/537: Loss=1.7186 (C:1.0345, R:0.0068)
Batch 425/537: Loss=1.6867 (C:1.0053, R:0.0068)
Batch 450/537: Loss=1.7516 (C:1.0707, R:0.0068)
Batch 475/537: Loss=1.7297 (C:1.0505, R:0.0068)
Batch 500/537: Loss=1.7432 (C:1.0596, R:0.0068)
Batch 525/537: Loss=1.6750 (C:0.9971, R:0.0068)

============================================================
Epoch 47/200 completed in 22.7s
Train: Loss=1.7281 (C:1.0483, R:0.0068) Ratio=3.08x
Val:   Loss=1.7580 (C:1.1168, R:0.0064) Ratio=2.46x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=1.7056 (C:1.0181, R:0.0069)
Batch  25/537: Loss=1.7533 (C:1.0734, R:0.0068)
Batch  50/537: Loss=1.7165 (C:1.0361, R:0.0068)
Batch  75/537: Loss=1.7061 (C:1.0222, R:0.0068)
Batch 100/537: Loss=1.7024 (C:1.0225, R:0.0068)
Batch 125/537: Loss=1.7233 (C:1.0448, R:0.0068)
Batch 150/537: Loss=1.7348 (C:1.0561, R:0.0068)
Batch 175/537: Loss=1.7038 (C:1.0286, R:0.0068)
Batch 200/537: Loss=1.7481 (C:1.0663, R:0.0068)
Batch 225/537: Loss=1.7541 (C:1.0739, R:0.0068)
Batch 250/537: Loss=1.7349 (C:1.0538, R:0.0068)
Batch 275/537: Loss=1.6951 (C:1.0224, R:0.0067)
Batch 300/537: Loss=1.7169 (C:1.0358, R:0.0068)
Batch 325/537: Loss=1.7418 (C:1.0635, R:0.0068)
Batch 350/537: Loss=1.7029 (C:1.0291, R:0.0067)
Batch 375/537: Loss=1.7198 (C:1.0465, R:0.0067)
Batch 400/537: Loss=1.7139 (C:1.0326, R:0.0068)
Batch 425/537: Loss=1.7353 (C:1.0540, R:0.0068)
Batch 450/537: Loss=1.7122 (C:1.0329, R:0.0068)
Batch 475/537: Loss=1.7288 (C:1.0476, R:0.0068)
Batch 500/537: Loss=1.7212 (C:1.0438, R:0.0068)
Batch 525/537: Loss=1.7431 (C:1.0623, R:0.0068)

============================================================
Epoch 48/200 completed in 22.7s
Train: Loss=1.7260 (C:1.0468, R:0.0068) Ratio=3.11x
Val:   Loss=1.7556 (C:1.1162, R:0.0064) Ratio=2.44x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 49
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.408 ¬± 0.490
    Neg distances: 1.515 ¬± 0.880
    Separation ratio: 3.71x
    Gap: -2.440
    ‚úÖ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=1.7212 (C:1.0466, R:0.0067)
Batch  25/537: Loss=1.7350 (C:1.0544, R:0.0068)
Batch  50/537: Loss=1.7310 (C:1.0541, R:0.0068)
Batch  75/537: Loss=1.7288 (C:1.0473, R:0.0068)
Batch 100/537: Loss=1.7575 (C:1.0754, R:0.0068)
Batch 125/537: Loss=1.7531 (C:1.0686, R:0.0068)
Batch 150/537: Loss=1.6889 (C:1.0137, R:0.0068)
Batch 175/537: Loss=1.7221 (C:1.0477, R:0.0067)
Batch 200/537: Loss=1.7157 (C:1.0359, R:0.0068)
Batch 225/537: Loss=1.7191 (C:1.0456, R:0.0067)
Batch 250/537: Loss=1.7305 (C:1.0490, R:0.0068)
Batch 275/537: Loss=1.7366 (C:1.0611, R:0.0068)
Batch 300/537: Loss=1.6988 (C:1.0205, R:0.0068)
Batch 325/537: Loss=1.7496 (C:1.0627, R:0.0069)
Batch 350/537: Loss=1.7443 (C:1.0624, R:0.0068)
Batch 375/537: Loss=1.7398 (C:1.0610, R:0.0068)
Batch 400/537: Loss=1.6820 (C:1.0096, R:0.0067)
Batch 425/537: Loss=1.7587 (C:1.0800, R:0.0068)
Batch 450/537: Loss=1.7033 (C:1.0230, R:0.0068)
Batch 475/537: Loss=1.7425 (C:1.0626, R:0.0068)
Batch 500/537: Loss=1.6961 (C:1.0220, R:0.0067)
Batch 525/537: Loss=1.7012 (C:1.0283, R:0.0067)

============================================================
Epoch 49/200 completed in 29.5s
Train: Loss=1.7275 (C:1.0488, R:0.0068) Ratio=3.11x
Val:   Loss=1.7543 (C:1.1157, R:0.0064) Ratio=2.49x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=1.7474 (C:1.0707, R:0.0068)
Batch  25/537: Loss=1.7282 (C:1.0529, R:0.0068)
Batch  50/537: Loss=1.6943 (C:1.0194, R:0.0067)
Batch  75/537: Loss=1.7144 (C:1.0340, R:0.0068)
Batch 100/537: Loss=1.7210 (C:1.0445, R:0.0068)
Batch 125/537: Loss=1.7411 (C:1.0585, R:0.0068)
Batch 150/537: Loss=1.7144 (C:1.0373, R:0.0068)
Batch 175/537: Loss=1.7335 (C:1.0535, R:0.0068)
Batch 200/537: Loss=1.7474 (C:1.0670, R:0.0068)
Batch 225/537: Loss=1.6945 (C:1.0108, R:0.0068)
Batch 250/537: Loss=1.7149 (C:1.0396, R:0.0068)
Batch 275/537: Loss=1.7050 (C:1.0239, R:0.0068)
Batch 300/537: Loss=1.7148 (C:1.0393, R:0.0068)
Batch 325/537: Loss=1.7379 (C:1.0632, R:0.0067)
Batch 350/537: Loss=1.7496 (C:1.0721, R:0.0068)
Batch 375/537: Loss=1.7203 (C:1.0430, R:0.0068)
Batch 400/537: Loss=1.7680 (C:1.0856, R:0.0068)
Batch 425/537: Loss=1.7123 (C:1.0347, R:0.0068)
Batch 450/537: Loss=1.7352 (C:1.0547, R:0.0068)
Batch 475/537: Loss=1.6945 (C:1.0217, R:0.0067)
Batch 500/537: Loss=1.7493 (C:1.0818, R:0.0067)
Batch 525/537: Loss=1.7320 (C:1.0545, R:0.0068)

============================================================
Epoch 50/200 completed in 22.7s
Train: Loss=1.7273 (C:1.0493, R:0.0068) Ratio=3.13x
Val:   Loss=1.7530 (C:1.1152, R:0.0064) Ratio=2.47x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=1.7399 (C:1.0618, R:0.0068)
Batch  25/537: Loss=1.7303 (C:1.0491, R:0.0068)
Batch  50/537: Loss=1.7157 (C:1.0343, R:0.0068)
Batch  75/537: Loss=1.7060 (C:1.0316, R:0.0067)
Batch 100/537: Loss=1.7342 (C:1.0504, R:0.0068)
Batch 125/537: Loss=1.7392 (C:1.0599, R:0.0068)
Batch 150/537: Loss=1.7121 (C:1.0414, R:0.0067)
Batch 175/537: Loss=1.7052 (C:1.0273, R:0.0068)
Batch 200/537: Loss=1.7141 (C:1.0403, R:0.0067)
Batch 225/537: Loss=1.7355 (C:1.0598, R:0.0068)
Batch 250/537: Loss=1.7114 (C:1.0272, R:0.0068)
Batch 275/537: Loss=1.7095 (C:1.0299, R:0.0068)
Batch 300/537: Loss=1.7475 (C:1.0730, R:0.0067)
Batch 325/537: Loss=1.6985 (C:1.0246, R:0.0067)
Batch 350/537: Loss=1.7336 (C:1.0575, R:0.0068)
Batch 375/537: Loss=1.6888 (C:1.0084, R:0.0068)
Batch 400/537: Loss=1.7115 (C:1.0341, R:0.0068)
Batch 425/537: Loss=1.7455 (C:1.0618, R:0.0068)
Batch 450/537: Loss=1.7148 (C:1.0396, R:0.0068)
Batch 475/537: Loss=1.7409 (C:1.0702, R:0.0067)
Batch 500/537: Loss=1.7168 (C:1.0393, R:0.0068)
Batch 525/537: Loss=1.7187 (C:1.0461, R:0.0067)

============================================================
Epoch 51/200 completed in 22.6s
Train: Loss=1.7243 (C:1.0468, R:0.0068) Ratio=3.09x
Val:   Loss=1.7501 (C:1.1130, R:0.0064) Ratio=2.51x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 52
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.392 ¬± 0.467
    Neg distances: 1.506 ¬± 0.887
    Separation ratio: 3.84x
    Gap: -2.423
    ‚úÖ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=1.7037 (C:1.0285, R:0.0068)
Batch  25/537: Loss=1.7127 (C:1.0379, R:0.0067)
Batch  50/537: Loss=1.7079 (C:1.0293, R:0.0068)
Batch  75/537: Loss=1.7409 (C:1.0659, R:0.0067)
Batch 100/537: Loss=1.7107 (C:1.0310, R:0.0068)
Batch 125/537: Loss=1.7369 (C:1.0597, R:0.0068)
Batch 150/537: Loss=1.7027 (C:1.0254, R:0.0068)
Batch 175/537: Loss=1.7296 (C:1.0508, R:0.0068)
Batch 200/537: Loss=1.7271 (C:1.0465, R:0.0068)
Batch 225/537: Loss=1.7045 (C:1.0332, R:0.0067)
Batch 250/537: Loss=1.7360 (C:1.0610, R:0.0068)
Batch 275/537: Loss=1.7389 (C:1.0594, R:0.0068)
Batch 300/537: Loss=1.7407 (C:1.0563, R:0.0068)
Batch 325/537: Loss=1.7112 (C:1.0341, R:0.0068)
Batch 350/537: Loss=1.7230 (C:1.0432, R:0.0068)
Batch 375/537: Loss=1.7456 (C:1.0642, R:0.0068)
Batch 400/537: Loss=1.7303 (C:1.0530, R:0.0068)
Batch 425/537: Loss=1.7452 (C:1.0608, R:0.0068)
Batch 450/537: Loss=1.7022 (C:1.0285, R:0.0067)
Batch 475/537: Loss=1.7309 (C:1.0521, R:0.0068)
Batch 500/537: Loss=1.7636 (C:1.0809, R:0.0068)
Batch 525/537: Loss=1.7099 (C:1.0386, R:0.0067)

============================================================
Epoch 52/200 completed in 29.2s
Train: Loss=1.7200 (C:1.0431, R:0.0068) Ratio=3.13x
Val:   Loss=1.7412 (C:1.1043, R:0.0064) Ratio=2.50x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7412)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=1.6850 (C:1.0097, R:0.0068)
Batch  25/537: Loss=1.7249 (C:1.0478, R:0.0068)
Batch  50/537: Loss=1.7770 (C:1.1009, R:0.0068)
Batch  75/537: Loss=1.7088 (C:1.0303, R:0.0068)
Batch 100/537: Loss=1.7137 (C:1.0413, R:0.0067)
Batch 125/537: Loss=1.7038 (C:1.0265, R:0.0068)
Batch 150/537: Loss=1.7201 (C:1.0416, R:0.0068)
Batch 175/537: Loss=1.7157 (C:1.0371, R:0.0068)
Batch 200/537: Loss=1.7259 (C:1.0487, R:0.0068)
Batch 225/537: Loss=1.7081 (C:1.0312, R:0.0068)
Batch 250/537: Loss=1.7564 (C:1.0805, R:0.0068)
Batch 275/537: Loss=1.7166 (C:1.0436, R:0.0067)
Batch 300/537: Loss=1.7662 (C:1.0962, R:0.0067)
Batch 325/537: Loss=1.7205 (C:1.0434, R:0.0068)
Batch 350/537: Loss=1.7160 (C:1.0323, R:0.0068)
Batch 375/537: Loss=1.7280 (C:1.0479, R:0.0068)
Batch 400/537: Loss=1.7248 (C:1.0415, R:0.0068)
Batch 425/537: Loss=1.7427 (C:1.0691, R:0.0067)
Batch 450/537: Loss=1.6795 (C:1.0043, R:0.0068)
Batch 475/537: Loss=1.7420 (C:1.0622, R:0.0068)
Batch 500/537: Loss=1.7116 (C:1.0328, R:0.0068)
Batch 525/537: Loss=1.7422 (C:1.0664, R:0.0068)

============================================================
Epoch 53/200 completed in 23.0s
Train: Loss=1.7195 (C:1.0430, R:0.0068) Ratio=3.07x
Val:   Loss=1.7409 (C:1.1053, R:0.0064) Ratio=2.51x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7409)
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=1.6992 (C:1.0243, R:0.0067)
Batch  25/537: Loss=1.7216 (C:1.0445, R:0.0068)
Batch  50/537: Loss=1.7346 (C:1.0598, R:0.0067)
Batch  75/537: Loss=1.7309 (C:1.0614, R:0.0067)
Batch 100/537: Loss=1.7142 (C:1.0374, R:0.0068)
Batch 125/537: Loss=1.7078 (C:1.0300, R:0.0068)
Batch 150/537: Loss=1.7014 (C:1.0226, R:0.0068)
Batch 175/537: Loss=1.7397 (C:1.0621, R:0.0068)
Batch 200/537: Loss=1.7128 (C:1.0395, R:0.0067)
Batch 225/537: Loss=1.7164 (C:1.0455, R:0.0067)
Batch 250/537: Loss=1.6718 (C:1.0012, R:0.0067)
Batch 275/537: Loss=1.6905 (C:1.0179, R:0.0067)
Batch 300/537: Loss=1.7372 (C:1.0559, R:0.0068)
Batch 325/537: Loss=1.7415 (C:1.0634, R:0.0068)
Batch 350/537: Loss=1.7223 (C:1.0516, R:0.0067)
Batch 375/537: Loss=1.6980 (C:1.0233, R:0.0067)
Batch 400/537: Loss=1.7307 (C:1.0512, R:0.0068)
Batch 425/537: Loss=1.7225 (C:1.0464, R:0.0068)
Batch 450/537: Loss=1.7307 (C:1.0516, R:0.0068)
Batch 475/537: Loss=1.7353 (C:1.0546, R:0.0068)
Batch 500/537: Loss=1.6952 (C:1.0267, R:0.0067)
Batch 525/537: Loss=1.7474 (C:1.0663, R:0.0068)

============================================================
Epoch 54/200 completed in 22.9s
Train: Loss=1.7171 (C:1.0413, R:0.0068) Ratio=3.18x
Val:   Loss=1.7496 (C:1.1146, R:0.0064) Ratio=2.48x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 55
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.402 ¬± 0.477
    Neg distances: 1.494 ¬± 0.894
    Separation ratio: 3.72x
    Gap: -2.465
    ‚úÖ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=1.7378 (C:1.0640, R:0.0067)
Batch  25/537: Loss=1.7227 (C:1.0524, R:0.0067)
Batch  50/537: Loss=1.7338 (C:1.0585, R:0.0068)
Batch  75/537: Loss=1.7283 (C:1.0500, R:0.0068)
Batch 100/537: Loss=1.7271 (C:1.0525, R:0.0067)
Batch 125/537: Loss=1.7327 (C:1.0565, R:0.0068)
Batch 150/537: Loss=1.7121 (C:1.0373, R:0.0067)
Batch 175/537: Loss=1.7138 (C:1.0382, R:0.0068)
Batch 200/537: Loss=1.7163 (C:1.0440, R:0.0067)
Batch 225/537: Loss=1.7242 (C:1.0459, R:0.0068)
Batch 250/537: Loss=1.7344 (C:1.0629, R:0.0067)
Batch 275/537: Loss=1.7245 (C:1.0488, R:0.0068)
Batch 300/537: Loss=1.7225 (C:1.0533, R:0.0067)
Batch 325/537: Loss=1.7288 (C:1.0522, R:0.0068)
Batch 350/537: Loss=1.6680 (C:0.9962, R:0.0067)
Batch 375/537: Loss=1.7560 (C:1.0797, R:0.0068)
Batch 400/537: Loss=1.7176 (C:1.0437, R:0.0067)
Batch 425/537: Loss=1.7302 (C:1.0568, R:0.0067)
Batch 450/537: Loss=1.7412 (C:1.0635, R:0.0068)
Batch 475/537: Loss=1.7374 (C:1.0607, R:0.0068)
Batch 500/537: Loss=1.7491 (C:1.0740, R:0.0068)
Batch 525/537: Loss=1.7035 (C:1.0291, R:0.0067)

============================================================
Epoch 55/200 completed in 29.9s
Train: Loss=1.7285 (C:1.0533, R:0.0068) Ratio=3.20x
Val:   Loss=1.7516 (C:1.1173, R:0.0063) Ratio=2.53x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=1.7530 (C:1.0738, R:0.0068)
Batch  25/537: Loss=1.7366 (C:1.0573, R:0.0068)
Batch  50/537: Loss=1.7203 (C:1.0447, R:0.0068)
Batch  75/537: Loss=1.7355 (C:1.0574, R:0.0068)
Batch 100/537: Loss=1.7277 (C:1.0521, R:0.0068)
Batch 125/537: Loss=1.7184 (C:1.0456, R:0.0067)
Batch 150/537: Loss=1.7198 (C:1.0389, R:0.0068)
Batch 175/537: Loss=1.7430 (C:1.0692, R:0.0067)
Batch 200/537: Loss=1.7486 (C:1.0713, R:0.0068)
Batch 225/537: Loss=1.7662 (C:1.0958, R:0.0067)
Batch 250/537: Loss=1.7365 (C:1.0548, R:0.0068)
Batch 275/537: Loss=1.7568 (C:1.0779, R:0.0068)
Batch 300/537: Loss=1.7290 (C:1.0508, R:0.0068)
Batch 325/537: Loss=1.6919 (C:1.0184, R:0.0067)
Batch 350/537: Loss=1.7447 (C:1.0733, R:0.0067)
Batch 375/537: Loss=1.7073 (C:1.0371, R:0.0067)
Batch 400/537: Loss=1.7416 (C:1.0633, R:0.0068)
Batch 425/537: Loss=1.7177 (C:1.0321, R:0.0069)
Batch 450/537: Loss=1.7272 (C:1.0554, R:0.0067)
Batch 475/537: Loss=1.7150 (C:1.0402, R:0.0067)
Batch 500/537: Loss=1.7595 (C:1.0896, R:0.0067)
Batch 525/537: Loss=1.7447 (C:1.0698, R:0.0067)

============================================================
Epoch 56/200 completed in 22.9s
Train: Loss=1.7290 (C:1.0542, R:0.0067) Ratio=3.13x
Val:   Loss=1.7482 (C:1.1151, R:0.0063) Ratio=2.51x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=1.7347 (C:1.0648, R:0.0067)
Batch  25/537: Loss=1.7076 (C:1.0364, R:0.0067)
Batch  50/537: Loss=1.6951 (C:1.0210, R:0.0067)
Batch  75/537: Loss=1.7259 (C:1.0518, R:0.0067)
Batch 100/537: Loss=1.7337 (C:1.0607, R:0.0067)
Batch 125/537: Loss=1.7275 (C:1.0568, R:0.0067)
Batch 150/537: Loss=1.7259 (C:1.0561, R:0.0067)
Batch 175/537: Loss=1.7058 (C:1.0310, R:0.0067)
Batch 200/537: Loss=1.6817 (C:1.0064, R:0.0068)
Batch 225/537: Loss=1.7363 (C:1.0659, R:0.0067)
Batch 250/537: Loss=1.7259 (C:1.0487, R:0.0068)
Batch 275/537: Loss=1.7409 (C:1.0578, R:0.0068)
Batch 300/537: Loss=1.7282 (C:1.0565, R:0.0067)
Batch 325/537: Loss=1.7427 (C:1.0697, R:0.0067)
Batch 350/537: Loss=1.7178 (C:1.0426, R:0.0068)
Batch 375/537: Loss=1.6874 (C:1.0098, R:0.0068)
Batch 400/537: Loss=1.7160 (C:1.0430, R:0.0067)
Batch 425/537: Loss=1.7101 (C:1.0385, R:0.0067)
Batch 450/537: Loss=1.7289 (C:1.0616, R:0.0067)
Batch 475/537: Loss=1.7002 (C:1.0249, R:0.0068)
Batch 500/537: Loss=1.6877 (C:1.0236, R:0.0066)
Batch 525/537: Loss=1.7431 (C:1.0666, R:0.0068)

============================================================
Epoch 57/200 completed in 22.7s
Train: Loss=1.7262 (C:1.0520, R:0.0067) Ratio=3.25x
Val:   Loss=1.7508 (C:1.1185, R:0.0063) Ratio=2.52x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 58
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.418 ¬± 0.499
    Neg distances: 1.508 ¬± 0.900
    Separation ratio: 3.61x
    Gap: -2.511
    ‚úÖ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=1.7349 (C:1.0533, R:0.0068)
Batch  25/537: Loss=1.7341 (C:1.0567, R:0.0068)
Batch  50/537: Loss=1.7012 (C:1.0249, R:0.0068)
Batch  75/537: Loss=1.7459 (C:1.0708, R:0.0068)
Batch 100/537: Loss=1.7439 (C:1.0746, R:0.0067)
Batch 125/537: Loss=1.7176 (C:1.0449, R:0.0067)
Batch 150/537: Loss=1.7525 (C:1.0766, R:0.0068)
Batch 175/537: Loss=1.6948 (C:1.0238, R:0.0067)
Batch 200/537: Loss=1.7247 (C:1.0542, R:0.0067)
Batch 225/537: Loss=1.7300 (C:1.0567, R:0.0067)
Batch 250/537: Loss=1.7139 (C:1.0409, R:0.0067)
Batch 275/537: Loss=1.7149 (C:1.0463, R:0.0067)
Batch 300/537: Loss=1.7045 (C:1.0353, R:0.0067)
Batch 325/537: Loss=1.7658 (C:1.0907, R:0.0068)
Batch 350/537: Loss=1.7169 (C:1.0415, R:0.0068)
Batch 375/537: Loss=1.7290 (C:1.0560, R:0.0067)
Batch 400/537: Loss=1.7307 (C:1.0587, R:0.0067)
Batch 425/537: Loss=1.7576 (C:1.0817, R:0.0068)
Batch 450/537: Loss=1.7182 (C:1.0434, R:0.0067)
Batch 475/537: Loss=1.7426 (C:1.0691, R:0.0067)
Batch 500/537: Loss=1.7509 (C:1.0746, R:0.0068)
Batch 525/537: Loss=1.7369 (C:1.0624, R:0.0067)

============================================================
Epoch 58/200 completed in 29.3s
Train: Loss=1.7342 (C:1.0602, R:0.0067) Ratio=3.18x
Val:   Loss=1.7488 (C:1.1162, R:0.0063) Ratio=2.54x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=1.6965 (C:1.0190, R:0.0068)
Batch  25/537: Loss=1.7158 (C:1.0371, R:0.0068)
Batch  50/537: Loss=1.7498 (C:1.0731, R:0.0068)
Batch  75/537: Loss=1.7261 (C:1.0532, R:0.0067)
Batch 100/537: Loss=1.7120 (C:1.0343, R:0.0068)
Batch 125/537: Loss=1.6926 (C:1.0247, R:0.0067)
Batch 150/537: Loss=1.7625 (C:1.0860, R:0.0068)
Batch 175/537: Loss=1.7420 (C:1.0694, R:0.0067)
Batch 200/537: Loss=1.7424 (C:1.0645, R:0.0068)
Batch 225/537: Loss=1.7259 (C:1.0475, R:0.0068)
Batch 250/537: Loss=1.7149 (C:1.0433, R:0.0067)
Batch 275/537: Loss=1.7202 (C:1.0434, R:0.0068)
Batch 300/537: Loss=1.7221 (C:1.0485, R:0.0067)
Batch 325/537: Loss=1.6995 (C:1.0299, R:0.0067)
Batch 350/537: Loss=1.7376 (C:1.0682, R:0.0067)
Batch 375/537: Loss=1.7144 (C:1.0415, R:0.0067)
Batch 400/537: Loss=1.6977 (C:1.0247, R:0.0067)
Batch 425/537: Loss=1.7323 (C:1.0673, R:0.0067)
Batch 450/537: Loss=1.7181 (C:1.0482, R:0.0067)
Batch 475/537: Loss=1.7192 (C:1.0494, R:0.0067)
Batch 500/537: Loss=1.7294 (C:1.0617, R:0.0067)
Batch 525/537: Loss=1.7440 (C:1.0714, R:0.0067)

============================================================
Epoch 59/200 completed in 22.6s
Train: Loss=1.7345 (C:1.0610, R:0.0067) Ratio=3.26x
Val:   Loss=1.7646 (C:1.1329, R:0.0063) Ratio=2.48x
Reconstruction weight: 100.000
No improvement for 6 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=1.7413 (C:1.0634, R:0.0068)
Batch  25/537: Loss=1.7094 (C:1.0365, R:0.0067)
Batch  50/537: Loss=1.7250 (C:1.0452, R:0.0068)
Batch  75/537: Loss=1.7176 (C:1.0495, R:0.0067)
Batch 100/537: Loss=1.7198 (C:1.0493, R:0.0067)
Batch 125/537: Loss=1.7317 (C:1.0538, R:0.0068)
Batch 150/537: Loss=1.7258 (C:1.0544, R:0.0067)
Batch 175/537: Loss=1.7520 (C:1.0787, R:0.0067)
Batch 200/537: Loss=1.7459 (C:1.0739, R:0.0067)
Batch 225/537: Loss=1.7346 (C:1.0608, R:0.0067)
Batch 250/537: Loss=1.7392 (C:1.0657, R:0.0067)
Batch 275/537: Loss=1.7338 (C:1.0582, R:0.0068)
Batch 300/537: Loss=1.7150 (C:1.0416, R:0.0067)
Batch 325/537: Loss=1.7611 (C:1.0861, R:0.0068)
Batch 350/537: Loss=1.7220 (C:1.0472, R:0.0067)
Batch 375/537: Loss=1.7421 (C:1.0658, R:0.0068)
Batch 400/537: Loss=1.7182 (C:1.0483, R:0.0067)
Batch 425/537: Loss=1.7507 (C:1.0762, R:0.0067)
Batch 450/537: Loss=1.7508 (C:1.0815, R:0.0067)
Batch 475/537: Loss=1.7265 (C:1.0543, R:0.0067)
Batch 500/537: Loss=1.7427 (C:1.0749, R:0.0067)
Batch 525/537: Loss=1.7242 (C:1.0578, R:0.0067)

============================================================
Epoch 60/200 completed in 23.0s
Train: Loss=1.7316 (C:1.0587, R:0.0067) Ratio=3.25x
Val:   Loss=1.7590 (C:1.1277, R:0.0063) Ratio=2.54x
Reconstruction weight: 100.000
No improvement for 7 epochs
Checkpoint saved at epoch 60
============================================================

üåç Updating global dataset at epoch 61
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.397 ¬± 0.469
    Neg distances: 1.537 ¬± 0.885
    Separation ratio: 3.88x
    Gap: -2.491
    ‚úÖ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=1.6993 (C:1.0335, R:0.0067)
Batch  25/537: Loss=1.6918 (C:1.0217, R:0.0067)
Batch  50/537: Loss=1.7140 (C:1.0361, R:0.0068)
Batch  75/537: Loss=1.7174 (C:1.0457, R:0.0067)
Batch 100/537: Loss=1.7009 (C:1.0257, R:0.0068)
Batch 125/537: Loss=1.6989 (C:1.0298, R:0.0067)
Batch 150/537: Loss=1.6769 (C:1.0056, R:0.0067)
Batch 175/537: Loss=1.7150 (C:1.0445, R:0.0067)
Batch 200/537: Loss=1.7084 (C:1.0341, R:0.0067)
Batch 225/537: Loss=1.6886 (C:1.0146, R:0.0067)
Batch 250/537: Loss=1.7121 (C:1.0362, R:0.0068)
Batch 275/537: Loss=1.7049 (C:1.0309, R:0.0067)
Batch 300/537: Loss=1.6736 (C:1.0044, R:0.0067)
Batch 325/537: Loss=1.7119 (C:1.0379, R:0.0067)
Batch 350/537: Loss=1.6874 (C:1.0119, R:0.0068)
Batch 375/537: Loss=1.6866 (C:1.0103, R:0.0068)
Batch 400/537: Loss=1.7202 (C:1.0435, R:0.0068)
Batch 425/537: Loss=1.7271 (C:1.0522, R:0.0067)
Batch 450/537: Loss=1.6982 (C:1.0290, R:0.0067)
Batch 475/537: Loss=1.6679 (C:0.9886, R:0.0068)
Batch 500/537: Loss=1.7154 (C:1.0413, R:0.0067)
Batch 525/537: Loss=1.7297 (C:1.0492, R:0.0068)

============================================================
Epoch 61/200 completed in 29.5s
Train: Loss=1.7026 (C:1.0300, R:0.0067) Ratio=3.27x
Val:   Loss=1.7264 (C:1.0953, R:0.0063) Ratio=2.57x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7264)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=1.6837 (C:1.0080, R:0.0068)
Batch  25/537: Loss=1.7384 (C:1.0661, R:0.0067)
Batch  50/537: Loss=1.7092 (C:1.0388, R:0.0067)
Batch  75/537: Loss=1.6977 (C:1.0320, R:0.0067)
Batch 100/537: Loss=1.6791 (C:1.0096, R:0.0067)
Batch 125/537: Loss=1.7058 (C:1.0399, R:0.0067)
Batch 150/537: Loss=1.6839 (C:1.0066, R:0.0068)
Batch 175/537: Loss=1.6873 (C:1.0200, R:0.0067)
Batch 200/537: Loss=1.6464 (C:0.9790, R:0.0067)
Batch 225/537: Loss=1.7480 (C:1.0707, R:0.0068)
Batch 250/537: Loss=1.7197 (C:1.0491, R:0.0067)
Batch 275/537: Loss=1.7076 (C:1.0357, R:0.0067)
Batch 300/537: Loss=1.7270 (C:1.0569, R:0.0067)
Batch 325/537: Loss=1.7050 (C:1.0323, R:0.0067)
Batch 350/537: Loss=1.7099 (C:1.0362, R:0.0067)
Batch 375/537: Loss=1.7054 (C:1.0346, R:0.0067)
Batch 400/537: Loss=1.6844 (C:1.0078, R:0.0068)
Batch 425/537: Loss=1.7185 (C:1.0428, R:0.0068)
Batch 450/537: Loss=1.7089 (C:1.0336, R:0.0068)
Batch 475/537: Loss=1.7174 (C:1.0420, R:0.0068)
Batch 500/537: Loss=1.6737 (C:1.0023, R:0.0067)
Batch 525/537: Loss=1.7114 (C:1.0390, R:0.0067)

============================================================
Epoch 62/200 completed in 22.6s
Train: Loss=1.7001 (C:1.0278, R:0.0067) Ratio=3.23x
Val:   Loss=1.7333 (C:1.1028, R:0.0063) Ratio=2.55x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=1.6841 (C:1.0159, R:0.0067)
Batch  25/537: Loss=1.6761 (C:1.0014, R:0.0067)
Batch  50/537: Loss=1.6731 (C:0.9941, R:0.0068)
Batch  75/537: Loss=1.6974 (C:1.0276, R:0.0067)
Batch 100/537: Loss=1.6916 (C:1.0184, R:0.0067)
Batch 125/537: Loss=1.6971 (C:1.0294, R:0.0067)
Batch 150/537: Loss=1.6951 (C:1.0238, R:0.0067)
Batch 175/537: Loss=1.6658 (C:1.0024, R:0.0066)
Batch 200/537: Loss=1.6886 (C:1.0174, R:0.0067)
Batch 225/537: Loss=1.6817 (C:1.0139, R:0.0067)
Batch 250/537: Loss=1.7009 (C:1.0361, R:0.0066)
Batch 275/537: Loss=1.6844 (C:1.0123, R:0.0067)
Batch 300/537: Loss=1.7252 (C:1.0509, R:0.0067)
Batch 325/537: Loss=1.6839 (C:1.0087, R:0.0068)
Batch 350/537: Loss=1.7185 (C:1.0412, R:0.0068)
Batch 375/537: Loss=1.7521 (C:1.0840, R:0.0067)
Batch 400/537: Loss=1.7254 (C:1.0515, R:0.0067)
Batch 425/537: Loss=1.6875 (C:1.0235, R:0.0066)
Batch 450/537: Loss=1.7249 (C:1.0496, R:0.0068)
Batch 475/537: Loss=1.6867 (C:1.0134, R:0.0067)
Batch 500/537: Loss=1.6977 (C:1.0265, R:0.0067)
Batch 525/537: Loss=1.6838 (C:1.0111, R:0.0067)

============================================================
Epoch 63/200 completed in 22.7s
Train: Loss=1.6999 (C:1.0280, R:0.0067) Ratio=3.26x
Val:   Loss=1.7289 (C:1.0986, R:0.0063) Ratio=2.54x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 64
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.409 ¬± 0.486
    Neg distances: 1.556 ¬± 0.892
    Separation ratio: 3.81x
    Gap: -2.607
    ‚úÖ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=1.6959 (C:1.0264, R:0.0067)
Batch  25/537: Loss=1.6689 (C:1.0008, R:0.0067)
Batch  50/537: Loss=1.7082 (C:1.0383, R:0.0067)
Batch  75/537: Loss=1.7315 (C:1.0559, R:0.0068)
Batch 100/537: Loss=1.6667 (C:0.9932, R:0.0067)
Batch 125/537: Loss=1.7126 (C:1.0428, R:0.0067)
Batch 150/537: Loss=1.7166 (C:1.0473, R:0.0067)
Batch 175/537: Loss=1.7063 (C:1.0344, R:0.0067)
Batch 200/537: Loss=1.7088 (C:1.0391, R:0.0067)
Batch 225/537: Loss=1.6998 (C:1.0316, R:0.0067)
Batch 250/537: Loss=1.7181 (C:1.0440, R:0.0067)
Batch 275/537: Loss=1.7261 (C:1.0538, R:0.0067)
Batch 300/537: Loss=1.7022 (C:1.0287, R:0.0067)
Batch 325/537: Loss=1.7248 (C:1.0581, R:0.0067)
Batch 350/537: Loss=1.6959 (C:1.0321, R:0.0066)
Batch 375/537: Loss=1.7225 (C:1.0517, R:0.0067)
Batch 400/537: Loss=1.7108 (C:1.0432, R:0.0067)
Batch 425/537: Loss=1.6962 (C:1.0190, R:0.0068)
Batch 450/537: Loss=1.7237 (C:1.0482, R:0.0068)
Batch 475/537: Loss=1.6811 (C:1.0057, R:0.0068)
Batch 500/537: Loss=1.7340 (C:1.0608, R:0.0067)
Batch 525/537: Loss=1.7038 (C:1.0269, R:0.0068)

============================================================
Epoch 64/200 completed in 29.1s
Train: Loss=1.7028 (C:1.0313, R:0.0067) Ratio=3.25x
Val:   Loss=1.7313 (C:1.1018, R:0.0063) Ratio=2.53x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=1.7062 (C:1.0376, R:0.0067)
Batch  25/537: Loss=1.7103 (C:1.0349, R:0.0068)
Batch  50/537: Loss=1.7160 (C:1.0449, R:0.0067)
Batch  75/537: Loss=1.7425 (C:1.0712, R:0.0067)
Batch 100/537: Loss=1.6761 (C:1.0070, R:0.0067)
Batch 125/537: Loss=1.6916 (C:1.0241, R:0.0067)
Batch 150/537: Loss=1.7030 (C:1.0290, R:0.0067)
Batch 175/537: Loss=1.6508 (C:0.9717, R:0.0068)
Batch 200/537: Loss=1.6987 (C:1.0326, R:0.0067)
Batch 225/537: Loss=1.6965 (C:1.0234, R:0.0067)
Batch 250/537: Loss=1.6883 (C:1.0171, R:0.0067)
Batch 275/537: Loss=1.6997 (C:1.0195, R:0.0068)
Batch 300/537: Loss=1.6760 (C:1.0005, R:0.0068)
Batch 325/537: Loss=1.6771 (C:1.0015, R:0.0068)
Batch 350/537: Loss=1.6933 (C:1.0211, R:0.0067)
Batch 375/537: Loss=1.7082 (C:1.0366, R:0.0067)
Batch 400/537: Loss=1.6783 (C:1.0052, R:0.0067)
Batch 425/537: Loss=1.7185 (C:1.0501, R:0.0067)
Batch 450/537: Loss=1.7289 (C:1.0576, R:0.0067)
Batch 475/537: Loss=1.7042 (C:1.0334, R:0.0067)
Batch 500/537: Loss=1.6848 (C:1.0123, R:0.0067)
Batch 525/537: Loss=1.7025 (C:1.0336, R:0.0067)

============================================================
Epoch 65/200 completed in 22.9s
Train: Loss=1.6999 (C:1.0288, R:0.0067) Ratio=3.30x
Val:   Loss=1.7264 (C:1.0977, R:0.0063) Ratio=2.58x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=1.7064 (C:1.0323, R:0.0067)
Batch  25/537: Loss=1.7359 (C:1.0639, R:0.0067)
Batch  50/537: Loss=1.6973 (C:1.0240, R:0.0067)
Batch  75/537: Loss=1.6877 (C:1.0154, R:0.0067)
Batch 100/537: Loss=1.7129 (C:1.0402, R:0.0067)
Batch 125/537: Loss=1.7227 (C:1.0565, R:0.0067)
Batch 150/537: Loss=1.6763 (C:1.0015, R:0.0067)
Batch 175/537: Loss=1.6940 (C:1.0319, R:0.0066)
Batch 200/537: Loss=1.6912 (C:1.0230, R:0.0067)
Batch 225/537: Loss=1.6881 (C:1.0207, R:0.0067)
Batch 250/537: Loss=1.6909 (C:1.0214, R:0.0067)
Batch 275/537: Loss=1.6700 (C:1.0025, R:0.0067)
Batch 300/537: Loss=1.7018 (C:1.0361, R:0.0067)
Batch 325/537: Loss=1.7003 (C:1.0281, R:0.0067)
Batch 350/537: Loss=1.6791 (C:1.0124, R:0.0067)
Batch 375/537: Loss=1.6749 (C:1.0039, R:0.0067)
Batch 400/537: Loss=1.7229 (C:1.0531, R:0.0067)
Batch 425/537: Loss=1.6892 (C:1.0199, R:0.0067)
Batch 450/537: Loss=1.7073 (C:1.0347, R:0.0067)
Batch 475/537: Loss=1.7000 (C:1.0274, R:0.0067)
Batch 500/537: Loss=1.7048 (C:1.0287, R:0.0068)
Batch 525/537: Loss=1.7449 (C:1.0758, R:0.0067)

============================================================
Epoch 66/200 completed in 22.5s
Train: Loss=1.6983 (C:1.0275, R:0.0067) Ratio=3.27x
Val:   Loss=1.7265 (C:1.0973, R:0.0063) Ratio=2.55x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 67
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.401 ¬± 0.474
    Neg distances: 1.582 ¬± 0.900
    Separation ratio: 3.95x
    Gap: -2.594
    ‚úÖ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=1.7047 (C:1.0329, R:0.0067)
Batch  25/537: Loss=1.7133 (C:1.0385, R:0.0067)
Batch  50/537: Loss=1.7108 (C:1.0393, R:0.0067)
Batch  75/537: Loss=1.7261 (C:1.0590, R:0.0067)
Batch 100/537: Loss=1.6926 (C:1.0165, R:0.0068)
Batch 125/537: Loss=1.6872 (C:1.0120, R:0.0068)
Batch 150/537: Loss=1.6806 (C:1.0018, R:0.0068)
Batch 175/537: Loss=1.7278 (C:1.0597, R:0.0067)
Batch 200/537: Loss=1.6991 (C:1.0332, R:0.0067)
Batch 225/537: Loss=1.6920 (C:1.0324, R:0.0066)
Batch 250/537: Loss=1.6590 (C:0.9961, R:0.0066)
Batch 275/537: Loss=1.6818 (C:1.0157, R:0.0067)
Batch 300/537: Loss=1.6637 (C:0.9916, R:0.0067)
Batch 325/537: Loss=1.6750 (C:1.0077, R:0.0067)
Batch 350/537: Loss=1.7019 (C:1.0305, R:0.0067)
Batch 375/537: Loss=1.7014 (C:1.0301, R:0.0067)
Batch 400/537: Loss=1.6909 (C:1.0217, R:0.0067)
Batch 425/537: Loss=1.6898 (C:1.0190, R:0.0067)
Batch 450/537: Loss=1.6621 (C:0.9877, R:0.0067)
Batch 475/537: Loss=1.6779 (C:1.0124, R:0.0067)
Batch 500/537: Loss=1.6740 (C:1.0064, R:0.0067)
Batch 525/537: Loss=1.7240 (C:1.0556, R:0.0067)

============================================================
Epoch 67/200 completed in 29.0s
Train: Loss=1.6894 (C:1.0188, R:0.0067) Ratio=3.29x
Val:   Loss=1.7235 (C:1.0955, R:0.0063) Ratio=2.56x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7235)
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=1.6795 (C:1.0170, R:0.0066)
Batch  25/537: Loss=1.6983 (C:1.0293, R:0.0067)
Batch  50/537: Loss=1.6601 (C:0.9872, R:0.0067)
Batch  75/537: Loss=1.6655 (C:0.9960, R:0.0067)
Batch 100/537: Loss=1.6709 (C:0.9977, R:0.0067)
Batch 125/537: Loss=1.6936 (C:1.0236, R:0.0067)
Batch 150/537: Loss=1.6812 (C:1.0094, R:0.0067)
Batch 175/537: Loss=1.6756 (C:1.0028, R:0.0067)
Batch 200/537: Loss=1.6780 (C:1.0073, R:0.0067)
Batch 225/537: Loss=1.6787 (C:1.0085, R:0.0067)
Batch 250/537: Loss=1.6752 (C:1.0036, R:0.0067)
Batch 275/537: Loss=1.6682 (C:1.0053, R:0.0066)
Batch 300/537: Loss=1.6947 (C:1.0198, R:0.0067)
Batch 325/537: Loss=1.6858 (C:1.0096, R:0.0068)
Batch 350/537: Loss=1.6534 (C:0.9802, R:0.0067)
Batch 375/537: Loss=1.6792 (C:1.0012, R:0.0068)
Batch 400/537: Loss=1.7064 (C:1.0367, R:0.0067)
Batch 425/537: Loss=1.7273 (C:1.0561, R:0.0067)
Batch 450/537: Loss=1.6617 (C:0.9881, R:0.0067)
Batch 475/537: Loss=1.6783 (C:1.0087, R:0.0067)
Batch 500/537: Loss=1.6769 (C:1.0059, R:0.0067)
Batch 525/537: Loss=1.6720 (C:0.9928, R:0.0068)

============================================================
Epoch 68/200 completed in 22.6s
Train: Loss=1.6869 (C:1.0166, R:0.0067) Ratio=3.40x
Val:   Loss=1.7089 (C:1.0812, R:0.0063) Ratio=2.56x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7089)
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=1.6862 (C:1.0179, R:0.0067)
Batch  25/537: Loss=1.6927 (C:1.0203, R:0.0067)
Batch  50/537: Loss=1.6562 (C:0.9891, R:0.0067)
Batch  75/537: Loss=1.6851 (C:1.0117, R:0.0067)
Batch 100/537: Loss=1.6629 (C:0.9891, R:0.0067)
Batch 125/537: Loss=1.7062 (C:1.0378, R:0.0067)
Batch 150/537: Loss=1.6435 (C:0.9768, R:0.0067)
Batch 175/537: Loss=1.6655 (C:0.9929, R:0.0067)
Batch 200/537: Loss=1.6945 (C:1.0310, R:0.0066)
Batch 225/537: Loss=1.6785 (C:1.0068, R:0.0067)
Batch 250/537: Loss=1.6682 (C:0.9955, R:0.0067)
Batch 275/537: Loss=1.6578 (C:0.9865, R:0.0067)
Batch 300/537: Loss=1.7035 (C:1.0380, R:0.0067)
Batch 325/537: Loss=1.6879 (C:1.0119, R:0.0068)
Batch 350/537: Loss=1.6854 (C:1.0187, R:0.0067)
Batch 375/537: Loss=1.6955 (C:1.0218, R:0.0067)
Batch 400/537: Loss=1.7105 (C:1.0411, R:0.0067)
Batch 425/537: Loss=1.6758 (C:1.0080, R:0.0067)
Batch 450/537: Loss=1.6753 (C:1.0095, R:0.0067)
Batch 475/537: Loss=1.6941 (C:1.0286, R:0.0067)
Batch 500/537: Loss=1.7214 (C:1.0504, R:0.0067)
Batch 525/537: Loss=1.6989 (C:1.0272, R:0.0067)

============================================================
Epoch 69/200 completed in 22.6s
Train: Loss=1.6868 (C:1.0169, R:0.0067) Ratio=3.35x
Val:   Loss=1.7180 (C:1.0904, R:0.0063) Ratio=2.53x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 70
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.400 ¬± 0.464
    Neg distances: 1.624 ¬± 0.901
    Separation ratio: 4.06x
    Gap: -2.658
    ‚úÖ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=1.6567 (C:0.9904, R:0.0067)
Batch  25/537: Loss=1.6767 (C:1.0019, R:0.0067)
Batch  50/537: Loss=1.6655 (C:0.9945, R:0.0067)
Batch  75/537: Loss=1.6544 (C:0.9846, R:0.0067)
Batch 100/537: Loss=1.6717 (C:1.0060, R:0.0067)
Batch 125/537: Loss=1.6564 (C:0.9863, R:0.0067)
Batch 150/537: Loss=1.7149 (C:1.0434, R:0.0067)
Batch 175/537: Loss=1.6481 (C:0.9827, R:0.0067)
Batch 200/537: Loss=1.6589 (C:0.9889, R:0.0067)
Batch 225/537: Loss=1.6583 (C:0.9878, R:0.0067)
Batch 250/537: Loss=1.6730 (C:1.0115, R:0.0066)
Batch 275/537: Loss=1.6705 (C:1.0070, R:0.0066)
Batch 300/537: Loss=1.6600 (C:0.9904, R:0.0067)
Batch 325/537: Loss=1.6688 (C:1.0049, R:0.0066)
Batch 350/537: Loss=1.6756 (C:1.0120, R:0.0066)
Batch 375/537: Loss=1.7065 (C:1.0328, R:0.0067)
Batch 400/537: Loss=1.7020 (C:1.0301, R:0.0067)
Batch 425/537: Loss=1.6462 (C:0.9797, R:0.0067)
Batch 450/537: Loss=1.6763 (C:1.0056, R:0.0067)
Batch 475/537: Loss=1.6737 (C:1.0015, R:0.0067)
Batch 500/537: Loss=1.6613 (C:0.9946, R:0.0067)
Batch 525/537: Loss=1.6510 (C:0.9858, R:0.0067)

============================================================
Epoch 70/200 completed in 29.1s
Train: Loss=1.6720 (C:1.0024, R:0.0067) Ratio=3.34x
Val:   Loss=1.7055 (C:1.0775, R:0.0063) Ratio=2.54x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7055)
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=1.6558 (C:0.9802, R:0.0068)
Batch  25/537: Loss=1.6810 (C:1.0103, R:0.0067)
Batch  50/537: Loss=1.6593 (C:0.9934, R:0.0067)
Batch  75/537: Loss=1.6211 (C:0.9480, R:0.0067)
Batch 100/537: Loss=1.6640 (C:0.9946, R:0.0067)
Batch 125/537: Loss=1.6707 (C:1.0015, R:0.0067)
Batch 150/537: Loss=1.6725 (C:1.0078, R:0.0066)
Batch 175/537: Loss=1.6569 (C:0.9864, R:0.0067)
Batch 200/537: Loss=1.6516 (C:0.9816, R:0.0067)
Batch 225/537: Loss=1.6842 (C:1.0169, R:0.0067)
Batch 250/537: Loss=1.6797 (C:1.0120, R:0.0067)
Batch 275/537: Loss=1.6747 (C:1.0050, R:0.0067)
Batch 300/537: Loss=1.6593 (C:0.9929, R:0.0067)
Batch 325/537: Loss=1.6644 (C:0.9989, R:0.0067)
Batch 350/537: Loss=1.6642 (C:0.9966, R:0.0067)
Batch 375/537: Loss=1.6830 (C:1.0123, R:0.0067)
Batch 400/537: Loss=1.6668 (C:0.9976, R:0.0067)
Batch 425/537: Loss=1.6690 (C:1.0009, R:0.0067)
Batch 450/537: Loss=1.6704 (C:0.9998, R:0.0067)
Batch 475/537: Loss=1.6758 (C:1.0057, R:0.0067)
Batch 500/537: Loss=1.6805 (C:1.0138, R:0.0067)
Batch 525/537: Loss=1.6777 (C:1.0048, R:0.0067)

============================================================
Epoch 71/200 completed in 22.4s
Train: Loss=1.6693 (C:1.0000, R:0.0067) Ratio=3.40x
Val:   Loss=1.7100 (C:1.0830, R:0.0063) Ratio=2.57x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=1.6895 (C:1.0189, R:0.0067)
Batch  25/537: Loss=1.7037 (C:1.0298, R:0.0067)
Batch  50/537: Loss=1.6707 (C:1.0075, R:0.0066)
Batch  75/537: Loss=1.6466 (C:0.9794, R:0.0067)
Batch 100/537: Loss=1.6923 (C:1.0279, R:0.0066)
Batch 125/537: Loss=1.6411 (C:0.9769, R:0.0066)
Batch 150/537: Loss=1.6311 (C:0.9598, R:0.0067)
Batch 175/537: Loss=1.6827 (C:1.0054, R:0.0068)
Batch 200/537: Loss=1.6611 (C:0.9969, R:0.0066)
Batch 225/537: Loss=1.6778 (C:1.0068, R:0.0067)
Batch 250/537: Loss=1.6649 (C:0.9979, R:0.0067)
Batch 275/537: Loss=1.6881 (C:1.0147, R:0.0067)
Batch 300/537: Loss=1.6967 (C:1.0208, R:0.0068)
Batch 325/537: Loss=1.6702 (C:0.9986, R:0.0067)
Batch 350/537: Loss=1.6615 (C:0.9912, R:0.0067)
Batch 375/537: Loss=1.6582 (C:0.9924, R:0.0067)
Batch 400/537: Loss=1.6585 (C:0.9886, R:0.0067)
Batch 425/537: Loss=1.6924 (C:1.0294, R:0.0066)
Batch 450/537: Loss=1.6652 (C:0.9948, R:0.0067)
Batch 475/537: Loss=1.6997 (C:1.0263, R:0.0067)
Batch 500/537: Loss=1.6771 (C:1.0037, R:0.0067)
Batch 525/537: Loss=1.6424 (C:0.9739, R:0.0067)

============================================================
Epoch 72/200 completed in 22.6s
Train: Loss=1.6678 (C:0.9987, R:0.0067) Ratio=3.35x
Val:   Loss=1.6993 (C:1.0731, R:0.0063) Ratio=2.56x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6993)
============================================================

üåç Updating global dataset at epoch 73
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.385 ¬± 0.445
    Neg distances: 1.644 ¬± 0.893
    Separation ratio: 4.27x
    Gap: -2.611
    ‚úÖ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=1.6450 (C:0.9784, R:0.0067)
Batch  25/537: Loss=1.6590 (C:0.9886, R:0.0067)
Batch  50/537: Loss=1.6201 (C:0.9514, R:0.0067)
Batch  75/537: Loss=1.6590 (C:0.9949, R:0.0066)
Batch 100/537: Loss=1.6802 (C:1.0137, R:0.0067)
Batch 125/537: Loss=1.6441 (C:0.9749, R:0.0067)
Batch 150/537: Loss=1.6917 (C:1.0258, R:0.0067)
Batch 175/537: Loss=1.6438 (C:0.9790, R:0.0066)
Batch 200/537: Loss=1.6598 (C:0.9855, R:0.0067)
Batch 225/537: Loss=1.6325 (C:0.9647, R:0.0067)
Batch 250/537: Loss=1.6545 (C:0.9839, R:0.0067)
Batch 275/537: Loss=1.6552 (C:0.9890, R:0.0067)
Batch 300/537: Loss=1.6622 (C:0.9924, R:0.0067)
Batch 325/537: Loss=1.6671 (C:0.9962, R:0.0067)
Batch 350/537: Loss=1.6587 (C:0.9983, R:0.0066)
Batch 375/537: Loss=1.6703 (C:0.9995, R:0.0067)
Batch 400/537: Loss=1.6346 (C:0.9738, R:0.0066)
Batch 425/537: Loss=1.6667 (C:1.0026, R:0.0066)
Batch 450/537: Loss=1.6399 (C:0.9726, R:0.0067)
Batch 475/537: Loss=1.7009 (C:1.0318, R:0.0067)
Batch 500/537: Loss=1.6200 (C:0.9542, R:0.0067)
Batch 525/537: Loss=1.6657 (C:1.0015, R:0.0066)

============================================================
Epoch 73/200 completed in 29.2s
Train: Loss=1.6514 (C:0.9825, R:0.0067) Ratio=3.34x
Val:   Loss=1.6808 (C:1.0549, R:0.0063) Ratio=2.63x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6808)
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=1.6602 (C:0.9901, R:0.0067)
Batch  25/537: Loss=1.6284 (C:0.9642, R:0.0066)
Batch  50/537: Loss=1.6427 (C:0.9717, R:0.0067)
Batch  75/537: Loss=1.6363 (C:0.9692, R:0.0067)
Batch 100/537: Loss=1.6418 (C:0.9746, R:0.0067)
Batch 125/537: Loss=1.6452 (C:0.9735, R:0.0067)
Batch 150/537: Loss=1.6514 (C:0.9843, R:0.0067)
Batch 175/537: Loss=1.6586 (C:0.9932, R:0.0067)
Batch 200/537: Loss=1.6322 (C:0.9649, R:0.0067)
Batch 225/537: Loss=1.6361 (C:0.9744, R:0.0066)
Batch 250/537: Loss=1.6470 (C:0.9785, R:0.0067)
Batch 275/537: Loss=1.6493 (C:0.9793, R:0.0067)
Batch 300/537: Loss=1.6402 (C:0.9714, R:0.0067)
Batch 325/537: Loss=1.6128 (C:0.9425, R:0.0067)
Batch 350/537: Loss=1.6730 (C:1.0046, R:0.0067)
Batch 375/537: Loss=1.6207 (C:0.9492, R:0.0067)
Batch 400/537: Loss=1.6323 (C:0.9623, R:0.0067)
Batch 425/537: Loss=1.6533 (C:0.9830, R:0.0067)
Batch 450/537: Loss=1.6736 (C:1.0040, R:0.0067)
Batch 475/537: Loss=1.6313 (C:0.9670, R:0.0066)
Batch 500/537: Loss=1.7005 (C:1.0271, R:0.0067)
Batch 525/537: Loss=1.6502 (C:0.9840, R:0.0067)

============================================================
Epoch 74/200 completed in 22.4s
Train: Loss=1.6487 (C:0.9800, R:0.0067) Ratio=3.41x
Val:   Loss=1.6775 (C:1.0522, R:0.0063) Ratio=2.56x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6775)
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=1.6687 (C:1.0005, R:0.0067)
Batch  25/537: Loss=1.6583 (C:0.9895, R:0.0067)
Batch  50/537: Loss=1.7048 (C:1.0274, R:0.0068)
Batch  75/537: Loss=1.6443 (C:0.9781, R:0.0067)
Batch 100/537: Loss=1.6754 (C:1.0131, R:0.0066)
Batch 125/537: Loss=1.6250 (C:0.9564, R:0.0067)
Batch 150/537: Loss=1.6433 (C:0.9750, R:0.0067)
Batch 175/537: Loss=1.6509 (C:0.9844, R:0.0067)
Batch 200/537: Loss=1.6496 (C:0.9803, R:0.0067)
Batch 225/537: Loss=1.6455 (C:0.9797, R:0.0067)
Batch 250/537: Loss=1.6231 (C:0.9546, R:0.0067)
Batch 275/537: Loss=1.6851 (C:1.0122, R:0.0067)
Batch 300/537: Loss=1.6465 (C:0.9760, R:0.0067)
Batch 325/537: Loss=1.6683 (C:0.9956, R:0.0067)
Batch 350/537: Loss=1.6683 (C:1.0011, R:0.0067)
Batch 375/537: Loss=1.6624 (C:0.9941, R:0.0067)
Batch 400/537: Loss=1.6336 (C:0.9689, R:0.0066)
Batch 425/537: Loss=1.6315 (C:0.9648, R:0.0067)
Batch 450/537: Loss=1.6436 (C:0.9753, R:0.0067)
Batch 475/537: Loss=1.6607 (C:0.9915, R:0.0067)
Batch 500/537: Loss=1.6332 (C:0.9615, R:0.0067)
Batch 525/537: Loss=1.6707 (C:1.0032, R:0.0067)

============================================================
Epoch 75/200 completed in 22.7s
Train: Loss=1.6495 (C:0.9809, R:0.0067) Ratio=3.38x
Val:   Loss=1.6805 (C:1.0550, R:0.0063) Ratio=2.54x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 76
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.418 ¬± 0.494
    Neg distances: 1.671 ¬± 0.905
    Separation ratio: 3.99x
    Gap: -2.733
    ‚úÖ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=1.6450 (C:0.9826, R:0.0066)
Batch  25/537: Loss=1.6511 (C:0.9842, R:0.0067)
Batch  50/537: Loss=1.6783 (C:1.0145, R:0.0066)
Batch  75/537: Loss=1.6712 (C:1.0032, R:0.0067)
Batch 100/537: Loss=1.6504 (C:0.9867, R:0.0066)
Batch 125/537: Loss=1.6709 (C:1.0048, R:0.0067)
Batch 150/537: Loss=1.6403 (C:0.9685, R:0.0067)
Batch 175/537: Loss=1.6688 (C:0.9981, R:0.0067)
Batch 200/537: Loss=1.6783 (C:1.0033, R:0.0068)
Batch 225/537: Loss=1.6248 (C:0.9563, R:0.0067)
Batch 250/537: Loss=1.6595 (C:0.9917, R:0.0067)
Batch 275/537: Loss=1.6573 (C:0.9868, R:0.0067)
Batch 300/537: Loss=1.6536 (C:0.9849, R:0.0067)
Batch 325/537: Loss=1.6243 (C:0.9577, R:0.0067)
Batch 350/537: Loss=1.5995 (C:0.9409, R:0.0066)
Batch 375/537: Loss=1.6332 (C:0.9671, R:0.0067)
Batch 400/537: Loss=1.6472 (C:0.9802, R:0.0067)
Batch 425/537: Loss=1.6615 (C:0.9899, R:0.0067)
Batch 450/537: Loss=1.6447 (C:0.9808, R:0.0066)
Batch 475/537: Loss=1.6557 (C:0.9888, R:0.0067)
Batch 500/537: Loss=1.6467 (C:0.9787, R:0.0067)
Batch 525/537: Loss=1.6554 (C:0.9844, R:0.0067)

============================================================
Epoch 76/200 completed in 29.2s
Train: Loss=1.6630 (C:0.9949, R:0.0067) Ratio=3.46x
Val:   Loss=1.6875 (C:1.0633, R:0.0062) Ratio=2.67x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=1.6973 (C:1.0294, R:0.0067)
Batch  25/537: Loss=1.6532 (C:0.9873, R:0.0067)
Batch  50/537: Loss=1.6568 (C:0.9912, R:0.0067)
Batch  75/537: Loss=1.6631 (C:0.9964, R:0.0067)
Batch 100/537: Loss=1.6861 (C:1.0231, R:0.0066)
Batch 125/537: Loss=1.6937 (C:1.0234, R:0.0067)
Batch 150/537: Loss=1.6608 (C:0.9848, R:0.0068)
Batch 175/537: Loss=1.6851 (C:1.0144, R:0.0067)
Batch 200/537: Loss=1.6737 (C:1.0049, R:0.0067)
Batch 225/537: Loss=1.6551 (C:0.9931, R:0.0066)
Batch 250/537: Loss=1.6480 (C:0.9803, R:0.0067)
Batch 275/537: Loss=1.6278 (C:0.9556, R:0.0067)
Batch 300/537: Loss=1.6641 (C:1.0034, R:0.0066)
Batch 325/537: Loss=1.6557 (C:0.9902, R:0.0067)
Batch 350/537: Loss=1.6398 (C:0.9727, R:0.0067)
Batch 375/537: Loss=1.6778 (C:1.0165, R:0.0066)
Batch 400/537: Loss=1.6411 (C:0.9704, R:0.0067)
Batch 425/537: Loss=1.6145 (C:0.9496, R:0.0066)
Batch 450/537: Loss=1.6941 (C:1.0287, R:0.0067)
Batch 475/537: Loss=1.6633 (C:0.9994, R:0.0066)
Batch 500/537: Loss=1.6743 (C:1.0092, R:0.0067)
Batch 525/537: Loss=1.6220 (C:0.9526, R:0.0067)

============================================================
Epoch 77/200 completed in 22.5s
Train: Loss=1.6620 (C:0.9941, R:0.0067) Ratio=3.41x
Val:   Loss=1.6883 (C:1.0635, R:0.0062) Ratio=2.61x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=1.6322 (C:0.9654, R:0.0067)
Batch  25/537: Loss=1.6546 (C:0.9862, R:0.0067)
Batch  50/537: Loss=1.6423 (C:0.9758, R:0.0067)
Batch  75/537: Loss=1.6706 (C:1.0020, R:0.0067)
Batch 100/537: Loss=1.6427 (C:0.9702, R:0.0067)
Batch 125/537: Loss=1.6633 (C:0.9968, R:0.0067)
Batch 150/537: Loss=1.6384 (C:0.9758, R:0.0066)
Batch 175/537: Loss=1.6844 (C:1.0159, R:0.0067)
Batch 200/537: Loss=1.6287 (C:0.9634, R:0.0067)
Batch 225/537: Loss=1.6667 (C:0.9979, R:0.0067)
Batch 250/537: Loss=1.6543 (C:0.9848, R:0.0067)
Batch 275/537: Loss=1.6793 (C:1.0094, R:0.0067)
Batch 300/537: Loss=1.6403 (C:0.9727, R:0.0067)
Batch 325/537: Loss=1.6731 (C:1.0051, R:0.0067)
Batch 350/537: Loss=1.6701 (C:1.0019, R:0.0067)
Batch 375/537: Loss=1.6657 (C:0.9970, R:0.0067)
Batch 400/537: Loss=1.6599 (C:0.9939, R:0.0067)
Batch 425/537: Loss=1.6774 (C:1.0177, R:0.0066)
Batch 450/537: Loss=1.6730 (C:1.0083, R:0.0066)
Batch 475/537: Loss=1.6521 (C:0.9835, R:0.0067)
Batch 500/537: Loss=1.6343 (C:0.9713, R:0.0066)
Batch 525/537: Loss=1.6775 (C:1.0015, R:0.0068)

============================================================
Epoch 78/200 completed in 22.6s
Train: Loss=1.6614 (C:0.9936, R:0.0067) Ratio=3.47x
Val:   Loss=1.6970 (C:1.0724, R:0.0062) Ratio=2.63x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 79
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.411 ¬± 0.483
    Neg distances: 1.693 ¬± 0.901
    Separation ratio: 4.12x
    Gap: -2.786
    ‚úÖ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=1.6534 (C:0.9881, R:0.0067)
Batch  25/537: Loss=1.6642 (C:0.9917, R:0.0067)
Batch  50/537: Loss=1.5986 (C:0.9279, R:0.0067)
Batch  75/537: Loss=1.6533 (C:0.9882, R:0.0067)
Batch 100/537: Loss=1.6512 (C:0.9871, R:0.0066)
Batch 125/537: Loss=1.6367 (C:0.9672, R:0.0067)
Batch 150/537: Loss=1.6709 (C:1.0007, R:0.0067)
Batch 175/537: Loss=1.6360 (C:0.9701, R:0.0067)
Batch 200/537: Loss=1.6545 (C:0.9856, R:0.0067)
Batch 225/537: Loss=1.6864 (C:1.0206, R:0.0067)
Batch 250/537: Loss=1.6875 (C:1.0249, R:0.0066)
Batch 275/537: Loss=1.6577 (C:0.9908, R:0.0067)
Batch 300/537: Loss=1.6408 (C:0.9757, R:0.0067)
Batch 325/537: Loss=1.6484 (C:0.9779, R:0.0067)
Batch 350/537: Loss=1.6693 (C:0.9956, R:0.0067)
Batch 375/537: Loss=1.6316 (C:0.9709, R:0.0066)
Batch 400/537: Loss=1.6337 (C:0.9700, R:0.0066)
Batch 425/537: Loss=1.6718 (C:1.0034, R:0.0067)
Batch 450/537: Loss=1.6396 (C:0.9719, R:0.0067)
Batch 475/537: Loss=1.6307 (C:0.9588, R:0.0067)
Batch 500/537: Loss=1.6117 (C:0.9423, R:0.0067)
Batch 525/537: Loss=1.6105 (C:0.9397, R:0.0067)

============================================================
Epoch 79/200 completed in 29.2s
Train: Loss=1.6489 (C:0.9812, R:0.0067) Ratio=3.42x
Val:   Loss=1.6914 (C:1.0672, R:0.0062) Ratio=2.63x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=1.6095 (C:0.9463, R:0.0066)
Batch  25/537: Loss=1.6214 (C:0.9553, R:0.0067)
Batch  50/537: Loss=1.6622 (C:0.9885, R:0.0067)
Batch  75/537: Loss=1.6321 (C:0.9693, R:0.0066)
Batch 100/537: Loss=1.6762 (C:1.0109, R:0.0067)
Batch 125/537: Loss=1.6283 (C:0.9610, R:0.0067)
Batch 150/537: Loss=1.6271 (C:0.9650, R:0.0066)
Batch 175/537: Loss=1.6412 (C:0.9753, R:0.0067)
Batch 200/537: Loss=1.6530 (C:0.9812, R:0.0067)
Batch 225/537: Loss=1.6412 (C:0.9679, R:0.0067)
Batch 250/537: Loss=1.6670 (C:0.9989, R:0.0067)
Batch 275/537: Loss=1.6568 (C:0.9829, R:0.0067)
Batch 300/537: Loss=1.6293 (C:0.9584, R:0.0067)
Batch 325/537: Loss=1.6249 (C:0.9625, R:0.0066)
Batch 350/537: Loss=1.6501 (C:0.9819, R:0.0067)
Batch 375/537: Loss=1.6663 (C:0.9976, R:0.0067)
Batch 400/537: Loss=1.6336 (C:0.9634, R:0.0067)
Batch 425/537: Loss=1.6308 (C:0.9709, R:0.0066)
Batch 450/537: Loss=1.6383 (C:0.9743, R:0.0066)
Batch 475/537: Loss=1.6532 (C:0.9862, R:0.0067)
Batch 500/537: Loss=1.6464 (C:0.9816, R:0.0066)
Batch 525/537: Loss=1.6533 (C:0.9878, R:0.0067)

============================================================
Epoch 80/200 completed in 23.2s
Train: Loss=1.6472 (C:0.9797, R:0.0067) Ratio=3.45x
Val:   Loss=1.6910 (C:1.0672, R:0.0062) Ratio=2.61x
Reconstruction weight: 100.000
No improvement for 6 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=1.6308 (C:0.9609, R:0.0067)
Batch  25/537: Loss=1.6465 (C:0.9779, R:0.0067)
Batch  50/537: Loss=1.6500 (C:0.9804, R:0.0067)
Batch  75/537: Loss=1.6459 (C:0.9803, R:0.0067)
Batch 100/537: Loss=1.6516 (C:0.9854, R:0.0067)
Batch 125/537: Loss=1.6654 (C:0.9945, R:0.0067)
Batch 150/537: Loss=1.6476 (C:0.9804, R:0.0067)
Batch 175/537: Loss=1.6685 (C:0.9973, R:0.0067)
Batch 200/537: Loss=1.6354 (C:0.9720, R:0.0066)
Batch 225/537: Loss=1.6292 (C:0.9624, R:0.0067)
Batch 250/537: Loss=1.6288 (C:0.9583, R:0.0067)
Batch 275/537: Loss=1.6771 (C:1.0131, R:0.0066)
Batch 300/537: Loss=1.6417 (C:0.9776, R:0.0066)
Batch 325/537: Loss=1.6554 (C:0.9910, R:0.0066)
Batch 350/537: Loss=1.7134 (C:1.0438, R:0.0067)
Batch 375/537: Loss=1.6321 (C:0.9656, R:0.0067)
Batch 400/537: Loss=1.6201 (C:0.9510, R:0.0067)
Batch 425/537: Loss=1.6706 (C:0.9998, R:0.0067)
Batch 450/537: Loss=1.6477 (C:0.9745, R:0.0067)
Batch 475/537: Loss=1.6631 (C:0.9960, R:0.0067)
Batch 500/537: Loss=1.6260 (C:0.9607, R:0.0067)
Batch 525/537: Loss=1.6386 (C:0.9778, R:0.0066)

============================================================
Epoch 81/200 completed in 23.0s
Train: Loss=1.6451 (C:0.9779, R:0.0067) Ratio=3.43x
Val:   Loss=1.6769 (C:1.0535, R:0.0062) Ratio=2.63x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6769)
============================================================

üåç Updating global dataset at epoch 82
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.401 ¬± 0.440
    Neg distances: 1.754 ¬± 0.902
    Separation ratio: 4.37x
    Gap: -2.843
    ‚úÖ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=1.5956 (C:0.9376, R:0.0066)
Batch  25/537: Loss=1.6316 (C:0.9675, R:0.0066)
Batch  50/537: Loss=1.6019 (C:0.9335, R:0.0067)
Batch  75/537: Loss=1.5982 (C:0.9287, R:0.0067)
Batch 100/537: Loss=1.5931 (C:0.9256, R:0.0067)
Batch 125/537: Loss=1.6182 (C:0.9498, R:0.0067)
Batch 150/537: Loss=1.6374 (C:0.9724, R:0.0066)
Batch 175/537: Loss=1.6373 (C:0.9688, R:0.0067)
Batch 200/537: Loss=1.5922 (C:0.9243, R:0.0067)
Batch 225/537: Loss=1.6221 (C:0.9620, R:0.0066)
Batch 250/537: Loss=1.6385 (C:0.9644, R:0.0067)
Batch 275/537: Loss=1.6193 (C:0.9536, R:0.0067)
Batch 300/537: Loss=1.6187 (C:0.9450, R:0.0067)
Batch 325/537: Loss=1.6188 (C:0.9529, R:0.0067)
Batch 350/537: Loss=1.6017 (C:0.9379, R:0.0066)
Batch 375/537: Loss=1.6533 (C:0.9841, R:0.0067)
Batch 400/537: Loss=1.6258 (C:0.9568, R:0.0067)
Batch 425/537: Loss=1.6031 (C:0.9367, R:0.0067)
Batch 450/537: Loss=1.6066 (C:0.9441, R:0.0066)
Batch 475/537: Loss=1.6213 (C:0.9546, R:0.0067)
Batch 500/537: Loss=1.6194 (C:0.9555, R:0.0066)
Batch 525/537: Loss=1.6317 (C:0.9605, R:0.0067)

============================================================
Epoch 82/200 completed in 29.1s
Train: Loss=1.6190 (C:0.9517, R:0.0067) Ratio=3.52x
Val:   Loss=1.6586 (C:1.0351, R:0.0062) Ratio=2.64x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6586)
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=1.6277 (C:0.9621, R:0.0067)
Batch  25/537: Loss=1.6251 (C:0.9528, R:0.0067)
Batch  50/537: Loss=1.6094 (C:0.9410, R:0.0067)
Batch  75/537: Loss=1.6479 (C:0.9746, R:0.0067)
Batch 100/537: Loss=1.6356 (C:0.9690, R:0.0067)
Batch 125/537: Loss=1.6287 (C:0.9598, R:0.0067)
Batch 150/537: Loss=1.6134 (C:0.9476, R:0.0067)
Batch 175/537: Loss=1.5734 (C:0.9081, R:0.0067)
Batch 200/537: Loss=1.6273 (C:0.9686, R:0.0066)
Batch 225/537: Loss=1.5988 (C:0.9319, R:0.0067)
Batch 250/537: Loss=1.6027 (C:0.9330, R:0.0067)
Batch 275/537: Loss=1.6257 (C:0.9541, R:0.0067)
Batch 300/537: Loss=1.6360 (C:0.9657, R:0.0067)
Batch 325/537: Loss=1.6343 (C:0.9658, R:0.0067)
Batch 350/537: Loss=1.6197 (C:0.9552, R:0.0066)
Batch 375/537: Loss=1.6272 (C:0.9581, R:0.0067)
Batch 400/537: Loss=1.6092 (C:0.9460, R:0.0066)
Batch 425/537: Loss=1.6048 (C:0.9382, R:0.0067)
Batch 450/537: Loss=1.5921 (C:0.9242, R:0.0067)
Batch 475/537: Loss=1.6216 (C:0.9577, R:0.0066)
Batch 500/537: Loss=1.6415 (C:0.9749, R:0.0067)
Batch 525/537: Loss=1.6336 (C:0.9706, R:0.0066)

============================================================
Epoch 83/200 completed in 23.0s
Train: Loss=1.6173 (C:0.9500, R:0.0067) Ratio=3.46x
Val:   Loss=1.6503 (C:1.0275, R:0.0062) Ratio=2.64x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6503)
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=1.6005 (C:0.9317, R:0.0067)
Batch  25/537: Loss=1.6352 (C:0.9685, R:0.0067)
Batch  50/537: Loss=1.6171 (C:0.9484, R:0.0067)
Batch  75/537: Loss=1.5946 (C:0.9334, R:0.0066)
Batch 100/537: Loss=1.5671 (C:0.9036, R:0.0066)
Batch 125/537: Loss=1.6343 (C:0.9648, R:0.0067)
Batch 150/537: Loss=1.6414 (C:0.9749, R:0.0067)
Batch 175/537: Loss=1.6065 (C:0.9473, R:0.0066)
Batch 200/537: Loss=1.5763 (C:0.9120, R:0.0066)
Batch 225/537: Loss=1.6054 (C:0.9395, R:0.0067)
Batch 250/537: Loss=1.5822 (C:0.9131, R:0.0067)
Batch 275/537: Loss=1.6203 (C:0.9530, R:0.0067)
Batch 300/537: Loss=1.6193 (C:0.9490, R:0.0067)
Batch 325/537: Loss=1.6177 (C:0.9518, R:0.0067)
Batch 350/537: Loss=1.5638 (C:0.8958, R:0.0067)
Batch 375/537: Loss=1.6187 (C:0.9531, R:0.0067)
Batch 400/537: Loss=1.6306 (C:0.9616, R:0.0067)
Batch 425/537: Loss=1.6182 (C:0.9409, R:0.0068)
Batch 450/537: Loss=1.6172 (C:0.9489, R:0.0067)
Batch 475/537: Loss=1.6425 (C:0.9738, R:0.0067)
Batch 500/537: Loss=1.5975 (C:0.9358, R:0.0066)
Batch 525/537: Loss=1.6311 (C:0.9628, R:0.0067)

============================================================
Epoch 84/200 completed in 23.4s
Train: Loss=1.6151 (C:0.9480, R:0.0067) Ratio=3.53x
Val:   Loss=1.6499 (C:1.0266, R:0.0062) Ratio=2.64x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6499)
============================================================

üåç Updating global dataset at epoch 85
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.423 ¬± 0.483
    Neg distances: 1.802 ¬± 0.912
    Separation ratio: 4.26x
    Gap: -2.947
    ‚úÖ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=1.5847 (C:0.9157, R:0.0067)
Batch  25/537: Loss=1.6161 (C:0.9567, R:0.0066)
Batch  50/537: Loss=1.6383 (C:0.9662, R:0.0067)
Batch  75/537: Loss=1.6158 (C:0.9468, R:0.0067)
Batch 100/537: Loss=1.6090 (C:0.9428, R:0.0067)
Batch 125/537: Loss=1.6103 (C:0.9380, R:0.0067)
Batch 150/537: Loss=1.6095 (C:0.9503, R:0.0066)
Batch 175/537: Loss=1.6358 (C:0.9679, R:0.0067)
Batch 200/537: Loss=1.5934 (C:0.9287, R:0.0066)
Batch 225/537: Loss=1.6142 (C:0.9528, R:0.0066)
Batch 250/537: Loss=1.6348 (C:0.9569, R:0.0068)
Batch 275/537: Loss=1.6113 (C:0.9435, R:0.0067)
Batch 300/537: Loss=1.6508 (C:0.9827, R:0.0067)
Batch 325/537: Loss=1.6255 (C:0.9548, R:0.0067)
Batch 350/537: Loss=1.6451 (C:0.9741, R:0.0067)
Batch 375/537: Loss=1.6372 (C:0.9807, R:0.0066)
Batch 400/537: Loss=1.6174 (C:0.9512, R:0.0067)
Batch 425/537: Loss=1.6157 (C:0.9468, R:0.0067)
Batch 450/537: Loss=1.6027 (C:0.9403, R:0.0066)
Batch 475/537: Loss=1.6156 (C:0.9494, R:0.0067)
Batch 500/537: Loss=1.6172 (C:0.9557, R:0.0066)
Batch 525/537: Loss=1.6093 (C:0.9399, R:0.0067)

============================================================
Epoch 85/200 completed in 29.4s
Train: Loss=1.6137 (C:0.9467, R:0.0067) Ratio=3.43x
Val:   Loss=1.6610 (C:1.0382, R:0.0062) Ratio=2.66x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=1.6154 (C:0.9509, R:0.0066)
Batch  25/537: Loss=1.6155 (C:0.9528, R:0.0066)
Batch  50/537: Loss=1.5954 (C:0.9232, R:0.0067)
Batch  75/537: Loss=1.6158 (C:0.9554, R:0.0066)
Batch 100/537: Loss=1.5800 (C:0.9215, R:0.0066)
Batch 125/537: Loss=1.6165 (C:0.9501, R:0.0067)
Batch 150/537: Loss=1.6074 (C:0.9459, R:0.0066)
Batch 175/537: Loss=1.5870 (C:0.9180, R:0.0067)
Batch 200/537: Loss=1.5783 (C:0.9065, R:0.0067)
Batch 225/537: Loss=1.6460 (C:0.9710, R:0.0068)
Batch 250/537: Loss=1.6448 (C:0.9784, R:0.0067)
Batch 275/537: Loss=1.6311 (C:0.9709, R:0.0066)
Batch 300/537: Loss=1.6308 (C:0.9640, R:0.0067)
Batch 325/537: Loss=1.5912 (C:0.9262, R:0.0067)
Batch 350/537: Loss=1.6128 (C:0.9471, R:0.0067)
Batch 375/537: Loss=1.6512 (C:0.9802, R:0.0067)
Batch 400/537: Loss=1.5986 (C:0.9298, R:0.0067)
Batch 425/537: Loss=1.5997 (C:0.9340, R:0.0067)
Batch 450/537: Loss=1.6270 (C:0.9546, R:0.0067)
Batch 475/537: Loss=1.6176 (C:0.9507, R:0.0067)
Batch 500/537: Loss=1.6490 (C:0.9826, R:0.0067)
Batch 525/537: Loss=1.6140 (C:0.9482, R:0.0067)

============================================================
Epoch 86/200 completed in 23.0s
Train: Loss=1.6122 (C:0.9452, R:0.0067) Ratio=3.48x
Val:   Loss=1.6666 (C:1.0439, R:0.0062) Ratio=2.63x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=1.6052 (C:0.9437, R:0.0066)
Batch  25/537: Loss=1.6271 (C:0.9639, R:0.0066)
Batch  50/537: Loss=1.5958 (C:0.9325, R:0.0066)
Batch  75/537: Loss=1.6084 (C:0.9418, R:0.0067)
Batch 100/537: Loss=1.6220 (C:0.9506, R:0.0067)
Batch 125/537: Loss=1.5946 (C:0.9232, R:0.0067)
Batch 150/537: Loss=1.6091 (C:0.9513, R:0.0066)
Batch 175/537: Loss=1.6009 (C:0.9333, R:0.0067)
Batch 200/537: Loss=1.5969 (C:0.9267, R:0.0067)
Batch 225/537: Loss=1.6119 (C:0.9451, R:0.0067)
Batch 250/537: Loss=1.5958 (C:0.9333, R:0.0066)
Batch 275/537: Loss=1.5898 (C:0.9234, R:0.0067)
Batch 300/537: Loss=1.6275 (C:0.9633, R:0.0066)
Batch 325/537: Loss=1.5942 (C:0.9295, R:0.0066)
Batch 350/537: Loss=1.6041 (C:0.9398, R:0.0066)
Batch 375/537: Loss=1.6168 (C:0.9501, R:0.0067)
Batch 400/537: Loss=1.5804 (C:0.9140, R:0.0067)
Batch 425/537: Loss=1.5983 (C:0.9298, R:0.0067)
Batch 450/537: Loss=1.6012 (C:0.9338, R:0.0067)
Batch 475/537: Loss=1.6268 (C:0.9591, R:0.0067)
Batch 500/537: Loss=1.6113 (C:0.9452, R:0.0067)
Batch 525/537: Loss=1.5989 (C:0.9326, R:0.0067)

============================================================
Epoch 87/200 completed in 23.2s
Train: Loss=1.6131 (C:0.9461, R:0.0067) Ratio=3.61x
Val:   Loss=1.6524 (C:1.0302, R:0.0062) Ratio=2.65x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

üåç Updating global dataset at epoch 88
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.404 ¬± 0.444
    Neg distances: 1.854 ¬± 0.915
    Separation ratio: 4.59x
    Gap: -3.003
    ‚úÖ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=1.5853 (C:0.9170, R:0.0067)
Batch  25/537: Loss=1.5909 (C:0.9217, R:0.0067)
Batch  50/537: Loss=1.6038 (C:0.9353, R:0.0067)
Batch  75/537: Loss=1.5567 (C:0.8949, R:0.0066)
Batch 100/537: Loss=1.5726 (C:0.9053, R:0.0067)
Batch 125/537: Loss=1.5798 (C:0.9041, R:0.0068)
Batch 150/537: Loss=1.5927 (C:0.9226, R:0.0067)
Batch 175/537: Loss=1.5772 (C:0.9107, R:0.0067)
Batch 200/537: Loss=1.5735 (C:0.9095, R:0.0066)
Batch 225/537: Loss=1.5849 (C:0.9163, R:0.0067)
Batch 250/537: Loss=1.5825 (C:0.9141, R:0.0067)
Batch 275/537: Loss=1.5715 (C:0.9103, R:0.0066)
Batch 300/537: Loss=1.5478 (C:0.8803, R:0.0067)
Batch 325/537: Loss=1.5723 (C:0.9047, R:0.0067)
Batch 350/537: Loss=1.5804 (C:0.9198, R:0.0066)
Batch 375/537: Loss=1.5644 (C:0.9068, R:0.0066)
Batch 400/537: Loss=1.5752 (C:0.9068, R:0.0067)
Batch 425/537: Loss=1.5767 (C:0.9160, R:0.0066)
Batch 450/537: Loss=1.5447 (C:0.8803, R:0.0066)
Batch 475/537: Loss=1.5711 (C:0.9077, R:0.0066)
Batch 500/537: Loss=1.5804 (C:0.9154, R:0.0067)
Batch 525/537: Loss=1.5679 (C:0.9006, R:0.0067)

============================================================
Epoch 88/200 completed in 29.9s
Train: Loss=1.5828 (C:0.9161, R:0.0067) Ratio=3.60x
Val:   Loss=1.6281 (C:1.0060, R:0.0062) Ratio=2.66x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6281)
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=1.5813 (C:0.9175, R:0.0066)
Batch  25/537: Loss=1.5738 (C:0.9076, R:0.0067)
Batch  50/537: Loss=1.6010 (C:0.9335, R:0.0067)
Batch  75/537: Loss=1.5596 (C:0.8912, R:0.0067)
Batch 100/537: Loss=1.6208 (C:0.9581, R:0.0066)
Batch 125/537: Loss=1.5706 (C:0.9023, R:0.0067)
Batch 150/537: Loss=1.5950 (C:0.9280, R:0.0067)
Batch 175/537: Loss=1.5864 (C:0.9173, R:0.0067)
Batch 200/537: Loss=1.5576 (C:0.8880, R:0.0067)
Batch 225/537: Loss=1.5991 (C:0.9329, R:0.0067)
Batch 250/537: Loss=1.6105 (C:0.9469, R:0.0066)
Batch 275/537: Loss=1.5929 (C:0.9233, R:0.0067)
Batch 300/537: Loss=1.5558 (C:0.8867, R:0.0067)
Batch 325/537: Loss=1.5419 (C:0.8702, R:0.0067)
Batch 350/537: Loss=1.6221 (C:0.9513, R:0.0067)
Batch 375/537: Loss=1.5800 (C:0.9152, R:0.0066)
Batch 400/537: Loss=1.5823 (C:0.9135, R:0.0067)
Batch 425/537: Loss=1.5808 (C:0.9167, R:0.0066)
Batch 450/537: Loss=1.6252 (C:0.9620, R:0.0066)
Batch 475/537: Loss=1.5884 (C:0.9238, R:0.0066)
Batch 500/537: Loss=1.5714 (C:0.9077, R:0.0066)
Batch 525/537: Loss=1.5661 (C:0.8970, R:0.0067)

============================================================
Epoch 89/200 completed in 23.1s
Train: Loss=1.5823 (C:0.9155, R:0.0067) Ratio=3.54x
Val:   Loss=1.6228 (C:1.0001, R:0.0062) Ratio=2.64x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6228)
============================================================

Epoch 90 Training
----------------------------------------
Batch   0/537: Loss=1.5915 (C:0.9205, R:0.0067)
Batch  25/537: Loss=1.5875 (C:0.9230, R:0.0066)
Batch  50/537: Loss=1.5871 (C:0.9251, R:0.0066)
Batch  75/537: Loss=1.5685 (C:0.9040, R:0.0066)
Batch 100/537: Loss=1.5563 (C:0.8917, R:0.0066)
Batch 125/537: Loss=1.5785 (C:0.9062, R:0.0067)
Batch 150/537: Loss=1.5705 (C:0.9002, R:0.0067)
Batch 175/537: Loss=1.6007 (C:0.9343, R:0.0067)
Batch 200/537: Loss=1.5858 (C:0.9147, R:0.0067)
Batch 225/537: Loss=1.5708 (C:0.9056, R:0.0067)
Batch 250/537: Loss=1.5473 (C:0.8818, R:0.0067)
Batch 275/537: Loss=1.5709 (C:0.9061, R:0.0066)
Batch 300/537: Loss=1.6002 (C:0.9372, R:0.0066)
Batch 325/537: Loss=1.5624 (C:0.9037, R:0.0066)
Batch 350/537: Loss=1.5400 (C:0.8749, R:0.0067)
Batch 375/537: Loss=1.5952 (C:0.9272, R:0.0067)
Batch 400/537: Loss=1.5842 (C:0.9248, R:0.0066)
Batch 425/537: Loss=1.6052 (C:0.9389, R:0.0067)
Batch 450/537: Loss=1.5730 (C:0.9031, R:0.0067)
Batch 475/537: Loss=1.5680 (C:0.9075, R:0.0066)
Batch 500/537: Loss=1.5702 (C:0.9061, R:0.0066)
Batch 525/537: Loss=1.5890 (C:0.9246, R:0.0066)

============================================================
Epoch 90/200 completed in 23.3s
Train: Loss=1.5810 (C:0.9144, R:0.0067) Ratio=3.53x
Val:   Loss=1.6230 (C:1.0002, R:0.0062) Ratio=2.70x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 91
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.425 ¬± 0.486
    Neg distances: 1.949 ¬± 0.937
    Separation ratio: 4.59x
    Gap: -3.227
    ‚úÖ Excellent global separation!

Epoch 91 Training
----------------------------------------
Batch   0/537: Loss=1.5793 (C:0.9207, R:0.0066)
Batch  25/537: Loss=1.5721 (C:0.9036, R:0.0067)
Batch  50/537: Loss=1.5349 (C:0.8686, R:0.0067)
Batch  75/537: Loss=1.6084 (C:0.9432, R:0.0067)
Batch 100/537: Loss=1.5446 (C:0.8838, R:0.0066)
Batch 125/537: Loss=1.5091 (C:0.8436, R:0.0067)
Batch 150/537: Loss=1.5656 (C:0.8979, R:0.0067)
Batch 175/537: Loss=1.5594 (C:0.8871, R:0.0067)
Batch 200/537: Loss=1.5469 (C:0.8753, R:0.0067)
Batch 225/537: Loss=1.5845 (C:0.9149, R:0.0067)
Batch 250/537: Loss=1.5972 (C:0.9349, R:0.0066)
Batch 275/537: Loss=1.5673 (C:0.9018, R:0.0067)
Batch 300/537: Loss=1.5902 (C:0.9191, R:0.0067)
Batch 325/537: Loss=1.5880 (C:0.9229, R:0.0067)
Batch 350/537: Loss=1.5797 (C:0.9190, R:0.0066)
Batch 375/537: Loss=1.5730 (C:0.9115, R:0.0066)
Batch 400/537: Loss=1.5373 (C:0.8684, R:0.0067)
Batch 425/537: Loss=1.5982 (C:0.9327, R:0.0067)
Batch 450/537: Loss=1.5749 (C:0.9018, R:0.0067)
Batch 475/537: Loss=1.5863 (C:0.9169, R:0.0067)
Batch 500/537: Loss=1.5547 (C:0.8829, R:0.0067)
Batch 525/537: Loss=1.5908 (C:0.9237, R:0.0067)

============================================================
Epoch 91/200 completed in 29.7s
Train: Loss=1.5650 (C:0.8981, R:0.0067) Ratio=3.52x
Val:   Loss=1.6081 (C:0.9863, R:0.0062) Ratio=2.71x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6081)
============================================================

Epoch 92 Training
----------------------------------------
Batch   0/537: Loss=1.5592 (C:0.8874, R:0.0067)
Batch  25/537: Loss=1.5775 (C:0.9121, R:0.0067)
Batch  50/537: Loss=1.5356 (C:0.8746, R:0.0066)
Batch  75/537: Loss=1.5504 (C:0.8874, R:0.0066)
Batch 100/537: Loss=1.5417 (C:0.8764, R:0.0067)
Batch 125/537: Loss=1.5548 (C:0.8908, R:0.0066)
Batch 150/537: Loss=1.5796 (C:0.9158, R:0.0066)
Batch 175/537: Loss=1.5481 (C:0.8839, R:0.0066)
Batch 200/537: Loss=1.5673 (C:0.9027, R:0.0066)
Batch 225/537: Loss=1.5561 (C:0.8925, R:0.0066)
Batch 250/537: Loss=1.5785 (C:0.9140, R:0.0066)
Batch 275/537: Loss=1.6083 (C:0.9362, R:0.0067)
Batch 300/537: Loss=1.5605 (C:0.8904, R:0.0067)
Batch 325/537: Loss=1.5619 (C:0.8920, R:0.0067)
Batch 350/537: Loss=1.5765 (C:0.9082, R:0.0067)
Batch 375/537: Loss=1.5994 (C:0.9291, R:0.0067)
Batch 400/537: Loss=1.5579 (C:0.8878, R:0.0067)
Batch 425/537: Loss=1.5808 (C:0.9167, R:0.0066)
Batch 450/537: Loss=1.5620 (C:0.8955, R:0.0067)
Batch 475/537: Loss=1.5280 (C:0.8635, R:0.0066)
Batch 500/537: Loss=1.5846 (C:0.9115, R:0.0067)
Batch 525/537: Loss=1.5414 (C:0.8724, R:0.0067)

============================================================
Epoch 92/200 completed in 22.4s
Train: Loss=1.5629 (C:0.8962, R:0.0067) Ratio=3.56x
Val:   Loss=1.6100 (C:0.9874, R:0.0062) Ratio=2.70x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 93 Training
----------------------------------------
Batch   0/537: Loss=1.5893 (C:0.9225, R:0.0067)
Batch  25/537: Loss=1.5552 (C:0.8889, R:0.0067)
Batch  50/537: Loss=1.5883 (C:0.9220, R:0.0067)
Batch  75/537: Loss=1.5945 (C:0.9302, R:0.0066)
Batch 100/537: Loss=1.5461 (C:0.8694, R:0.0068)
Batch 125/537: Loss=1.5859 (C:0.9132, R:0.0067)
Batch 150/537: Loss=1.5819 (C:0.9126, R:0.0067)
Batch 175/537: Loss=1.5370 (C:0.8635, R:0.0067)
Batch 200/537: Loss=1.5810 (C:0.9112, R:0.0067)
Batch 225/537: Loss=1.5967 (C:0.9298, R:0.0067)
Batch 250/537: Loss=1.5682 (C:0.9008, R:0.0067)
Batch 275/537: Loss=1.5448 (C:0.8767, R:0.0067)
Batch 300/537: Loss=1.5244 (C:0.8616, R:0.0066)
Batch 325/537: Loss=1.5246 (C:0.8596, R:0.0067)
Batch 350/537: Loss=1.5450 (C:0.8806, R:0.0066)
Batch 375/537: Loss=1.5785 (C:0.9132, R:0.0067)
Batch 400/537: Loss=1.5356 (C:0.8709, R:0.0066)
Batch 425/537: Loss=1.5768 (C:0.9085, R:0.0067)
Batch 450/537: Loss=1.5730 (C:0.9031, R:0.0067)
Batch 475/537: Loss=1.5373 (C:0.8718, R:0.0067)
Batch 500/537: Loss=1.5444 (C:0.8800, R:0.0066)
Batch 525/537: Loss=1.5604 (C:0.8917, R:0.0067)

============================================================
Epoch 93/200 completed in 22.7s
Train: Loss=1.5612 (C:0.8943, R:0.0067) Ratio=3.63x
Val:   Loss=1.6117 (C:0.9897, R:0.0062) Ratio=2.72x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 94
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.433 ¬± 0.504
    Neg distances: 1.986 ¬± 0.933
    Separation ratio: 4.59x
    Gap: -3.254
    ‚úÖ Excellent global separation!

Epoch 94 Training
----------------------------------------
Batch   0/537: Loss=1.5527 (C:0.8862, R:0.0067)
Batch  25/537: Loss=1.5582 (C:0.8949, R:0.0066)
Batch  50/537: Loss=1.5328 (C:0.8602, R:0.0067)
Batch  75/537: Loss=1.5265 (C:0.8603, R:0.0067)
Batch 100/537: Loss=1.5227 (C:0.8581, R:0.0066)
Batch 125/537: Loss=1.5311 (C:0.8631, R:0.0067)
Batch 150/537: Loss=1.5474 (C:0.8823, R:0.0067)
Batch 175/537: Loss=1.5183 (C:0.8489, R:0.0067)
Batch 200/537: Loss=1.5987 (C:0.9364, R:0.0066)
Batch 225/537: Loss=1.5489 (C:0.8823, R:0.0067)
Batch 250/537: Loss=1.5571 (C:0.8872, R:0.0067)
Batch 275/537: Loss=1.5701 (C:0.9004, R:0.0067)
Batch 300/537: Loss=1.5160 (C:0.8511, R:0.0066)
Batch 325/537: Loss=1.5763 (C:0.9059, R:0.0067)
Batch 350/537: Loss=1.5130 (C:0.8465, R:0.0067)
Batch 375/537: Loss=1.5720 (C:0.9124, R:0.0066)
Batch 400/537: Loss=1.5220 (C:0.8486, R:0.0067)
Batch 425/537: Loss=1.5440 (C:0.8775, R:0.0067)
Batch 450/537: Loss=1.5491 (C:0.8839, R:0.0067)
Batch 475/537: Loss=1.5364 (C:0.8733, R:0.0066)
Batch 500/537: Loss=1.5596 (C:0.8925, R:0.0067)
Batch 525/537: Loss=1.5330 (C:0.8680, R:0.0067)

============================================================
Epoch 94/200 completed in 29.5s
Train: Loss=1.5455 (C:0.8787, R:0.0067) Ratio=3.59x
Val:   Loss=1.5871 (C:0.9657, R:0.0062) Ratio=2.73x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5871)
============================================================

Epoch 95 Training
----------------------------------------
Batch   0/537: Loss=1.5382 (C:0.8731, R:0.0067)
Batch  25/537: Loss=1.5399 (C:0.8699, R:0.0067)
Batch  50/537: Loss=1.5565 (C:0.8876, R:0.0067)
Batch  75/537: Loss=1.5145 (C:0.8479, R:0.0067)
Batch 100/537: Loss=1.5206 (C:0.8521, R:0.0067)
Batch 125/537: Loss=1.5293 (C:0.8626, R:0.0067)
Batch 150/537: Loss=1.5587 (C:0.8845, R:0.0067)
Batch 175/537: Loss=1.5100 (C:0.8434, R:0.0067)
Batch 200/537: Loss=1.5447 (C:0.8783, R:0.0067)
Batch 225/537: Loss=1.5726 (C:0.9032, R:0.0067)
Batch 250/537: Loss=1.5363 (C:0.8697, R:0.0067)
Batch 275/537: Loss=1.5833 (C:0.9167, R:0.0067)
Batch 300/537: Loss=1.5273 (C:0.8642, R:0.0066)
Batch 325/537: Loss=1.5319 (C:0.8694, R:0.0066)
Batch 350/537: Loss=1.5444 (C:0.8711, R:0.0067)
Batch 375/537: Loss=1.5044 (C:0.8391, R:0.0067)
Batch 400/537: Loss=1.5400 (C:0.8712, R:0.0067)
Batch 425/537: Loss=1.5395 (C:0.8718, R:0.0067)
Batch 450/537: Loss=1.5272 (C:0.8652, R:0.0066)
Batch 475/537: Loss=1.5284 (C:0.8626, R:0.0067)
Batch 500/537: Loss=1.5562 (C:0.8879, R:0.0067)
Batch 525/537: Loss=1.5782 (C:0.9081, R:0.0067)

============================================================
Epoch 95/200 completed in 22.5s
Train: Loss=1.5446 (C:0.8777, R:0.0067) Ratio=3.63x
Val:   Loss=1.5957 (C:0.9738, R:0.0062) Ratio=2.72x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 96 Training
----------------------------------------
Batch   0/537: Loss=1.5183 (C:0.8498, R:0.0067)
Batch  25/537: Loss=1.5268 (C:0.8583, R:0.0067)
Batch  50/537: Loss=1.5788 (C:0.9128, R:0.0067)
Batch  75/537: Loss=1.5309 (C:0.8648, R:0.0067)
Batch 100/537: Loss=1.5477 (C:0.8808, R:0.0067)
Batch 125/537: Loss=1.5737 (C:0.9061, R:0.0067)
Batch 150/537: Loss=1.5377 (C:0.8746, R:0.0066)
Batch 175/537: Loss=1.5286 (C:0.8668, R:0.0066)
Batch 200/537: Loss=1.5220 (C:0.8484, R:0.0067)
Batch 225/537: Loss=1.5471 (C:0.8823, R:0.0066)
Batch 250/537: Loss=1.5420 (C:0.8726, R:0.0067)
Batch 275/537: Loss=1.5130 (C:0.8482, R:0.0066)
Batch 300/537: Loss=1.5325 (C:0.8546, R:0.0068)
Batch 325/537: Loss=1.4984 (C:0.8288, R:0.0067)
Batch 350/537: Loss=1.5931 (C:0.9281, R:0.0066)
Batch 375/537: Loss=1.5504 (C:0.8797, R:0.0067)
Batch 400/537: Loss=1.5251 (C:0.8592, R:0.0067)
Batch 425/537: Loss=1.5299 (C:0.8586, R:0.0067)
Batch 450/537: Loss=1.5452 (C:0.8775, R:0.0067)
Batch 475/537: Loss=1.5313 (C:0.8680, R:0.0066)
Batch 500/537: Loss=1.5641 (C:0.9001, R:0.0066)
Batch 525/537: Loss=1.5432 (C:0.8758, R:0.0067)

============================================================
Epoch 96/200 completed in 22.4s
Train: Loss=1.5431 (C:0.8764, R:0.0067) Ratio=3.69x
Val:   Loss=1.5960 (C:0.9742, R:0.0062) Ratio=2.73x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 97
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.461 ¬± 0.526
    Neg distances: 2.068 ¬± 0.964
    Separation ratio: 4.48x
    Gap: -3.427
    ‚úÖ Excellent global separation!

Epoch 97 Training
----------------------------------------
Batch   0/537: Loss=1.5352 (C:0.8677, R:0.0067)
Batch  25/537: Loss=1.5425 (C:0.8687, R:0.0067)
Batch  50/537: Loss=1.5263 (C:0.8631, R:0.0066)
Batch  75/537: Loss=1.5093 (C:0.8476, R:0.0066)
Batch 100/537: Loss=1.5246 (C:0.8569, R:0.0067)
Batch 125/537: Loss=1.4986 (C:0.8407, R:0.0066)
Batch 150/537: Loss=1.5253 (C:0.8612, R:0.0066)
Batch 175/537: Loss=1.5096 (C:0.8426, R:0.0067)
Batch 200/537: Loss=1.4884 (C:0.8204, R:0.0067)
Batch 225/537: Loss=1.5398 (C:0.8749, R:0.0066)
Batch 250/537: Loss=1.5569 (C:0.8888, R:0.0067)
Batch 275/537: Loss=1.5167 (C:0.8462, R:0.0067)
Batch 300/537: Loss=1.5106 (C:0.8451, R:0.0067)
Batch 325/537: Loss=1.5248 (C:0.8538, R:0.0067)
Batch 350/537: Loss=1.5246 (C:0.8486, R:0.0068)
Batch 375/537: Loss=1.5433 (C:0.8667, R:0.0068)
Batch 400/537: Loss=1.4972 (C:0.8279, R:0.0067)
Batch 425/537: Loss=1.5193 (C:0.8570, R:0.0066)
Batch 450/537: Loss=1.5922 (C:0.9265, R:0.0067)
Batch 475/537: Loss=1.5628 (C:0.8946, R:0.0067)
Batch 500/537: Loss=1.5327 (C:0.8640, R:0.0067)
Batch 525/537: Loss=1.5166 (C:0.8447, R:0.0067)

============================================================
Epoch 97/200 completed in 28.9s
Train: Loss=1.5335 (C:0.8666, R:0.0067) Ratio=3.68x
Val:   Loss=1.5832 (C:0.9616, R:0.0062) Ratio=2.75x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5832)
============================================================

Epoch 98 Training
----------------------------------------
Batch   0/537: Loss=1.5408 (C:0.8727, R:0.0067)
Batch  25/537: Loss=1.5032 (C:0.8402, R:0.0066)
Batch  50/537: Loss=1.5179 (C:0.8523, R:0.0067)
Batch  75/537: Loss=1.5349 (C:0.8743, R:0.0066)
Batch 100/537: Loss=1.5275 (C:0.8544, R:0.0067)
Batch 125/537: Loss=1.5434 (C:0.8773, R:0.0067)
Batch 150/537: Loss=1.5100 (C:0.8397, R:0.0067)
Batch 175/537: Loss=1.5176 (C:0.8546, R:0.0066)
Batch 200/537: Loss=1.5517 (C:0.8808, R:0.0067)
Batch 225/537: Loss=1.5340 (C:0.8632, R:0.0067)
Batch 250/537: Loss=1.5357 (C:0.8692, R:0.0067)
Batch 275/537: Loss=1.5326 (C:0.8613, R:0.0067)
Batch 300/537: Loss=1.5428 (C:0.8742, R:0.0067)
Batch 325/537: Loss=1.5395 (C:0.8730, R:0.0067)
Batch 350/537: Loss=1.5247 (C:0.8611, R:0.0066)
Batch 375/537: Loss=1.5463 (C:0.8845, R:0.0066)
Batch 400/537: Loss=1.5429 (C:0.8707, R:0.0067)
Batch 425/537: Loss=1.5321 (C:0.8648, R:0.0067)
Batch 450/537: Loss=1.5422 (C:0.8706, R:0.0067)
Batch 475/537: Loss=1.4913 (C:0.8276, R:0.0066)
Batch 500/537: Loss=1.5484 (C:0.8800, R:0.0067)
Batch 525/537: Loss=1.5260 (C:0.8602, R:0.0067)

============================================================
Epoch 98/200 completed in 22.6s
Train: Loss=1.5332 (C:0.8663, R:0.0067) Ratio=3.65x
Val:   Loss=1.5811 (C:0.9590, R:0.0062) Ratio=2.77x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5811)
============================================================

Epoch 99 Training
----------------------------------------
Batch   0/537: Loss=1.5324 (C:0.8618, R:0.0067)
Batch  25/537: Loss=1.5031 (C:0.8422, R:0.0066)
Batch  50/537: Loss=1.4840 (C:0.8121, R:0.0067)
Batch  75/537: Loss=1.5253 (C:0.8555, R:0.0067)
Batch 100/537: Loss=1.5290 (C:0.8564, R:0.0067)
Batch 125/537: Loss=1.5067 (C:0.8376, R:0.0067)
Batch 150/537: Loss=1.5273 (C:0.8577, R:0.0067)
Batch 175/537: Loss=1.5395 (C:0.8766, R:0.0066)
Batch 200/537: Loss=1.5079 (C:0.8452, R:0.0066)
Batch 225/537: Loss=1.5318 (C:0.8693, R:0.0066)
Batch 250/537: Loss=1.5082 (C:0.8473, R:0.0066)
Batch 275/537: Loss=1.5316 (C:0.8639, R:0.0067)
Batch 300/537: Loss=1.5376 (C:0.8739, R:0.0066)
Batch 325/537: Loss=1.5316 (C:0.8657, R:0.0067)
Batch 350/537: Loss=1.5633 (C:0.8876, R:0.0068)
Batch 375/537: Loss=1.5511 (C:0.8789, R:0.0067)
Batch 400/537: Loss=1.5731 (C:0.9091, R:0.0066)
Batch 425/537: Loss=1.4982 (C:0.8372, R:0.0066)
Batch 450/537: Loss=1.5372 (C:0.8683, R:0.0067)
Batch 475/537: Loss=1.5565 (C:0.8883, R:0.0067)
Batch 500/537: Loss=1.5357 (C:0.8686, R:0.0067)
Batch 525/537: Loss=1.5152 (C:0.8422, R:0.0067)

============================================================
Epoch 99/200 completed in 22.5s
Train: Loss=1.5311 (C:0.8642, R:0.0067) Ratio=3.76x
Val:   Loss=1.5678 (C:0.9467, R:0.0062) Ratio=2.76x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5678)
============================================================

üåç Updating global dataset at epoch 100
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.455 ¬± 0.559
    Neg distances: 2.209 ¬± 0.991
    Separation ratio: 4.85x
    Gap: -3.571
    ‚úÖ Excellent global separation!

Epoch 100 Training
----------------------------------------
Batch   0/537: Loss=1.4839 (C:0.8149, R:0.0067)
Batch  25/537: Loss=1.4385 (C:0.7632, R:0.0068)
Batch  50/537: Loss=1.4851 (C:0.8225, R:0.0066)
Batch  75/537: Loss=1.4894 (C:0.8199, R:0.0067)
Batch 100/537: Loss=1.4868 (C:0.8204, R:0.0067)
Batch 125/537: Loss=1.4715 (C:0.8142, R:0.0066)
Batch 150/537: Loss=1.4531 (C:0.7911, R:0.0066)
Batch 175/537: Loss=1.4883 (C:0.8142, R:0.0067)
Batch 200/537: Loss=1.4829 (C:0.8158, R:0.0067)
Batch 225/537: Loss=1.5028 (C:0.8367, R:0.0067)
Batch 250/537: Loss=1.4791 (C:0.8070, R:0.0067)
Batch 275/537: Loss=1.4867 (C:0.8202, R:0.0067)
Batch 300/537: Loss=1.4741 (C:0.8003, R:0.0067)
Batch 325/537: Loss=1.4820 (C:0.8182, R:0.0066)
Batch 350/537: Loss=1.4492 (C:0.7853, R:0.0066)
Batch 375/537: Loss=1.5135 (C:0.8457, R:0.0067)
Batch 400/537: Loss=1.5109 (C:0.8411, R:0.0067)
Batch 425/537: Loss=1.4987 (C:0.8354, R:0.0066)
Batch 450/537: Loss=1.4676 (C:0.8014, R:0.0067)
Batch 475/537: Loss=1.4648 (C:0.7974, R:0.0067)
Batch 500/537: Loss=1.5024 (C:0.8415, R:0.0066)
Batch 525/537: Loss=1.4944 (C:0.8303, R:0.0066)

============================================================
Epoch 100/200 completed in 29.1s
Train: Loss=1.4821 (C:0.8148, R:0.0067) Ratio=3.70x
Val:   Loss=1.5356 (C:0.9139, R:0.0062) Ratio=2.80x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5356)
Checkpoint saved at epoch 100
============================================================

Epoch 101 Training
----------------------------------------
Batch   0/537: Loss=1.4286 (C:0.7598, R:0.0067)
Batch  25/537: Loss=1.4906 (C:0.8223, R:0.0067)
Batch  50/537: Loss=1.4661 (C:0.7981, R:0.0067)
Batch  75/537: Loss=1.4727 (C:0.8058, R:0.0067)
Batch 100/537: Loss=1.5058 (C:0.8385, R:0.0067)
Batch 125/537: Loss=1.4860 (C:0.8165, R:0.0067)
Batch 150/537: Loss=1.5143 (C:0.8382, R:0.0068)
Batch 175/537: Loss=1.4532 (C:0.7872, R:0.0067)
Batch 200/537: Loss=1.4622 (C:0.7915, R:0.0067)
Batch 225/537: Loss=1.4736 (C:0.8040, R:0.0067)
Batch 250/537: Loss=1.4436 (C:0.7761, R:0.0067)
Batch 275/537: Loss=1.4948 (C:0.8282, R:0.0067)
Batch 300/537: Loss=1.4783 (C:0.8081, R:0.0067)
Batch 325/537: Loss=1.4879 (C:0.8294, R:0.0066)
Batch 350/537: Loss=1.4538 (C:0.7876, R:0.0067)
Batch 375/537: Loss=1.4755 (C:0.8095, R:0.0067)
Batch 400/537: Loss=1.4700 (C:0.8044, R:0.0067)
Batch 425/537: Loss=1.4806 (C:0.8097, R:0.0067)
Batch 450/537: Loss=1.4963 (C:0.8223, R:0.0067)
Batch 475/537: Loss=1.5026 (C:0.8424, R:0.0066)
Batch 500/537: Loss=1.4990 (C:0.8256, R:0.0067)
Batch 525/537: Loss=1.4717 (C:0.8042, R:0.0067)

============================================================
Epoch 101/200 completed in 22.9s
Train: Loss=1.4807 (C:0.8132, R:0.0067) Ratio=3.76x
Val:   Loss=1.5417 (C:0.9198, R:0.0062) Ratio=2.76x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 102 Training
----------------------------------------
Batch   0/537: Loss=1.4763 (C:0.8127, R:0.0066)
Batch  25/537: Loss=1.4448 (C:0.7792, R:0.0067)
Batch  50/537: Loss=1.5276 (C:0.8652, R:0.0066)
Batch  75/537: Loss=1.4561 (C:0.7900, R:0.0067)
Batch 100/537: Loss=1.4555 (C:0.7929, R:0.0066)
Batch 125/537: Loss=1.4792 (C:0.8151, R:0.0066)
Batch 150/537: Loss=1.4837 (C:0.8168, R:0.0067)
Batch 175/537: Loss=1.4877 (C:0.8183, R:0.0067)
Batch 200/537: Loss=1.4709 (C:0.8046, R:0.0067)
Batch 225/537: Loss=1.5180 (C:0.8497, R:0.0067)
Batch 250/537: Loss=1.4879 (C:0.8212, R:0.0067)
Batch 275/537: Loss=1.5025 (C:0.8372, R:0.0067)
Batch 300/537: Loss=1.5038 (C:0.8415, R:0.0066)
Batch 325/537: Loss=1.4867 (C:0.8107, R:0.0068)
Batch 350/537: Loss=1.4832 (C:0.8194, R:0.0066)
Batch 375/537: Loss=1.4827 (C:0.8105, R:0.0067)
Batch 400/537: Loss=1.5149 (C:0.8512, R:0.0066)
Batch 425/537: Loss=1.4419 (C:0.7773, R:0.0066)
Batch 450/537: Loss=1.4843 (C:0.8207, R:0.0066)
Batch 475/537: Loss=1.5098 (C:0.8391, R:0.0067)
Batch 500/537: Loss=1.4902 (C:0.8244, R:0.0067)
Batch 525/537: Loss=1.5273 (C:0.8565, R:0.0067)

============================================================
Epoch 102/200 completed in 23.1s
Train: Loss=1.4786 (C:0.8111, R:0.0067) Ratio=3.70x
Val:   Loss=1.5372 (C:0.9157, R:0.0062) Ratio=2.79x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 103
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.481 ¬± 0.616
    Neg distances: 2.308 ¬± 1.033
    Separation ratio: 4.80x
    Gap: -3.736
    ‚úÖ Excellent global separation!

Epoch 103 Training
----------------------------------------
Batch   0/537: Loss=1.4789 (C:0.8087, R:0.0067)
Batch  25/537: Loss=1.4547 (C:0.7815, R:0.0067)
Batch  50/537: Loss=1.4930 (C:0.8271, R:0.0067)
Batch  75/537: Loss=1.4799 (C:0.8123, R:0.0067)
Batch 100/537: Loss=1.4415 (C:0.7678, R:0.0067)
Batch 125/537: Loss=1.4395 (C:0.7707, R:0.0067)
Batch 150/537: Loss=1.4761 (C:0.8073, R:0.0067)
Batch 175/537: Loss=1.4580 (C:0.7911, R:0.0067)
Batch 200/537: Loss=1.4637 (C:0.7987, R:0.0067)
Batch 225/537: Loss=1.4714 (C:0.8054, R:0.0067)
Batch 250/537: Loss=1.4845 (C:0.8063, R:0.0068)
Batch 275/537: Loss=1.4863 (C:0.8113, R:0.0068)
Batch 300/537: Loss=1.4375 (C:0.7716, R:0.0067)
Batch 325/537: Loss=1.5277 (C:0.8570, R:0.0067)
Batch 350/537: Loss=1.4782 (C:0.8111, R:0.0067)
Batch 375/537: Loss=1.4478 (C:0.7774, R:0.0067)
Batch 400/537: Loss=1.4405 (C:0.7747, R:0.0067)
Batch 425/537: Loss=1.4852 (C:0.8141, R:0.0067)
Batch 450/537: Loss=1.4851 (C:0.8130, R:0.0067)
Batch 475/537: Loss=1.4649 (C:0.7908, R:0.0067)
Batch 500/537: Loss=1.4612 (C:0.7930, R:0.0067)
Batch 525/537: Loss=1.4862 (C:0.8199, R:0.0067)

============================================================
Epoch 103/200 completed in 30.1s
Train: Loss=1.4613 (C:0.7934, R:0.0067) Ratio=3.71x
Val:   Loss=1.5172 (C:0.8945, R:0.0062) Ratio=2.81x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5172)
============================================================

Epoch 104 Training
----------------------------------------
Batch   0/537: Loss=1.4429 (C:0.7672, R:0.0068)
Batch  25/537: Loss=1.4545 (C:0.7888, R:0.0067)
Batch  50/537: Loss=1.4626 (C:0.7989, R:0.0066)
Batch  75/537: Loss=1.4514 (C:0.7879, R:0.0066)
Batch 100/537: Loss=1.4782 (C:0.8158, R:0.0066)
Batch 125/537: Loss=1.4549 (C:0.7826, R:0.0067)
Batch 150/537: Loss=1.4995 (C:0.8311, R:0.0067)
Batch 175/537: Loss=1.4456 (C:0.7768, R:0.0067)
Batch 200/537: Loss=1.4813 (C:0.8055, R:0.0068)
Batch 225/537: Loss=1.4583 (C:0.7875, R:0.0067)
Batch 250/537: Loss=1.4445 (C:0.7800, R:0.0066)
Batch 275/537: Loss=1.4722 (C:0.8035, R:0.0067)
Batch 300/537: Loss=1.4692 (C:0.7963, R:0.0067)
Batch 325/537: Loss=1.4683 (C:0.8027, R:0.0067)
Batch 350/537: Loss=1.4532 (C:0.7833, R:0.0067)
Batch 375/537: Loss=1.4659 (C:0.7952, R:0.0067)
Batch 400/537: Loss=1.4835 (C:0.8166, R:0.0067)
Batch 425/537: Loss=1.4642 (C:0.7977, R:0.0067)
Batch 450/537: Loss=1.4435 (C:0.7710, R:0.0067)
Batch 475/537: Loss=1.4783 (C:0.8078, R:0.0067)
Batch 500/537: Loss=1.4563 (C:0.7867, R:0.0067)
Batch 525/537: Loss=1.4153 (C:0.7499, R:0.0067)

============================================================
Epoch 104/200 completed in 23.0s
Train: Loss=1.4603 (C:0.7925, R:0.0067) Ratio=3.76x
Val:   Loss=1.5260 (C:0.9037, R:0.0062) Ratio=2.75x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 105 Training
----------------------------------------
Batch   0/537: Loss=1.4152 (C:0.7420, R:0.0067)
Batch  25/537: Loss=1.4635 (C:0.7935, R:0.0067)
Batch  50/537: Loss=1.4825 (C:0.8100, R:0.0067)
Batch  75/537: Loss=1.4624 (C:0.7928, R:0.0067)
Batch 100/537: Loss=1.4554 (C:0.7921, R:0.0066)
Batch 125/537: Loss=1.4980 (C:0.8241, R:0.0067)
Batch 150/537: Loss=1.4766 (C:0.8163, R:0.0066)
Batch 175/537: Loss=1.4853 (C:0.8135, R:0.0067)
Batch 200/537: Loss=1.4547 (C:0.7837, R:0.0067)
Batch 225/537: Loss=1.4328 (C:0.7722, R:0.0066)
Batch 250/537: Loss=1.4849 (C:0.8193, R:0.0067)
Batch 275/537: Loss=1.4629 (C:0.7951, R:0.0067)
Batch 300/537: Loss=1.4467 (C:0.7784, R:0.0067)
Batch 325/537: Loss=1.4814 (C:0.8124, R:0.0067)
Batch 350/537: Loss=1.4630 (C:0.7921, R:0.0067)
Batch 375/537: Loss=1.4464 (C:0.7740, R:0.0067)
Batch 400/537: Loss=1.4459 (C:0.7783, R:0.0067)
Batch 425/537: Loss=1.4570 (C:0.7917, R:0.0067)
Batch 450/537: Loss=1.4121 (C:0.7481, R:0.0066)
Batch 475/537: Loss=1.4671 (C:0.7984, R:0.0067)
Batch 500/537: Loss=1.4982 (C:0.8328, R:0.0067)
Batch 525/537: Loss=1.4987 (C:0.8280, R:0.0067)

============================================================
Epoch 105/200 completed in 22.9s
Train: Loss=1.4587 (C:0.7908, R:0.0067) Ratio=3.78x
Val:   Loss=1.5234 (C:0.9012, R:0.0062) Ratio=2.74x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 106
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.485 ¬± 0.634
    Neg distances: 2.353 ¬± 1.047
    Separation ratio: 4.85x
    Gap: -3.799
    ‚úÖ Excellent global separation!

Epoch 106 Training
----------------------------------------
Batch   0/537: Loss=1.4513 (C:0.7790, R:0.0067)
Batch  25/537: Loss=1.4152 (C:0.7492, R:0.0067)
Batch  50/537: Loss=1.4325 (C:0.7653, R:0.0067)
Batch  75/537: Loss=1.4496 (C:0.7830, R:0.0067)
Batch 100/537: Loss=1.4541 (C:0.7859, R:0.0067)
Batch 125/537: Loss=1.4523 (C:0.7905, R:0.0066)
Batch 150/537: Loss=1.4493 (C:0.7818, R:0.0067)
Batch 175/537: Loss=1.3879 (C:0.7157, R:0.0067)
Batch 200/537: Loss=1.4281 (C:0.7551, R:0.0067)
Batch 225/537: Loss=1.4382 (C:0.7657, R:0.0067)
Batch 250/537: Loss=1.4714 (C:0.8003, R:0.0067)
Batch 275/537: Loss=1.4290 (C:0.7658, R:0.0066)
Batch 300/537: Loss=1.4443 (C:0.7746, R:0.0067)
Batch 325/537: Loss=1.4024 (C:0.7294, R:0.0067)
Batch 350/537: Loss=1.4744 (C:0.8080, R:0.0067)
Batch 375/537: Loss=1.4401 (C:0.7631, R:0.0068)
Batch 400/537: Loss=1.5037 (C:0.8412, R:0.0066)
Batch 425/537: Loss=1.4449 (C:0.7744, R:0.0067)
Batch 450/537: Loss=1.4564 (C:0.7879, R:0.0067)
Batch 475/537: Loss=1.4438 (C:0.7767, R:0.0067)
Batch 500/537: Loss=1.4363 (C:0.7652, R:0.0067)
Batch 525/537: Loss=1.4302 (C:0.7623, R:0.0067)

============================================================
Epoch 106/200 completed in 29.3s
Train: Loss=1.4447 (C:0.7766, R:0.0067) Ratio=3.83x
Val:   Loss=1.4954 (C:0.8725, R:0.0062) Ratio=2.82x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4954)
============================================================

Epoch 107 Training
----------------------------------------
Batch   0/537: Loss=1.4325 (C:0.7627, R:0.0067)
Batch  25/537: Loss=1.4460 (C:0.7791, R:0.0067)
Batch  50/537: Loss=1.4600 (C:0.7863, R:0.0067)
Batch  75/537: Loss=1.4440 (C:0.7783, R:0.0067)
Batch 100/537: Loss=1.4649 (C:0.7951, R:0.0067)
Batch 125/537: Loss=1.4619 (C:0.7978, R:0.0066)
Batch 150/537: Loss=1.4347 (C:0.7622, R:0.0067)
Batch 175/537: Loss=1.4480 (C:0.7803, R:0.0067)
Batch 200/537: Loss=1.4152 (C:0.7482, R:0.0067)
Batch 225/537: Loss=1.4562 (C:0.7877, R:0.0067)
Batch 250/537: Loss=1.4473 (C:0.7814, R:0.0067)
Batch 275/537: Loss=1.4724 (C:0.8050, R:0.0067)
Batch 300/537: Loss=1.3940 (C:0.7288, R:0.0067)
Batch 325/537: Loss=1.4598 (C:0.7943, R:0.0067)
Batch 350/537: Loss=1.4481 (C:0.7813, R:0.0067)
Batch 375/537: Loss=1.4499 (C:0.7824, R:0.0067)
Batch 400/537: Loss=1.4336 (C:0.7621, R:0.0067)
Batch 425/537: Loss=1.4363 (C:0.7726, R:0.0066)
Batch 450/537: Loss=1.4220 (C:0.7502, R:0.0067)
Batch 475/537: Loss=1.4411 (C:0.7697, R:0.0067)
Batch 500/537: Loss=1.4115 (C:0.7418, R:0.0067)
Batch 525/537: Loss=1.4437 (C:0.7766, R:0.0067)

============================================================
Epoch 107/200 completed in 22.7s
Train: Loss=1.4419 (C:0.7736, R:0.0067) Ratio=3.81x
Val:   Loss=1.5060 (C:0.8838, R:0.0062) Ratio=2.80x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 108 Training
----------------------------------------
Batch   0/537: Loss=1.4340 (C:0.7703, R:0.0066)
Batch  25/537: Loss=1.4505 (C:0.7781, R:0.0067)
Batch  50/537: Loss=1.4507 (C:0.7842, R:0.0067)
Batch  75/537: Loss=1.4390 (C:0.7717, R:0.0067)
Batch 100/537: Loss=1.4162 (C:0.7529, R:0.0066)
Batch 125/537: Loss=1.4133 (C:0.7460, R:0.0067)
Batch 150/537: Loss=1.4568 (C:0.7900, R:0.0067)
Batch 175/537: Loss=1.3993 (C:0.7274, R:0.0067)
Batch 200/537: Loss=1.4148 (C:0.7414, R:0.0067)
Batch 225/537: Loss=1.4515 (C:0.7843, R:0.0067)
Batch 250/537: Loss=1.3984 (C:0.7361, R:0.0066)
Batch 275/537: Loss=1.4395 (C:0.7659, R:0.0067)
Batch 300/537: Loss=1.4758 (C:0.8107, R:0.0067)
Batch 325/537: Loss=1.4085 (C:0.7357, R:0.0067)
Batch 350/537: Loss=1.4141 (C:0.7509, R:0.0066)
Batch 375/537: Loss=1.4021 (C:0.7308, R:0.0067)
Batch 400/537: Loss=1.4327 (C:0.7670, R:0.0067)
Batch 425/537: Loss=1.4344 (C:0.7655, R:0.0067)
Batch 450/537: Loss=1.4393 (C:0.7762, R:0.0066)
Batch 475/537: Loss=1.4368 (C:0.7669, R:0.0067)
Batch 500/537: Loss=1.4482 (C:0.7803, R:0.0067)
Batch 525/537: Loss=1.4178 (C:0.7482, R:0.0067)

============================================================
Epoch 108/200 completed in 23.0s
Train: Loss=1.4384 (C:0.7702, R:0.0067) Ratio=3.83x
Val:   Loss=1.5002 (C:0.8773, R:0.0062) Ratio=2.80x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 109
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.502 ¬± 0.687
    Neg distances: 2.426 ¬± 1.084
    Separation ratio: 4.83x
    Gap: -3.899
    ‚úÖ Excellent global separation!

Epoch 109 Training
----------------------------------------
Batch   0/537: Loss=1.4655 (C:0.7942, R:0.0067)
Batch  25/537: Loss=1.4204 (C:0.7515, R:0.0067)
Batch  50/537: Loss=1.4153 (C:0.7496, R:0.0067)
Batch  75/537: Loss=1.4320 (C:0.7644, R:0.0067)
Batch 100/537: Loss=1.4534 (C:0.7837, R:0.0067)
Batch 125/537: Loss=1.4350 (C:0.7684, R:0.0067)
Batch 150/537: Loss=1.4324 (C:0.7590, R:0.0067)
Batch 175/537: Loss=1.4366 (C:0.7754, R:0.0066)
Batch 200/537: Loss=1.4422 (C:0.7740, R:0.0067)
Batch 225/537: Loss=1.4579 (C:0.7905, R:0.0067)
Batch 250/537: Loss=1.4228 (C:0.7565, R:0.0067)
Batch 275/537: Loss=1.3913 (C:0.7234, R:0.0067)
Batch 300/537: Loss=1.4760 (C:0.8056, R:0.0067)
Batch 325/537: Loss=1.4014 (C:0.7305, R:0.0067)
Batch 350/537: Loss=1.4520 (C:0.7811, R:0.0067)
Batch 375/537: Loss=1.4668 (C:0.7941, R:0.0067)
Batch 400/537: Loss=1.3774 (C:0.7132, R:0.0066)
Batch 425/537: Loss=1.4403 (C:0.7728, R:0.0067)
Batch 450/537: Loss=1.4293 (C:0.7580, R:0.0067)
Batch 475/537: Loss=1.3800 (C:0.7122, R:0.0067)
Batch 500/537: Loss=1.4440 (C:0.7798, R:0.0066)
Batch 525/537: Loss=1.4512 (C:0.7826, R:0.0067)

============================================================
Epoch 109/200 completed in 29.8s
Train: Loss=1.4351 (C:0.7665, R:0.0067) Ratio=3.86x
Val:   Loss=1.4930 (C:0.8706, R:0.0062) Ratio=2.84x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4930)
============================================================

Epoch 110 Training
----------------------------------------
Batch   0/537: Loss=1.4347 (C:0.7673, R:0.0067)
Batch  25/537: Loss=1.3922 (C:0.7241, R:0.0067)
Batch  50/537: Loss=1.4423 (C:0.7749, R:0.0067)
Batch  75/537: Loss=1.4241 (C:0.7559, R:0.0067)
Batch 100/537: Loss=1.4769 (C:0.8048, R:0.0067)
Batch 125/537: Loss=1.4438 (C:0.7725, R:0.0067)
Batch 150/537: Loss=1.4328 (C:0.7648, R:0.0067)
Batch 175/537: Loss=1.4067 (C:0.7316, R:0.0068)
Batch 200/537: Loss=1.4292 (C:0.7665, R:0.0066)
Batch 225/537: Loss=1.4275 (C:0.7617, R:0.0067)
Batch 250/537: Loss=1.4753 (C:0.8070, R:0.0067)
Batch 275/537: Loss=1.4107 (C:0.7410, R:0.0067)
Batch 300/537: Loss=1.4501 (C:0.7755, R:0.0067)
Batch 325/537: Loss=1.4152 (C:0.7474, R:0.0067)
Batch 350/537: Loss=1.4128 (C:0.7399, R:0.0067)
Batch 375/537: Loss=1.4163 (C:0.7570, R:0.0066)
Batch 400/537: Loss=1.4115 (C:0.7376, R:0.0067)
Batch 425/537: Loss=1.4383 (C:0.7708, R:0.0067)
Batch 450/537: Loss=1.4179 (C:0.7517, R:0.0067)
Batch 475/537: Loss=1.4487 (C:0.7800, R:0.0067)
Batch 500/537: Loss=1.4588 (C:0.7870, R:0.0067)
Batch 525/537: Loss=1.4481 (C:0.7772, R:0.0067)

============================================================
Epoch 110/200 completed in 23.4s
Train: Loss=1.4343 (C:0.7657, R:0.0067) Ratio=3.88x
Val:   Loss=1.5123 (C:0.8888, R:0.0062) Ratio=2.83x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 111 Training
----------------------------------------
Batch   0/537: Loss=1.4313 (C:0.7610, R:0.0067)
Batch  25/537: Loss=1.4233 (C:0.7547, R:0.0067)
Batch  50/537: Loss=1.4061 (C:0.7333, R:0.0067)
Batch  75/537: Loss=1.4197 (C:0.7547, R:0.0067)
Batch 100/537: Loss=1.3753 (C:0.7089, R:0.0067)
Batch 125/537: Loss=1.4081 (C:0.7378, R:0.0067)
Batch 150/537: Loss=1.3778 (C:0.7163, R:0.0066)
Batch 175/537: Loss=1.4223 (C:0.7535, R:0.0067)
Batch 200/537: Loss=1.4000 (C:0.7281, R:0.0067)
Batch 225/537: Loss=1.4392 (C:0.7723, R:0.0067)
Batch 250/537: Loss=1.4430 (C:0.7719, R:0.0067)
Batch 275/537: Loss=1.4280 (C:0.7636, R:0.0066)
Batch 300/537: Loss=1.4359 (C:0.7647, R:0.0067)
Batch 325/537: Loss=1.4497 (C:0.7818, R:0.0067)
Batch 350/537: Loss=1.4618 (C:0.7941, R:0.0067)
Batch 375/537: Loss=1.4229 (C:0.7508, R:0.0067)
Batch 400/537: Loss=1.4570 (C:0.7865, R:0.0067)
Batch 425/537: Loss=1.4410 (C:0.7766, R:0.0066)
Batch 450/537: Loss=1.4067 (C:0.7459, R:0.0066)
Batch 475/537: Loss=1.4597 (C:0.7926, R:0.0067)
Batch 500/537: Loss=1.4297 (C:0.7590, R:0.0067)
Batch 525/537: Loss=1.4443 (C:0.7807, R:0.0066)

============================================================
Epoch 111/200 completed in 22.7s
Train: Loss=1.4326 (C:0.7640, R:0.0067) Ratio=3.91x
Val:   Loss=1.4896 (C:0.8667, R:0.0062) Ratio=2.87x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4896)
============================================================

üåç Updating global dataset at epoch 112
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.486 ¬± 0.698
    Neg distances: 2.452 ¬± 1.077
    Separation ratio: 5.04x
    Gap: -3.900
    ‚úÖ Excellent global separation!

Epoch 112 Training
----------------------------------------
Batch   0/537: Loss=1.4143 (C:0.7577, R:0.0066)
Batch  25/537: Loss=1.4306 (C:0.7657, R:0.0066)
Batch  50/537: Loss=1.4087 (C:0.7401, R:0.0067)
Batch  75/537: Loss=1.4229 (C:0.7450, R:0.0068)
Batch 100/537: Loss=1.3891 (C:0.7166, R:0.0067)
Batch 125/537: Loss=1.4598 (C:0.7908, R:0.0067)
Batch 150/537: Loss=1.3890 (C:0.7200, R:0.0067)
Batch 175/537: Loss=1.4343 (C:0.7609, R:0.0067)
Batch 200/537: Loss=1.3784 (C:0.7126, R:0.0067)
Batch 225/537: Loss=1.4218 (C:0.7582, R:0.0066)
Batch 250/537: Loss=1.3391 (C:0.6606, R:0.0068)
Batch 275/537: Loss=1.4233 (C:0.7579, R:0.0067)
Batch 300/537: Loss=1.4175 (C:0.7496, R:0.0067)
Batch 325/537: Loss=1.4437 (C:0.7750, R:0.0067)
Batch 350/537: Loss=1.4360 (C:0.7664, R:0.0067)
Batch 375/537: Loss=1.4023 (C:0.7324, R:0.0067)
Batch 400/537: Loss=1.4258 (C:0.7601, R:0.0067)
Batch 425/537: Loss=1.4011 (C:0.7322, R:0.0067)
Batch 450/537: Loss=1.4422 (C:0.7752, R:0.0067)
Batch 475/537: Loss=1.4205 (C:0.7508, R:0.0067)
Batch 500/537: Loss=1.4244 (C:0.7560, R:0.0067)
Batch 525/537: Loss=1.4031 (C:0.7336, R:0.0067)

============================================================
Epoch 112/200 completed in 29.4s
Train: Loss=1.4126 (C:0.7437, R:0.0067) Ratio=3.83x
Val:   Loss=1.4783 (C:0.8555, R:0.0062) Ratio=2.85x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4783)
============================================================

Epoch 113 Training
----------------------------------------
Batch   0/537: Loss=1.4394 (C:0.7658, R:0.0067)
Batch  25/537: Loss=1.4002 (C:0.7357, R:0.0066)
Batch  50/537: Loss=1.3881 (C:0.7208, R:0.0067)
Batch  75/537: Loss=1.4472 (C:0.7718, R:0.0068)
Batch 100/537: Loss=1.3989 (C:0.7358, R:0.0066)
Batch 125/537: Loss=1.4387 (C:0.7650, R:0.0067)
Batch 150/537: Loss=1.4355 (C:0.7712, R:0.0066)
Batch 175/537: Loss=1.3797 (C:0.7084, R:0.0067)
Batch 200/537: Loss=1.4385 (C:0.7688, R:0.0067)
Batch 225/537: Loss=1.4380 (C:0.7707, R:0.0067)
Batch 250/537: Loss=1.4153 (C:0.7505, R:0.0066)
Batch 275/537: Loss=1.3627 (C:0.6845, R:0.0068)
Batch 300/537: Loss=1.4139 (C:0.7436, R:0.0067)
Batch 325/537: Loss=1.4238 (C:0.7521, R:0.0067)
Batch 350/537: Loss=1.4018 (C:0.7207, R:0.0068)
Batch 375/537: Loss=1.3968 (C:0.7291, R:0.0067)
Batch 400/537: Loss=1.4021 (C:0.7387, R:0.0066)
Batch 425/537: Loss=1.4108 (C:0.7405, R:0.0067)
Batch 450/537: Loss=1.4477 (C:0.7718, R:0.0068)
Batch 475/537: Loss=1.4397 (C:0.7612, R:0.0068)
Batch 500/537: Loss=1.4252 (C:0.7519, R:0.0067)
Batch 525/537: Loss=1.4040 (C:0.7321, R:0.0067)

============================================================
Epoch 113/200 completed in 22.7s
Train: Loss=1.4087 (C:0.7398, R:0.0067) Ratio=3.89x
Val:   Loss=1.4777 (C:0.8550, R:0.0062) Ratio=2.86x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4777)
============================================================

Epoch 114 Training
----------------------------------------
Batch   0/537: Loss=1.3984 (C:0.7258, R:0.0067)
Batch  25/537: Loss=1.3936 (C:0.7259, R:0.0067)
Batch  50/537: Loss=1.4219 (C:0.7548, R:0.0067)
Batch  75/537: Loss=1.3910 (C:0.7201, R:0.0067)
Batch 100/537: Loss=1.3801 (C:0.7091, R:0.0067)
Batch 125/537: Loss=1.4051 (C:0.7324, R:0.0067)
Batch 150/537: Loss=1.3756 (C:0.7023, R:0.0067)
Batch 175/537: Loss=1.3894 (C:0.7172, R:0.0067)
Batch 200/537: Loss=1.3853 (C:0.7162, R:0.0067)
Batch 225/537: Loss=1.3392 (C:0.6773, R:0.0066)
Batch 250/537: Loss=1.4080 (C:0.7384, R:0.0067)
Batch 275/537: Loss=1.4038 (C:0.7337, R:0.0067)
Batch 300/537: Loss=1.4461 (C:0.7802, R:0.0067)
Batch 325/537: Loss=1.3904 (C:0.7201, R:0.0067)
Batch 350/537: Loss=1.4711 (C:0.7971, R:0.0067)
Batch 375/537: Loss=1.4134 (C:0.7444, R:0.0067)
Batch 400/537: Loss=1.4133 (C:0.7376, R:0.0068)
Batch 425/537: Loss=1.4075 (C:0.7358, R:0.0067)
Batch 450/537: Loss=1.4225 (C:0.7526, R:0.0067)
Batch 475/537: Loss=1.4602 (C:0.7884, R:0.0067)
Batch 500/537: Loss=1.4006 (C:0.7314, R:0.0067)
Batch 525/537: Loss=1.4149 (C:0.7425, R:0.0067)

============================================================
Epoch 114/200 completed in 22.7s
Train: Loss=1.4073 (C:0.7383, R:0.0067) Ratio=3.96x
Val:   Loss=1.4802 (C:0.8570, R:0.0062) Ratio=2.82x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 115
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.467 ¬± 0.662
    Neg distances: 2.476 ¬± 1.078
    Separation ratio: 5.30x
    Gap: -3.896
    ‚úÖ Excellent global separation!

Epoch 115 Training
----------------------------------------
Batch   0/537: Loss=1.3309 (C:0.6637, R:0.0067)
Batch  25/537: Loss=1.4249 (C:0.7513, R:0.0067)
Batch  50/537: Loss=1.4001 (C:0.7334, R:0.0067)
Batch  75/537: Loss=1.4052 (C:0.7366, R:0.0067)
Batch 100/537: Loss=1.4409 (C:0.7705, R:0.0067)
Batch 125/537: Loss=1.3886 (C:0.7197, R:0.0067)
Batch 150/537: Loss=1.3798 (C:0.7120, R:0.0067)
Batch 175/537: Loss=1.3928 (C:0.7275, R:0.0067)
Batch 200/537: Loss=1.3666 (C:0.6923, R:0.0067)
Batch 225/537: Loss=1.3936 (C:0.7256, R:0.0067)
Batch 250/537: Loss=1.4050 (C:0.7342, R:0.0067)
Batch 275/537: Loss=1.4364 (C:0.7648, R:0.0067)
Batch 300/537: Loss=1.4512 (C:0.7827, R:0.0067)
Batch 325/537: Loss=1.3932 (C:0.7249, R:0.0067)
Batch 350/537: Loss=1.3985 (C:0.7235, R:0.0068)
Batch 375/537: Loss=1.4070 (C:0.7434, R:0.0066)
Batch 400/537: Loss=1.3909 (C:0.7161, R:0.0067)
Batch 425/537: Loss=1.3576 (C:0.6869, R:0.0067)
Batch 450/537: Loss=1.3552 (C:0.6841, R:0.0067)
Batch 475/537: Loss=1.4099 (C:0.7416, R:0.0067)
Batch 500/537: Loss=1.4015 (C:0.7323, R:0.0067)
Batch 525/537: Loss=1.3998 (C:0.7315, R:0.0067)

============================================================
Epoch 115/200 completed in 29.3s
Train: Loss=1.3903 (C:0.7213, R:0.0067) Ratio=3.92x
Val:   Loss=1.4492 (C:0.8259, R:0.0062) Ratio=2.85x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4492)
============================================================

Epoch 116 Training
----------------------------------------
Batch   0/537: Loss=1.4222 (C:0.7560, R:0.0067)
Batch  25/537: Loss=1.3871 (C:0.7174, R:0.0067)
Batch  50/537: Loss=1.3899 (C:0.7239, R:0.0067)
Batch  75/537: Loss=1.3961 (C:0.7259, R:0.0067)
Batch 100/537: Loss=1.4060 (C:0.7327, R:0.0067)
Batch 125/537: Loss=1.3838 (C:0.7146, R:0.0067)
Batch 150/537: Loss=1.3461 (C:0.6774, R:0.0067)
Batch 175/537: Loss=1.3799 (C:0.7132, R:0.0067)
Batch 200/537: Loss=1.3427 (C:0.6735, R:0.0067)
Batch 225/537: Loss=1.3597 (C:0.6894, R:0.0067)
Batch 250/537: Loss=1.3717 (C:0.7061, R:0.0067)
Batch 275/537: Loss=1.4168 (C:0.7494, R:0.0067)
Batch 300/537: Loss=1.3986 (C:0.7260, R:0.0067)
Batch 325/537: Loss=1.4007 (C:0.7323, R:0.0067)
Batch 350/537: Loss=1.3697 (C:0.6968, R:0.0067)
Batch 375/537: Loss=1.3972 (C:0.7273, R:0.0067)
Batch 400/537: Loss=1.3815 (C:0.7078, R:0.0067)
Batch 425/537: Loss=1.4105 (C:0.7397, R:0.0067)
Batch 450/537: Loss=1.3721 (C:0.6979, R:0.0067)
Batch 475/537: Loss=1.4154 (C:0.7497, R:0.0067)
Batch 500/537: Loss=1.3844 (C:0.7127, R:0.0067)
Batch 525/537: Loss=1.4221 (C:0.7534, R:0.0067)

============================================================
Epoch 116/200 completed in 22.4s
Train: Loss=1.3869 (C:0.7178, R:0.0067) Ratio=3.92x
Val:   Loss=1.4564 (C:0.8330, R:0.0062) Ratio=2.82x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 117 Training
----------------------------------------
Batch   0/537: Loss=1.3928 (C:0.7257, R:0.0067)
Batch  25/537: Loss=1.3917 (C:0.7246, R:0.0067)
Batch  50/537: Loss=1.3533 (C:0.6838, R:0.0067)
Batch  75/537: Loss=1.3922 (C:0.7248, R:0.0067)
Batch 100/537: Loss=1.3785 (C:0.7162, R:0.0066)
Batch 125/537: Loss=1.3445 (C:0.6821, R:0.0066)
Batch 150/537: Loss=1.4183 (C:0.7486, R:0.0067)
Batch 175/537: Loss=1.3886 (C:0.7151, R:0.0067)
Batch 200/537: Loss=1.4007 (C:0.7334, R:0.0067)
Batch 225/537: Loss=1.3700 (C:0.7047, R:0.0067)
Batch 250/537: Loss=1.3536 (C:0.6872, R:0.0067)
Batch 275/537: Loss=1.4071 (C:0.7372, R:0.0067)
Batch 300/537: Loss=1.3828 (C:0.7144, R:0.0067)
Batch 325/537: Loss=1.3778 (C:0.7037, R:0.0067)
Batch 350/537: Loss=1.3739 (C:0.7066, R:0.0067)
Batch 375/537: Loss=1.4010 (C:0.7273, R:0.0067)
Batch 400/537: Loss=1.3839 (C:0.7123, R:0.0067)
Batch 425/537: Loss=1.4029 (C:0.7348, R:0.0067)
Batch 450/537: Loss=1.3500 (C:0.6828, R:0.0067)
Batch 475/537: Loss=1.3944 (C:0.7234, R:0.0067)
Batch 500/537: Loss=1.4102 (C:0.7437, R:0.0067)
Batch 525/537: Loss=1.3879 (C:0.7110, R:0.0068)

============================================================
Epoch 117/200 completed in 22.5s
Train: Loss=1.3862 (C:0.7171, R:0.0067) Ratio=3.97x
Val:   Loss=1.4677 (C:0.8447, R:0.0062) Ratio=2.79x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 118
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.461 ¬± 0.649
    Neg distances: 2.500 ¬± 1.085
    Separation ratio: 5.43x
    Gap: -3.956
    ‚úÖ Excellent global separation!

Epoch 118 Training
----------------------------------------
Batch   0/537: Loss=1.3976 (C:0.7338, R:0.0066)
Batch  25/537: Loss=1.3543 (C:0.6787, R:0.0068)
Batch  50/537: Loss=1.4146 (C:0.7375, R:0.0068)
Batch  75/537: Loss=1.3576 (C:0.6882, R:0.0067)
Batch 100/537: Loss=1.4181 (C:0.7445, R:0.0067)
Batch 125/537: Loss=1.3853 (C:0.7143, R:0.0067)
Batch 150/537: Loss=1.3812 (C:0.7137, R:0.0067)
Batch 175/537: Loss=1.3821 (C:0.7185, R:0.0066)
Batch 200/537: Loss=1.3681 (C:0.6973, R:0.0067)
Batch 225/537: Loss=1.3908 (C:0.7224, R:0.0067)
Batch 250/537: Loss=1.3925 (C:0.7216, R:0.0067)
Batch 275/537: Loss=1.3843 (C:0.7161, R:0.0067)
Batch 300/537: Loss=1.4174 (C:0.7465, R:0.0067)
Batch 325/537: Loss=1.3697 (C:0.7012, R:0.0067)
Batch 350/537: Loss=1.3821 (C:0.7168, R:0.0067)
Batch 375/537: Loss=1.3684 (C:0.6996, R:0.0067)
Batch 400/537: Loss=1.3422 (C:0.6782, R:0.0066)
Batch 425/537: Loss=1.3816 (C:0.7104, R:0.0067)
Batch 450/537: Loss=1.3988 (C:0.7249, R:0.0067)
Batch 475/537: Loss=1.3505 (C:0.6766, R:0.0067)
Batch 500/537: Loss=1.4050 (C:0.7423, R:0.0066)
Batch 525/537: Loss=1.3577 (C:0.6873, R:0.0067)

============================================================
Epoch 118/200 completed in 28.9s
Train: Loss=1.3734 (C:0.7041, R:0.0067) Ratio=3.94x
Val:   Loss=1.4367 (C:0.8129, R:0.0062) Ratio=2.88x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4367)
============================================================

Epoch 119 Training
----------------------------------------
Batch   0/537: Loss=1.3273 (C:0.6585, R:0.0067)
Batch  25/537: Loss=1.3749 (C:0.7025, R:0.0067)
Batch  50/537: Loss=1.3523 (C:0.6825, R:0.0067)
Batch  75/537: Loss=1.3766 (C:0.7089, R:0.0067)
Batch 100/537: Loss=1.3755 (C:0.7007, R:0.0067)
Batch 125/537: Loss=1.3908 (C:0.7151, R:0.0068)
Batch 150/537: Loss=1.3896 (C:0.7199, R:0.0067)
Batch 175/537: Loss=1.3491 (C:0.6839, R:0.0067)
Batch 200/537: Loss=1.3218 (C:0.6589, R:0.0066)
Batch 225/537: Loss=1.3636 (C:0.6973, R:0.0067)
Batch 250/537: Loss=1.3768 (C:0.7058, R:0.0067)
Batch 275/537: Loss=1.3718 (C:0.7004, R:0.0067)
Batch 300/537: Loss=1.3738 (C:0.7069, R:0.0067)
Batch 325/537: Loss=1.3688 (C:0.7027, R:0.0067)
Batch 350/537: Loss=1.4051 (C:0.7309, R:0.0067)
Batch 375/537: Loss=1.3553 (C:0.6891, R:0.0067)
Batch 400/537: Loss=1.3778 (C:0.7085, R:0.0067)
Batch 425/537: Loss=1.3605 (C:0.6917, R:0.0067)
Batch 450/537: Loss=1.3331 (C:0.6678, R:0.0067)
Batch 475/537: Loss=1.3657 (C:0.6927, R:0.0067)
Batch 500/537: Loss=1.3904 (C:0.7202, R:0.0067)
Batch 525/537: Loss=1.3513 (C:0.6830, R:0.0067)

============================================================
Epoch 119/200 completed in 22.6s
Train: Loss=1.3724 (C:0.7030, R:0.0067) Ratio=4.08x
Val:   Loss=1.4418 (C:0.8179, R:0.0062) Ratio=2.86x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 120 Training
----------------------------------------
Batch   0/537: Loss=1.3157 (C:0.6364, R:0.0068)
Batch  25/537: Loss=1.3700 (C:0.7106, R:0.0066)
Batch  50/537: Loss=1.3840 (C:0.7169, R:0.0067)
Batch  75/537: Loss=1.3696 (C:0.7039, R:0.0067)
Batch 100/537: Loss=1.3560 (C:0.6865, R:0.0067)
Batch 125/537: Loss=1.3565 (C:0.6837, R:0.0067)
Batch 150/537: Loss=1.3406 (C:0.6752, R:0.0067)
Batch 175/537: Loss=1.3463 (C:0.6787, R:0.0067)
Batch 200/537: Loss=1.3924 (C:0.7200, R:0.0067)
Batch 225/537: Loss=1.3596 (C:0.6845, R:0.0068)
Batch 250/537: Loss=1.4086 (C:0.7364, R:0.0067)
Batch 275/537: Loss=1.3697 (C:0.7003, R:0.0067)
Batch 300/537: Loss=1.3230 (C:0.6555, R:0.0067)
Batch 325/537: Loss=1.3329 (C:0.6625, R:0.0067)
Batch 350/537: Loss=1.4113 (C:0.7377, R:0.0067)
Batch 375/537: Loss=1.3886 (C:0.7191, R:0.0067)
Batch 400/537: Loss=1.3987 (C:0.7265, R:0.0067)
Batch 425/537: Loss=1.3684 (C:0.7065, R:0.0066)
Batch 450/537: Loss=1.3791 (C:0.7108, R:0.0067)
Batch 475/537: Loss=1.3926 (C:0.7154, R:0.0068)
Batch 500/537: Loss=1.3881 (C:0.7130, R:0.0068)
Batch 525/537: Loss=1.3933 (C:0.7217, R:0.0067)

============================================================
Epoch 120/200 completed in 22.4s
Train: Loss=1.3721 (C:0.7027, R:0.0067) Ratio=4.05x
Val:   Loss=1.4422 (C:0.8179, R:0.0062) Ratio=2.85x
Reconstruction weight: 100.000
No improvement for 2 epochs
Checkpoint saved at epoch 120
============================================================

üåç Updating global dataset at epoch 121
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.460 ¬± 0.644
    Neg distances: 2.537 ¬± 1.104
    Separation ratio: 5.51x
    Gap: -4.023
    ‚úÖ Excellent global separation!

Epoch 121 Training
----------------------------------------
Batch   0/537: Loss=1.3756 (C:0.6965, R:0.0068)
Batch  25/537: Loss=1.3611 (C:0.6960, R:0.0067)
Batch  50/537: Loss=1.3563 (C:0.6884, R:0.0067)
Batch  75/537: Loss=1.3686 (C:0.6932, R:0.0068)
Batch 100/537: Loss=1.3444 (C:0.6760, R:0.0067)
Batch 125/537: Loss=1.3673 (C:0.6979, R:0.0067)
Batch 150/537: Loss=1.3553 (C:0.6840, R:0.0067)
Batch 175/537: Loss=1.3700 (C:0.6995, R:0.0067)
Batch 200/537: Loss=1.3687 (C:0.6984, R:0.0067)
Batch 225/537: Loss=1.3993 (C:0.7288, R:0.0067)
Batch 250/537: Loss=1.3297 (C:0.6606, R:0.0067)
Batch 275/537: Loss=1.3605 (C:0.6899, R:0.0067)
Batch 300/537: Loss=1.3433 (C:0.6737, R:0.0067)
Batch 325/537: Loss=1.3769 (C:0.7071, R:0.0067)
Batch 350/537: Loss=1.3358 (C:0.6665, R:0.0067)
Batch 375/537: Loss=1.3841 (C:0.7096, R:0.0067)
Batch 400/537: Loss=1.3679 (C:0.7027, R:0.0067)
Batch 425/537: Loss=1.3406 (C:0.6713, R:0.0067)
Batch 450/537: Loss=1.3474 (C:0.6771, R:0.0067)
Batch 475/537: Loss=1.3396 (C:0.6672, R:0.0067)
Batch 500/537: Loss=1.3658 (C:0.6973, R:0.0067)
Batch 525/537: Loss=1.3407 (C:0.6685, R:0.0067)

============================================================
Epoch 121/200 completed in 29.2s
Train: Loss=1.3670 (C:0.6975, R:0.0067) Ratio=4.08x
Val:   Loss=1.4197 (C:0.7961, R:0.0062) Ratio=2.93x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4197)
============================================================

Epoch 122 Training
----------------------------------------
Batch   0/537: Loss=1.4248 (C:0.7528, R:0.0067)
Batch  25/537: Loss=1.3483 (C:0.6797, R:0.0067)
Batch  50/537: Loss=1.3600 (C:0.6937, R:0.0067)
Batch  75/537: Loss=1.3923 (C:0.7259, R:0.0067)
Batch 100/537: Loss=1.3472 (C:0.6789, R:0.0067)
Batch 125/537: Loss=1.3870 (C:0.7219, R:0.0067)
Batch 150/537: Loss=1.3496 (C:0.6820, R:0.0067)
Batch 175/537: Loss=1.4030 (C:0.7318, R:0.0067)
Batch 200/537: Loss=1.3698 (C:0.6980, R:0.0067)
Batch 225/537: Loss=1.3618 (C:0.6939, R:0.0067)
Batch 250/537: Loss=1.3627 (C:0.6904, R:0.0067)
Batch 275/537: Loss=1.3562 (C:0.6827, R:0.0067)
Batch 300/537: Loss=1.3575 (C:0.6876, R:0.0067)
Batch 325/537: Loss=1.3655 (C:0.6847, R:0.0068)
Batch 350/537: Loss=1.3537 (C:0.6875, R:0.0067)
Batch 375/537: Loss=1.3606 (C:0.6934, R:0.0067)
Batch 400/537: Loss=1.3557 (C:0.6896, R:0.0067)
Batch 425/537: Loss=1.3332 (C:0.6610, R:0.0067)
Batch 450/537: Loss=1.3847 (C:0.7193, R:0.0067)
Batch 475/537: Loss=1.3394 (C:0.6666, R:0.0067)
Batch 500/537: Loss=1.3618 (C:0.6954, R:0.0067)
Batch 525/537: Loss=1.4073 (C:0.7380, R:0.0067)

============================================================
Epoch 122/200 completed in 22.5s
Train: Loss=1.3699 (C:0.7004, R:0.0067) Ratio=4.09x
Val:   Loss=1.4348 (C:0.8114, R:0.0062) Ratio=2.93x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 123 Training
----------------------------------------
Batch   0/537: Loss=1.3905 (C:0.7212, R:0.0067)
Batch  25/537: Loss=1.3980 (C:0.7221, R:0.0068)
Batch  50/537: Loss=1.4114 (C:0.7407, R:0.0067)
Batch  75/537: Loss=1.3952 (C:0.7268, R:0.0067)
Batch 100/537: Loss=1.3379 (C:0.6648, R:0.0067)
Batch 125/537: Loss=1.3483 (C:0.6759, R:0.0067)
Batch 150/537: Loss=1.3708 (C:0.7013, R:0.0067)
Batch 175/537: Loss=1.3525 (C:0.6844, R:0.0067)
Batch 200/537: Loss=1.3286 (C:0.6632, R:0.0067)
Batch 225/537: Loss=1.3342 (C:0.6646, R:0.0067)
Batch 250/537: Loss=1.3924 (C:0.7202, R:0.0067)
Batch 275/537: Loss=1.3512 (C:0.6808, R:0.0067)
Batch 300/537: Loss=1.3348 (C:0.6638, R:0.0067)
Batch 325/537: Loss=1.3688 (C:0.6950, R:0.0067)
Batch 350/537: Loss=1.4054 (C:0.7397, R:0.0067)
Batch 375/537: Loss=1.4009 (C:0.7324, R:0.0067)
Batch 400/537: Loss=1.3652 (C:0.6959, R:0.0067)
Batch 425/537: Loss=1.3592 (C:0.6936, R:0.0067)
Batch 450/537: Loss=1.4235 (C:0.7481, R:0.0068)
Batch 475/537: Loss=1.3556 (C:0.6891, R:0.0067)
Batch 500/537: Loss=1.3482 (C:0.6830, R:0.0067)
Batch 525/537: Loss=1.3770 (C:0.7036, R:0.0067)

============================================================
Epoch 123/200 completed in 22.8s
Train: Loss=1.3668 (C:0.6971, R:0.0067) Ratio=4.03x
Val:   Loss=1.4468 (C:0.8233, R:0.0062) Ratio=2.82x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 124
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.466 ¬± 0.657
    Neg distances: 2.529 ¬± 1.104
    Separation ratio: 5.42x
    Gap: -4.026
    ‚úÖ Excellent global separation!

Epoch 124 Training
----------------------------------------
Batch   0/537: Loss=1.3677 (C:0.6997, R:0.0067)
Batch  25/537: Loss=1.3278 (C:0.6614, R:0.0067)
Batch  50/537: Loss=1.3793 (C:0.7056, R:0.0067)
Batch  75/537: Loss=1.3950 (C:0.7172, R:0.0068)
Batch 100/537: Loss=1.3130 (C:0.6470, R:0.0067)
Batch 125/537: Loss=1.2961 (C:0.6255, R:0.0067)
Batch 150/537: Loss=1.3441 (C:0.6843, R:0.0066)
Batch 175/537: Loss=1.3908 (C:0.7195, R:0.0067)
Batch 200/537: Loss=1.3637 (C:0.6913, R:0.0067)
Batch 225/537: Loss=1.3656 (C:0.7034, R:0.0066)
Batch 250/537: Loss=1.3380 (C:0.6724, R:0.0067)
Batch 275/537: Loss=1.3510 (C:0.6834, R:0.0067)
Batch 300/537: Loss=1.3740 (C:0.7096, R:0.0066)
Batch 325/537: Loss=1.3703 (C:0.6956, R:0.0067)
Batch 350/537: Loss=1.3985 (C:0.7347, R:0.0066)
Batch 375/537: Loss=1.3369 (C:0.6695, R:0.0067)
Batch 400/537: Loss=1.3827 (C:0.7152, R:0.0067)
Batch 425/537: Loss=1.3856 (C:0.7066, R:0.0068)
Batch 450/537: Loss=1.3556 (C:0.6867, R:0.0067)
Batch 475/537: Loss=1.3647 (C:0.6922, R:0.0067)
Batch 500/537: Loss=1.3697 (C:0.7049, R:0.0066)
Batch 525/537: Loss=1.3647 (C:0.6936, R:0.0067)

============================================================
Epoch 124/200 completed in 29.3s
Train: Loss=1.3654 (C:0.6958, R:0.0067) Ratio=4.03x
Val:   Loss=1.4534 (C:0.8299, R:0.0062) Ratio=2.86x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 125 Training
----------------------------------------
Batch   0/537: Loss=1.3837 (C:0.7117, R:0.0067)
Batch  25/537: Loss=1.3670 (C:0.6971, R:0.0067)
Batch  50/537: Loss=1.3749 (C:0.7030, R:0.0067)
Batch  75/537: Loss=1.3537 (C:0.6827, R:0.0067)
Batch 100/537: Loss=1.3691 (C:0.6929, R:0.0068)
Batch 125/537: Loss=1.3578 (C:0.6875, R:0.0067)
Batch 150/537: Loss=1.3459 (C:0.6792, R:0.0067)
Batch 175/537: Loss=1.3868 (C:0.7170, R:0.0067)
Batch 200/537: Loss=1.4020 (C:0.7307, R:0.0067)
Batch 225/537: Loss=1.3114 (C:0.6447, R:0.0067)
Batch 250/537: Loss=1.3437 (C:0.6746, R:0.0067)
Batch 275/537: Loss=1.3793 (C:0.7122, R:0.0067)
Batch 300/537: Loss=1.3599 (C:0.6940, R:0.0067)
Batch 325/537: Loss=1.3857 (C:0.7167, R:0.0067)
Batch 350/537: Loss=1.3632 (C:0.6978, R:0.0067)
Batch 375/537: Loss=1.3576 (C:0.6909, R:0.0067)
Batch 400/537: Loss=1.4044 (C:0.7394, R:0.0066)
Batch 425/537: Loss=1.3630 (C:0.6939, R:0.0067)
Batch 450/537: Loss=1.3294 (C:0.6562, R:0.0067)
Batch 475/537: Loss=1.3670 (C:0.6956, R:0.0067)
Batch 500/537: Loss=1.3738 (C:0.7120, R:0.0066)
Batch 525/537: Loss=1.3615 (C:0.6818, R:0.0068)

============================================================
Epoch 125/200 completed in 22.5s
Train: Loss=1.3643 (C:0.6946, R:0.0067) Ratio=4.03x
Val:   Loss=1.4381 (C:0.8147, R:0.0062) Ratio=2.86x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 126 Training
----------------------------------------
Batch   0/537: Loss=1.3142 (C:0.6458, R:0.0067)
Batch  25/537: Loss=1.3501 (C:0.6874, R:0.0066)
Batch  50/537: Loss=1.3625 (C:0.6933, R:0.0067)
Batch  75/537: Loss=1.3028 (C:0.6368, R:0.0067)
Batch 100/537: Loss=1.3563 (C:0.6867, R:0.0067)
Batch 125/537: Loss=1.3573 (C:0.6815, R:0.0068)
Batch 150/537: Loss=1.3524 (C:0.6884, R:0.0066)
Batch 175/537: Loss=1.3500 (C:0.6807, R:0.0067)
Batch 200/537: Loss=1.3565 (C:0.6883, R:0.0067)
Batch 225/537: Loss=1.3669 (C:0.6966, R:0.0067)
Batch 250/537: Loss=1.3359 (C:0.6658, R:0.0067)
Batch 275/537: Loss=1.3662 (C:0.6947, R:0.0067)
Batch 300/537: Loss=1.3289 (C:0.6587, R:0.0067)
Batch 325/537: Loss=1.3742 (C:0.7042, R:0.0067)
Batch 350/537: Loss=1.3515 (C:0.6864, R:0.0067)
Batch 375/537: Loss=1.3578 (C:0.6882, R:0.0067)
Batch 400/537: Loss=1.3745 (C:0.7047, R:0.0067)
Batch 425/537: Loss=1.3590 (C:0.6922, R:0.0067)
Batch 450/537: Loss=1.3532 (C:0.6832, R:0.0067)
Batch 475/537: Loss=1.3497 (C:0.6813, R:0.0067)
Batch 500/537: Loss=1.3273 (C:0.6589, R:0.0067)
Batch 525/537: Loss=1.3950 (C:0.7308, R:0.0066)

============================================================
Epoch 126/200 completed in 22.7s
Train: Loss=1.3627 (C:0.6930, R:0.0067) Ratio=4.16x
Val:   Loss=1.4401 (C:0.8167, R:0.0062) Ratio=2.85x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 127
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.453 ¬± 0.651
    Neg distances: 2.551 ¬± 1.100
    Separation ratio: 5.63x
    Gap: -4.053
    ‚úÖ Excellent global separation!

Epoch 127 Training
----------------------------------------
Batch   0/537: Loss=1.3098 (C:0.6373, R:0.0067)
Batch  25/537: Loss=1.3287 (C:0.6637, R:0.0067)
Batch  50/537: Loss=1.3639 (C:0.6915, R:0.0067)
Batch  75/537: Loss=1.3669 (C:0.6987, R:0.0067)
Batch 100/537: Loss=1.3626 (C:0.6929, R:0.0067)
Batch 125/537: Loss=1.3437 (C:0.6759, R:0.0067)
Batch 150/537: Loss=1.2864 (C:0.6151, R:0.0067)
Batch 175/537: Loss=1.3488 (C:0.6768, R:0.0067)
Batch 200/537: Loss=1.3317 (C:0.6674, R:0.0066)
Batch 225/537: Loss=1.3403 (C:0.6637, R:0.0068)
Batch 250/537: Loss=1.3374 (C:0.6658, R:0.0067)
Batch 275/537: Loss=1.3079 (C:0.6358, R:0.0067)
Batch 300/537: Loss=1.3471 (C:0.6774, R:0.0067)
Batch 325/537: Loss=1.3203 (C:0.6467, R:0.0067)
Batch 350/537: Loss=1.3594 (C:0.6835, R:0.0068)
Batch 375/537: Loss=1.3341 (C:0.6649, R:0.0067)
Batch 400/537: Loss=1.3310 (C:0.6640, R:0.0067)
Batch 425/537: Loss=1.3252 (C:0.6523, R:0.0067)
Batch 450/537: Loss=1.3617 (C:0.6942, R:0.0067)
Batch 475/537: Loss=1.4013 (C:0.7311, R:0.0067)
Batch 500/537: Loss=1.3844 (C:0.7097, R:0.0067)
Batch 525/537: Loss=1.3575 (C:0.6873, R:0.0067)

============================================================
Epoch 127/200 completed in 29.7s
Train: Loss=1.3485 (C:0.6785, R:0.0067) Ratio=4.14x
Val:   Loss=1.4152 (C:0.7922, R:0.0062) Ratio=2.89x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4152)
============================================================

Epoch 128 Training
----------------------------------------
Batch   0/537: Loss=1.3691 (C:0.7013, R:0.0067)
Batch  25/537: Loss=1.3365 (C:0.6651, R:0.0067)
Batch  50/537: Loss=1.3598 (C:0.6923, R:0.0067)
Batch  75/537: Loss=1.3693 (C:0.6935, R:0.0068)
Batch 100/537: Loss=1.3079 (C:0.6353, R:0.0067)
Batch 125/537: Loss=1.3835 (C:0.7193, R:0.0066)
Batch 150/537: Loss=1.3443 (C:0.6753, R:0.0067)
Batch 175/537: Loss=1.3757 (C:0.7072, R:0.0067)
Batch 200/537: Loss=1.3198 (C:0.6474, R:0.0067)
Batch 225/537: Loss=1.3464 (C:0.6726, R:0.0067)
Batch 250/537: Loss=1.3245 (C:0.6531, R:0.0067)
Batch 275/537: Loss=1.3450 (C:0.6723, R:0.0067)
Batch 300/537: Loss=1.3297 (C:0.6619, R:0.0067)
Batch 325/537: Loss=1.3550 (C:0.6824, R:0.0067)
Batch 350/537: Loss=1.3874 (C:0.7156, R:0.0067)
Batch 375/537: Loss=1.3185 (C:0.6528, R:0.0067)
Batch 400/537: Loss=1.3166 (C:0.6389, R:0.0068)
Batch 425/537: Loss=1.3533 (C:0.6822, R:0.0067)
Batch 450/537: Loss=1.3663 (C:0.6983, R:0.0067)
Batch 475/537: Loss=1.3490 (C:0.6766, R:0.0067)
Batch 500/537: Loss=1.3674 (C:0.6998, R:0.0067)
Batch 525/537: Loss=1.3280 (C:0.6687, R:0.0066)

============================================================
Epoch 128/200 completed in 22.9s
Train: Loss=1.3478 (C:0.6779, R:0.0067) Ratio=4.13x
Val:   Loss=1.4185 (C:0.7958, R:0.0062) Ratio=2.88x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 129 Training
----------------------------------------
Batch   0/537: Loss=1.3326 (C:0.6673, R:0.0067)
Batch  25/537: Loss=1.3103 (C:0.6373, R:0.0067)
Batch  50/537: Loss=1.3107 (C:0.6343, R:0.0068)
Batch  75/537: Loss=1.3223 (C:0.6486, R:0.0067)
Batch 100/537: Loss=1.3505 (C:0.6796, R:0.0067)
Batch 125/537: Loss=1.3455 (C:0.6764, R:0.0067)
Batch 150/537: Loss=1.3086 (C:0.6414, R:0.0067)
Batch 175/537: Loss=1.3756 (C:0.7079, R:0.0067)
Batch 200/537: Loss=1.3524 (C:0.6715, R:0.0068)
Batch 225/537: Loss=1.3732 (C:0.7016, R:0.0067)
Batch 250/537: Loss=1.3649 (C:0.6985, R:0.0067)
Batch 275/537: Loss=1.3641 (C:0.6817, R:0.0068)
Batch 300/537: Loss=1.3617 (C:0.6928, R:0.0067)
Batch 325/537: Loss=1.3169 (C:0.6513, R:0.0067)
Batch 350/537: Loss=1.3920 (C:0.7171, R:0.0067)
Batch 375/537: Loss=1.3314 (C:0.6619, R:0.0067)
Batch 400/537: Loss=1.3893 (C:0.7255, R:0.0066)
Batch 425/537: Loss=1.3461 (C:0.6830, R:0.0066)
Batch 450/537: Loss=1.3293 (C:0.6584, R:0.0067)
Batch 475/537: Loss=1.3823 (C:0.7154, R:0.0067)
Batch 500/537: Loss=1.3381 (C:0.6647, R:0.0067)
Batch 525/537: Loss=1.3612 (C:0.6915, R:0.0067)

============================================================
Epoch 129/200 completed in 22.5s
Train: Loss=1.3467 (C:0.6769, R:0.0067) Ratio=4.08x
Val:   Loss=1.4329 (C:0.8095, R:0.0062) Ratio=2.85x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 130
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.458 ¬± 0.646
    Neg distances: 2.545 ¬± 1.105
    Separation ratio: 5.56x
    Gap: -4.046
    ‚úÖ Excellent global separation!

Epoch 130 Training
----------------------------------------
Batch   0/537: Loss=1.3415 (C:0.6679, R:0.0067)
Batch  25/537: Loss=1.3493 (C:0.6764, R:0.0067)
Batch  50/537: Loss=1.3615 (C:0.6879, R:0.0067)
Batch  75/537: Loss=1.3468 (C:0.6773, R:0.0067)
Batch 100/537: Loss=1.3413 (C:0.6670, R:0.0067)
Batch 125/537: Loss=1.3476 (C:0.6774, R:0.0067)
Batch 150/537: Loss=1.3479 (C:0.6764, R:0.0067)
Batch 175/537: Loss=1.3605 (C:0.6911, R:0.0067)
Batch 200/537: Loss=1.3761 (C:0.7053, R:0.0067)
Batch 225/537: Loss=1.3882 (C:0.7187, R:0.0067)
Batch 250/537: Loss=1.3616 (C:0.6902, R:0.0067)
Batch 275/537: Loss=1.3975 (C:0.7261, R:0.0067)
Batch 300/537: Loss=1.3854 (C:0.7141, R:0.0067)
Batch 325/537: Loss=1.3705 (C:0.7022, R:0.0067)
Batch 350/537: Loss=1.3439 (C:0.6722, R:0.0067)
Batch 375/537: Loss=1.3905 (C:0.7151, R:0.0068)
Batch 400/537: Loss=1.3852 (C:0.7113, R:0.0067)
Batch 425/537: Loss=1.4012 (C:0.7332, R:0.0067)
Batch 450/537: Loss=1.3103 (C:0.6395, R:0.0067)
Batch 475/537: Loss=1.3963 (C:0.7231, R:0.0067)
Batch 500/537: Loss=1.3627 (C:0.6916, R:0.0067)
Batch 525/537: Loss=1.3563 (C:0.6935, R:0.0066)

============================================================
Epoch 130/200 completed in 29.2s
Train: Loss=1.3507 (C:0.6808, R:0.0067) Ratio=3.99x
Val:   Loss=1.4289 (C:0.8056, R:0.0062) Ratio=2.85x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 131 Training
----------------------------------------
Batch   0/537: Loss=1.3348 (C:0.6693, R:0.0067)
Batch  25/537: Loss=1.3462 (C:0.6754, R:0.0067)
Batch  50/537: Loss=1.3520 (C:0.6832, R:0.0067)
Batch  75/537: Loss=1.3471 (C:0.6811, R:0.0067)
Batch 100/537: Loss=1.3815 (C:0.7087, R:0.0067)
Batch 125/537: Loss=1.3921 (C:0.7218, R:0.0067)
Batch 150/537: Loss=1.3472 (C:0.6767, R:0.0067)
Batch 175/537: Loss=1.3598 (C:0.6950, R:0.0066)
Batch 200/537: Loss=1.3827 (C:0.7165, R:0.0067)
Batch 225/537: Loss=1.3117 (C:0.6422, R:0.0067)
Batch 250/537: Loss=1.3396 (C:0.6650, R:0.0067)
Batch 275/537: Loss=1.3668 (C:0.6981, R:0.0067)
Batch 300/537: Loss=1.3892 (C:0.7120, R:0.0068)
Batch 325/537: Loss=1.3050 (C:0.6354, R:0.0067)
Batch 350/537: Loss=1.3388 (C:0.6722, R:0.0067)
Batch 375/537: Loss=1.3699 (C:0.6997, R:0.0067)
Batch 400/537: Loss=1.3403 (C:0.6745, R:0.0067)
Batch 425/537: Loss=1.3492 (C:0.6709, R:0.0068)
Batch 450/537: Loss=1.3740 (C:0.7077, R:0.0067)
Batch 475/537: Loss=1.3477 (C:0.6758, R:0.0067)
Batch 500/537: Loss=1.3800 (C:0.7118, R:0.0067)
Batch 525/537: Loss=1.3528 (C:0.6777, R:0.0068)

============================================================
Epoch 131/200 completed in 22.6s
Train: Loss=1.3508 (C:0.6810, R:0.0067) Ratio=4.08x
Val:   Loss=1.4213 (C:0.7974, R:0.0062) Ratio=2.90x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 132 Training
----------------------------------------
Batch   0/537: Loss=1.3798 (C:0.7112, R:0.0067)
Batch  25/537: Loss=1.3256 (C:0.6546, R:0.0067)
Batch  50/537: Loss=1.2989 (C:0.6253, R:0.0067)
Batch  75/537: Loss=1.3187 (C:0.6466, R:0.0067)
Batch 100/537: Loss=1.3235 (C:0.6519, R:0.0067)
Batch 125/537: Loss=1.3019 (C:0.6283, R:0.0067)
Batch 150/537: Loss=1.3521 (C:0.6832, R:0.0067)
Batch 175/537: Loss=1.3194 (C:0.6515, R:0.0067)
Batch 200/537: Loss=1.3387 (C:0.6682, R:0.0067)
Batch 225/537: Loss=1.3196 (C:0.6515, R:0.0067)
Batch 250/537: Loss=1.3247 (C:0.6515, R:0.0067)
Batch 275/537: Loss=1.3273 (C:0.6522, R:0.0068)
Batch 300/537: Loss=1.3650 (C:0.6953, R:0.0067)
Batch 325/537: Loss=1.3330 (C:0.6610, R:0.0067)
Batch 350/537: Loss=1.3339 (C:0.6694, R:0.0066)
Batch 375/537: Loss=1.3353 (C:0.6638, R:0.0067)
Batch 400/537: Loss=1.3214 (C:0.6480, R:0.0067)
Batch 425/537: Loss=1.3627 (C:0.6943, R:0.0067)
Batch 450/537: Loss=1.3893 (C:0.7240, R:0.0067)
Batch 475/537: Loss=1.3353 (C:0.6689, R:0.0067)
Batch 500/537: Loss=1.3400 (C:0.6680, R:0.0067)
Batch 525/537: Loss=1.3761 (C:0.7049, R:0.0067)

============================================================
Epoch 132/200 completed in 22.6s
Train: Loss=1.3488 (C:0.6790, R:0.0067) Ratio=4.21x
Val:   Loss=1.4278 (C:0.8049, R:0.0062) Ratio=2.92x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 133
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.438 ¬± 0.630
    Neg distances: 2.595 ¬± 1.109
    Separation ratio: 5.92x
    Gap: -4.060
    ‚úÖ Excellent global separation!

Epoch 133 Training
----------------------------------------
Batch   0/537: Loss=1.3221 (C:0.6548, R:0.0067)
Batch  25/537: Loss=1.3403 (C:0.6690, R:0.0067)
Batch  50/537: Loss=1.3353 (C:0.6634, R:0.0067)
Batch  75/537: Loss=1.3288 (C:0.6675, R:0.0066)
Batch 100/537: Loss=1.3359 (C:0.6614, R:0.0067)
Batch 125/537: Loss=1.3170 (C:0.6396, R:0.0068)
Batch 150/537: Loss=1.3062 (C:0.6395, R:0.0067)
Batch 175/537: Loss=1.3240 (C:0.6552, R:0.0067)
Batch 200/537: Loss=1.3089 (C:0.6363, R:0.0067)
Batch 225/537: Loss=1.3509 (C:0.6811, R:0.0067)
Batch 250/537: Loss=1.3032 (C:0.6312, R:0.0067)
Batch 275/537: Loss=1.3321 (C:0.6599, R:0.0067)
Batch 300/537: Loss=1.3404 (C:0.6663, R:0.0067)
Batch 325/537: Loss=1.2904 (C:0.6148, R:0.0068)
Batch 350/537: Loss=1.3258 (C:0.6556, R:0.0067)
Batch 375/537: Loss=1.3428 (C:0.6779, R:0.0066)
Batch 400/537: Loss=1.3328 (C:0.6643, R:0.0067)
Batch 425/537: Loss=1.3277 (C:0.6618, R:0.0067)
Batch 450/537: Loss=1.3274 (C:0.6543, R:0.0067)
Batch 475/537: Loss=1.3274 (C:0.6567, R:0.0067)
Batch 500/537: Loss=1.3569 (C:0.6828, R:0.0067)
Batch 525/537: Loss=1.3430 (C:0.6655, R:0.0068)

============================================================
Epoch 133/200 completed in 29.2s
Train: Loss=1.3293 (C:0.6592, R:0.0067) Ratio=4.19x
Val:   Loss=1.4072 (C:0.7838, R:0.0062) Ratio=2.93x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4072)
============================================================

Epoch 134 Training
----------------------------------------
Batch   0/537: Loss=1.3671 (C:0.6889, R:0.0068)
Batch  25/537: Loss=1.2635 (C:0.5979, R:0.0067)
Batch  50/537: Loss=1.3409 (C:0.6713, R:0.0067)
Batch  75/537: Loss=1.3042 (C:0.6351, R:0.0067)
Batch 100/537: Loss=1.3521 (C:0.6787, R:0.0067)
Batch 125/537: Loss=1.3530 (C:0.6845, R:0.0067)
Batch 150/537: Loss=1.3555 (C:0.6867, R:0.0067)
Batch 175/537: Loss=1.3302 (C:0.6597, R:0.0067)
Batch 200/537: Loss=1.3385 (C:0.6730, R:0.0067)
Batch 225/537: Loss=1.3342 (C:0.6546, R:0.0068)
Batch 250/537: Loss=1.3236 (C:0.6500, R:0.0067)
Batch 275/537: Loss=1.3407 (C:0.6665, R:0.0067)
Batch 300/537: Loss=1.3064 (C:0.6417, R:0.0066)
Batch 325/537: Loss=1.3205 (C:0.6529, R:0.0067)
Batch 350/537: Loss=1.3571 (C:0.6828, R:0.0067)
Batch 375/537: Loss=1.3214 (C:0.6602, R:0.0066)
Batch 400/537: Loss=1.3129 (C:0.6437, R:0.0067)
Batch 425/537: Loss=1.2879 (C:0.6256, R:0.0066)
Batch 450/537: Loss=1.3274 (C:0.6618, R:0.0067)
Batch 475/537: Loss=1.3190 (C:0.6477, R:0.0067)
Batch 500/537: Loss=1.3267 (C:0.6560, R:0.0067)
Batch 525/537: Loss=1.3556 (C:0.6849, R:0.0067)

============================================================
Epoch 134/200 completed in 22.6s
Train: Loss=1.3279 (C:0.6580, R:0.0067) Ratio=4.21x
Val:   Loss=1.4150 (C:0.7917, R:0.0062) Ratio=2.88x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 135 Training
----------------------------------------
Batch   0/537: Loss=1.3129 (C:0.6440, R:0.0067)
Batch  25/537: Loss=1.3556 (C:0.6837, R:0.0067)
Batch  50/537: Loss=1.3465 (C:0.6754, R:0.0067)
Batch  75/537: Loss=1.3308 (C:0.6581, R:0.0067)
Batch 100/537: Loss=1.3141 (C:0.6410, R:0.0067)
Batch 125/537: Loss=1.3662 (C:0.6944, R:0.0067)
Batch 150/537: Loss=1.3142 (C:0.6516, R:0.0066)
Batch 175/537: Loss=1.2861 (C:0.6111, R:0.0067)
Batch 200/537: Loss=1.3028 (C:0.6243, R:0.0068)
Batch 225/537: Loss=1.3222 (C:0.6532, R:0.0067)
Batch 250/537: Loss=1.3787 (C:0.7038, R:0.0067)
Batch 275/537: Loss=1.3062 (C:0.6387, R:0.0067)
Batch 300/537: Loss=1.3370 (C:0.6667, R:0.0067)
Batch 325/537: Loss=1.3026 (C:0.6349, R:0.0067)
Batch 350/537: Loss=1.3170 (C:0.6439, R:0.0067)
Batch 375/537: Loss=1.3015 (C:0.6280, R:0.0067)
Batch 400/537: Loss=1.3520 (C:0.6858, R:0.0067)
Batch 425/537: Loss=1.2925 (C:0.6254, R:0.0067)
Batch 450/537: Loss=1.2895 (C:0.6210, R:0.0067)
Batch 475/537: Loss=1.3497 (C:0.6804, R:0.0067)
Batch 500/537: Loss=1.3253 (C:0.6589, R:0.0067)
Batch 525/537: Loss=1.3233 (C:0.6579, R:0.0067)

============================================================
Epoch 135/200 completed in 22.5s
Train: Loss=1.3289 (C:0.6589, R:0.0067) Ratio=4.18x
Val:   Loss=1.4107 (C:0.7871, R:0.0062) Ratio=2.89x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 136
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.443 ¬± 0.641
    Neg distances: 2.570 ¬± 1.101
    Separation ratio: 5.81x
    Gap: -4.070
    ‚úÖ Excellent global separation!

Epoch 136 Training
----------------------------------------
Batch   0/537: Loss=1.3135 (C:0.6422, R:0.0067)
Batch  25/537: Loss=1.3419 (C:0.6679, R:0.0067)
Batch  50/537: Loss=1.3035 (C:0.6325, R:0.0067)
Batch  75/537: Loss=1.3510 (C:0.6803, R:0.0067)
Batch 100/537: Loss=1.3001 (C:0.6326, R:0.0067)
Batch 125/537: Loss=1.3780 (C:0.7057, R:0.0067)
Batch 150/537: Loss=1.3282 (C:0.6618, R:0.0067)
Batch 175/537: Loss=1.3343 (C:0.6610, R:0.0067)
Batch 200/537: Loss=1.3478 (C:0.6752, R:0.0067)
Batch 225/537: Loss=1.3462 (C:0.6787, R:0.0067)
Batch 250/537: Loss=1.3474 (C:0.6721, R:0.0068)
Batch 275/537: Loss=1.3419 (C:0.6729, R:0.0067)
Batch 300/537: Loss=1.3312 (C:0.6616, R:0.0067)
Batch 325/537: Loss=1.2949 (C:0.6295, R:0.0067)
Batch 350/537: Loss=1.3370 (C:0.6698, R:0.0067)
Batch 375/537: Loss=1.3148 (C:0.6483, R:0.0067)
Batch 400/537: Loss=1.3145 (C:0.6475, R:0.0067)
Batch 425/537: Loss=1.3032 (C:0.6356, R:0.0067)
Batch 450/537: Loss=1.3306 (C:0.6628, R:0.0067)
Batch 475/537: Loss=1.2933 (C:0.6216, R:0.0067)
Batch 500/537: Loss=1.3262 (C:0.6540, R:0.0067)
Batch 525/537: Loss=1.3473 (C:0.6826, R:0.0066)

============================================================
Epoch 136/200 completed in 29.2s
Train: Loss=1.3301 (C:0.6601, R:0.0067) Ratio=4.16x
Val:   Loss=1.4163 (C:0.7928, R:0.0062) Ratio=2.86x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 137 Training
----------------------------------------
Batch   0/537: Loss=1.3549 (C:0.6848, R:0.0067)
Batch  25/537: Loss=1.3466 (C:0.6807, R:0.0067)
Batch  50/537: Loss=1.3130 (C:0.6493, R:0.0066)
Batch  75/537: Loss=1.2942 (C:0.6281, R:0.0067)
Batch 100/537: Loss=1.3236 (C:0.6508, R:0.0067)
Batch 125/537: Loss=1.3113 (C:0.6428, R:0.0067)
Batch 150/537: Loss=1.3151 (C:0.6468, R:0.0067)
Batch 175/537: Loss=1.3023 (C:0.6308, R:0.0067)
Batch 200/537: Loss=1.3147 (C:0.6456, R:0.0067)
Batch 225/537: Loss=1.3018 (C:0.6304, R:0.0067)
Batch 250/537: Loss=1.3404 (C:0.6755, R:0.0066)
Batch 275/537: Loss=1.3271 (C:0.6558, R:0.0067)
Batch 300/537: Loss=1.3167 (C:0.6477, R:0.0067)
Batch 325/537: Loss=1.3499 (C:0.6838, R:0.0067)
Batch 350/537: Loss=1.3464 (C:0.6761, R:0.0067)
Batch 375/537: Loss=1.3868 (C:0.7044, R:0.0068)
Batch 400/537: Loss=1.2936 (C:0.6198, R:0.0067)
Batch 425/537: Loss=1.3760 (C:0.7092, R:0.0067)
Batch 450/537: Loss=1.3391 (C:0.6738, R:0.0067)
Batch 475/537: Loss=1.2950 (C:0.6252, R:0.0067)
Batch 500/537: Loss=1.3680 (C:0.6932, R:0.0067)
Batch 525/537: Loss=1.3335 (C:0.6621, R:0.0067)

============================================================
Epoch 137/200 completed in 22.6s
Train: Loss=1.3287 (C:0.6586, R:0.0067) Ratio=4.22x
Val:   Loss=1.4190 (C:0.7955, R:0.0062) Ratio=2.84x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 138 Training
----------------------------------------
Batch   0/537: Loss=1.2633 (C:0.5937, R:0.0067)
Batch  25/537: Loss=1.3434 (C:0.6717, R:0.0067)
Batch  50/537: Loss=1.2989 (C:0.6275, R:0.0067)
Batch  75/537: Loss=1.3429 (C:0.6716, R:0.0067)
Batch 100/537: Loss=1.3318 (C:0.6655, R:0.0067)
Batch 125/537: Loss=1.3138 (C:0.6487, R:0.0067)
Batch 150/537: Loss=1.2943 (C:0.6274, R:0.0067)
Batch 175/537: Loss=1.3493 (C:0.6840, R:0.0067)
Batch 200/537: Loss=1.2993 (C:0.6286, R:0.0067)
Batch 225/537: Loss=1.3421 (C:0.6652, R:0.0068)
Batch 250/537: Loss=1.3673 (C:0.6971, R:0.0067)
Batch 275/537: Loss=1.3437 (C:0.6727, R:0.0067)
Batch 300/537: Loss=1.3095 (C:0.6330, R:0.0068)
Batch 325/537: Loss=1.3163 (C:0.6402, R:0.0068)
Batch 350/537: Loss=1.3406 (C:0.6707, R:0.0067)
Batch 375/537: Loss=1.3419 (C:0.6736, R:0.0067)
Batch 400/537: Loss=1.3692 (C:0.7021, R:0.0067)
Batch 425/537: Loss=1.3560 (C:0.6863, R:0.0067)
Batch 450/537: Loss=1.3088 (C:0.6420, R:0.0067)
Batch 475/537: Loss=1.3762 (C:0.7039, R:0.0067)
Batch 500/537: Loss=1.3038 (C:0.6314, R:0.0067)
Batch 525/537: Loss=1.3149 (C:0.6426, R:0.0067)

============================================================
Epoch 138/200 completed in 22.6s
Train: Loss=1.3279 (C:0.6578, R:0.0067) Ratio=4.25x
Val:   Loss=1.4232 (C:0.7992, R:0.0062) Ratio=2.86x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 139
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.444 ¬± 0.641
    Neg distances: 2.604 ¬± 1.119
    Separation ratio: 5.87x
    Gap: -4.091
    ‚úÖ Excellent global separation!

Epoch 139 Training
----------------------------------------
Batch   0/537: Loss=1.3179 (C:0.6493, R:0.0067)
Batch  25/537: Loss=1.3071 (C:0.6299, R:0.0068)
Batch  50/537: Loss=1.2834 (C:0.6137, R:0.0067)
Batch  75/537: Loss=1.3104 (C:0.6336, R:0.0068)
Batch 100/537: Loss=1.3454 (C:0.6761, R:0.0067)
Batch 125/537: Loss=1.3173 (C:0.6457, R:0.0067)
Batch 150/537: Loss=1.3391 (C:0.6658, R:0.0067)
Batch 175/537: Loss=1.3594 (C:0.6842, R:0.0068)
Batch 200/537: Loss=1.3374 (C:0.6666, R:0.0067)
Batch 225/537: Loss=1.3185 (C:0.6488, R:0.0067)
Batch 250/537: Loss=1.3885 (C:0.7259, R:0.0066)
Batch 275/537: Loss=1.3410 (C:0.6741, R:0.0067)
Batch 300/537: Loss=1.3235 (C:0.6495, R:0.0067)
Batch 325/537: Loss=1.2894 (C:0.6201, R:0.0067)
Batch 350/537: Loss=1.3356 (C:0.6692, R:0.0067)
Batch 375/537: Loss=1.3218 (C:0.6520, R:0.0067)
Batch 400/537: Loss=1.3535 (C:0.6765, R:0.0068)
Batch 425/537: Loss=1.3187 (C:0.6530, R:0.0067)
Batch 450/537: Loss=1.3208 (C:0.6493, R:0.0067)
Batch 475/537: Loss=1.3219 (C:0.6458, R:0.0068)
Batch 500/537: Loss=1.3144 (C:0.6405, R:0.0067)
Batch 525/537: Loss=1.2901 (C:0.6230, R:0.0067)

============================================================
Epoch 139/200 completed in 29.2s
Train: Loss=1.3285 (C:0.6584, R:0.0067) Ratio=4.26x
Val:   Loss=1.4211 (C:0.7977, R:0.0062) Ratio=2.90x
Reconstruction weight: 100.000
No improvement for 6 epochs
============================================================

Epoch 140 Training
----------------------------------------
Batch   0/537: Loss=1.3012 (C:0.6304, R:0.0067)
Batch  25/537: Loss=1.3337 (C:0.6580, R:0.0068)
Batch  50/537: Loss=1.2896 (C:0.6177, R:0.0067)
Batch  75/537: Loss=1.3204 (C:0.6475, R:0.0067)
Batch 100/537: Loss=1.3217 (C:0.6526, R:0.0067)
Batch 125/537: Loss=1.3016 (C:0.6258, R:0.0068)
Batch 150/537: Loss=1.3487 (C:0.6814, R:0.0067)
Batch 175/537: Loss=1.3280 (C:0.6589, R:0.0067)
Batch 200/537: Loss=1.3525 (C:0.6832, R:0.0067)
Batch 225/537: Loss=1.3290 (C:0.6599, R:0.0067)
Batch 250/537: Loss=1.3446 (C:0.6817, R:0.0066)
Batch 275/537: Loss=1.3067 (C:0.6433, R:0.0066)
Batch 300/537: Loss=1.3245 (C:0.6579, R:0.0067)
Batch 325/537: Loss=1.3246 (C:0.6557, R:0.0067)
Batch 350/537: Loss=1.2974 (C:0.6322, R:0.0067)
Batch 375/537: Loss=1.3088 (C:0.6399, R:0.0067)
Batch 400/537: Loss=1.3362 (C:0.6673, R:0.0067)
Batch 425/537: Loss=1.3124 (C:0.6393, R:0.0067)
Batch 450/537: Loss=1.3648 (C:0.6942, R:0.0067)
Batch 475/537: Loss=1.3345 (C:0.6662, R:0.0067)
Batch 500/537: Loss=1.3453 (C:0.6764, R:0.0067)
Batch 525/537: Loss=1.2813 (C:0.6105, R:0.0067)

============================================================
Epoch 140/200 completed in 22.4s
Train: Loss=1.3270 (C:0.6571, R:0.0067) Ratio=4.27x
Val:   Loss=1.4094 (C:0.7864, R:0.0062) Ratio=2.93x
Reconstruction weight: 100.000
No improvement for 7 epochs
Checkpoint saved at epoch 140
============================================================

Epoch 141 Training
----------------------------------------
Batch   0/537: Loss=1.3479 (C:0.6812, R:0.0067)
Batch  25/537: Loss=1.3328 (C:0.6619, R:0.0067)
Batch  50/537: Loss=1.3450 (C:0.6769, R:0.0067)
Batch  75/537: Loss=1.3045 (C:0.6355, R:0.0067)
Batch 100/537: Loss=1.3211 (C:0.6504, R:0.0067)
Batch 125/537: Loss=1.3280 (C:0.6542, R:0.0067)
Batch 150/537: Loss=1.2995 (C:0.6359, R:0.0066)
Batch 175/537: Loss=1.3298 (C:0.6604, R:0.0067)
Batch 200/537: Loss=1.3453 (C:0.6698, R:0.0068)
Batch 225/537: Loss=1.3336 (C:0.6588, R:0.0067)
Batch 250/537: Loss=1.3292 (C:0.6570, R:0.0067)
Batch 275/537: Loss=1.2775 (C:0.6114, R:0.0067)
Batch 300/537: Loss=1.3100 (C:0.6415, R:0.0067)
Batch 325/537: Loss=1.3340 (C:0.6686, R:0.0067)
Batch 350/537: Loss=1.3501 (C:0.6740, R:0.0068)
Batch 375/537: Loss=1.3115 (C:0.6363, R:0.0068)
Batch 400/537: Loss=1.3503 (C:0.6779, R:0.0067)
Batch 425/537: Loss=1.3551 (C:0.6847, R:0.0067)
Batch 450/537: Loss=1.3330 (C:0.6563, R:0.0068)
Batch 475/537: Loss=1.3224 (C:0.6508, R:0.0067)
Batch 500/537: Loss=1.2733 (C:0.6014, R:0.0067)
Batch 525/537: Loss=1.3437 (C:0.6749, R:0.0067)

============================================================
Epoch 141/200 completed in 22.6s
Train: Loss=1.3280 (C:0.6580, R:0.0067) Ratio=4.29x
Val:   Loss=1.4139 (C:0.7905, R:0.0062) Ratio=2.91x
Reconstruction weight: 100.000
No improvement for 8 epochs

Early stopping triggered after 141 epochs
Best model was at epoch 133 with Val Loss: 1.4072

Global Dataset Training Completed!
Best epoch: 133
Best validation loss: 1.4072
Final separation ratios: Train=4.29x, Val=2.91x
Training completed!
Creating loss plots...
Loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/plots/global_concat_test_attention_training_losses.png
Loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/plots/global_concat_test_attention_training_losses.png
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4661
  Adjusted Rand Score: 0.5565
  Clustering Accuracy: 0.8272
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8281
  Per-class F1: [0.8481880509304603, 0.7720672214064285, 0.8642096642096642]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.006185
Evaluating separation quality...
Separation Results:
  Positive distances: 0.815 ¬± 0.994
  Negative distances: 2.368 ¬± 1.252
  Separation ratio: 2.91x
  Gap: -4.100
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4661
  Clustering Accuracy: 0.8272
  Adjusted Rand Score: 0.5565

Classification Performance:
  Accuracy: 0.8281

Separation Quality:
  Separation Ratio: 2.91x
  Gap: -4.100
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.006185
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/results/evaluation_results_20250724_213756.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/results/evaluation_results_20250724_213756.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_test_attention_20250724_203741/final_results.json

Key Results:
  Separation ratio: 2.91x
  Perfect separation: False
  Classification accuracy: 0.8281

Analysis completed with exit code: 0
Time: Thu 24 Jul 21:37:57 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
