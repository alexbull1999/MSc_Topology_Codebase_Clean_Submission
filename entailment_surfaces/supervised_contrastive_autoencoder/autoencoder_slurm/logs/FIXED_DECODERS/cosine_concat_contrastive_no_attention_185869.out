Starting Surface Distance Metric Analysis job...
Job ID: 185869
Node: gpuvm19
Time: Thu 24 Jul 19:28:15 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Thu Jul 24 19:28:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   56C    P8             14W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-24 19:28:30.114352
Using device: cuda

Configuration:
  Embedding type: cosine_concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 200
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'cosine_concat'
Output dimension will be: 1537
GlobalDataLoader initialized:
  Embedding type: cosine_concat
  Output dimension: 1537
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating cosine_concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated cosine_concat embeddings: torch.Size([549367, 1537])
Generating embeddings for validation...
Generating cosine_concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9842, 1537])
Generating embeddings for test...
Generating cosine_concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9824, 1537])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1537])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1537])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1537])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1537
Updated model input_dim to: 1537
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
ContrastiveAutoencoder initialized:
  Input dim: 1537
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 3,046,551
Model created with 3,046,551 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 100.0
Optimizer created: Adam (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 3,046,551
Starting training...
========================================
Starting Global Dataset Training...
============================================================

üåç Updating global dataset at epoch 1
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.006 ¬± 0.001
    Neg distances: 0.006 ¬± 0.001
    Separation ratio: 1.00x
    Gap: -0.010
    ‚ùå Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=3.6015 (C:2.0000, R:0.0160)
Batch  25/537: Loss=3.4470 (C:2.0000, R:0.0145)
Batch  50/537: Loss=3.3392 (C:1.9997, R:0.0134)
Batch  75/537: Loss=3.2351 (C:1.9996, R:0.0124)
Batch 100/537: Loss=3.1347 (C:1.9998, R:0.0113)
Batch 125/537: Loss=3.0735 (C:1.9994, R:0.0107)
Batch 150/537: Loss=3.0381 (C:1.9995, R:0.0104)
Batch 175/537: Loss=3.0199 (C:1.9992, R:0.0102)
Batch 200/537: Loss=3.0031 (C:1.9998, R:0.0100)
Batch 225/537: Loss=2.9922 (C:1.9975, R:0.0099)
Batch 250/537: Loss=2.9807 (C:1.9985, R:0.0098)
Batch 275/537: Loss=2.9699 (C:1.9981, R:0.0097)
Batch 300/537: Loss=2.9602 (C:1.9974, R:0.0096)
Batch 325/537: Loss=2.9577 (C:1.9979, R:0.0096)
Batch 350/537: Loss=2.9406 (C:1.9973, R:0.0094)
Batch 375/537: Loss=2.9345 (C:1.9990, R:0.0094)
Batch 400/537: Loss=2.9207 (C:1.9982, R:0.0092)
Batch 425/537: Loss=2.9105 (C:1.9951, R:0.0092)
Batch 450/537: Loss=2.9060 (C:1.9959, R:0.0091)
Batch 475/537: Loss=2.8959 (C:1.9929, R:0.0090)
Batch 500/537: Loss=2.8882 (C:1.9897, R:0.0090)
Batch 525/537: Loss=2.8742 (C:1.9867, R:0.0089)

============================================================
Epoch 1/200 completed in 33.8s
Train: Loss=3.0340 (C:1.9973, R:0.0104) Ratio=1.03x
Val:   Loss=2.8475 (C:1.9836, R:0.0086) Ratio=1.15x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.8475)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=2.8796 (C:1.9891, R:0.0089)
Batch  25/537: Loss=2.8634 (C:1.9786, R:0.0088)
Batch  50/537: Loss=2.8653 (C:1.9829, R:0.0088)
Batch  75/537: Loss=2.8428 (C:1.9704, R:0.0087)
Batch 100/537: Loss=2.8390 (C:1.9676, R:0.0087)
Batch 125/537: Loss=2.8461 (C:1.9721, R:0.0087)
Batch 150/537: Loss=2.8284 (C:1.9576, R:0.0087)
Batch 175/537: Loss=2.8011 (C:1.9341, R:0.0087)
Batch 200/537: Loss=2.8047 (C:1.9448, R:0.0086)
Batch 225/537: Loss=2.7849 (C:1.9316, R:0.0085)
Batch 250/537: Loss=2.7759 (C:1.9235, R:0.0085)
Batch 275/537: Loss=2.7866 (C:1.9357, R:0.0085)
Batch 300/537: Loss=2.7827 (C:1.9398, R:0.0084)
Batch 325/537: Loss=2.7707 (C:1.9259, R:0.0084)
Batch 350/537: Loss=2.7698 (C:1.9299, R:0.0084)
Batch 375/537: Loss=2.7632 (C:1.9236, R:0.0084)
Batch 400/537: Loss=2.7670 (C:1.9299, R:0.0084)
Batch 425/537: Loss=2.7590 (C:1.9241, R:0.0083)
Batch 450/537: Loss=2.7457 (C:1.9147, R:0.0083)
Batch 475/537: Loss=2.7491 (C:1.9187, R:0.0083)
Batch 500/537: Loss=2.7592 (C:1.9288, R:0.0083)
Batch 525/537: Loss=2.7454 (C:1.9245, R:0.0082)

============================================================
Epoch 2/200 completed in 24.3s
Train: Loss=2.7951 (C:1.9437, R:0.0085) Ratio=1.44x
Val:   Loss=2.7123 (C:1.9162, R:0.0080) Ratio=1.65x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.7123)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=2.7387 (C:1.9125, R:0.0083)
Batch  25/537: Loss=2.7326 (C:1.9148, R:0.0082)
Batch  50/537: Loss=2.7412 (C:1.9271, R:0.0081)
Batch  75/537: Loss=2.7348 (C:1.9186, R:0.0082)
Batch 100/537: Loss=2.7315 (C:1.9225, R:0.0081)
Batch 125/537: Loss=2.7372 (C:1.9279, R:0.0081)
Batch 150/537: Loss=2.7359 (C:1.9306, R:0.0081)
Batch 175/537: Loss=2.7178 (C:1.9169, R:0.0080)
Batch 200/537: Loss=2.7242 (C:1.9293, R:0.0079)
Batch 225/537: Loss=2.7188 (C:1.9192, R:0.0080)
Batch 250/537: Loss=2.7317 (C:1.9260, R:0.0081)
Batch 275/537: Loss=2.7157 (C:1.9193, R:0.0080)
Batch 300/537: Loss=2.7097 (C:1.9161, R:0.0079)
Batch 325/537: Loss=2.7124 (C:1.9188, R:0.0079)
Batch 350/537: Loss=2.7050 (C:1.9119, R:0.0079)
Batch 375/537: Loss=2.7111 (C:1.9181, R:0.0079)
Batch 400/537: Loss=2.7144 (C:1.9236, R:0.0079)
Batch 425/537: Loss=2.7129 (C:1.9232, R:0.0079)
Batch 450/537: Loss=2.7056 (C:1.9174, R:0.0079)
Batch 475/537: Loss=2.7092 (C:1.9237, R:0.0079)
Batch 500/537: Loss=2.7011 (C:1.9185, R:0.0078)
Batch 525/537: Loss=2.7029 (C:1.9183, R:0.0078)

============================================================
Epoch 3/200 completed in 24.6s
Train: Loss=2.7188 (C:1.9196, R:0.0080) Ratio=1.61x
Val:   Loss=2.6685 (C:1.9122, R:0.0076) Ratio=1.69x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.6685)
============================================================

üåç Updating global dataset at epoch 4
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.876 ¬± 0.475
    Neg distances: 1.508 ¬± 0.652
    Separation ratio: 1.72x
    Gap: -2.584
    ‚ö†Ô∏è  Moderate global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=2.2390 (C:1.4591, R:0.0078)
Batch  25/537: Loss=2.2426 (C:1.4535, R:0.0079)
Batch  50/537: Loss=2.2424 (C:1.4546, R:0.0079)
Batch  75/537: Loss=2.2226 (C:1.4339, R:0.0079)
Batch 100/537: Loss=2.2062 (C:1.4149, R:0.0079)
Batch 125/537: Loss=2.2347 (C:1.4386, R:0.0080)
Batch 150/537: Loss=2.2117 (C:1.4283, R:0.0078)
Batch 175/537: Loss=2.2384 (C:1.4525, R:0.0079)
Batch 200/537: Loss=2.2447 (C:1.4631, R:0.0078)
Batch 225/537: Loss=2.1916 (C:1.4026, R:0.0079)
Batch 250/537: Loss=2.2182 (C:1.4324, R:0.0079)
Batch 275/537: Loss=2.2276 (C:1.4449, R:0.0078)
Batch 300/537: Loss=2.2126 (C:1.4369, R:0.0078)
Batch 325/537: Loss=2.2141 (C:1.4273, R:0.0079)
Batch 350/537: Loss=2.2086 (C:1.4291, R:0.0078)
Batch 375/537: Loss=2.2387 (C:1.4579, R:0.0078)
Batch 400/537: Loss=2.1973 (C:1.4224, R:0.0077)
Batch 425/537: Loss=2.2193 (C:1.4390, R:0.0078)
Batch 450/537: Loss=2.2338 (C:1.4563, R:0.0078)
Batch 475/537: Loss=2.1991 (C:1.4257, R:0.0077)
Batch 500/537: Loss=2.1820 (C:1.4095, R:0.0077)
Batch 525/537: Loss=2.2173 (C:1.4395, R:0.0078)

============================================================
Epoch 4/200 completed in 33.7s
Train: Loss=2.2218 (C:1.4389, R:0.0078) Ratio=1.79x
Val:   Loss=2.1615 (C:1.4127, R:0.0075) Ratio=1.91x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1615)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=2.1894 (C:1.4177, R:0.0077)
Batch  25/537: Loss=2.1795 (C:1.4074, R:0.0077)
Batch  50/537: Loss=2.2018 (C:1.4260, R:0.0078)
Batch  75/537: Loss=2.2025 (C:1.4242, R:0.0078)
Batch 100/537: Loss=2.2048 (C:1.4317, R:0.0077)
Batch 125/537: Loss=2.1840 (C:1.4171, R:0.0077)
Batch 150/537: Loss=2.1658 (C:1.3967, R:0.0077)
Batch 175/537: Loss=2.2045 (C:1.4382, R:0.0077)
Batch 200/537: Loss=2.1949 (C:1.4268, R:0.0077)
Batch 225/537: Loss=2.1888 (C:1.4233, R:0.0077)
Batch 250/537: Loss=2.2223 (C:1.4501, R:0.0077)
Batch 275/537: Loss=2.1688 (C:1.4083, R:0.0076)
Batch 300/537: Loss=2.1805 (C:1.4126, R:0.0077)
Batch 325/537: Loss=2.1462 (C:1.3890, R:0.0076)
Batch 350/537: Loss=2.1687 (C:1.4029, R:0.0077)
Batch 375/537: Loss=2.1868 (C:1.4282, R:0.0076)
Batch 400/537: Loss=2.1810 (C:1.4200, R:0.0076)
Batch 425/537: Loss=2.1587 (C:1.3979, R:0.0076)
Batch 450/537: Loss=2.2025 (C:1.4453, R:0.0076)
Batch 475/537: Loss=2.1967 (C:1.4351, R:0.0076)
Batch 500/537: Loss=2.1781 (C:1.4197, R:0.0076)
Batch 525/537: Loss=2.1860 (C:1.4258, R:0.0076)

============================================================
Epoch 5/200 completed in 24.1s
Train: Loss=2.1871 (C:1.4216, R:0.0077) Ratio=1.91x
Val:   Loss=2.1404 (C:1.4061, R:0.0073) Ratio=1.96x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1404)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=2.1576 (C:1.4029, R:0.0075)
Batch  25/537: Loss=2.1715 (C:1.4095, R:0.0076)
Batch  50/537: Loss=2.1633 (C:1.3996, R:0.0076)
Batch  75/537: Loss=2.1615 (C:1.4027, R:0.0076)
Batch 100/537: Loss=2.1823 (C:1.4214, R:0.0076)
Batch 125/537: Loss=2.1657 (C:1.4117, R:0.0075)
Batch 150/537: Loss=2.1868 (C:1.4335, R:0.0075)
Batch 175/537: Loss=2.1757 (C:1.4233, R:0.0075)
Batch 200/537: Loss=2.1628 (C:1.4122, R:0.0075)
Batch 225/537: Loss=2.1590 (C:1.4031, R:0.0076)
Batch 250/537: Loss=2.1432 (C:1.3900, R:0.0075)
Batch 275/537: Loss=2.1443 (C:1.3895, R:0.0075)
Batch 300/537: Loss=2.2047 (C:1.4478, R:0.0076)
Batch 325/537: Loss=2.1730 (C:1.4228, R:0.0075)
Batch 350/537: Loss=2.1820 (C:1.4319, R:0.0075)
Batch 375/537: Loss=2.1508 (C:1.4008, R:0.0075)
Batch 400/537: Loss=2.1474 (C:1.3939, R:0.0075)
Batch 425/537: Loss=2.1850 (C:1.4313, R:0.0075)
Batch 450/537: Loss=2.1522 (C:1.4032, R:0.0075)
Batch 475/537: Loss=2.1669 (C:1.4148, R:0.0075)
Batch 500/537: Loss=2.1633 (C:1.4171, R:0.0075)
Batch 525/537: Loss=2.1628 (C:1.4104, R:0.0075)

============================================================
Epoch 6/200 completed in 24.0s
Train: Loss=2.1630 (C:1.4097, R:0.0075) Ratio=1.98x
Val:   Loss=2.1277 (C:1.4049, R:0.0072) Ratio=2.03x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1277)
============================================================

üåç Updating global dataset at epoch 7
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.695 ¬± 0.530
    Neg distances: 1.520 ¬± 0.755
    Separation ratio: 2.19x
    Gap: -2.534
    ‚úÖ Good global separation

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=2.0363 (C:1.2889, R:0.0075)
Batch  25/537: Loss=2.0416 (C:1.2912, R:0.0075)
Batch  50/537: Loss=2.0399 (C:1.2935, R:0.0075)
Batch  75/537: Loss=2.0640 (C:1.3147, R:0.0075)
Batch 100/537: Loss=2.0725 (C:1.3189, R:0.0075)
Batch 125/537: Loss=2.0715 (C:1.3118, R:0.0076)
Batch 150/537: Loss=2.0449 (C:1.2961, R:0.0075)
Batch 175/537: Loss=2.0430 (C:1.2975, R:0.0075)
Batch 200/537: Loss=2.0447 (C:1.2953, R:0.0075)
Batch 225/537: Loss=2.0855 (C:1.3299, R:0.0076)
Batch 250/537: Loss=2.0331 (C:1.2871, R:0.0075)
Batch 275/537: Loss=2.0676 (C:1.3210, R:0.0075)
Batch 300/537: Loss=2.0254 (C:1.2848, R:0.0074)
Batch 325/537: Loss=2.0523 (C:1.3070, R:0.0075)
Batch 350/537: Loss=2.0423 (C:1.2884, R:0.0075)
Batch 375/537: Loss=2.0201 (C:1.2716, R:0.0075)
Batch 400/537: Loss=2.0522 (C:1.3076, R:0.0074)
Batch 425/537: Loss=2.0556 (C:1.3061, R:0.0075)
Batch 450/537: Loss=2.0516 (C:1.3091, R:0.0074)
Batch 475/537: Loss=2.0547 (C:1.3166, R:0.0074)
Batch 500/537: Loss=2.0409 (C:1.3022, R:0.0074)
Batch 525/537: Loss=2.0561 (C:1.3087, R:0.0075)

============================================================
Epoch 7/200 completed in 32.4s
Train: Loss=2.0465 (C:1.2993, R:0.0075) Ratio=2.06x
Val:   Loss=2.0207 (C:1.3046, R:0.0072) Ratio=2.11x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.0207)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=2.0030 (C:1.2662, R:0.0074)
Batch  25/537: Loss=2.0068 (C:1.2647, R:0.0074)
Batch  50/537: Loss=2.0014 (C:1.2573, R:0.0074)
Batch  75/537: Loss=2.0015 (C:1.2574, R:0.0074)
Batch 100/537: Loss=2.0314 (C:1.2930, R:0.0074)
Batch 125/537: Loss=2.0387 (C:1.2970, R:0.0074)
Batch 150/537: Loss=2.0187 (C:1.2742, R:0.0074)
Batch 175/537: Loss=2.0481 (C:1.3073, R:0.0074)
Batch 200/537: Loss=2.0377 (C:1.2931, R:0.0074)
Batch 225/537: Loss=2.0555 (C:1.3088, R:0.0075)
Batch 250/537: Loss=2.0297 (C:1.2926, R:0.0074)
Batch 275/537: Loss=2.0423 (C:1.3053, R:0.0074)
Batch 300/537: Loss=2.0104 (C:1.2651, R:0.0075)
Batch 325/537: Loss=2.0151 (C:1.2778, R:0.0074)
Batch 350/537: Loss=2.0497 (C:1.3074, R:0.0074)
Batch 375/537: Loss=2.0136 (C:1.2718, R:0.0074)
Batch 400/537: Loss=2.0378 (C:1.2982, R:0.0074)
Batch 425/537: Loss=2.0417 (C:1.3056, R:0.0074)
Batch 450/537: Loss=2.0496 (C:1.3132, R:0.0074)
Batch 475/537: Loss=2.0264 (C:1.2878, R:0.0074)
Batch 500/537: Loss=2.0247 (C:1.2805, R:0.0074)
Batch 525/537: Loss=2.0281 (C:1.2835, R:0.0074)

============================================================
Epoch 8/200 completed in 24.2s
Train: Loss=2.0299 (C:1.2892, R:0.0074) Ratio=2.18x
Val:   Loss=1.9999 (C:1.2894, R:0.0071) Ratio=2.18x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9999)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=2.0149 (C:1.2762, R:0.0074)
Batch  25/537: Loss=1.9920 (C:1.2563, R:0.0074)
Batch  50/537: Loss=2.0231 (C:1.2799, R:0.0074)
Batch  75/537: Loss=2.0715 (C:1.3365, R:0.0074)
Batch 100/537: Loss=1.9928 (C:1.2589, R:0.0073)
Batch 125/537: Loss=2.0220 (C:1.2853, R:0.0074)
Batch 150/537: Loss=2.0277 (C:1.2916, R:0.0074)
Batch 175/537: Loss=1.9953 (C:1.2573, R:0.0074)
Batch 200/537: Loss=2.0228 (C:1.2864, R:0.0074)
Batch 225/537: Loss=1.9821 (C:1.2471, R:0.0074)
Batch 250/537: Loss=1.9965 (C:1.2620, R:0.0073)
Batch 275/537: Loss=2.0386 (C:1.3051, R:0.0073)
Batch 300/537: Loss=2.0094 (C:1.2792, R:0.0073)
Batch 325/537: Loss=2.0103 (C:1.2743, R:0.0074)
Batch 350/537: Loss=2.0208 (C:1.2845, R:0.0074)
Batch 375/537: Loss=1.9968 (C:1.2568, R:0.0074)
Batch 400/537: Loss=2.0343 (C:1.2980, R:0.0074)
Batch 425/537: Loss=1.9983 (C:1.2707, R:0.0073)
Batch 450/537: Loss=2.0075 (C:1.2761, R:0.0073)
Batch 475/537: Loss=2.0172 (C:1.2785, R:0.0074)
Batch 500/537: Loss=2.0490 (C:1.3177, R:0.0073)
Batch 525/537: Loss=2.0350 (C:1.3001, R:0.0073)

============================================================
Epoch 9/200 completed in 23.9s
Train: Loss=2.0186 (C:1.2831, R:0.0074) Ratio=2.21x
Val:   Loss=1.9973 (C:1.2919, R:0.0071) Ratio=2.19x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9973)
============================================================

üåç Updating global dataset at epoch 10
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.620 ¬± 0.539
    Neg distances: 1.520 ¬± 0.795
    Separation ratio: 2.45x
    Gap: -2.550
    ‚úÖ Good global separation

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=1.9888 (C:1.2519, R:0.0074)
Batch  25/537: Loss=1.9737 (C:1.2420, R:0.0073)
Batch  50/537: Loss=2.0038 (C:1.2658, R:0.0074)
Batch  75/537: Loss=1.9461 (C:1.2141, R:0.0073)
Batch 100/537: Loss=1.9890 (C:1.2554, R:0.0073)
Batch 125/537: Loss=1.9695 (C:1.2336, R:0.0074)
Batch 150/537: Loss=1.9831 (C:1.2448, R:0.0074)
Batch 175/537: Loss=1.9291 (C:1.1963, R:0.0073)
Batch 200/537: Loss=1.9997 (C:1.2659, R:0.0073)
Batch 225/537: Loss=1.9780 (C:1.2420, R:0.0074)
Batch 250/537: Loss=1.9765 (C:1.2502, R:0.0073)
Batch 275/537: Loss=1.9502 (C:1.2218, R:0.0073)
Batch 300/537: Loss=1.9630 (C:1.2385, R:0.0072)
Batch 325/537: Loss=2.0003 (C:1.2725, R:0.0073)
Batch 350/537: Loss=1.9879 (C:1.2628, R:0.0073)
Batch 375/537: Loss=1.9904 (C:1.2572, R:0.0073)
Batch 400/537: Loss=1.9682 (C:1.2291, R:0.0074)
Batch 425/537: Loss=1.9937 (C:1.2605, R:0.0073)
Batch 450/537: Loss=1.9908 (C:1.2565, R:0.0073)
Batch 475/537: Loss=1.9930 (C:1.2583, R:0.0073)
Batch 500/537: Loss=1.9448 (C:1.2095, R:0.0074)
Batch 525/537: Loss=1.9756 (C:1.2496, R:0.0073)

============================================================
Epoch 10/200 completed in 32.2s
Train: Loss=1.9729 (C:1.2404, R:0.0073) Ratio=2.30x
Val:   Loss=1.9385 (C:1.2362, R:0.0070) Ratio=2.27x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9385)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.9937 (C:1.2567, R:0.0074)
Batch  25/537: Loss=1.9693 (C:1.2390, R:0.0073)
Batch  50/537: Loss=1.9341 (C:1.2035, R:0.0073)
Batch  75/537: Loss=1.9404 (C:1.2074, R:0.0073)
Batch 100/537: Loss=1.9587 (C:1.2275, R:0.0073)
Batch 125/537: Loss=1.9462 (C:1.2149, R:0.0073)
Batch 150/537: Loss=1.9621 (C:1.2319, R:0.0073)
Batch 175/537: Loss=1.9476 (C:1.2178, R:0.0073)
Batch 200/537: Loss=1.9613 (C:1.2294, R:0.0073)
Batch 225/537: Loss=1.9779 (C:1.2427, R:0.0074)
Batch 250/537: Loss=1.9685 (C:1.2294, R:0.0074)
Batch 275/537: Loss=1.9530 (C:1.2259, R:0.0073)
Batch 300/537: Loss=1.9418 (C:1.2137, R:0.0073)
Batch 325/537: Loss=1.9674 (C:1.2379, R:0.0073)
Batch 350/537: Loss=1.9757 (C:1.2516, R:0.0072)
Batch 375/537: Loss=1.9845 (C:1.2528, R:0.0073)
Batch 400/537: Loss=1.9516 (C:1.2269, R:0.0072)
Batch 425/537: Loss=1.9567 (C:1.2355, R:0.0072)
Batch 450/537: Loss=1.9219 (C:1.1940, R:0.0073)
Batch 475/537: Loss=1.9912 (C:1.2620, R:0.0073)
Batch 500/537: Loss=1.9378 (C:1.2161, R:0.0072)
Batch 525/537: Loss=1.9722 (C:1.2385, R:0.0073)

============================================================
Epoch 11/200 completed in 24.4s
Train: Loss=1.9604 (C:1.2308, R:0.0073) Ratio=2.39x
Val:   Loss=1.9405 (C:1.2417, R:0.0070) Ratio=2.30x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.9354 (C:1.2058, R:0.0073)
Batch  25/537: Loss=1.9828 (C:1.2557, R:0.0073)
Batch  50/537: Loss=1.9460 (C:1.2222, R:0.0072)
Batch  75/537: Loss=1.9285 (C:1.2054, R:0.0072)
Batch 100/537: Loss=1.9534 (C:1.2274, R:0.0073)
Batch 125/537: Loss=1.9830 (C:1.2536, R:0.0073)
Batch 150/537: Loss=1.9659 (C:1.2360, R:0.0073)
Batch 175/537: Loss=1.9449 (C:1.2230, R:0.0072)
Batch 200/537: Loss=1.9346 (C:1.2084, R:0.0073)
Batch 225/537: Loss=1.9505 (C:1.2309, R:0.0072)
Batch 250/537: Loss=1.9409 (C:1.2131, R:0.0073)
Batch 275/537: Loss=1.9186 (C:1.1889, R:0.0073)
Batch 300/537: Loss=1.9823 (C:1.2547, R:0.0073)
Batch 325/537: Loss=1.9637 (C:1.2369, R:0.0073)
Batch 350/537: Loss=1.9557 (C:1.2193, R:0.0074)
Batch 375/537: Loss=1.9329 (C:1.2054, R:0.0073)
Batch 400/537: Loss=1.9474 (C:1.2225, R:0.0072)
Batch 425/537: Loss=1.9254 (C:1.1945, R:0.0073)
Batch 450/537: Loss=1.9780 (C:1.2521, R:0.0073)
Batch 475/537: Loss=1.9184 (C:1.1959, R:0.0072)
Batch 500/537: Loss=1.9670 (C:1.2433, R:0.0072)
Batch 525/537: Loss=1.9437 (C:1.2136, R:0.0073)

============================================================
Epoch 12/200 completed in 24.0s
Train: Loss=1.9521 (C:1.2250, R:0.0073) Ratio=2.44x
Val:   Loss=1.9317 (C:1.2354, R:0.0070) Ratio=2.34x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9317)
============================================================

üåç Updating global dataset at epoch 13
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.574 ¬± 0.553
    Neg distances: 1.554 ¬± 0.837
    Separation ratio: 2.71x
    Gap: -2.633
    ‚úÖ Good global separation

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=1.9172 (C:1.1919, R:0.0073)
Batch  25/537: Loss=1.9262 (C:1.1961, R:0.0073)
Batch  50/537: Loss=1.8933 (C:1.1625, R:0.0073)
Batch  75/537: Loss=1.9161 (C:1.1882, R:0.0073)
Batch 100/537: Loss=1.9443 (C:1.2159, R:0.0073)
Batch 125/537: Loss=1.9298 (C:1.2067, R:0.0072)
Batch 150/537: Loss=1.9188 (C:1.1869, R:0.0073)
Batch 175/537: Loss=1.9219 (C:1.1957, R:0.0073)
Batch 200/537: Loss=1.8952 (C:1.1701, R:0.0073)
Batch 225/537: Loss=1.8721 (C:1.1544, R:0.0072)
Batch 250/537: Loss=1.9100 (C:1.1843, R:0.0073)
Batch 275/537: Loss=1.8990 (C:1.1749, R:0.0072)
Batch 300/537: Loss=1.9336 (C:1.2054, R:0.0073)
Batch 325/537: Loss=1.9441 (C:1.2162, R:0.0073)
Batch 350/537: Loss=1.9085 (C:1.1824, R:0.0073)
Batch 375/537: Loss=1.9316 (C:1.2077, R:0.0072)
Batch 400/537: Loss=1.8997 (C:1.1757, R:0.0072)
Batch 425/537: Loss=1.9127 (C:1.1926, R:0.0072)
Batch 450/537: Loss=1.9025 (C:1.1790, R:0.0072)
Batch 475/537: Loss=1.9028 (C:1.1848, R:0.0072)
Batch 500/537: Loss=1.9318 (C:1.2061, R:0.0073)
Batch 525/537: Loss=1.9167 (C:1.1885, R:0.0073)

============================================================
Epoch 13/200 completed in 32.3s
Train: Loss=1.9131 (C:1.1876, R:0.0073) Ratio=2.50x
Val:   Loss=1.9024 (C:1.2082, R:0.0069) Ratio=2.35x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9024)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=1.9032 (C:1.1781, R:0.0073)
Batch  25/537: Loss=1.9454 (C:1.2231, R:0.0072)
Batch  50/537: Loss=1.9155 (C:1.1889, R:0.0073)
Batch  75/537: Loss=1.9146 (C:1.1845, R:0.0073)
Batch 100/537: Loss=1.8572 (C:1.1347, R:0.0072)
Batch 125/537: Loss=1.9081 (C:1.1847, R:0.0072)
Batch 150/537: Loss=1.9045 (C:1.1776, R:0.0073)
Batch 175/537: Loss=1.9030 (C:1.1752, R:0.0073)
Batch 200/537: Loss=1.9615 (C:1.2384, R:0.0072)
Batch 225/537: Loss=1.9109 (C:1.1849, R:0.0073)
Batch 250/537: Loss=1.9176 (C:1.1920, R:0.0073)
Batch 275/537: Loss=1.8791 (C:1.1621, R:0.0072)
Batch 300/537: Loss=1.9151 (C:1.1960, R:0.0072)
Batch 325/537: Loss=1.8958 (C:1.1739, R:0.0072)
Batch 350/537: Loss=1.8706 (C:1.1514, R:0.0072)
Batch 375/537: Loss=1.9218 (C:1.1983, R:0.0072)
Batch 400/537: Loss=1.8973 (C:1.1754, R:0.0072)
Batch 425/537: Loss=1.8978 (C:1.1745, R:0.0072)
Batch 450/537: Loss=1.9229 (C:1.1964, R:0.0073)
Batch 475/537: Loss=1.9213 (C:1.1952, R:0.0073)
Batch 500/537: Loss=1.8920 (C:1.1762, R:0.0072)
Batch 525/537: Loss=1.9128 (C:1.1942, R:0.0072)

============================================================
Epoch 14/200 completed in 24.1s
Train: Loss=1.9064 (C:1.1827, R:0.0072) Ratio=2.55x
Val:   Loss=1.9000 (C:1.2069, R:0.0069) Ratio=2.37x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9000)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=1.8854 (C:1.1599, R:0.0073)
Batch  25/537: Loss=1.8801 (C:1.1565, R:0.0072)
Batch  50/537: Loss=1.9027 (C:1.1736, R:0.0073)
Batch  75/537: Loss=1.9060 (C:1.1854, R:0.0072)
Batch 100/537: Loss=1.8715 (C:1.1473, R:0.0072)
Batch 125/537: Loss=1.8946 (C:1.1744, R:0.0072)
Batch 150/537: Loss=1.9296 (C:1.2081, R:0.0072)
Batch 175/537: Loss=1.8982 (C:1.1770, R:0.0072)
Batch 200/537: Loss=1.9338 (C:1.2019, R:0.0073)
Batch 225/537: Loss=1.8608 (C:1.1402, R:0.0072)
Batch 250/537: Loss=1.9133 (C:1.1979, R:0.0072)
Batch 275/537: Loss=1.8801 (C:1.1558, R:0.0072)
Batch 300/537: Loss=1.8792 (C:1.1615, R:0.0072)
Batch 325/537: Loss=1.8690 (C:1.1524, R:0.0072)
Batch 350/537: Loss=1.8895 (C:1.1718, R:0.0072)
Batch 375/537: Loss=1.9055 (C:1.1790, R:0.0073)
Batch 400/537: Loss=1.8689 (C:1.1499, R:0.0072)
Batch 425/537: Loss=1.8712 (C:1.1469, R:0.0072)
Batch 450/537: Loss=1.8698 (C:1.1444, R:0.0073)
Batch 475/537: Loss=1.9120 (C:1.1913, R:0.0072)
Batch 500/537: Loss=1.9162 (C:1.1907, R:0.0073)
Batch 525/537: Loss=1.9150 (C:1.1969, R:0.0072)

============================================================
Epoch 15/200 completed in 24.2s
Train: Loss=1.9009 (C:1.1790, R:0.0072) Ratio=2.63x
Val:   Loss=1.8909 (C:1.2005, R:0.0069) Ratio=2.42x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8909)
============================================================

üåç Updating global dataset at epoch 16
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.543 ¬± 0.546
    Neg distances: 1.580 ¬± 0.844
    Separation ratio: 2.91x
    Gap: -2.676
    ‚úÖ Good global separation

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=1.8572 (C:1.1382, R:0.0072)
Batch  25/537: Loss=1.8810 (C:1.1575, R:0.0072)
Batch  50/537: Loss=1.8537 (C:1.1298, R:0.0072)
Batch  75/537: Loss=1.8760 (C:1.1517, R:0.0072)
Batch 100/537: Loss=1.8508 (C:1.1291, R:0.0072)
Batch 125/537: Loss=1.8928 (C:1.1681, R:0.0072)
Batch 150/537: Loss=1.8882 (C:1.1630, R:0.0073)
Batch 175/537: Loss=1.8860 (C:1.1607, R:0.0073)
Batch 200/537: Loss=1.9031 (C:1.1743, R:0.0073)
Batch 225/537: Loss=1.8676 (C:1.1448, R:0.0072)
Batch 250/537: Loss=1.9030 (C:1.1767, R:0.0073)
Batch 275/537: Loss=1.8737 (C:1.1496, R:0.0072)
Batch 300/537: Loss=1.8831 (C:1.1632, R:0.0072)
Batch 325/537: Loss=1.8669 (C:1.1472, R:0.0072)
Batch 350/537: Loss=1.8375 (C:1.1150, R:0.0072)
Batch 375/537: Loss=1.8542 (C:1.1327, R:0.0072)
Batch 400/537: Loss=1.8589 (C:1.1438, R:0.0072)
Batch 425/537: Loss=1.8744 (C:1.1546, R:0.0072)
Batch 450/537: Loss=1.8565 (C:1.1328, R:0.0072)
Batch 475/537: Loss=1.8777 (C:1.1612, R:0.0072)
Batch 500/537: Loss=1.8504 (C:1.1276, R:0.0072)
Batch 525/537: Loss=1.8919 (C:1.1733, R:0.0072)

============================================================
Epoch 16/200 completed in 33.0s
Train: Loss=1.8677 (C:1.1472, R:0.0072) Ratio=2.63x
Val:   Loss=1.8590 (C:1.1696, R:0.0069) Ratio=2.45x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8590)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=1.8950 (C:1.1787, R:0.0072)
Batch  25/537: Loss=1.8782 (C:1.1570, R:0.0072)
Batch  50/537: Loss=1.8501 (C:1.1305, R:0.0072)
Batch  75/537: Loss=1.8474 (C:1.1288, R:0.0072)
Batch 100/537: Loss=1.8324 (C:1.1159, R:0.0072)
Batch 125/537: Loss=1.8607 (C:1.1414, R:0.0072)
Batch 150/537: Loss=1.8619 (C:1.1415, R:0.0072)
Batch 175/537: Loss=1.8327 (C:1.1154, R:0.0072)
Batch 200/537: Loss=1.8593 (C:1.1410, R:0.0072)
Batch 225/537: Loss=1.9262 (C:1.2031, R:0.0072)
Batch 250/537: Loss=1.8534 (C:1.1426, R:0.0071)
Batch 275/537: Loss=1.8697 (C:1.1518, R:0.0072)
Batch 300/537: Loss=1.8318 (C:1.1145, R:0.0072)
Batch 325/537: Loss=1.8630 (C:1.1398, R:0.0072)
Batch 350/537: Loss=1.8482 (C:1.1289, R:0.0072)
Batch 375/537: Loss=1.8419 (C:1.1210, R:0.0072)
Batch 400/537: Loss=1.9087 (C:1.1897, R:0.0072)
Batch 425/537: Loss=1.8421 (C:1.1236, R:0.0072)
Batch 450/537: Loss=1.8686 (C:1.1564, R:0.0071)
Batch 475/537: Loss=1.8219 (C:1.1105, R:0.0071)
Batch 500/537: Loss=1.8886 (C:1.1659, R:0.0072)
Batch 525/537: Loss=1.8528 (C:1.1368, R:0.0072)

============================================================
Epoch 17/200 completed in 24.7s
Train: Loss=1.8613 (C:1.1424, R:0.0072) Ratio=2.73x
Val:   Loss=1.8592 (C:1.1708, R:0.0069) Ratio=2.44x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=1.8742 (C:1.1561, R:0.0072)
Batch  25/537: Loss=1.8412 (C:1.1254, R:0.0072)
Batch  50/537: Loss=1.8445 (C:1.1303, R:0.0071)
Batch  75/537: Loss=1.8631 (C:1.1465, R:0.0072)
Batch 100/537: Loss=1.8675 (C:1.1494, R:0.0072)
Batch 125/537: Loss=1.8623 (C:1.1468, R:0.0072)
Batch 150/537: Loss=1.8702 (C:1.1534, R:0.0072)
Batch 175/537: Loss=1.8315 (C:1.1123, R:0.0072)
Batch 200/537: Loss=1.8562 (C:1.1334, R:0.0072)
Batch 225/537: Loss=1.8677 (C:1.1546, R:0.0071)
Batch 250/537: Loss=1.8547 (C:1.1345, R:0.0072)
Batch 275/537: Loss=1.8889 (C:1.1612, R:0.0073)
Batch 300/537: Loss=1.8392 (C:1.1233, R:0.0072)
Batch 325/537: Loss=1.8388 (C:1.1191, R:0.0072)
Batch 350/537: Loss=1.8427 (C:1.1279, R:0.0071)
Batch 375/537: Loss=1.8529 (C:1.1434, R:0.0071)
Batch 400/537: Loss=1.8519 (C:1.1399, R:0.0071)
Batch 425/537: Loss=1.8792 (C:1.1578, R:0.0072)
Batch 450/537: Loss=1.8677 (C:1.1504, R:0.0072)
Batch 475/537: Loss=1.8634 (C:1.1436, R:0.0072)
Batch 500/537: Loss=1.8581 (C:1.1367, R:0.0072)
Batch 525/537: Loss=1.8262 (C:1.1082, R:0.0072)

============================================================
Epoch 18/200 completed in 24.6s
Train: Loss=1.8568 (C:1.1395, R:0.0072) Ratio=2.76x
Val:   Loss=1.8549 (C:1.1692, R:0.0069) Ratio=2.48x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8549)
============================================================

üåç Updating global dataset at epoch 19
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.494 ¬± 0.524
    Neg distances: 1.677 ¬± 0.866
    Separation ratio: 3.39x
    Gap: -2.798
    ‚úÖ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=1.7950 (C:1.0767, R:0.0072)
Batch  25/537: Loss=1.8197 (C:1.1042, R:0.0072)
Batch  50/537: Loss=1.8206 (C:1.1039, R:0.0072)
Batch  75/537: Loss=1.7909 (C:1.0745, R:0.0072)
Batch 100/537: Loss=1.8174 (C:1.0983, R:0.0072)
Batch 125/537: Loss=1.8079 (C:1.0859, R:0.0072)
Batch 150/537: Loss=1.7970 (C:1.0830, R:0.0071)
Batch 175/537: Loss=1.7951 (C:1.0790, R:0.0072)
Batch 200/537: Loss=1.8175 (C:1.1048, R:0.0071)
Batch 225/537: Loss=1.7922 (C:1.0790, R:0.0071)
Batch 250/537: Loss=1.8067 (C:1.0945, R:0.0071)
Batch 275/537: Loss=1.7947 (C:1.0761, R:0.0072)
Batch 300/537: Loss=1.7949 (C:1.0759, R:0.0072)
Batch 325/537: Loss=1.8195 (C:1.1041, R:0.0072)
Batch 350/537: Loss=1.8217 (C:1.1057, R:0.0072)
Batch 375/537: Loss=1.8004 (C:1.0845, R:0.0072)
Batch 400/537: Loss=1.8080 (C:1.0845, R:0.0072)
Batch 425/537: Loss=1.7769 (C:1.0630, R:0.0071)
Batch 450/537: Loss=1.7894 (C:1.0707, R:0.0072)
Batch 475/537: Loss=1.7887 (C:1.0695, R:0.0072)
Batch 500/537: Loss=1.7791 (C:1.0667, R:0.0071)
Batch 525/537: Loss=1.8029 (C:1.0933, R:0.0071)

============================================================
Epoch 19/200 completed in 33.9s
Train: Loss=1.7941 (C:1.0776, R:0.0072) Ratio=2.76x
Val:   Loss=1.7956 (C:1.1113, R:0.0068) Ratio=2.54x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7956)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=1.7847 (C:1.0680, R:0.0072)
Batch  25/537: Loss=1.7740 (C:1.0575, R:0.0072)
Batch  50/537: Loss=1.8067 (C:1.0869, R:0.0072)
Batch  75/537: Loss=1.7775 (C:1.0630, R:0.0071)
Batch 100/537: Loss=1.8210 (C:1.1009, R:0.0072)
Batch 125/537: Loss=1.7602 (C:1.0456, R:0.0071)
Batch 150/537: Loss=1.7729 (C:1.0571, R:0.0072)
Batch 175/537: Loss=1.7939 (C:1.0719, R:0.0072)
Batch 200/537: Loss=1.8047 (C:1.0950, R:0.0071)
Batch 225/537: Loss=1.8093 (C:1.0966, R:0.0071)
Batch 250/537: Loss=1.8185 (C:1.1055, R:0.0071)
Batch 275/537: Loss=1.8082 (C:1.0952, R:0.0071)
Batch 300/537: Loss=1.7969 (C:1.0811, R:0.0072)
Batch 325/537: Loss=1.8032 (C:1.0822, R:0.0072)
Batch 350/537: Loss=1.7861 (C:1.0742, R:0.0071)
Batch 375/537: Loss=1.7796 (C:1.0659, R:0.0071)
Batch 400/537: Loss=1.7828 (C:1.0720, R:0.0071)
Batch 425/537: Loss=1.7326 (C:1.0147, R:0.0072)
Batch 450/537: Loss=1.7834 (C:1.0755, R:0.0071)
Batch 475/537: Loss=1.7469 (C:1.0353, R:0.0071)
Batch 500/537: Loss=1.8156 (C:1.0992, R:0.0072)
Batch 525/537: Loss=1.8038 (C:1.0843, R:0.0072)

============================================================
Epoch 20/200 completed in 24.7s
Train: Loss=1.7908 (C:1.0755, R:0.0072) Ratio=2.87x
Val:   Loss=1.7815 (C:1.0990, R:0.0068) Ratio=2.60x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7815)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=1.7679 (C:1.0575, R:0.0071)
Batch  25/537: Loss=1.7646 (C:1.0420, R:0.0072)
Batch  50/537: Loss=1.8046 (C:1.0978, R:0.0071)
Batch  75/537: Loss=1.7951 (C:1.0765, R:0.0072)
Batch 100/537: Loss=1.7646 (C:1.0486, R:0.0072)
Batch 125/537: Loss=1.8029 (C:1.0825, R:0.0072)
Batch 150/537: Loss=1.7689 (C:1.0529, R:0.0072)
Batch 175/537: Loss=1.8003 (C:1.0880, R:0.0071)
Batch 200/537: Loss=1.8150 (C:1.1034, R:0.0071)
Batch 225/537: Loss=1.7891 (C:1.0794, R:0.0071)
Batch 250/537: Loss=1.8056 (C:1.0831, R:0.0072)
Batch 275/537: Loss=1.7864 (C:1.0723, R:0.0071)
Batch 300/537: Loss=1.8171 (C:1.0950, R:0.0072)
Batch 325/537: Loss=1.7832 (C:1.0684, R:0.0071)
Batch 350/537: Loss=1.7861 (C:1.0697, R:0.0072)
Batch 375/537: Loss=1.7439 (C:1.0333, R:0.0071)
Batch 400/537: Loss=1.8101 (C:1.0920, R:0.0072)
Batch 425/537: Loss=1.7891 (C:1.0740, R:0.0072)
Batch 450/537: Loss=1.7747 (C:1.0641, R:0.0071)
Batch 475/537: Loss=1.7918 (C:1.0744, R:0.0072)
Batch 500/537: Loss=1.8069 (C:1.0920, R:0.0071)
Batch 525/537: Loss=1.7929 (C:1.0833, R:0.0071)

============================================================
Epoch 21/200 completed in 23.9s
Train: Loss=1.7851 (C:1.0708, R:0.0071) Ratio=2.86x
Val:   Loss=1.7915 (C:1.1096, R:0.0068) Ratio=2.53x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 22
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.518 ¬± 0.551
    Neg distances: 1.675 ¬± 0.877
    Separation ratio: 3.24x
    Gap: -2.828
    ‚úÖ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=1.8036 (C:1.0898, R:0.0071)
Batch  25/537: Loss=1.7987 (C:1.0847, R:0.0071)
Batch  50/537: Loss=1.7689 (C:1.0613, R:0.0071)
Batch  75/537: Loss=1.8118 (C:1.0968, R:0.0071)
Batch 100/537: Loss=1.7486 (C:1.0335, R:0.0072)
Batch 125/537: Loss=1.7894 (C:1.0750, R:0.0071)
Batch 150/537: Loss=1.7691 (C:1.0570, R:0.0071)
Batch 175/537: Loss=1.7753 (C:1.0569, R:0.0072)
Batch 200/537: Loss=1.7996 (C:1.0907, R:0.0071)
Batch 225/537: Loss=1.7765 (C:1.0619, R:0.0071)
Batch 250/537: Loss=1.8014 (C:1.0870, R:0.0071)
Batch 275/537: Loss=1.7987 (C:1.0842, R:0.0071)
Batch 300/537: Loss=1.8352 (C:1.1199, R:0.0072)
Batch 325/537: Loss=1.8087 (C:1.0972, R:0.0071)
Batch 350/537: Loss=1.7928 (C:1.0737, R:0.0072)
Batch 375/537: Loss=1.7896 (C:1.0773, R:0.0071)
Batch 400/537: Loss=1.7889 (C:1.0713, R:0.0072)
Batch 425/537: Loss=1.8268 (C:1.1134, R:0.0071)
Batch 450/537: Loss=1.8170 (C:1.1080, R:0.0071)
Batch 475/537: Loss=1.8064 (C:1.0969, R:0.0071)
Batch 500/537: Loss=1.8148 (C:1.0964, R:0.0072)
Batch 525/537: Loss=1.8000 (C:1.0839, R:0.0072)

============================================================
Epoch 22/200 completed in 31.2s
Train: Loss=1.7996 (C:1.0862, R:0.0071) Ratio=2.95x
Val:   Loss=1.7947 (C:1.1137, R:0.0068) Ratio=2.58x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=1.7847 (C:1.0724, R:0.0071)
Batch  25/537: Loss=1.7681 (C:1.0552, R:0.0071)
Batch  50/537: Loss=1.7742 (C:1.0676, R:0.0071)
Batch  75/537: Loss=1.8326 (C:1.1236, R:0.0071)
Batch 100/537: Loss=1.8026 (C:1.0894, R:0.0071)
Batch 125/537: Loss=1.8017 (C:1.0907, R:0.0071)
Batch 150/537: Loss=1.8085 (C:1.0952, R:0.0071)
Batch 175/537: Loss=1.8278 (C:1.1168, R:0.0071)
Batch 200/537: Loss=1.7749 (C:1.0580, R:0.0072)
Batch 225/537: Loss=1.8008 (C:1.0857, R:0.0072)
Batch 250/537: Loss=1.8215 (C:1.1187, R:0.0070)
Batch 275/537: Loss=1.7840 (C:1.0725, R:0.0071)
Batch 300/537: Loss=1.7819 (C:1.0655, R:0.0072)
Batch 325/537: Loss=1.7655 (C:1.0560, R:0.0071)
Batch 350/537: Loss=1.7773 (C:1.0688, R:0.0071)
Batch 375/537: Loss=1.7845 (C:1.0801, R:0.0070)
Batch 400/537: Loss=1.8024 (C:1.0896, R:0.0071)
Batch 425/537: Loss=1.8235 (C:1.1082, R:0.0072)
Batch 450/537: Loss=1.7816 (C:1.0686, R:0.0071)
Batch 475/537: Loss=1.7945 (C:1.0787, R:0.0072)
Batch 500/537: Loss=1.8107 (C:1.0957, R:0.0071)
Batch 525/537: Loss=1.8043 (C:1.0866, R:0.0072)

============================================================
Epoch 23/200 completed in 23.7s
Train: Loss=1.7943 (C:1.0819, R:0.0071) Ratio=2.91x
Val:   Loss=1.7945 (C:1.1145, R:0.0068) Ratio=2.63x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=1.7859 (C:1.0724, R:0.0071)
Batch  25/537: Loss=1.8131 (C:1.1013, R:0.0071)
Batch  50/537: Loss=1.7857 (C:1.0812, R:0.0070)
Batch  75/537: Loss=1.7936 (C:1.0780, R:0.0072)
Batch 100/537: Loss=1.7835 (C:1.0744, R:0.0071)
Batch 125/537: Loss=1.8090 (C:1.0981, R:0.0071)
Batch 150/537: Loss=1.8036 (C:1.0945, R:0.0071)
Batch 175/537: Loss=1.7818 (C:1.0722, R:0.0071)
Batch 200/537: Loss=1.7528 (C:1.0410, R:0.0071)
Batch 225/537: Loss=1.7942 (C:1.0891, R:0.0071)
Batch 250/537: Loss=1.7915 (C:1.0776, R:0.0071)
Batch 275/537: Loss=1.8371 (C:1.1208, R:0.0072)
Batch 300/537: Loss=1.7781 (C:1.0619, R:0.0072)
Batch 325/537: Loss=1.8059 (C:1.0977, R:0.0071)
Batch 350/537: Loss=1.8041 (C:1.0864, R:0.0072)
Batch 375/537: Loss=1.7906 (C:1.0775, R:0.0071)
Batch 400/537: Loss=1.7869 (C:1.0699, R:0.0072)
Batch 425/537: Loss=1.8121 (C:1.0996, R:0.0071)
Batch 450/537: Loss=1.7926 (C:1.0879, R:0.0070)
Batch 475/537: Loss=1.7825 (C:1.0653, R:0.0072)
Batch 500/537: Loss=1.7880 (C:1.0741, R:0.0071)
Batch 525/537: Loss=1.7834 (C:1.0754, R:0.0071)

============================================================
Epoch 24/200 completed in 23.5s
Train: Loss=1.7898 (C:1.0782, R:0.0071) Ratio=3.02x
Val:   Loss=1.7993 (C:1.1193, R:0.0068) Ratio=2.62x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 25
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.496 ¬± 0.547
    Neg distances: 1.752 ¬± 0.890
    Separation ratio: 3.53x
    Gap: -2.863
    ‚úÖ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=1.8037 (C:1.0935, R:0.0071)
Batch  25/537: Loss=1.7599 (C:1.0488, R:0.0071)
Batch  50/537: Loss=1.7540 (C:1.0398, R:0.0071)
Batch  75/537: Loss=1.7668 (C:1.0512, R:0.0072)
Batch 100/537: Loss=1.7556 (C:1.0447, R:0.0071)
Batch 125/537: Loss=1.7453 (C:1.0271, R:0.0072)
Batch 150/537: Loss=1.7713 (C:1.0632, R:0.0071)
Batch 175/537: Loss=1.7598 (C:1.0484, R:0.0071)
Batch 200/537: Loss=1.7301 (C:1.0204, R:0.0071)
Batch 225/537: Loss=1.7301 (C:1.0157, R:0.0071)
Batch 250/537: Loss=1.7324 (C:1.0155, R:0.0072)
Batch 275/537: Loss=1.7128 (C:1.0078, R:0.0070)
Batch 300/537: Loss=1.7516 (C:1.0377, R:0.0071)
Batch 325/537: Loss=1.7366 (C:1.0272, R:0.0071)
Batch 350/537: Loss=1.7507 (C:1.0380, R:0.0071)
Batch 375/537: Loss=1.7402 (C:1.0344, R:0.0071)
Batch 400/537: Loss=1.7749 (C:1.0653, R:0.0071)
Batch 425/537: Loss=1.8171 (C:1.1038, R:0.0071)
Batch 450/537: Loss=1.7538 (C:1.0497, R:0.0070)
Batch 475/537: Loss=1.7559 (C:1.0433, R:0.0071)
Batch 500/537: Loss=1.7260 (C:1.0224, R:0.0070)
Batch 525/537: Loss=1.7332 (C:1.0178, R:0.0072)

============================================================
Epoch 25/200 completed in 31.5s
Train: Loss=1.7533 (C:1.0420, R:0.0071) Ratio=3.02x
Val:   Loss=1.7653 (C:1.0864, R:0.0068) Ratio=2.65x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7653)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=1.7434 (C:1.0402, R:0.0070)
Batch  25/537: Loss=1.7498 (C:1.0374, R:0.0071)
Batch  50/537: Loss=1.7696 (C:1.0629, R:0.0071)
Batch  75/537: Loss=1.7155 (C:1.0025, R:0.0071)
Batch 100/537: Loss=1.7570 (C:1.0421, R:0.0071)
Batch 125/537: Loss=1.7420 (C:1.0315, R:0.0071)
Batch 150/537: Loss=1.7764 (C:1.0697, R:0.0071)
Batch 175/537: Loss=1.6961 (C:0.9890, R:0.0071)
Batch 200/537: Loss=1.7386 (C:1.0236, R:0.0071)
Batch 225/537: Loss=1.7488 (C:1.0363, R:0.0071)
Batch 250/537: Loss=1.7437 (C:1.0374, R:0.0071)
Batch 275/537: Loss=1.7423 (C:1.0362, R:0.0071)
Batch 300/537: Loss=1.7364 (C:1.0285, R:0.0071)
Batch 325/537: Loss=1.7701 (C:1.0601, R:0.0071)
Batch 350/537: Loss=1.7838 (C:1.0718, R:0.0071)
Batch 375/537: Loss=1.7733 (C:1.0622, R:0.0071)
Batch 400/537: Loss=1.7713 (C:1.0584, R:0.0071)
Batch 425/537: Loss=1.6998 (C:0.9883, R:0.0071)
Batch 450/537: Loss=1.7685 (C:1.0598, R:0.0071)
Batch 475/537: Loss=1.7403 (C:1.0278, R:0.0071)
Batch 500/537: Loss=1.7899 (C:1.0725, R:0.0072)
Batch 525/537: Loss=1.7777 (C:1.0701, R:0.0071)

============================================================
Epoch 26/200 completed in 23.7s
Train: Loss=1.7478 (C:1.0373, R:0.0071) Ratio=3.08x
Val:   Loss=1.7546 (C:1.0762, R:0.0068) Ratio=2.63x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7546)
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=1.7327 (C:1.0192, R:0.0071)
Batch  25/537: Loss=1.7302 (C:1.0209, R:0.0071)
Batch  50/537: Loss=1.7208 (C:1.0070, R:0.0071)
Batch  75/537: Loss=1.7692 (C:1.0599, R:0.0071)
Batch 100/537: Loss=1.7532 (C:1.0441, R:0.0071)
Batch 125/537: Loss=1.7494 (C:1.0363, R:0.0071)
Batch 150/537: Loss=1.7108 (C:0.9940, R:0.0072)
Batch 175/537: Loss=1.7659 (C:1.0517, R:0.0071)
Batch 200/537: Loss=1.7523 (C:1.0425, R:0.0071)
Batch 225/537: Loss=1.7604 (C:1.0462, R:0.0071)
Batch 250/537: Loss=1.7514 (C:1.0404, R:0.0071)
Batch 275/537: Loss=1.7471 (C:1.0349, R:0.0071)
Batch 300/537: Loss=1.7474 (C:1.0371, R:0.0071)
Batch 325/537: Loss=1.7369 (C:1.0299, R:0.0071)
Batch 350/537: Loss=1.7208 (C:1.0173, R:0.0070)
Batch 375/537: Loss=1.7610 (C:1.0562, R:0.0070)
Batch 400/537: Loss=1.7720 (C:1.0631, R:0.0071)
Batch 425/537: Loss=1.7526 (C:1.0425, R:0.0071)
Batch 450/537: Loss=1.7498 (C:1.0468, R:0.0070)
Batch 475/537: Loss=1.7369 (C:1.0221, R:0.0071)
Batch 500/537: Loss=1.7679 (C:1.0599, R:0.0071)
Batch 525/537: Loss=1.7434 (C:1.0362, R:0.0071)

============================================================
Epoch 27/200 completed in 23.8s
Train: Loss=1.7455 (C:1.0357, R:0.0071) Ratio=3.11x
Val:   Loss=1.7490 (C:1.0727, R:0.0068) Ratio=2.68x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7490)
============================================================

üåç Updating global dataset at epoch 28
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.468 ¬± 0.516
    Neg distances: 1.833 ¬± 0.902
    Separation ratio: 3.92x
    Gap: -3.028
    ‚úÖ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=1.7143 (C:1.0030, R:0.0071)
Batch  25/537: Loss=1.6851 (C:0.9734, R:0.0071)
Batch  50/537: Loss=1.6837 (C:0.9787, R:0.0070)
Batch  75/537: Loss=1.7035 (C:0.9958, R:0.0071)
Batch 100/537: Loss=1.7197 (C:1.0042, R:0.0072)
Batch 125/537: Loss=1.7117 (C:0.9987, R:0.0071)
Batch 150/537: Loss=1.6904 (C:0.9854, R:0.0070)
Batch 175/537: Loss=1.7074 (C:0.9923, R:0.0072)
Batch 200/537: Loss=1.7352 (C:1.0274, R:0.0071)
Batch 225/537: Loss=1.6576 (C:0.9545, R:0.0070)
Batch 250/537: Loss=1.6994 (C:0.9861, R:0.0071)
Batch 275/537: Loss=1.7013 (C:0.9920, R:0.0071)
Batch 300/537: Loss=1.7071 (C:0.9962, R:0.0071)
Batch 325/537: Loss=1.6802 (C:0.9728, R:0.0071)
Batch 350/537: Loss=1.6783 (C:0.9693, R:0.0071)
Batch 375/537: Loss=1.6975 (C:0.9876, R:0.0071)
Batch 400/537: Loss=1.6930 (C:0.9765, R:0.0072)
Batch 425/537: Loss=1.7181 (C:1.0085, R:0.0071)
Batch 450/537: Loss=1.6981 (C:0.9856, R:0.0071)
Batch 475/537: Loss=1.7001 (C:0.9951, R:0.0071)
Batch 500/537: Loss=1.7085 (C:0.9986, R:0.0071)
Batch 525/537: Loss=1.7207 (C:1.0137, R:0.0071)

============================================================
Epoch 28/200 completed in 31.6s
Train: Loss=1.7003 (C:0.9907, R:0.0071) Ratio=3.12x
Val:   Loss=1.7099 (C:1.0345, R:0.0068) Ratio=2.71x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7099)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=1.6959 (C:0.9867, R:0.0071)
Batch  25/537: Loss=1.6738 (C:0.9704, R:0.0070)
Batch  50/537: Loss=1.7025 (C:0.9942, R:0.0071)
Batch  75/537: Loss=1.6802 (C:0.9665, R:0.0071)
Batch 100/537: Loss=1.6817 (C:0.9672, R:0.0071)
Batch 125/537: Loss=1.6493 (C:0.9485, R:0.0070)
Batch 150/537: Loss=1.6512 (C:0.9439, R:0.0071)
Batch 175/537: Loss=1.7089 (C:0.9977, R:0.0071)
Batch 200/537: Loss=1.6557 (C:0.9484, R:0.0071)
Batch 225/537: Loss=1.6967 (C:0.9837, R:0.0071)
Batch 250/537: Loss=1.7120 (C:1.0073, R:0.0070)
Batch 275/537: Loss=1.6549 (C:0.9467, R:0.0071)
Batch 300/537: Loss=1.6774 (C:0.9679, R:0.0071)
Batch 325/537: Loss=1.6776 (C:0.9739, R:0.0070)
Batch 350/537: Loss=1.6579 (C:0.9452, R:0.0071)
Batch 375/537: Loss=1.6891 (C:0.9814, R:0.0071)
Batch 400/537: Loss=1.7011 (C:0.9955, R:0.0071)
Batch 425/537: Loss=1.7060 (C:0.9969, R:0.0071)
Batch 450/537: Loss=1.7092 (C:1.0079, R:0.0070)
Batch 475/537: Loss=1.6800 (C:0.9716, R:0.0071)
Batch 500/537: Loss=1.6972 (C:0.9894, R:0.0071)
Batch 525/537: Loss=1.7103 (C:1.0006, R:0.0071)

============================================================
Epoch 29/200 completed in 24.0s
Train: Loss=1.6955 (C:0.9865, R:0.0071) Ratio=3.19x
Val:   Loss=1.7085 (C:1.0327, R:0.0068) Ratio=2.75x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7085)
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=1.7056 (C:0.9980, R:0.0071)
Batch  25/537: Loss=1.7129 (C:0.9997, R:0.0071)
Batch  50/537: Loss=1.6728 (C:0.9636, R:0.0071)
Batch  75/537: Loss=1.7151 (C:1.0097, R:0.0071)
Batch 100/537: Loss=1.7115 (C:0.9990, R:0.0071)
Batch 125/537: Loss=1.7100 (C:0.9999, R:0.0071)
Batch 150/537: Loss=1.6988 (C:0.9901, R:0.0071)
Batch 175/537: Loss=1.7233 (C:1.0161, R:0.0071)
Batch 200/537: Loss=1.6963 (C:0.9930, R:0.0070)
Batch 225/537: Loss=1.6979 (C:0.9869, R:0.0071)
Batch 250/537: Loss=1.7043 (C:0.9981, R:0.0071)
Batch 275/537: Loss=1.6850 (C:0.9776, R:0.0071)
Batch 300/537: Loss=1.6775 (C:0.9689, R:0.0071)
Batch 325/537: Loss=1.6557 (C:0.9500, R:0.0071)
Batch 350/537: Loss=1.7082 (C:0.9978, R:0.0071)
Batch 375/537: Loss=1.7313 (C:1.0233, R:0.0071)
Batch 400/537: Loss=1.7225 (C:1.0147, R:0.0071)
Batch 425/537: Loss=1.7181 (C:1.0062, R:0.0071)
Batch 450/537: Loss=1.6979 (C:0.9917, R:0.0071)
Batch 475/537: Loss=1.6941 (C:0.9889, R:0.0071)
Batch 500/537: Loss=1.7269 (C:1.0155, R:0.0071)
Batch 525/537: Loss=1.6824 (C:0.9695, R:0.0071)

============================================================
Epoch 30/200 completed in 24.0s
Train: Loss=1.6924 (C:0.9840, R:0.0071) Ratio=3.14x
Val:   Loss=1.7121 (C:1.0374, R:0.0067) Ratio=2.70x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 31
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.475 ¬± 0.551
    Neg distances: 1.887 ¬± 0.901
    Separation ratio: 3.97x
    Gap: -3.061
    ‚úÖ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=1.6177 (C:0.9065, R:0.0071)
Batch  25/537: Loss=1.6490 (C:0.9398, R:0.0071)
Batch  50/537: Loss=1.6522 (C:0.9433, R:0.0071)
Batch  75/537: Loss=1.6900 (C:0.9785, R:0.0071)
Batch 100/537: Loss=1.6295 (C:0.9202, R:0.0071)
Batch 125/537: Loss=1.6601 (C:0.9497, R:0.0071)
Batch 150/537: Loss=1.7326 (C:1.0289, R:0.0070)
Batch 175/537: Loss=1.6430 (C:0.9370, R:0.0071)
Batch 200/537: Loss=1.6546 (C:0.9505, R:0.0070)
Batch 225/537: Loss=1.6566 (C:0.9494, R:0.0071)
Batch 250/537: Loss=1.6446 (C:0.9398, R:0.0070)
Batch 275/537: Loss=1.7042 (C:1.0011, R:0.0070)
Batch 300/537: Loss=1.6887 (C:0.9775, R:0.0071)
Batch 325/537: Loss=1.6369 (C:0.9314, R:0.0071)
Batch 350/537: Loss=1.6903 (C:0.9832, R:0.0071)
Batch 375/537: Loss=1.6560 (C:0.9459, R:0.0071)
Batch 400/537: Loss=1.6542 (C:0.9493, R:0.0070)
Batch 425/537: Loss=1.6841 (C:0.9792, R:0.0070)
Batch 450/537: Loss=1.7204 (C:1.0054, R:0.0072)
Batch 475/537: Loss=1.6838 (C:0.9780, R:0.0071)
Batch 500/537: Loss=1.7186 (C:1.0060, R:0.0071)
Batch 525/537: Loss=1.6752 (C:0.9637, R:0.0071)

============================================================
Epoch 31/200 completed in 31.6s
Train: Loss=1.6725 (C:0.9644, R:0.0071) Ratio=3.25x
Val:   Loss=1.6923 (C:1.0177, R:0.0067) Ratio=2.77x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6923)
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=1.6599 (C:0.9515, R:0.0071)
Batch  25/537: Loss=1.6965 (C:0.9866, R:0.0071)
Batch  50/537: Loss=1.6482 (C:0.9411, R:0.0071)
Batch  75/537: Loss=1.6716 (C:0.9575, R:0.0071)
Batch 100/537: Loss=1.6853 (C:0.9751, R:0.0071)
Batch 125/537: Loss=1.6316 (C:0.9261, R:0.0071)
Batch 150/537: Loss=1.6723 (C:0.9678, R:0.0070)
Batch 175/537: Loss=1.6402 (C:0.9279, R:0.0071)
Batch 200/537: Loss=1.6863 (C:0.9772, R:0.0071)
Batch 225/537: Loss=1.6459 (C:0.9417, R:0.0070)
Batch 250/537: Loss=1.6654 (C:0.9512, R:0.0071)
Batch 275/537: Loss=1.7062 (C:1.0013, R:0.0070)
Batch 300/537: Loss=1.6522 (C:0.9527, R:0.0070)
Batch 325/537: Loss=1.6983 (C:0.9832, R:0.0072)
Batch 350/537: Loss=1.6627 (C:0.9519, R:0.0071)
Batch 375/537: Loss=1.7058 (C:0.9971, R:0.0071)
Batch 400/537: Loss=1.6878 (C:0.9747, R:0.0071)
Batch 425/537: Loss=1.6771 (C:0.9688, R:0.0071)
Batch 450/537: Loss=1.6100 (C:0.9053, R:0.0070)
Batch 475/537: Loss=1.6576 (C:0.9563, R:0.0070)
Batch 500/537: Loss=1.7038 (C:0.9946, R:0.0071)
Batch 525/537: Loss=1.6732 (C:0.9671, R:0.0071)

============================================================
Epoch 32/200 completed in 23.9s
Train: Loss=1.6675 (C:0.9599, R:0.0071) Ratio=3.22x
Val:   Loss=1.6729 (C:0.9994, R:0.0067) Ratio=2.80x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6729)
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=1.6668 (C:0.9609, R:0.0071)
Batch  25/537: Loss=1.7117 (C:1.0078, R:0.0070)
Batch  50/537: Loss=1.6767 (C:0.9708, R:0.0071)
Batch  75/537: Loss=1.6569 (C:0.9467, R:0.0071)
Batch 100/537: Loss=1.6972 (C:0.9957, R:0.0070)
Batch 125/537: Loss=1.6582 (C:0.9453, R:0.0071)
Batch 150/537: Loss=1.6509 (C:0.9454, R:0.0071)
Batch 175/537: Loss=1.6698 (C:0.9669, R:0.0070)
Batch 200/537: Loss=1.7204 (C:1.0108, R:0.0071)
Batch 225/537: Loss=1.6492 (C:0.9428, R:0.0071)
Batch 250/537: Loss=1.6586 (C:0.9523, R:0.0071)
Batch 275/537: Loss=1.6854 (C:0.9841, R:0.0070)
Batch 300/537: Loss=1.6663 (C:0.9626, R:0.0070)
Batch 325/537: Loss=1.6848 (C:0.9772, R:0.0071)
Batch 350/537: Loss=1.6643 (C:0.9588, R:0.0071)
Batch 375/537: Loss=1.6524 (C:0.9476, R:0.0070)
Batch 400/537: Loss=1.6659 (C:0.9683, R:0.0070)
Batch 425/537: Loss=1.6801 (C:0.9702, R:0.0071)
Batch 450/537: Loss=1.6836 (C:0.9779, R:0.0071)
Batch 475/537: Loss=1.6432 (C:0.9412, R:0.0070)
Batch 500/537: Loss=1.6359 (C:0.9267, R:0.0071)
Batch 525/537: Loss=1.6530 (C:0.9553, R:0.0070)

============================================================
Epoch 33/200 completed in 23.9s
Train: Loss=1.6653 (C:0.9582, R:0.0071) Ratio=3.29x
Val:   Loss=1.6913 (C:1.0185, R:0.0067) Ratio=2.74x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 34
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.492 ¬± 0.596
    Neg distances: 2.006 ¬± 0.947
    Separation ratio: 4.08x
    Gap: -3.238
    ‚úÖ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=1.6220 (C:0.9127, R:0.0071)
Batch  25/537: Loss=1.6317 (C:0.9265, R:0.0071)
Batch  50/537: Loss=1.6630 (C:0.9557, R:0.0071)
Batch  75/537: Loss=1.6225 (C:0.9137, R:0.0071)
Batch 100/537: Loss=1.5998 (C:0.8945, R:0.0071)
Batch 125/537: Loss=1.6392 (C:0.9310, R:0.0071)
Batch 150/537: Loss=1.6560 (C:0.9506, R:0.0071)
Batch 175/537: Loss=1.5945 (C:0.8879, R:0.0071)
Batch 200/537: Loss=1.6625 (C:0.9549, R:0.0071)
Batch 225/537: Loss=1.6422 (C:0.9341, R:0.0071)
Batch 250/537: Loss=1.6627 (C:0.9532, R:0.0071)
Batch 275/537: Loss=1.6339 (C:0.9236, R:0.0071)
Batch 300/537: Loss=1.6563 (C:0.9490, R:0.0071)
Batch 325/537: Loss=1.6310 (C:0.9208, R:0.0071)
Batch 350/537: Loss=1.6579 (C:0.9507, R:0.0071)
Batch 375/537: Loss=1.6488 (C:0.9412, R:0.0071)
Batch 400/537: Loss=1.6544 (C:0.9385, R:0.0072)
Batch 425/537: Loss=1.6124 (C:0.9062, R:0.0071)
Batch 450/537: Loss=1.6528 (C:0.9509, R:0.0070)
Batch 475/537: Loss=1.6536 (C:0.9525, R:0.0070)
Batch 500/537: Loss=1.6137 (C:0.9029, R:0.0071)
Batch 525/537: Loss=1.6170 (C:0.9158, R:0.0070)

============================================================
Epoch 34/200 completed in 31.8s
Train: Loss=1.6422 (C:0.9352, R:0.0071) Ratio=3.35x
Val:   Loss=1.6647 (C:0.9919, R:0.0067) Ratio=2.79x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6647)
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=1.6044 (C:0.8965, R:0.0071)
Batch  25/537: Loss=1.6381 (C:0.9417, R:0.0070)
Batch  50/537: Loss=1.6130 (C:0.9108, R:0.0070)
Batch  75/537: Loss=1.6343 (C:0.9275, R:0.0071)
Batch 100/537: Loss=1.6539 (C:0.9520, R:0.0070)
Batch 125/537: Loss=1.6350 (C:0.9302, R:0.0070)
Batch 150/537: Loss=1.6578 (C:0.9501, R:0.0071)
Batch 175/537: Loss=1.6190 (C:0.9090, R:0.0071)
Batch 200/537: Loss=1.6492 (C:0.9429, R:0.0071)
Batch 225/537: Loss=1.6481 (C:0.9372, R:0.0071)
Batch 250/537: Loss=1.6281 (C:0.9223, R:0.0071)
Batch 275/537: Loss=1.6355 (C:0.9276, R:0.0071)
Batch 300/537: Loss=1.6120 (C:0.9046, R:0.0071)
Batch 325/537: Loss=1.6328 (C:0.9247, R:0.0071)
Batch 350/537: Loss=1.6093 (C:0.9036, R:0.0071)
Batch 375/537: Loss=1.6326 (C:0.9222, R:0.0071)
Batch 400/537: Loss=1.6695 (C:0.9661, R:0.0070)
Batch 425/537: Loss=1.7059 (C:0.9982, R:0.0071)
Batch 450/537: Loss=1.6431 (C:0.9353, R:0.0071)
Batch 475/537: Loss=1.6369 (C:0.9298, R:0.0071)
Batch 500/537: Loss=1.6409 (C:0.9288, R:0.0071)
Batch 525/537: Loss=1.6293 (C:0.9258, R:0.0070)

============================================================
Epoch 35/200 completed in 24.0s
Train: Loss=1.6410 (C:0.9345, R:0.0071) Ratio=3.42x
Val:   Loss=1.6754 (C:1.0030, R:0.0067) Ratio=2.78x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=1.6103 (C:0.9099, R:0.0070)
Batch  25/537: Loss=1.6054 (C:0.8990, R:0.0071)
Batch  50/537: Loss=1.6372 (C:0.9258, R:0.0071)
Batch  75/537: Loss=1.6732 (C:0.9671, R:0.0071)
Batch 100/537: Loss=1.6518 (C:0.9443, R:0.0071)
Batch 125/537: Loss=1.6465 (C:0.9442, R:0.0070)
Batch 150/537: Loss=1.6464 (C:0.9369, R:0.0071)
Batch 175/537: Loss=1.6390 (C:0.9238, R:0.0072)
Batch 200/537: Loss=1.6494 (C:0.9407, R:0.0071)
Batch 225/537: Loss=1.6268 (C:0.9187, R:0.0071)
Batch 250/537: Loss=1.6441 (C:0.9356, R:0.0071)
Batch 275/537: Loss=1.6365 (C:0.9288, R:0.0071)
Batch 300/537: Loss=1.6229 (C:0.9130, R:0.0071)
Batch 325/537: Loss=1.6403 (C:0.9341, R:0.0071)
Batch 350/537: Loss=1.6447 (C:0.9326, R:0.0071)
Batch 375/537: Loss=1.6369 (C:0.9331, R:0.0070)
Batch 400/537: Loss=1.5988 (C:0.8968, R:0.0070)
Batch 425/537: Loss=1.7140 (C:1.0075, R:0.0071)
Batch 450/537: Loss=1.6251 (C:0.9123, R:0.0071)
Batch 475/537: Loss=1.6300 (C:0.9249, R:0.0071)
Batch 500/537: Loss=1.6295 (C:0.9240, R:0.0071)
Batch 525/537: Loss=1.6554 (C:0.9512, R:0.0070)

============================================================
Epoch 36/200 completed in 24.2s
Train: Loss=1.6379 (C:0.9318, R:0.0071) Ratio=3.36x
Val:   Loss=1.6708 (C:0.9990, R:0.0067) Ratio=2.75x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 37
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.500 ¬± 0.601
    Neg distances: 2.043 ¬± 0.955
    Separation ratio: 4.09x
    Gap: -3.321
    ‚úÖ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=1.6091 (C:0.8980, R:0.0071)
Batch  25/537: Loss=1.6424 (C:0.9368, R:0.0071)
Batch  50/537: Loss=1.6298 (C:0.9226, R:0.0071)
Batch  75/537: Loss=1.6260 (C:0.9193, R:0.0071)
Batch 100/537: Loss=1.6051 (C:0.9065, R:0.0070)
Batch 125/537: Loss=1.6640 (C:0.9596, R:0.0070)
Batch 150/537: Loss=1.5892 (C:0.8890, R:0.0070)
Batch 175/537: Loss=1.6491 (C:0.9424, R:0.0071)
Batch 200/537: Loss=1.6313 (C:0.9254, R:0.0071)
Batch 225/537: Loss=1.6314 (C:0.9256, R:0.0071)
Batch 250/537: Loss=1.5870 (C:0.8759, R:0.0071)
Batch 275/537: Loss=1.6477 (C:0.9397, R:0.0071)
Batch 300/537: Loss=1.6368 (C:0.9342, R:0.0070)
Batch 325/537: Loss=1.6592 (C:0.9486, R:0.0071)
Batch 350/537: Loss=1.6314 (C:0.9247, R:0.0071)
Batch 375/537: Loss=1.6338 (C:0.9257, R:0.0071)
Batch 400/537: Loss=1.6118 (C:0.9101, R:0.0070)
Batch 425/537: Loss=1.6170 (C:0.9151, R:0.0070)
Batch 450/537: Loss=1.6416 (C:0.9345, R:0.0071)
Batch 475/537: Loss=1.6048 (C:0.8993, R:0.0071)
Batch 500/537: Loss=1.6080 (C:0.9064, R:0.0070)
Batch 525/537: Loss=1.6436 (C:0.9353, R:0.0071)

============================================================
Epoch 37/200 completed in 32.8s
Train: Loss=1.6219 (C:0.9162, R:0.0071) Ratio=3.37x
Val:   Loss=1.6498 (C:0.9781, R:0.0067) Ratio=2.81x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6498)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=1.6197 (C:0.9120, R:0.0071)
Batch  25/537: Loss=1.6195 (C:0.9155, R:0.0070)
Batch  50/537: Loss=1.6338 (C:0.9224, R:0.0071)
Batch  75/537: Loss=1.6213 (C:0.9131, R:0.0071)
Batch 100/537: Loss=1.6329 (C:0.9296, R:0.0070)
Batch 125/537: Loss=1.6030 (C:0.8962, R:0.0071)
Batch 150/537: Loss=1.6073 (C:0.8999, R:0.0071)
Batch 175/537: Loss=1.5816 (C:0.8758, R:0.0071)
Batch 200/537: Loss=1.6086 (C:0.9049, R:0.0070)
Batch 225/537: Loss=1.6334 (C:0.9251, R:0.0071)
Batch 250/537: Loss=1.5943 (C:0.8850, R:0.0071)
Batch 275/537: Loss=1.6319 (C:0.9252, R:0.0071)
Batch 300/537: Loss=1.6444 (C:0.9327, R:0.0071)
Batch 325/537: Loss=1.6236 (C:0.9182, R:0.0071)
Batch 350/537: Loss=1.6039 (C:0.8981, R:0.0071)
Batch 375/537: Loss=1.6330 (C:0.9271, R:0.0071)
Batch 400/537: Loss=1.6127 (C:0.9083, R:0.0070)
Batch 425/537: Loss=1.6009 (C:0.8907, R:0.0071)
Batch 450/537: Loss=1.6304 (C:0.9295, R:0.0070)
Batch 475/537: Loss=1.6001 (C:0.9004, R:0.0070)
Batch 500/537: Loss=1.6248 (C:0.9200, R:0.0070)
Batch 525/537: Loss=1.6483 (C:0.9458, R:0.0070)

============================================================
Epoch 38/200 completed in 24.5s
Train: Loss=1.6187 (C:0.9133, R:0.0071) Ratio=3.39x
Val:   Loss=1.6577 (C:0.9866, R:0.0067) Ratio=2.79x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=1.5773 (C:0.8698, R:0.0071)
Batch  25/537: Loss=1.6135 (C:0.9095, R:0.0070)
Batch  50/537: Loss=1.6032 (C:0.8906, R:0.0071)
Batch  75/537: Loss=1.6390 (C:0.9372, R:0.0070)
Batch 100/537: Loss=1.5878 (C:0.8771, R:0.0071)
Batch 125/537: Loss=1.6080 (C:0.9073, R:0.0070)
Batch 150/537: Loss=1.5938 (C:0.8917, R:0.0070)
Batch 175/537: Loss=1.5948 (C:0.8860, R:0.0071)
Batch 200/537: Loss=1.5867 (C:0.8801, R:0.0071)
Batch 225/537: Loss=1.6315 (C:0.9309, R:0.0070)
Batch 250/537: Loss=1.5936 (C:0.8887, R:0.0070)
Batch 275/537: Loss=1.6119 (C:0.9103, R:0.0070)
Batch 300/537: Loss=1.6201 (C:0.9199, R:0.0070)
Batch 325/537: Loss=1.5883 (C:0.8860, R:0.0070)
Batch 350/537: Loss=1.5535 (C:0.8478, R:0.0071)
Batch 375/537: Loss=1.5938 (C:0.8894, R:0.0070)
Batch 400/537: Loss=1.6389 (C:0.9350, R:0.0070)
Batch 425/537: Loss=1.6173 (C:0.9166, R:0.0070)
Batch 450/537: Loss=1.6449 (C:0.9358, R:0.0071)
Batch 475/537: Loss=1.5673 (C:0.8675, R:0.0070)
Batch 500/537: Loss=1.5880 (C:0.8846, R:0.0070)
Batch 525/537: Loss=1.5950 (C:0.8886, R:0.0071)

============================================================
Epoch 39/200 completed in 24.0s
Train: Loss=1.6161 (C:0.9112, R:0.0070) Ratio=3.48x
Val:   Loss=1.6432 (C:0.9738, R:0.0067) Ratio=2.84x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6432)
============================================================

üåç Updating global dataset at epoch 40
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.497 ¬± 0.623
    Neg distances: 2.149 ¬± 0.976
    Separation ratio: 4.33x
    Gap: -3.455
    ‚úÖ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=1.5819 (C:0.8780, R:0.0070)
Batch  25/537: Loss=1.6054 (C:0.9003, R:0.0071)
Batch  50/537: Loss=1.5658 (C:0.8610, R:0.0070)
Batch  75/537: Loss=1.5453 (C:0.8390, R:0.0071)
Batch 100/537: Loss=1.5729 (C:0.8720, R:0.0070)
Batch 125/537: Loss=1.5716 (C:0.8651, R:0.0071)
Batch 150/537: Loss=1.6170 (C:0.9104, R:0.0071)
Batch 175/537: Loss=1.5610 (C:0.8556, R:0.0071)
Batch 200/537: Loss=1.5920 (C:0.8777, R:0.0071)
Batch 225/537: Loss=1.5483 (C:0.8469, R:0.0070)
Batch 250/537: Loss=1.5941 (C:0.8897, R:0.0070)
Batch 275/537: Loss=1.5714 (C:0.8611, R:0.0071)
Batch 300/537: Loss=1.5863 (C:0.8827, R:0.0070)
Batch 325/537: Loss=1.5613 (C:0.8518, R:0.0071)
Batch 350/537: Loss=1.5744 (C:0.8659, R:0.0071)
Batch 375/537: Loss=1.6074 (C:0.9014, R:0.0071)
Batch 400/537: Loss=1.6121 (C:0.9116, R:0.0070)
Batch 425/537: Loss=1.5617 (C:0.8527, R:0.0071)
Batch 450/537: Loss=1.5711 (C:0.8667, R:0.0070)
Batch 475/537: Loss=1.5743 (C:0.8684, R:0.0071)
Batch 500/537: Loss=1.5840 (C:0.8765, R:0.0071)
Batch 525/537: Loss=1.6274 (C:0.9230, R:0.0070)

============================================================
Epoch 40/200 completed in 32.8s
Train: Loss=1.5786 (C:0.8739, R:0.0070) Ratio=3.43x
Val:   Loss=1.6093 (C:0.9392, R:0.0067) Ratio=2.89x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6093)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=1.5457 (C:0.8436, R:0.0070)
Batch  25/537: Loss=1.5409 (C:0.8354, R:0.0071)
Batch  50/537: Loss=1.5829 (C:0.8808, R:0.0070)
Batch  75/537: Loss=1.5924 (C:0.8840, R:0.0071)
Batch 100/537: Loss=1.5941 (C:0.8905, R:0.0070)
Batch 125/537: Loss=1.5765 (C:0.8715, R:0.0071)
Batch 150/537: Loss=1.5836 (C:0.8827, R:0.0070)
Batch 175/537: Loss=1.6008 (C:0.8997, R:0.0070)
Batch 200/537: Loss=1.5610 (C:0.8532, R:0.0071)
Batch 225/537: Loss=1.5422 (C:0.8386, R:0.0070)
Batch 250/537: Loss=1.5685 (C:0.8665, R:0.0070)
Batch 275/537: Loss=1.5462 (C:0.8428, R:0.0070)
Batch 300/537: Loss=1.5866 (C:0.8793, R:0.0071)
Batch 325/537: Loss=1.5787 (C:0.8742, R:0.0070)
Batch 350/537: Loss=1.5989 (C:0.8931, R:0.0071)
Batch 375/537: Loss=1.6067 (C:0.8986, R:0.0071)
Batch 400/537: Loss=1.5725 (C:0.8618, R:0.0071)
Batch 425/537: Loss=1.5735 (C:0.8685, R:0.0071)
Batch 450/537: Loss=1.5670 (C:0.8621, R:0.0070)
Batch 475/537: Loss=1.6095 (C:0.9062, R:0.0070)
Batch 500/537: Loss=1.5776 (C:0.8763, R:0.0070)
Batch 525/537: Loss=1.5935 (C:0.8904, R:0.0070)

============================================================
Epoch 41/200 completed in 24.4s
Train: Loss=1.5767 (C:0.8722, R:0.0070) Ratio=3.48x
Val:   Loss=1.6049 (C:0.9357, R:0.0067) Ratio=2.89x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6049)
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=1.5386 (C:0.8408, R:0.0070)
Batch  25/537: Loss=1.5573 (C:0.8526, R:0.0070)
Batch  50/537: Loss=1.5515 (C:0.8458, R:0.0071)
Batch  75/537: Loss=1.6058 (C:0.9009, R:0.0070)
Batch 100/537: Loss=1.5573 (C:0.8518, R:0.0071)
Batch 125/537: Loss=1.5398 (C:0.8359, R:0.0070)
Batch 150/537: Loss=1.5671 (C:0.8629, R:0.0070)
Batch 175/537: Loss=1.5670 (C:0.8663, R:0.0070)
Batch 200/537: Loss=1.5696 (C:0.8663, R:0.0070)
Batch 225/537: Loss=1.5602 (C:0.8585, R:0.0070)
Batch 250/537: Loss=1.5786 (C:0.8792, R:0.0070)
Batch 275/537: Loss=1.5573 (C:0.8522, R:0.0071)
Batch 300/537: Loss=1.6006 (C:0.8981, R:0.0070)
Batch 325/537: Loss=1.5967 (C:0.8906, R:0.0071)
Batch 350/537: Loss=1.5860 (C:0.8851, R:0.0070)
Batch 375/537: Loss=1.5394 (C:0.8393, R:0.0070)
Batch 400/537: Loss=1.5359 (C:0.8332, R:0.0070)
Batch 425/537: Loss=1.5804 (C:0.8851, R:0.0070)
Batch 450/537: Loss=1.5889 (C:0.8861, R:0.0070)
Batch 475/537: Loss=1.5463 (C:0.8462, R:0.0070)
Batch 500/537: Loss=1.5802 (C:0.8641, R:0.0072)
Batch 525/537: Loss=1.5534 (C:0.8550, R:0.0070)

============================================================
Epoch 42/200 completed in 24.0s
Train: Loss=1.5740 (C:0.8699, R:0.0070) Ratio=3.51x
Val:   Loss=1.6111 (C:0.9422, R:0.0067) Ratio=2.87x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 43
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.520 ¬± 0.665
    Neg distances: 2.254 ¬± 1.012
    Separation ratio: 4.33x
    Gap: -3.615
    ‚úÖ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=1.5703 (C:0.8649, R:0.0071)
Batch  25/537: Loss=1.5039 (C:0.8002, R:0.0070)
Batch  50/537: Loss=1.5387 (C:0.8346, R:0.0070)
Batch  75/537: Loss=1.5681 (C:0.8591, R:0.0071)
Batch 100/537: Loss=1.5386 (C:0.8369, R:0.0070)
Batch 125/537: Loss=1.5352 (C:0.8345, R:0.0070)
Batch 150/537: Loss=1.5836 (C:0.8805, R:0.0070)
Batch 175/537: Loss=1.5441 (C:0.8386, R:0.0071)
Batch 200/537: Loss=1.5322 (C:0.8183, R:0.0071)
Batch 225/537: Loss=1.5401 (C:0.8325, R:0.0071)
Batch 250/537: Loss=1.5722 (C:0.8661, R:0.0071)
Batch 275/537: Loss=1.5700 (C:0.8593, R:0.0071)
Batch 300/537: Loss=1.5505 (C:0.8439, R:0.0071)
Batch 325/537: Loss=1.5269 (C:0.8193, R:0.0071)
Batch 350/537: Loss=1.5256 (C:0.8201, R:0.0071)
Batch 375/537: Loss=1.5560 (C:0.8475, R:0.0071)
Batch 400/537: Loss=1.5226 (C:0.8201, R:0.0070)
Batch 425/537: Loss=1.5867 (C:0.8757, R:0.0071)
Batch 450/537: Loss=1.5463 (C:0.8377, R:0.0071)
Batch 475/537: Loss=1.5932 (C:0.8892, R:0.0070)
Batch 500/537: Loss=1.5476 (C:0.8444, R:0.0070)
Batch 525/537: Loss=1.5605 (C:0.8519, R:0.0071)

============================================================
Epoch 43/200 completed in 32.2s
Train: Loss=1.5503 (C:0.8460, R:0.0070) Ratio=3.55x
Val:   Loss=1.5795 (C:0.9103, R:0.0067) Ratio=2.86x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5795)
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=1.5242 (C:0.8222, R:0.0070)
Batch  25/537: Loss=1.5732 (C:0.8690, R:0.0070)
Batch  50/537: Loss=1.5206 (C:0.8143, R:0.0071)
Batch  75/537: Loss=1.5240 (C:0.8209, R:0.0070)
Batch 100/537: Loss=1.5665 (C:0.8627, R:0.0070)
Batch 125/537: Loss=1.5682 (C:0.8658, R:0.0070)
Batch 150/537: Loss=1.5511 (C:0.8500, R:0.0070)
Batch 175/537: Loss=1.5199 (C:0.8182, R:0.0070)
Batch 200/537: Loss=1.5921 (C:0.8857, R:0.0071)
Batch 225/537: Loss=1.5645 (C:0.8596, R:0.0070)
Batch 250/537: Loss=1.5552 (C:0.8510, R:0.0070)
Batch 275/537: Loss=1.5462 (C:0.8378, R:0.0071)
Batch 300/537: Loss=1.5889 (C:0.8832, R:0.0071)
Batch 325/537: Loss=1.5544 (C:0.8524, R:0.0070)
Batch 350/537: Loss=1.5189 (C:0.8095, R:0.0071)
Batch 375/537: Loss=1.5663 (C:0.8615, R:0.0070)
Batch 400/537: Loss=1.5111 (C:0.8085, R:0.0070)
Batch 425/537: Loss=1.5622 (C:0.8544, R:0.0071)
Batch 450/537: Loss=1.5241 (C:0.8181, R:0.0071)
Batch 475/537: Loss=1.5255 (C:0.8188, R:0.0071)
Batch 500/537: Loss=1.5418 (C:0.8336, R:0.0071)
Batch 525/537: Loss=1.5968 (C:0.8865, R:0.0071)

============================================================
Epoch 44/200 completed in 24.1s
Train: Loss=1.5475 (C:0.8436, R:0.0070) Ratio=3.57x
Val:   Loss=1.5838 (C:0.9153, R:0.0067) Ratio=2.80x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=1.5438 (C:0.8386, R:0.0071)
Batch  25/537: Loss=1.5084 (C:0.8069, R:0.0070)
Batch  50/537: Loss=1.5634 (C:0.8577, R:0.0071)
Batch  75/537: Loss=1.5757 (C:0.8732, R:0.0070)
Batch 100/537: Loss=1.5270 (C:0.8189, R:0.0071)
Batch 125/537: Loss=1.5700 (C:0.8687, R:0.0070)
Batch 150/537: Loss=1.5488 (C:0.8376, R:0.0071)
Batch 175/537: Loss=1.5294 (C:0.8228, R:0.0071)
Batch 200/537: Loss=1.5745 (C:0.8666, R:0.0071)
Batch 225/537: Loss=1.5352 (C:0.8262, R:0.0071)
Batch 250/537: Loss=1.5320 (C:0.8266, R:0.0071)
Batch 275/537: Loss=1.5828 (C:0.8834, R:0.0070)
Batch 300/537: Loss=1.5420 (C:0.8405, R:0.0070)
Batch 325/537: Loss=1.5451 (C:0.8393, R:0.0071)
Batch 350/537: Loss=1.5480 (C:0.8502, R:0.0070)
Batch 375/537: Loss=1.5110 (C:0.8097, R:0.0070)
Batch 400/537: Loss=1.5690 (C:0.8705, R:0.0070)
Batch 425/537: Loss=1.5444 (C:0.8405, R:0.0070)
Batch 450/537: Loss=1.5247 (C:0.8259, R:0.0070)
Batch 475/537: Loss=1.5061 (C:0.8057, R:0.0070)
Batch 500/537: Loss=1.5348 (C:0.8295, R:0.0071)
Batch 525/537: Loss=1.5571 (C:0.8537, R:0.0070)

============================================================
Epoch 45/200 completed in 23.9s
Train: Loss=1.5447 (C:0.8412, R:0.0070) Ratio=3.56x
Val:   Loss=1.5744 (C:0.9070, R:0.0067) Ratio=2.88x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5744)
============================================================

üåç Updating global dataset at epoch 46
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.522 ¬± 0.693
    Neg distances: 2.335 ¬± 1.030
    Separation ratio: 4.47x
    Gap: -3.697
    ‚úÖ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=1.5260 (C:0.8176, R:0.0071)
Batch  25/537: Loss=1.5192 (C:0.8176, R:0.0070)
Batch  50/537: Loss=1.4993 (C:0.7950, R:0.0070)
Batch  75/537: Loss=1.5163 (C:0.8141, R:0.0070)
Batch 100/537: Loss=1.5314 (C:0.8242, R:0.0071)
Batch 125/537: Loss=1.5215 (C:0.8140, R:0.0071)
Batch 150/537: Loss=1.5226 (C:0.8178, R:0.0070)
Batch 175/537: Loss=1.5355 (C:0.8291, R:0.0071)
Batch 200/537: Loss=1.5079 (C:0.8106, R:0.0070)
Batch 225/537: Loss=1.4708 (C:0.7636, R:0.0071)
Batch 250/537: Loss=1.5116 (C:0.8105, R:0.0070)
Batch 275/537: Loss=1.4731 (C:0.7664, R:0.0071)
Batch 300/537: Loss=1.5271 (C:0.8203, R:0.0071)
Batch 325/537: Loss=1.4953 (C:0.7927, R:0.0070)
Batch 350/537: Loss=1.5272 (C:0.8237, R:0.0070)
Batch 375/537: Loss=1.5219 (C:0.8172, R:0.0070)
Batch 400/537: Loss=1.4663 (C:0.7672, R:0.0070)
Batch 425/537: Loss=1.5113 (C:0.8077, R:0.0070)
Batch 450/537: Loss=1.5070 (C:0.8071, R:0.0070)
Batch 475/537: Loss=1.5541 (C:0.8466, R:0.0071)
Batch 500/537: Loss=1.5214 (C:0.8116, R:0.0071)
Batch 525/537: Loss=1.5076 (C:0.8088, R:0.0070)

============================================================
Epoch 46/200 completed in 31.9s
Train: Loss=1.5170 (C:0.8135, R:0.0070) Ratio=3.56x
Val:   Loss=1.5535 (C:0.8863, R:0.0067) Ratio=2.87x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5535)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=1.4844 (C:0.7905, R:0.0069)
Batch  25/537: Loss=1.5571 (C:0.8546, R:0.0070)
Batch  50/537: Loss=1.5465 (C:0.8504, R:0.0070)
Batch  75/537: Loss=1.5054 (C:0.8032, R:0.0070)
Batch 100/537: Loss=1.4873 (C:0.7872, R:0.0070)
Batch 125/537: Loss=1.5154 (C:0.8089, R:0.0071)
Batch 150/537: Loss=1.5129 (C:0.8123, R:0.0070)
Batch 175/537: Loss=1.4963 (C:0.7977, R:0.0070)
Batch 200/537: Loss=1.4934 (C:0.7969, R:0.0070)
Batch 225/537: Loss=1.4689 (C:0.7629, R:0.0071)
Batch 250/537: Loss=1.5075 (C:0.7964, R:0.0071)
Batch 275/537: Loss=1.5303 (C:0.8236, R:0.0071)
Batch 300/537: Loss=1.4990 (C:0.7887, R:0.0071)
Batch 325/537: Loss=1.4967 (C:0.7940, R:0.0070)
Batch 350/537: Loss=1.5172 (C:0.8184, R:0.0070)
Batch 375/537: Loss=1.5493 (C:0.8481, R:0.0070)
Batch 400/537: Loss=1.4927 (C:0.7898, R:0.0070)
Batch 425/537: Loss=1.5487 (C:0.8422, R:0.0071)
Batch 450/537: Loss=1.5006 (C:0.7919, R:0.0071)
Batch 475/537: Loss=1.5323 (C:0.8259, R:0.0071)
Batch 500/537: Loss=1.5606 (C:0.8605, R:0.0070)
Batch 525/537: Loss=1.5196 (C:0.8206, R:0.0070)

============================================================
Epoch 47/200 completed in 23.8s
Train: Loss=1.5136 (C:0.8102, R:0.0070) Ratio=3.58x
Val:   Loss=1.5494 (C:0.8822, R:0.0067) Ratio=2.91x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5494)
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=1.4755 (C:0.7731, R:0.0070)
Batch  25/537: Loss=1.4918 (C:0.7921, R:0.0070)
Batch  50/537: Loss=1.5099 (C:0.8046, R:0.0071)
Batch  75/537: Loss=1.4713 (C:0.7714, R:0.0070)
Batch 100/537: Loss=1.4929 (C:0.7854, R:0.0071)
Batch 125/537: Loss=1.5055 (C:0.8017, R:0.0070)
Batch 150/537: Loss=1.5283 (C:0.8216, R:0.0071)
Batch 175/537: Loss=1.4983 (C:0.7964, R:0.0070)
Batch 200/537: Loss=1.5182 (C:0.8141, R:0.0070)
Batch 225/537: Loss=1.4889 (C:0.7868, R:0.0070)
Batch 250/537: Loss=1.5279 (C:0.8240, R:0.0070)
Batch 275/537: Loss=1.5026 (C:0.7965, R:0.0071)
Batch 300/537: Loss=1.5271 (C:0.8293, R:0.0070)
Batch 325/537: Loss=1.5104 (C:0.8092, R:0.0070)
Batch 350/537: Loss=1.5661 (C:0.8572, R:0.0071)
Batch 375/537: Loss=1.4814 (C:0.7812, R:0.0070)
Batch 400/537: Loss=1.5198 (C:0.8152, R:0.0070)
Batch 425/537: Loss=1.5093 (C:0.8065, R:0.0070)
Batch 450/537: Loss=1.4793 (C:0.7701, R:0.0071)
Batch 475/537: Loss=1.5249 (C:0.8199, R:0.0070)
Batch 500/537: Loss=1.5320 (C:0.8285, R:0.0070)
Batch 525/537: Loss=1.5242 (C:0.8162, R:0.0071)

============================================================
Epoch 48/200 completed in 24.2s
Train: Loss=1.5114 (C:0.8083, R:0.0070) Ratio=3.59x
Val:   Loss=1.5575 (C:0.8903, R:0.0067) Ratio=2.85x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 49
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.507 ¬± 0.685
    Neg distances: 2.390 ¬± 1.036
    Separation ratio: 4.72x
    Gap: -3.712
    ‚úÖ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=1.4817 (C:0.7777, R:0.0070)
Batch  25/537: Loss=1.4880 (C:0.7839, R:0.0070)
Batch  50/537: Loss=1.4601 (C:0.7548, R:0.0071)
Batch  75/537: Loss=1.4250 (C:0.7266, R:0.0070)
Batch 100/537: Loss=1.5036 (C:0.7963, R:0.0071)
Batch 125/537: Loss=1.4715 (C:0.7640, R:0.0071)
Batch 150/537: Loss=1.4681 (C:0.7712, R:0.0070)
Batch 175/537: Loss=1.4726 (C:0.7732, R:0.0070)
Batch 200/537: Loss=1.4908 (C:0.7915, R:0.0070)
Batch 225/537: Loss=1.5107 (C:0.8061, R:0.0070)
Batch 250/537: Loss=1.4881 (C:0.7846, R:0.0070)
Batch 275/537: Loss=1.4688 (C:0.7663, R:0.0070)
Batch 300/537: Loss=1.5006 (C:0.7928, R:0.0071)
Batch 325/537: Loss=1.5562 (C:0.8544, R:0.0070)
Batch 350/537: Loss=1.5179 (C:0.8083, R:0.0071)
Batch 375/537: Loss=1.4834 (C:0.7807, R:0.0070)
Batch 400/537: Loss=1.5132 (C:0.8062, R:0.0071)
Batch 425/537: Loss=1.5178 (C:0.8216, R:0.0070)
Batch 450/537: Loss=1.4795 (C:0.7747, R:0.0070)
Batch 475/537: Loss=1.5644 (C:0.8605, R:0.0070)
Batch 500/537: Loss=1.4966 (C:0.7909, R:0.0071)
Batch 525/537: Loss=1.5135 (C:0.8149, R:0.0070)

============================================================
Epoch 49/200 completed in 32.0s
Train: Loss=1.4856 (C:0.7826, R:0.0070) Ratio=3.54x
Val:   Loss=1.5295 (C:0.8627, R:0.0067) Ratio=2.94x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5295)
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=1.4697 (C:0.7683, R:0.0070)
Batch  25/537: Loss=1.4426 (C:0.7385, R:0.0070)
Batch  50/537: Loss=1.4924 (C:0.7900, R:0.0070)
Batch  75/537: Loss=1.5003 (C:0.8029, R:0.0070)
Batch 100/537: Loss=1.4893 (C:0.7864, R:0.0070)
Batch 125/537: Loss=1.4825 (C:0.7772, R:0.0071)
Batch 150/537: Loss=1.4636 (C:0.7617, R:0.0070)
Batch 175/537: Loss=1.5033 (C:0.7959, R:0.0071)
Batch 200/537: Loss=1.4647 (C:0.7676, R:0.0070)
Batch 225/537: Loss=1.4987 (C:0.7961, R:0.0070)
Batch 250/537: Loss=1.4741 (C:0.7755, R:0.0070)
Batch 275/537: Loss=1.5022 (C:0.7989, R:0.0070)
Batch 300/537: Loss=1.4803 (C:0.7787, R:0.0070)
Batch 325/537: Loss=1.5122 (C:0.8094, R:0.0070)
Batch 350/537: Loss=1.4469 (C:0.7544, R:0.0069)
Batch 375/537: Loss=1.4332 (C:0.7342, R:0.0070)
Batch 400/537: Loss=1.4804 (C:0.7829, R:0.0070)
Batch 425/537: Loss=1.4522 (C:0.7450, R:0.0071)
Batch 450/537: Loss=1.4659 (C:0.7515, R:0.0071)
Batch 475/537: Loss=1.5106 (C:0.8064, R:0.0070)
Batch 500/537: Loss=1.4839 (C:0.7786, R:0.0071)
Batch 525/537: Loss=1.5093 (C:0.8026, R:0.0071)

============================================================
Epoch 50/200 completed in 23.8s
Train: Loss=1.4831 (C:0.7803, R:0.0070) Ratio=3.64x
Val:   Loss=1.5229 (C:0.8565, R:0.0067) Ratio=2.83x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5229)
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=1.4755 (C:0.7742, R:0.0070)
Batch  25/537: Loss=1.5162 (C:0.8073, R:0.0071)
Batch  50/537: Loss=1.4629 (C:0.7551, R:0.0071)
Batch  75/537: Loss=1.4719 (C:0.7689, R:0.0070)
Batch 100/537: Loss=1.4519 (C:0.7494, R:0.0070)
Batch 125/537: Loss=1.4715 (C:0.7671, R:0.0070)
Batch 150/537: Loss=1.4552 (C:0.7579, R:0.0070)
Batch 175/537: Loss=1.4896 (C:0.7812, R:0.0071)
Batch 200/537: Loss=1.4933 (C:0.7876, R:0.0071)
Batch 225/537: Loss=1.4502 (C:0.7517, R:0.0070)
Batch 250/537: Loss=1.4654 (C:0.7641, R:0.0070)
Batch 275/537: Loss=1.5146 (C:0.8164, R:0.0070)
Batch 300/537: Loss=1.4406 (C:0.7362, R:0.0070)
Batch 325/537: Loss=1.5076 (C:0.7967, R:0.0071)
Batch 350/537: Loss=1.4518 (C:0.7517, R:0.0070)
Batch 375/537: Loss=1.5190 (C:0.8149, R:0.0070)
Batch 400/537: Loss=1.4573 (C:0.7518, R:0.0071)
Batch 425/537: Loss=1.4643 (C:0.7677, R:0.0070)
Batch 450/537: Loss=1.5050 (C:0.8003, R:0.0070)
Batch 475/537: Loss=1.4783 (C:0.7710, R:0.0071)
Batch 500/537: Loss=1.5085 (C:0.8073, R:0.0070)
Batch 525/537: Loss=1.5009 (C:0.7982, R:0.0070)

============================================================
Epoch 51/200 completed in 24.2s
Train: Loss=1.4801 (C:0.7777, R:0.0070) Ratio=3.61x
Val:   Loss=1.5357 (C:0.8704, R:0.0067) Ratio=2.84x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 52
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.534 ¬± 0.720
    Neg distances: 2.354 ¬± 1.029
    Separation ratio: 4.41x
    Gap: -3.744
    ‚úÖ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=1.4557 (C:0.7626, R:0.0069)
Batch  25/537: Loss=1.4579 (C:0.7526, R:0.0071)
Batch  50/537: Loss=1.4413 (C:0.7387, R:0.0070)
Batch  75/537: Loss=1.5197 (C:0.8158, R:0.0070)
Batch 100/537: Loss=1.5030 (C:0.7976, R:0.0071)
Batch 125/537: Loss=1.4772 (C:0.7695, R:0.0071)
Batch 150/537: Loss=1.4789 (C:0.7779, R:0.0070)
Batch 175/537: Loss=1.4644 (C:0.7644, R:0.0070)
Batch 200/537: Loss=1.4793 (C:0.7816, R:0.0070)
Batch 225/537: Loss=1.5086 (C:0.8042, R:0.0070)
Batch 250/537: Loss=1.5170 (C:0.8200, R:0.0070)
Batch 275/537: Loss=1.4838 (C:0.7811, R:0.0070)
Batch 300/537: Loss=1.4885 (C:0.7844, R:0.0070)
Batch 325/537: Loss=1.5144 (C:0.8076, R:0.0071)
Batch 350/537: Loss=1.5150 (C:0.8131, R:0.0070)
Batch 375/537: Loss=1.4539 (C:0.7568, R:0.0070)
Batch 400/537: Loss=1.4948 (C:0.7926, R:0.0070)
Batch 425/537: Loss=1.4795 (C:0.7756, R:0.0070)
Batch 450/537: Loss=1.5331 (C:0.8354, R:0.0070)
Batch 475/537: Loss=1.4942 (C:0.7948, R:0.0070)
Batch 500/537: Loss=1.4925 (C:0.7880, R:0.0070)
Batch 525/537: Loss=1.5131 (C:0.8105, R:0.0070)

============================================================
Epoch 52/200 completed in 32.0s
Train: Loss=1.4944 (C:0.7923, R:0.0070) Ratio=3.63x
Val:   Loss=1.5267 (C:0.8614, R:0.0067) Ratio=2.85x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=1.4644 (C:0.7590, R:0.0071)
Batch  25/537: Loss=1.4633 (C:0.7617, R:0.0070)
Batch  50/537: Loss=1.4661 (C:0.7609, R:0.0071)
Batch  75/537: Loss=1.4713 (C:0.7718, R:0.0070)
Batch 100/537: Loss=1.4849 (C:0.7862, R:0.0070)
Batch 125/537: Loss=1.4527 (C:0.7469, R:0.0071)
Batch 150/537: Loss=1.4572 (C:0.7563, R:0.0070)
Batch 175/537: Loss=1.5346 (C:0.8292, R:0.0071)
Batch 200/537: Loss=1.4889 (C:0.7898, R:0.0070)
Batch 225/537: Loss=1.4567 (C:0.7612, R:0.0070)
Batch 250/537: Loss=1.5068 (C:0.8012, R:0.0071)
Batch 275/537: Loss=1.4712 (C:0.7660, R:0.0071)
Batch 300/537: Loss=1.5095 (C:0.8086, R:0.0070)
Batch 325/537: Loss=1.5195 (C:0.8119, R:0.0071)
Batch 350/537: Loss=1.4760 (C:0.7743, R:0.0070)
Batch 375/537: Loss=1.4960 (C:0.7966, R:0.0070)
Batch 400/537: Loss=1.4665 (C:0.7689, R:0.0070)
Batch 425/537: Loss=1.5269 (C:0.8258, R:0.0070)
Batch 450/537: Loss=1.4433 (C:0.7460, R:0.0070)
Batch 475/537: Loss=1.4758 (C:0.7759, R:0.0070)
Batch 500/537: Loss=1.4955 (C:0.7887, R:0.0071)
Batch 525/537: Loss=1.5185 (C:0.8174, R:0.0070)

============================================================
Epoch 53/200 completed in 23.8s
Train: Loss=1.4908 (C:0.7891, R:0.0070) Ratio=3.71x
Val:   Loss=1.5355 (C:0.8703, R:0.0067) Ratio=2.88x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=1.4745 (C:0.7739, R:0.0070)
Batch  25/537: Loss=1.5049 (C:0.8026, R:0.0070)
Batch  50/537: Loss=1.5443 (C:0.8379, R:0.0071)
Batch  75/537: Loss=1.4876 (C:0.7873, R:0.0070)
Batch 100/537: Loss=1.4978 (C:0.7927, R:0.0071)
Batch 125/537: Loss=1.4614 (C:0.7526, R:0.0071)
Batch 150/537: Loss=1.4831 (C:0.7805, R:0.0070)
Batch 175/537: Loss=1.5269 (C:0.8252, R:0.0070)
Batch 200/537: Loss=1.4930 (C:0.7859, R:0.0071)
Batch 225/537: Loss=1.4850 (C:0.7782, R:0.0071)
Batch 250/537: Loss=1.4666 (C:0.7585, R:0.0071)
Batch 275/537: Loss=1.5163 (C:0.8169, R:0.0070)
Batch 300/537: Loss=1.4809 (C:0.7826, R:0.0070)
Batch 325/537: Loss=1.4939 (C:0.7928, R:0.0070)
Batch 350/537: Loss=1.4612 (C:0.7571, R:0.0070)
Batch 375/537: Loss=1.4477 (C:0.7481, R:0.0070)
Batch 400/537: Loss=1.4972 (C:0.7965, R:0.0070)
Batch 425/537: Loss=1.4884 (C:0.7858, R:0.0070)
Batch 450/537: Loss=1.5233 (C:0.8202, R:0.0070)
Batch 475/537: Loss=1.4567 (C:0.7532, R:0.0070)
Batch 500/537: Loss=1.4742 (C:0.7708, R:0.0070)
Batch 525/537: Loss=1.4615 (C:0.7584, R:0.0070)

============================================================
Epoch 54/200 completed in 23.8s
Train: Loss=1.4886 (C:0.7872, R:0.0070) Ratio=3.74x
Val:   Loss=1.5341 (C:0.8695, R:0.0066) Ratio=2.90x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 55
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.522 ¬± 0.722
    Neg distances: 2.399 ¬± 1.028
    Separation ratio: 4.60x
    Gap: -3.803
    ‚úÖ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=1.4567 (C:0.7536, R:0.0070)
Batch  25/537: Loss=1.4666 (C:0.7614, R:0.0071)
Batch  50/537: Loss=1.4935 (C:0.7874, R:0.0071)
Batch  75/537: Loss=1.4318 (C:0.7268, R:0.0071)
Batch 100/537: Loss=1.4983 (C:0.7935, R:0.0070)
Batch 125/537: Loss=1.4764 (C:0.7769, R:0.0070)
Batch 150/537: Loss=1.5419 (C:0.8369, R:0.0071)
Batch 175/537: Loss=1.5066 (C:0.8038, R:0.0070)
Batch 200/537: Loss=1.4773 (C:0.7820, R:0.0070)
Batch 225/537: Loss=1.4758 (C:0.7645, R:0.0071)
Batch 250/537: Loss=1.4474 (C:0.7468, R:0.0070)
Batch 275/537: Loss=1.4950 (C:0.7951, R:0.0070)
Batch 300/537: Loss=1.4340 (C:0.7299, R:0.0070)
Batch 325/537: Loss=1.4445 (C:0.7391, R:0.0071)
Batch 350/537: Loss=1.4677 (C:0.7642, R:0.0070)
Batch 375/537: Loss=1.4696 (C:0.7681, R:0.0070)
Batch 400/537: Loss=1.4733 (C:0.7757, R:0.0070)
Batch 425/537: Loss=1.4629 (C:0.7652, R:0.0070)
Batch 450/537: Loss=1.5353 (C:0.8299, R:0.0071)
Batch 475/537: Loss=1.4931 (C:0.7894, R:0.0070)
Batch 500/537: Loss=1.4757 (C:0.7729, R:0.0070)
Batch 525/537: Loss=1.4551 (C:0.7555, R:0.0070)

============================================================
Epoch 55/200 completed in 32.4s
Train: Loss=1.4664 (C:0.7651, R:0.0070) Ratio=3.59x
Val:   Loss=1.5121 (C:0.8471, R:0.0066) Ratio=2.90x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5121)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=1.5167 (C:0.8182, R:0.0070)
Batch  25/537: Loss=1.4798 (C:0.7745, R:0.0071)
Batch  50/537: Loss=1.4942 (C:0.7990, R:0.0070)
Batch  75/537: Loss=1.4831 (C:0.7786, R:0.0070)
Batch 100/537: Loss=1.4612 (C:0.7606, R:0.0070)
Batch 125/537: Loss=1.4349 (C:0.7320, R:0.0070)
Batch 150/537: Loss=1.4957 (C:0.7937, R:0.0070)
Batch 175/537: Loss=1.5046 (C:0.8029, R:0.0070)
Batch 200/537: Loss=1.4847 (C:0.7846, R:0.0070)
Batch 225/537: Loss=1.4463 (C:0.7545, R:0.0069)
Batch 250/537: Loss=1.4702 (C:0.7706, R:0.0070)
Batch 275/537: Loss=1.4714 (C:0.7678, R:0.0070)
Batch 300/537: Loss=1.4929 (C:0.7887, R:0.0070)
Batch 325/537: Loss=1.4729 (C:0.7632, R:0.0071)
Batch 350/537: Loss=1.4502 (C:0.7495, R:0.0070)
Batch 375/537: Loss=1.4531 (C:0.7536, R:0.0070)
Batch 400/537: Loss=1.4949 (C:0.7915, R:0.0070)
Batch 425/537: Loss=1.4538 (C:0.7524, R:0.0070)
Batch 450/537: Loss=1.4640 (C:0.7576, R:0.0071)
Batch 475/537: Loss=1.4662 (C:0.7590, R:0.0071)
Batch 500/537: Loss=1.4835 (C:0.7841, R:0.0070)
Batch 525/537: Loss=1.5170 (C:0.8158, R:0.0070)

============================================================
Epoch 56/200 completed in 24.0s
Train: Loss=1.4666 (C:0.7655, R:0.0070) Ratio=3.66x
Val:   Loss=1.5154 (C:0.8514, R:0.0066) Ratio=2.95x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=1.4210 (C:0.7210, R:0.0070)
Batch  25/537: Loss=1.4536 (C:0.7491, R:0.0070)
Batch  50/537: Loss=1.4769 (C:0.7785, R:0.0070)
Batch  75/537: Loss=1.4919 (C:0.7892, R:0.0070)
Batch 100/537: Loss=1.4718 (C:0.7671, R:0.0070)
Batch 125/537: Loss=1.4503 (C:0.7481, R:0.0070)
Batch 150/537: Loss=1.4398 (C:0.7333, R:0.0071)
Batch 175/537: Loss=1.4510 (C:0.7514, R:0.0070)
Batch 200/537: Loss=1.4642 (C:0.7633, R:0.0070)
Batch 225/537: Loss=1.4935 (C:0.7894, R:0.0070)
Batch 250/537: Loss=1.4578 (C:0.7555, R:0.0070)
Batch 275/537: Loss=1.4784 (C:0.7742, R:0.0070)
Batch 300/537: Loss=1.4645 (C:0.7641, R:0.0070)
Batch 325/537: Loss=1.4554 (C:0.7494, R:0.0071)
Batch 350/537: Loss=1.4817 (C:0.7787, R:0.0070)
Batch 375/537: Loss=1.4799 (C:0.7777, R:0.0070)
Batch 400/537: Loss=1.4616 (C:0.7614, R:0.0070)
Batch 425/537: Loss=1.4978 (C:0.7953, R:0.0070)
Batch 450/537: Loss=1.4505 (C:0.7487, R:0.0070)
Batch 475/537: Loss=1.4397 (C:0.7431, R:0.0070)
Batch 500/537: Loss=1.4987 (C:0.7985, R:0.0070)
Batch 525/537: Loss=1.4943 (C:0.7941, R:0.0070)

============================================================
Epoch 57/200 completed in 24.0s
Train: Loss=1.4632 (C:0.7626, R:0.0070) Ratio=3.72x
Val:   Loss=1.5056 (C:0.8428, R:0.0066) Ratio=2.91x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5056)
============================================================

üåç Updating global dataset at epoch 58
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.511 ¬± 0.726
    Neg distances: 2.445 ¬± 1.036
    Separation ratio: 4.78x
    Gap: -3.785
    ‚úÖ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=1.4993 (C:0.8021, R:0.0070)
Batch  25/537: Loss=1.4858 (C:0.7811, R:0.0070)
Batch  50/537: Loss=1.4446 (C:0.7470, R:0.0070)
Batch  75/537: Loss=1.4262 (C:0.7201, R:0.0071)
Batch 100/537: Loss=1.4168 (C:0.7151, R:0.0070)
Batch 125/537: Loss=1.4431 (C:0.7443, R:0.0070)
Batch 150/537: Loss=1.4189 (C:0.7196, R:0.0070)
Batch 175/537: Loss=1.4471 (C:0.7469, R:0.0070)
Batch 200/537: Loss=1.4174 (C:0.7181, R:0.0070)
Batch 225/537: Loss=1.4524 (C:0.7541, R:0.0070)
Batch 250/537: Loss=1.4394 (C:0.7387, R:0.0070)
Batch 275/537: Loss=1.4402 (C:0.7498, R:0.0069)
Batch 300/537: Loss=1.4456 (C:0.7381, R:0.0071)
Batch 325/537: Loss=1.4817 (C:0.7779, R:0.0070)
Batch 350/537: Loss=1.4408 (C:0.7427, R:0.0070)
Batch 375/537: Loss=1.5059 (C:0.8046, R:0.0070)
Batch 400/537: Loss=1.4770 (C:0.7800, R:0.0070)
Batch 425/537: Loss=1.4539 (C:0.7573, R:0.0070)
Batch 450/537: Loss=1.4543 (C:0.7535, R:0.0070)
Batch 475/537: Loss=1.4380 (C:0.7360, R:0.0070)
Batch 500/537: Loss=1.4272 (C:0.7325, R:0.0069)
Batch 525/537: Loss=1.4625 (C:0.7653, R:0.0070)

============================================================
Epoch 58/200 completed in 32.0s
Train: Loss=1.4504 (C:0.7500, R:0.0070) Ratio=3.74x
Val:   Loss=1.4908 (C:0.8269, R:0.0066) Ratio=2.89x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4908)
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=1.4978 (C:0.7929, R:0.0070)
Batch  25/537: Loss=1.4398 (C:0.7394, R:0.0070)
Batch  50/537: Loss=1.4577 (C:0.7535, R:0.0070)
Batch  75/537: Loss=1.4455 (C:0.7451, R:0.0070)
Batch 100/537: Loss=1.4264 (C:0.7243, R:0.0070)
Batch 125/537: Loss=1.4820 (C:0.7764, R:0.0071)
Batch 150/537: Loss=1.4121 (C:0.7117, R:0.0070)
Batch 175/537: Loss=1.4391 (C:0.7402, R:0.0070)
Batch 200/537: Loss=1.4536 (C:0.7570, R:0.0070)
Batch 225/537: Loss=1.4268 (C:0.7245, R:0.0070)
Batch 250/537: Loss=1.4605 (C:0.7620, R:0.0070)
Batch 275/537: Loss=1.4385 (C:0.7423, R:0.0070)
Batch 300/537: Loss=1.4466 (C:0.7480, R:0.0070)
Batch 325/537: Loss=1.4376 (C:0.7370, R:0.0070)
Batch 350/537: Loss=1.4373 (C:0.7415, R:0.0070)
Batch 375/537: Loss=1.4499 (C:0.7487, R:0.0070)
Batch 400/537: Loss=1.4454 (C:0.7471, R:0.0070)
Batch 425/537: Loss=1.4387 (C:0.7422, R:0.0070)
Batch 450/537: Loss=1.4985 (C:0.8043, R:0.0069)
Batch 475/537: Loss=1.4334 (C:0.7309, R:0.0070)
Batch 500/537: Loss=1.4404 (C:0.7419, R:0.0070)
Batch 525/537: Loss=1.4431 (C:0.7420, R:0.0070)

============================================================
Epoch 59/200 completed in 24.0s
Train: Loss=1.4490 (C:0.7490, R:0.0070) Ratio=3.75x
Val:   Loss=1.4988 (C:0.8356, R:0.0066) Ratio=2.93x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=1.4322 (C:0.7415, R:0.0069)
Batch  25/537: Loss=1.4558 (C:0.7552, R:0.0070)
Batch  50/537: Loss=1.4929 (C:0.7862, R:0.0071)
Batch  75/537: Loss=1.4522 (C:0.7537, R:0.0070)
Batch 100/537: Loss=1.4531 (C:0.7517, R:0.0070)
Batch 125/537: Loss=1.4484 (C:0.7524, R:0.0070)
Batch 150/537: Loss=1.4895 (C:0.7940, R:0.0070)
Batch 175/537: Loss=1.4430 (C:0.7413, R:0.0070)
Batch 200/537: Loss=1.4622 (C:0.7586, R:0.0070)
Batch 225/537: Loss=1.4746 (C:0.7808, R:0.0069)
Batch 250/537: Loss=1.4518 (C:0.7531, R:0.0070)
Batch 275/537: Loss=1.4809 (C:0.7793, R:0.0070)
Batch 300/537: Loss=1.4763 (C:0.7787, R:0.0070)
Batch 325/537: Loss=1.4673 (C:0.7622, R:0.0071)
Batch 350/537: Loss=1.4873 (C:0.7892, R:0.0070)
Batch 375/537: Loss=1.4275 (C:0.7221, R:0.0071)
Batch 400/537: Loss=1.4358 (C:0.7439, R:0.0069)
Batch 425/537: Loss=1.4700 (C:0.7772, R:0.0069)
Batch 450/537: Loss=1.4531 (C:0.7563, R:0.0070)
Batch 475/537: Loss=1.4315 (C:0.7320, R:0.0070)
Batch 500/537: Loss=1.4056 (C:0.7095, R:0.0070)
Batch 525/537: Loss=1.4555 (C:0.7599, R:0.0070)

============================================================
Epoch 60/200 completed in 24.4s
Train: Loss=1.4477 (C:0.7481, R:0.0070) Ratio=3.68x
Val:   Loss=1.4866 (C:0.8236, R:0.0066) Ratio=2.91x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4866)
Checkpoint saved at epoch 60
============================================================

üåç Updating global dataset at epoch 61
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.490 ¬± 0.696
    Neg distances: 2.461 ¬± 1.030
    Separation ratio: 5.03x
    Gap: -3.764
    ‚úÖ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=1.4472 (C:0.7524, R:0.0069)
Batch  25/537: Loss=1.4200 (C:0.7240, R:0.0070)
Batch  50/537: Loss=1.4240 (C:0.7241, R:0.0070)
Batch  75/537: Loss=1.4257 (C:0.7272, R:0.0070)
Batch 100/537: Loss=1.4348 (C:0.7350, R:0.0070)
Batch 125/537: Loss=1.4403 (C:0.7376, R:0.0070)
Batch 150/537: Loss=1.4073 (C:0.7062, R:0.0070)
Batch 175/537: Loss=1.4445 (C:0.7447, R:0.0070)
Batch 200/537: Loss=1.4791 (C:0.7813, R:0.0070)
Batch 225/537: Loss=1.4270 (C:0.7256, R:0.0070)
Batch 250/537: Loss=1.4377 (C:0.7330, R:0.0070)
Batch 275/537: Loss=1.3761 (C:0.6813, R:0.0069)
Batch 300/537: Loss=1.4269 (C:0.7204, R:0.0071)
Batch 325/537: Loss=1.4403 (C:0.7324, R:0.0071)
Batch 350/537: Loss=1.4493 (C:0.7521, R:0.0070)
Batch 375/537: Loss=1.4212 (C:0.7250, R:0.0070)
Batch 400/537: Loss=1.4299 (C:0.7340, R:0.0070)
Batch 425/537: Loss=1.4157 (C:0.7154, R:0.0070)
Batch 450/537: Loss=1.4374 (C:0.7411, R:0.0070)
Batch 475/537: Loss=1.4537 (C:0.7537, R:0.0070)
Batch 500/537: Loss=1.4062 (C:0.7073, R:0.0070)
Batch 525/537: Loss=1.4156 (C:0.7166, R:0.0070)

============================================================
Epoch 61/200 completed in 32.0s
Train: Loss=1.4277 (C:0.7283, R:0.0070) Ratio=3.76x
Val:   Loss=1.4876 (C:0.8254, R:0.0066) Ratio=2.88x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=1.4359 (C:0.7365, R:0.0070)
Batch  25/537: Loss=1.4095 (C:0.7060, R:0.0070)
Batch  50/537: Loss=1.4806 (C:0.7760, R:0.0070)
Batch  75/537: Loss=1.4146 (C:0.7111, R:0.0070)
Batch 100/537: Loss=1.3885 (C:0.6855, R:0.0070)
Batch 125/537: Loss=1.4720 (C:0.7715, R:0.0070)
Batch 150/537: Loss=1.4694 (C:0.7755, R:0.0069)
Batch 175/537: Loss=1.4041 (C:0.7005, R:0.0070)
Batch 200/537: Loss=1.4616 (C:0.7648, R:0.0070)
Batch 225/537: Loss=1.4504 (C:0.7507, R:0.0070)
Batch 250/537: Loss=1.4273 (C:0.7300, R:0.0070)
Batch 275/537: Loss=1.4440 (C:0.7417, R:0.0070)
Batch 300/537: Loss=1.4254 (C:0.7246, R:0.0070)
Batch 325/537: Loss=1.4538 (C:0.7542, R:0.0070)
Batch 350/537: Loss=1.4523 (C:0.7485, R:0.0070)
Batch 375/537: Loss=1.4361 (C:0.7491, R:0.0069)
Batch 400/537: Loss=1.4161 (C:0.7120, R:0.0070)
Batch 425/537: Loss=1.4918 (C:0.7906, R:0.0070)
Batch 450/537: Loss=1.3924 (C:0.6911, R:0.0070)
Batch 475/537: Loss=1.4366 (C:0.7390, R:0.0070)
Batch 500/537: Loss=1.4204 (C:0.7210, R:0.0070)
Batch 525/537: Loss=1.4551 (C:0.7513, R:0.0070)

============================================================
Epoch 62/200 completed in 24.1s
Train: Loss=1.4269 (C:0.7279, R:0.0070) Ratio=3.73x
Val:   Loss=1.4698 (C:0.8089, R:0.0066) Ratio=2.92x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4698)
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=1.4185 (C:0.7213, R:0.0070)
Batch  25/537: Loss=1.3893 (C:0.6891, R:0.0070)
Batch  50/537: Loss=1.4411 (C:0.7462, R:0.0069)
Batch  75/537: Loss=1.4276 (C:0.7256, R:0.0070)
Batch 100/537: Loss=1.3836 (C:0.6871, R:0.0070)
Batch 125/537: Loss=1.4124 (C:0.7155, R:0.0070)
Batch 150/537: Loss=1.4273 (C:0.7302, R:0.0070)
Batch 175/537: Loss=1.4315 (C:0.7365, R:0.0070)
Batch 200/537: Loss=1.4195 (C:0.7131, R:0.0071)
Batch 225/537: Loss=1.4172 (C:0.7165, R:0.0070)
Batch 250/537: Loss=1.4402 (C:0.7416, R:0.0070)
Batch 275/537: Loss=1.4335 (C:0.7311, R:0.0070)
Batch 300/537: Loss=1.4174 (C:0.7159, R:0.0070)
Batch 325/537: Loss=1.4381 (C:0.7413, R:0.0070)
Batch 350/537: Loss=1.4405 (C:0.7452, R:0.0070)
Batch 375/537: Loss=1.4118 (C:0.7130, R:0.0070)
Batch 400/537: Loss=1.4749 (C:0.7740, R:0.0070)
Batch 425/537: Loss=1.4269 (C:0.7276, R:0.0070)
Batch 450/537: Loss=1.4007 (C:0.7014, R:0.0070)
Batch 475/537: Loss=1.4187 (C:0.7165, R:0.0070)
Batch 500/537: Loss=1.4232 (C:0.7245, R:0.0070)
Batch 525/537: Loss=1.4810 (C:0.7851, R:0.0070)

============================================================
Epoch 63/200 completed in 23.9s
Train: Loss=1.4263 (C:0.7277, R:0.0070) Ratio=3.81x
Val:   Loss=1.4728 (C:0.8120, R:0.0066) Ratio=2.90x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 64
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.498 ¬± 0.719
    Neg distances: 2.439 ¬± 1.022
    Separation ratio: 4.89x
    Gap: -3.748
    ‚úÖ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=1.4007 (C:0.6939, R:0.0071)
Batch  25/537: Loss=1.4016 (C:0.7063, R:0.0070)
Batch  50/537: Loss=1.4460 (C:0.7495, R:0.0070)
Batch  75/537: Loss=1.4500 (C:0.7488, R:0.0070)
Batch 100/537: Loss=1.4202 (C:0.7184, R:0.0070)
Batch 125/537: Loss=1.4707 (C:0.7691, R:0.0070)
Batch 150/537: Loss=1.4244 (C:0.7254, R:0.0070)
Batch 175/537: Loss=1.4515 (C:0.7560, R:0.0070)
Batch 200/537: Loss=1.3972 (C:0.7002, R:0.0070)
Batch 225/537: Loss=1.4244 (C:0.7231, R:0.0070)
Batch 250/537: Loss=1.4066 (C:0.7159, R:0.0069)
Batch 275/537: Loss=1.4858 (C:0.7890, R:0.0070)
Batch 300/537: Loss=1.4216 (C:0.7235, R:0.0070)
Batch 325/537: Loss=1.5019 (C:0.8030, R:0.0070)
Batch 350/537: Loss=1.4257 (C:0.7299, R:0.0070)
Batch 375/537: Loss=1.4353 (C:0.7364, R:0.0070)
Batch 400/537: Loss=1.4153 (C:0.7167, R:0.0070)
Batch 425/537: Loss=1.3962 (C:0.6977, R:0.0070)
Batch 450/537: Loss=1.4426 (C:0.7482, R:0.0069)
Batch 475/537: Loss=1.4297 (C:0.7346, R:0.0070)
Batch 500/537: Loss=1.4613 (C:0.7600, R:0.0070)
Batch 525/537: Loss=1.4352 (C:0.7382, R:0.0070)

============================================================
Epoch 64/200 completed in 31.5s
Train: Loss=1.4302 (C:0.7320, R:0.0070) Ratio=3.73x
Val:   Loss=1.4755 (C:0.8148, R:0.0066) Ratio=2.96x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=1.4506 (C:0.7546, R:0.0070)
Batch  25/537: Loss=1.4062 (C:0.7024, R:0.0070)
Batch  50/537: Loss=1.4275 (C:0.7248, R:0.0070)
Batch  75/537: Loss=1.4468 (C:0.7490, R:0.0070)
Batch 100/537: Loss=1.3972 (C:0.7025, R:0.0069)
Batch 125/537: Loss=1.4128 (C:0.7135, R:0.0070)
Batch 150/537: Loss=1.4174 (C:0.7147, R:0.0070)
Batch 175/537: Loss=1.4038 (C:0.7035, R:0.0070)
Batch 200/537: Loss=1.4384 (C:0.7398, R:0.0070)
Batch 225/537: Loss=1.4317 (C:0.7331, R:0.0070)
Batch 250/537: Loss=1.4340 (C:0.7373, R:0.0070)
Batch 275/537: Loss=1.4450 (C:0.7386, R:0.0071)
Batch 300/537: Loss=1.4405 (C:0.7412, R:0.0070)
Batch 325/537: Loss=1.3997 (C:0.7000, R:0.0070)
Batch 350/537: Loss=1.4540 (C:0.7523, R:0.0070)
Batch 375/537: Loss=1.4545 (C:0.7601, R:0.0069)
Batch 400/537: Loss=1.4318 (C:0.7300, R:0.0070)
Batch 425/537: Loss=1.4241 (C:0.7276, R:0.0070)
Batch 450/537: Loss=1.4161 (C:0.7239, R:0.0069)
Batch 475/537: Loss=1.4450 (C:0.7486, R:0.0070)
Batch 500/537: Loss=1.4258 (C:0.7265, R:0.0070)
Batch 525/537: Loss=1.4132 (C:0.7121, R:0.0070)

============================================================
Epoch 65/200 completed in 23.9s
Train: Loss=1.4284 (C:0.7304, R:0.0070) Ratio=3.80x
Val:   Loss=1.4735 (C:0.8131, R:0.0066) Ratio=2.90x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=1.4423 (C:0.7488, R:0.0069)
Batch  25/537: Loss=1.3645 (C:0.6644, R:0.0070)
Batch  50/537: Loss=1.4240 (C:0.7264, R:0.0070)
Batch  75/537: Loss=1.3939 (C:0.6921, R:0.0070)
Batch 100/537: Loss=1.4018 (C:0.7066, R:0.0070)
Batch 125/537: Loss=1.4542 (C:0.7549, R:0.0070)
Batch 150/537: Loss=1.4418 (C:0.7381, R:0.0070)
Batch 175/537: Loss=1.4104 (C:0.7161, R:0.0069)
Batch 200/537: Loss=1.4188 (C:0.7269, R:0.0069)
Batch 225/537: Loss=1.4295 (C:0.7358, R:0.0069)
Batch 250/537: Loss=1.4516 (C:0.7465, R:0.0071)
Batch 275/537: Loss=1.4294 (C:0.7344, R:0.0069)
Batch 300/537: Loss=1.4215 (C:0.7289, R:0.0069)
Batch 325/537: Loss=1.4019 (C:0.7015, R:0.0070)
Batch 350/537: Loss=1.4130 (C:0.7174, R:0.0070)
Batch 375/537: Loss=1.4534 (C:0.7571, R:0.0070)
Batch 400/537: Loss=1.4630 (C:0.7613, R:0.0070)
Batch 425/537: Loss=1.4459 (C:0.7505, R:0.0070)
Batch 450/537: Loss=1.4266 (C:0.7311, R:0.0070)
Batch 475/537: Loss=1.3741 (C:0.6743, R:0.0070)
Batch 500/537: Loss=1.4353 (C:0.7405, R:0.0069)
Batch 525/537: Loss=1.4368 (C:0.7355, R:0.0070)

============================================================
Epoch 66/200 completed in 23.7s
Train: Loss=1.4292 (C:0.7315, R:0.0070) Ratio=3.76x
Val:   Loss=1.4679 (C:0.8082, R:0.0066) Ratio=2.92x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4679)
============================================================

üåç Updating global dataset at epoch 67
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.491 ¬± 0.703
    Neg distances: 2.451 ¬± 1.026
    Separation ratio: 4.99x
    Gap: -3.776
    ‚úÖ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=1.3636 (C:0.6680, R:0.0070)
Batch  25/537: Loss=1.4299 (C:0.7364, R:0.0069)
Batch  50/537: Loss=1.4437 (C:0.7447, R:0.0070)
Batch  75/537: Loss=1.4621 (C:0.7660, R:0.0070)
Batch 100/537: Loss=1.4127 (C:0.7180, R:0.0069)
Batch 125/537: Loss=1.3939 (C:0.6955, R:0.0070)
Batch 150/537: Loss=1.4165 (C:0.7203, R:0.0070)
Batch 175/537: Loss=1.4504 (C:0.7546, R:0.0070)
Batch 200/537: Loss=1.4562 (C:0.7546, R:0.0070)
Batch 225/537: Loss=1.3887 (C:0.6900, R:0.0070)
Batch 250/537: Loss=1.4184 (C:0.7203, R:0.0070)
Batch 275/537: Loss=1.4518 (C:0.7524, R:0.0070)
Batch 300/537: Loss=1.4298 (C:0.7407, R:0.0069)
Batch 325/537: Loss=1.4654 (C:0.7709, R:0.0069)
Batch 350/537: Loss=1.4479 (C:0.7545, R:0.0069)
Batch 375/537: Loss=1.4412 (C:0.7376, R:0.0070)
Batch 400/537: Loss=1.3848 (C:0.6870, R:0.0070)
Batch 425/537: Loss=1.4313 (C:0.7345, R:0.0070)
Batch 450/537: Loss=1.3872 (C:0.6871, R:0.0070)
Batch 475/537: Loss=1.4107 (C:0.7130, R:0.0070)
Batch 500/537: Loss=1.4406 (C:0.7428, R:0.0070)
Batch 525/537: Loss=1.3987 (C:0.6951, R:0.0070)

============================================================
Epoch 67/200 completed in 32.1s
Train: Loss=1.4223 (C:0.7250, R:0.0070) Ratio=3.77x
Val:   Loss=1.4832 (C:0.8231, R:0.0066) Ratio=2.89x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=1.3981 (C:0.7070, R:0.0069)
Batch  25/537: Loss=1.4429 (C:0.7479, R:0.0070)
Batch  50/537: Loss=1.3887 (C:0.6995, R:0.0069)
Batch  75/537: Loss=1.4403 (C:0.7440, R:0.0070)
Batch 100/537: Loss=1.3915 (C:0.6935, R:0.0070)
Batch 125/537: Loss=1.4398 (C:0.7423, R:0.0070)
Batch 150/537: Loss=1.3717 (C:0.6822, R:0.0069)
Batch 175/537: Loss=1.4040 (C:0.7118, R:0.0069)
Batch 200/537: Loss=1.4040 (C:0.7115, R:0.0069)
Batch 225/537: Loss=1.3415 (C:0.6456, R:0.0070)
Batch 250/537: Loss=1.4251 (C:0.7186, R:0.0071)
Batch 275/537: Loss=1.4167 (C:0.7184, R:0.0070)
Batch 300/537: Loss=1.4720 (C:0.7756, R:0.0070)
Batch 325/537: Loss=1.4322 (C:0.7387, R:0.0069)
Batch 350/537: Loss=1.4136 (C:0.7269, R:0.0069)
Batch 375/537: Loss=1.4226 (C:0.7279, R:0.0069)
Batch 400/537: Loss=1.4335 (C:0.7338, R:0.0070)
Batch 425/537: Loss=1.4071 (C:0.7065, R:0.0070)
Batch 450/537: Loss=1.4347 (C:0.7368, R:0.0070)
Batch 475/537: Loss=1.4281 (C:0.7305, R:0.0070)
Batch 500/537: Loss=1.4134 (C:0.7129, R:0.0070)
Batch 525/537: Loss=1.4167 (C:0.7167, R:0.0070)

============================================================
Epoch 68/200 completed in 24.2s
Train: Loss=1.4226 (C:0.7256, R:0.0070) Ratio=3.83x
Val:   Loss=1.4628 (C:0.8042, R:0.0066) Ratio=2.93x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4628)
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=1.4380 (C:0.7449, R:0.0069)
Batch  25/537: Loss=1.4645 (C:0.7727, R:0.0069)
Batch  50/537: Loss=1.4314 (C:0.7360, R:0.0070)
Batch  75/537: Loss=1.3705 (C:0.6723, R:0.0070)
Batch 100/537: Loss=1.4157 (C:0.7122, R:0.0070)
Batch 125/537: Loss=1.4310 (C:0.7326, R:0.0070)
Batch 150/537: Loss=1.4134 (C:0.7169, R:0.0070)
Batch 175/537: Loss=1.4046 (C:0.7067, R:0.0070)
Batch 200/537: Loss=1.4012 (C:0.7061, R:0.0070)
Batch 225/537: Loss=1.3960 (C:0.6993, R:0.0070)
Batch 250/537: Loss=1.4079 (C:0.7119, R:0.0070)
Batch 275/537: Loss=1.3727 (C:0.6717, R:0.0070)
Batch 300/537: Loss=1.4358 (C:0.7433, R:0.0069)
Batch 325/537: Loss=1.4021 (C:0.7057, R:0.0070)
Batch 350/537: Loss=1.4353 (C:0.7428, R:0.0069)
Batch 375/537: Loss=1.4750 (C:0.7800, R:0.0070)
Batch 400/537: Loss=1.4417 (C:0.7378, R:0.0070)
Batch 425/537: Loss=1.4289 (C:0.7322, R:0.0070)
Batch 450/537: Loss=1.4758 (C:0.7746, R:0.0070)
Batch 475/537: Loss=1.4197 (C:0.7245, R:0.0070)
Batch 500/537: Loss=1.4160 (C:0.7201, R:0.0070)
Batch 525/537: Loss=1.4110 (C:0.7145, R:0.0070)

============================================================
Epoch 69/200 completed in 24.1s
Train: Loss=1.4205 (C:0.7239, R:0.0070) Ratio=3.82x
Val:   Loss=1.4743 (C:0.8160, R:0.0066) Ratio=2.91x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 70
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.482 ¬± 0.686
    Neg distances: 2.432 ¬± 1.006
    Separation ratio: 5.04x
    Gap: -3.749
    ‚úÖ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=1.4056 (C:0.7094, R:0.0070)
Batch  25/537: Loss=1.4296 (C:0.7355, R:0.0069)
Batch  50/537: Loss=1.3990 (C:0.7001, R:0.0070)
Batch  75/537: Loss=1.4546 (C:0.7560, R:0.0070)
Batch 100/537: Loss=1.4673 (C:0.7721, R:0.0070)
Batch 125/537: Loss=1.4153 (C:0.7124, R:0.0070)
Batch 150/537: Loss=1.3887 (C:0.6891, R:0.0070)
Batch 175/537: Loss=1.4414 (C:0.7483, R:0.0069)
Batch 200/537: Loss=1.4359 (C:0.7383, R:0.0070)
Batch 225/537: Loss=1.4252 (C:0.7293, R:0.0070)
Batch 250/537: Loss=1.3815 (C:0.6877, R:0.0069)
Batch 275/537: Loss=1.4163 (C:0.7269, R:0.0069)
Batch 300/537: Loss=1.4026 (C:0.6992, R:0.0070)
Batch 325/537: Loss=1.3908 (C:0.6978, R:0.0069)
Batch 350/537: Loss=1.4203 (C:0.7274, R:0.0069)
Batch 375/537: Loss=1.3935 (C:0.6973, R:0.0070)
Batch 400/537: Loss=1.4423 (C:0.7441, R:0.0070)
Batch 425/537: Loss=1.4340 (C:0.7453, R:0.0069)
Batch 450/537: Loss=1.4206 (C:0.7285, R:0.0069)
Batch 475/537: Loss=1.4204 (C:0.7147, R:0.0071)
Batch 500/537: Loss=1.4243 (C:0.7281, R:0.0070)
Batch 525/537: Loss=1.4427 (C:0.7467, R:0.0070)

============================================================
Epoch 70/200 completed in 32.7s
Train: Loss=1.4098 (C:0.7134, R:0.0070) Ratio=3.80x
Val:   Loss=1.4580 (C:0.8006, R:0.0066) Ratio=2.90x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4580)
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=1.3882 (C:0.6988, R:0.0069)
Batch  25/537: Loss=1.3758 (C:0.6799, R:0.0070)
Batch  50/537: Loss=1.4111 (C:0.7122, R:0.0070)
Batch  75/537: Loss=1.4309 (C:0.7366, R:0.0069)
Batch 100/537: Loss=1.4318 (C:0.7286, R:0.0070)
Batch 125/537: Loss=1.3797 (C:0.6817, R:0.0070)
Batch 150/537: Loss=1.4007 (C:0.7075, R:0.0069)
Batch 175/537: Loss=1.4215 (C:0.7227, R:0.0070)
Batch 200/537: Loss=1.4156 (C:0.7106, R:0.0070)
Batch 225/537: Loss=1.4154 (C:0.7252, R:0.0069)
Batch 250/537: Loss=1.4149 (C:0.7221, R:0.0069)
Batch 275/537: Loss=1.4342 (C:0.7428, R:0.0069)
Batch 300/537: Loss=1.3774 (C:0.6848, R:0.0069)
Batch 325/537: Loss=1.4320 (C:0.7399, R:0.0069)
Batch 350/537: Loss=1.4272 (C:0.7282, R:0.0070)
Batch 375/537: Loss=1.4010 (C:0.7036, R:0.0070)
Batch 400/537: Loss=1.4369 (C:0.7394, R:0.0070)
Batch 425/537: Loss=1.4140 (C:0.7168, R:0.0070)
Batch 450/537: Loss=1.4422 (C:0.7426, R:0.0070)
Batch 475/537: Loss=1.3756 (C:0.6739, R:0.0070)
Batch 500/537: Loss=1.4084 (C:0.7136, R:0.0069)
Batch 525/537: Loss=1.4381 (C:0.7490, R:0.0069)

============================================================
Epoch 71/200 completed in 24.2s
Train: Loss=1.4087 (C:0.7127, R:0.0070) Ratio=3.85x
Val:   Loss=1.4626 (C:0.8052, R:0.0066) Ratio=2.91x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=1.4078 (C:0.7127, R:0.0070)
Batch  25/537: Loss=1.3885 (C:0.6950, R:0.0069)
Batch  50/537: Loss=1.3989 (C:0.6997, R:0.0070)
Batch  75/537: Loss=1.4170 (C:0.7145, R:0.0070)
Batch 100/537: Loss=1.4338 (C:0.7372, R:0.0070)
Batch 125/537: Loss=1.4295 (C:0.7318, R:0.0070)
Batch 150/537: Loss=1.4478 (C:0.7448, R:0.0070)
Batch 175/537: Loss=1.4633 (C:0.7654, R:0.0070)
Batch 200/537: Loss=1.3918 (C:0.6998, R:0.0069)
Batch 225/537: Loss=1.3888 (C:0.6925, R:0.0070)
Batch 250/537: Loss=1.3699 (C:0.6750, R:0.0069)
Batch 275/537: Loss=1.3865 (C:0.6956, R:0.0069)
Batch 300/537: Loss=1.4224 (C:0.7249, R:0.0070)
Batch 325/537: Loss=1.4472 (C:0.7558, R:0.0069)
Batch 350/537: Loss=1.3886 (C:0.6912, R:0.0070)
Batch 375/537: Loss=1.4019 (C:0.7045, R:0.0070)
Batch 400/537: Loss=1.4197 (C:0.7265, R:0.0069)
Batch 425/537: Loss=1.3876 (C:0.6947, R:0.0069)
Batch 450/537: Loss=1.4204 (C:0.7243, R:0.0070)
Batch 475/537: Loss=1.4334 (C:0.7432, R:0.0069)
Batch 500/537: Loss=1.4188 (C:0.7268, R:0.0069)
Batch 525/537: Loss=1.4217 (C:0.7244, R:0.0070)

============================================================
Epoch 72/200 completed in 24.5s
Train: Loss=1.4077 (C:0.7120, R:0.0070) Ratio=3.82x
Val:   Loss=1.4437 (C:0.7861, R:0.0066) Ratio=2.94x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.4437)
============================================================

üåç Updating global dataset at epoch 73
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.477 ¬± 0.702
    Neg distances: 2.457 ¬± 1.005
    Separation ratio: 5.15x
    Gap: -3.789
    ‚úÖ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=1.3600 (C:0.6611, R:0.0070)
Batch  25/537: Loss=1.3610 (C:0.6699, R:0.0069)
Batch  50/537: Loss=1.3556 (C:0.6602, R:0.0070)
Batch  75/537: Loss=1.4371 (C:0.7411, R:0.0070)
Batch 100/537: Loss=1.3685 (C:0.6757, R:0.0069)
Batch 125/537: Loss=1.3953 (C:0.6939, R:0.0070)
Batch 150/537: Loss=1.4057 (C:0.7082, R:0.0070)
Batch 175/537: Loss=1.4145 (C:0.7173, R:0.0070)
Batch 200/537: Loss=1.4407 (C:0.7440, R:0.0070)
Batch 225/537: Loss=1.3970 (C:0.7101, R:0.0069)
Batch 250/537: Loss=1.4216 (C:0.7292, R:0.0069)
Batch 275/537: Loss=1.4287 (C:0.7329, R:0.0070)
Batch 300/537: Loss=1.3973 (C:0.7030, R:0.0069)
Batch 325/537: Loss=1.3858 (C:0.6922, R:0.0069)
Batch 350/537: Loss=1.3875 (C:0.6963, R:0.0069)
Batch 375/537: Loss=1.3534 (C:0.6583, R:0.0070)
Batch 400/537: Loss=1.3962 (C:0.6948, R:0.0070)
Batch 425/537: Loss=1.3713 (C:0.6789, R:0.0069)
Batch 450/537: Loss=1.3992 (C:0.7022, R:0.0070)
Batch 475/537: Loss=1.4959 (C:0.7987, R:0.0070)
Batch 500/537: Loss=1.4202 (C:0.7225, R:0.0070)
Batch 525/537: Loss=1.3844 (C:0.6880, R:0.0070)

============================================================
Epoch 73/200 completed in 32.8s
Train: Loss=1.3984 (C:0.7030, R:0.0070) Ratio=3.83x
Val:   Loss=1.4641 (C:0.8069, R:0.0066) Ratio=2.88x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=1.3620 (C:0.6624, R:0.0070)
Batch  25/537: Loss=1.3831 (C:0.6857, R:0.0070)
Batch  50/537: Loss=1.3884 (C:0.6905, R:0.0070)
Batch  75/537: Loss=1.4314 (C:0.7373, R:0.0069)
Batch 100/537: Loss=1.3992 (C:0.6996, R:0.0070)
Batch 125/537: Loss=1.4046 (C:0.7126, R:0.0069)
Batch 150/537: Loss=1.4256 (C:0.7269, R:0.0070)
Batch 175/537: Loss=1.4053 (C:0.7098, R:0.0070)
Batch 200/537: Loss=1.4086 (C:0.7139, R:0.0069)
Batch 225/537: Loss=1.4267 (C:0.7288, R:0.0070)
Batch 250/537: Loss=1.3854 (C:0.6889, R:0.0070)
Batch 275/537: Loss=1.3926 (C:0.6988, R:0.0069)
Batch 300/537: Loss=1.3804 (C:0.6831, R:0.0070)
Batch 325/537: Loss=1.4352 (C:0.7385, R:0.0070)
Batch 350/537: Loss=1.4396 (C:0.7394, R:0.0070)
Batch 375/537: Loss=1.4672 (C:0.7771, R:0.0069)
Batch 400/537: Loss=1.4213 (C:0.7252, R:0.0070)
Batch 425/537: Loss=1.3978 (C:0.7059, R:0.0069)
Batch 450/537: Loss=1.3912 (C:0.7008, R:0.0069)
Batch 475/537: Loss=1.4185 (C:0.7216, R:0.0070)
Batch 500/537: Loss=1.4066 (C:0.7112, R:0.0070)
Batch 525/537: Loss=1.3648 (C:0.6700, R:0.0069)

============================================================
Epoch 74/200 completed in 24.3s
Train: Loss=1.3969 (C:0.7017, R:0.0070) Ratio=3.83x
Val:   Loss=1.4610 (C:0.8048, R:0.0066) Ratio=2.93x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=1.4021 (C:0.7064, R:0.0070)
Batch  25/537: Loss=1.4118 (C:0.7156, R:0.0070)
Batch  50/537: Loss=1.3929 (C:0.6993, R:0.0069)
Batch  75/537: Loss=1.4022 (C:0.7150, R:0.0069)
Batch 100/537: Loss=1.3885 (C:0.6897, R:0.0070)
Batch 125/537: Loss=1.3761 (C:0.6813, R:0.0069)
Batch 150/537: Loss=1.4341 (C:0.7373, R:0.0070)
Batch 175/537: Loss=1.3948 (C:0.7035, R:0.0069)
Batch 200/537: Loss=1.4093 (C:0.7191, R:0.0069)
Batch 225/537: Loss=1.3738 (C:0.6840, R:0.0069)
Batch 250/537: Loss=1.4430 (C:0.7472, R:0.0070)
Batch 275/537: Loss=1.4362 (C:0.7391, R:0.0070)
Batch 300/537: Loss=1.4018 (C:0.7064, R:0.0070)
Batch 325/537: Loss=1.3630 (C:0.6711, R:0.0069)
Batch 350/537: Loss=1.3653 (C:0.6737, R:0.0069)
Batch 375/537: Loss=1.4281 (C:0.7267, R:0.0070)
Batch 400/537: Loss=1.3816 (C:0.6952, R:0.0069)
Batch 425/537: Loss=1.4034 (C:0.7052, R:0.0070)
Batch 450/537: Loss=1.4294 (C:0.7302, R:0.0070)
Batch 475/537: Loss=1.3988 (C:0.7032, R:0.0070)
Batch 500/537: Loss=1.4248 (C:0.7283, R:0.0070)
Batch 525/537: Loss=1.4007 (C:0.7039, R:0.0070)

============================================================
Epoch 75/200 completed in 24.3s
Train: Loss=1.3979 (C:0.7030, R:0.0069) Ratio=3.77x
Val:   Loss=1.4606 (C:0.8041, R:0.0066) Ratio=2.90x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

üåç Updating global dataset at epoch 76
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.480 ¬± 0.692
    Neg distances: 2.457 ¬± 1.015
    Separation ratio: 5.11x
    Gap: -3.773
    ‚úÖ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=1.4045 (C:0.7006, R:0.0070)
Batch  25/537: Loss=1.3875 (C:0.6985, R:0.0069)
Batch  50/537: Loss=1.4015 (C:0.7097, R:0.0069)
Batch  75/537: Loss=1.4268 (C:0.7252, R:0.0070)
Batch 100/537: Loss=1.4079 (C:0.7129, R:0.0070)
Batch 125/537: Loss=1.4206 (C:0.7245, R:0.0070)
Batch 150/537: Loss=1.3837 (C:0.6869, R:0.0070)
Batch 175/537: Loss=1.4067 (C:0.7146, R:0.0069)
Batch 200/537: Loss=1.3380 (C:0.6473, R:0.0069)
Batch 225/537: Loss=1.4241 (C:0.7259, R:0.0070)
Batch 250/537: Loss=1.4294 (C:0.7346, R:0.0069)
Batch 275/537: Loss=1.4041 (C:0.7108, R:0.0069)
Batch 300/537: Loss=1.3951 (C:0.7011, R:0.0069)
Batch 325/537: Loss=1.3968 (C:0.7035, R:0.0069)
Batch 350/537: Loss=1.4074 (C:0.7097, R:0.0070)
Batch 375/537: Loss=1.4352 (C:0.7448, R:0.0069)
Batch 400/537: Loss=1.4256 (C:0.7296, R:0.0070)
Batch 425/537: Loss=1.3917 (C:0.7019, R:0.0069)
Batch 450/537: Loss=1.3832 (C:0.6869, R:0.0070)
Batch 475/537: Loss=1.4351 (C:0.7344, R:0.0070)
Batch 500/537: Loss=1.4188 (C:0.7269, R:0.0069)
Batch 525/537: Loss=1.3743 (C:0.6773, R:0.0070)

============================================================
Epoch 76/200 completed in 32.1s
Train: Loss=1.4022 (C:0.7076, R:0.0069) Ratio=3.85x
Val:   Loss=1.4510 (C:0.7955, R:0.0066) Ratio=2.95x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=1.3467 (C:0.6524, R:0.0069)
Batch  25/537: Loss=1.4264 (C:0.7311, R:0.0070)
Batch  50/537: Loss=1.3550 (C:0.6668, R:0.0069)
Batch  75/537: Loss=1.4071 (C:0.7126, R:0.0069)
Batch 100/537: Loss=1.4029 (C:0.7058, R:0.0070)
Batch 125/537: Loss=1.3851 (C:0.6927, R:0.0069)
Batch 150/537: Loss=1.3978 (C:0.6992, R:0.0070)
Batch 175/537: Loss=1.3889 (C:0.6961, R:0.0069)
Batch 200/537: Loss=1.4025 (C:0.7092, R:0.0069)
Batch 225/537: Loss=1.3883 (C:0.6949, R:0.0069)
Batch 250/537: Loss=1.3750 (C:0.6776, R:0.0070)
Batch 275/537: Loss=1.4078 (C:0.7136, R:0.0069)
Batch 300/537: Loss=1.4143 (C:0.7217, R:0.0069)
Batch 325/537: Loss=1.3920 (C:0.6958, R:0.0070)
Batch 350/537: Loss=1.4085 (C:0.7124, R:0.0070)
Batch 375/537: Loss=1.4355 (C:0.7465, R:0.0069)
Batch 400/537: Loss=1.4510 (C:0.7555, R:0.0070)
Batch 425/537: Loss=1.4121 (C:0.7189, R:0.0069)
Batch 450/537: Loss=1.4220 (C:0.7271, R:0.0069)
Batch 475/537: Loss=1.3699 (C:0.6798, R:0.0069)
Batch 500/537: Loss=1.3805 (C:0.6854, R:0.0070)
Batch 525/537: Loss=1.3964 (C:0.7064, R:0.0069)

============================================================
Epoch 77/200 completed in 23.5s
Train: Loss=1.4004 (C:0.7061, R:0.0069) Ratio=3.92x
Val:   Loss=1.4515 (C:0.7964, R:0.0066) Ratio=2.94x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=1.4040 (C:0.7123, R:0.0069)
Batch  25/537: Loss=1.3738 (C:0.6758, R:0.0070)
Batch  50/537: Loss=1.4069 (C:0.7133, R:0.0069)
Batch  75/537: Loss=1.4255 (C:0.7226, R:0.0070)
Batch 100/537: Loss=1.4483 (C:0.7549, R:0.0069)
Batch 125/537: Loss=1.4185 (C:0.7205, R:0.0070)
Batch 150/537: Loss=1.3923 (C:0.6996, R:0.0069)
Batch 175/537: Loss=1.3952 (C:0.7023, R:0.0069)
Batch 200/537: Loss=1.3778 (C:0.6817, R:0.0070)
Batch 225/537: Loss=1.4006 (C:0.7041, R:0.0070)
Batch 250/537: Loss=1.4225 (C:0.7251, R:0.0070)
Batch 275/537: Loss=1.3819 (C:0.6839, R:0.0070)
Batch 300/537: Loss=1.4100 (C:0.7124, R:0.0070)
Batch 325/537: Loss=1.3830 (C:0.6951, R:0.0069)
Batch 350/537: Loss=1.4003 (C:0.7042, R:0.0070)
Batch 375/537: Loss=1.3795 (C:0.6900, R:0.0069)
Batch 400/537: Loss=1.3627 (C:0.6709, R:0.0069)
Batch 425/537: Loss=1.3826 (C:0.6955, R:0.0069)
Batch 450/537: Loss=1.4438 (C:0.7482, R:0.0070)
Batch 475/537: Loss=1.3841 (C:0.6932, R:0.0069)
Batch 500/537: Loss=1.4392 (C:0.7423, R:0.0070)
Batch 525/537: Loss=1.4499 (C:0.7551, R:0.0069)

============================================================
Epoch 78/200 completed in 23.8s
Train: Loss=1.3994 (C:0.7054, R:0.0069) Ratio=3.84x
Val:   Loss=1.4532 (C:0.7980, R:0.0066) Ratio=2.96x
Reconstruction weight: 100.000
No improvement for 6 epochs
============================================================

üåç Updating global dataset at epoch 79
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.470 ¬± 0.689
    Neg distances: 2.460 ¬± 1.008
    Separation ratio: 5.23x
    Gap: -3.795
    ‚úÖ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=1.3620 (C:0.6674, R:0.0069)
Batch  25/537: Loss=1.3719 (C:0.6724, R:0.0070)
Batch  50/537: Loss=1.4049 (C:0.7088, R:0.0070)
Batch  75/537: Loss=1.3512 (C:0.6563, R:0.0069)
Batch 100/537: Loss=1.4178 (C:0.7282, R:0.0069)
Batch 125/537: Loss=1.3779 (C:0.6868, R:0.0069)
Batch 150/537: Loss=1.3953 (C:0.6980, R:0.0070)
Batch 175/537: Loss=1.4238 (C:0.7285, R:0.0070)
Batch 200/537: Loss=1.4096 (C:0.7241, R:0.0069)
Batch 225/537: Loss=1.3889 (C:0.6932, R:0.0070)
Batch 250/537: Loss=1.4058 (C:0.7110, R:0.0069)
Batch 275/537: Loss=1.3996 (C:0.7073, R:0.0069)
Batch 300/537: Loss=1.4466 (C:0.7508, R:0.0070)
Batch 325/537: Loss=1.4122 (C:0.7209, R:0.0069)
Batch 350/537: Loss=1.3821 (C:0.6876, R:0.0069)
Batch 375/537: Loss=1.3844 (C:0.6870, R:0.0070)
Batch 400/537: Loss=1.3652 (C:0.6741, R:0.0069)
Batch 425/537: Loss=1.4128 (C:0.7225, R:0.0069)
Batch 450/537: Loss=1.3818 (C:0.6891, R:0.0069)
Batch 475/537: Loss=1.3940 (C:0.6962, R:0.0070)
Batch 500/537: Loss=1.3480 (C:0.6543, R:0.0069)
Batch 525/537: Loss=1.4045 (C:0.7083, R:0.0070)

============================================================
Epoch 79/200 completed in 30.9s
Train: Loss=1.3880 (C:0.6941, R:0.0069) Ratio=3.89x
Val:   Loss=1.4537 (C:0.7981, R:0.0066) Ratio=2.89x
Reconstruction weight: 100.000
No improvement for 7 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=1.4051 (C:0.7118, R:0.0069)
Batch  25/537: Loss=1.3760 (C:0.6908, R:0.0069)
Batch  50/537: Loss=1.4158 (C:0.7205, R:0.0070)
Batch  75/537: Loss=1.3960 (C:0.6985, R:0.0070)
Batch 100/537: Loss=1.3808 (C:0.6874, R:0.0069)
Batch 125/537: Loss=1.4372 (C:0.7377, R:0.0070)
Batch 150/537: Loss=1.3609 (C:0.6708, R:0.0069)
Batch 175/537: Loss=1.4200 (C:0.7247, R:0.0070)
Batch 200/537: Loss=1.3955 (C:0.7054, R:0.0069)
Batch 225/537: Loss=1.3716 (C:0.6793, R:0.0069)
Batch 250/537: Loss=1.3674 (C:0.6724, R:0.0070)
Batch 275/537: Loss=1.3741 (C:0.6847, R:0.0069)
Batch 300/537: Loss=1.4327 (C:0.7347, R:0.0070)
Batch 325/537: Loss=1.3896 (C:0.6945, R:0.0070)
Batch 350/537: Loss=1.3894 (C:0.6976, R:0.0069)
Batch 375/537: Loss=1.3926 (C:0.6955, R:0.0070)
Batch 400/537: Loss=1.3717 (C:0.6776, R:0.0069)
Batch 425/537: Loss=1.3680 (C:0.6723, R:0.0070)
Batch 450/537: Loss=1.3972 (C:0.7027, R:0.0069)
Batch 475/537: Loss=1.4133 (C:0.7196, R:0.0069)
Batch 500/537: Loss=1.4075 (C:0.7127, R:0.0069)
Batch 525/537: Loss=1.3490 (C:0.6540, R:0.0070)

============================================================
Epoch 80/200 completed in 23.6s
Train: Loss=1.3881 (C:0.6946, R:0.0069) Ratio=3.94x
Val:   Loss=1.4475 (C:0.7930, R:0.0065) Ratio=2.88x
Reconstruction weight: 100.000
No improvement for 8 epochs
Checkpoint saved at epoch 80

Early stopping triggered after 80 epochs
Best model was at epoch 72 with Val Loss: 1.4437

Global Dataset Training Completed!
Best epoch: 72
Best validation loss: 1.4437
Final separation ratios: Train=3.94x, Val=2.88x
Training completed!
Creating loss plots...
Loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/plots/global_cosine_test_no_attention_training_losses.png
Loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/plots/global_cosine_test_no_attention_training_losses.png
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.4778
  Adjusted Rand Score: 0.5636
  Clustering Accuracy: 0.8303
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8328
  Per-class F1: [0.8509529239289786, 0.7789713541666666, 0.8688497613954255]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.006533
Evaluating separation quality...
Separation Results:
  Positive distances: 0.783 ¬± 0.952
  Negative distances: 2.283 ¬± 1.140
  Separation ratio: 2.91x
  Gap: -3.819
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.4778
  Clustering Accuracy: 0.8303
  Adjusted Rand Score: 0.5636

Classification Performance:
  Accuracy: 0.8328

Separation Quality:
  Separation Ratio: 2.91x
  Gap: -3.819
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.006533
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/results/evaluation_results_20250724_200450.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/results/evaluation_results_20250724_200450.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_no_attention_20250724_192830/final_results.json

Key Results:
  Separation ratio: 2.91x
  Perfect separation: False
  Classification accuracy: 0.8328

Analysis completed with exit code: 0
Time: Thu 24 Jul 20:04:52 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
