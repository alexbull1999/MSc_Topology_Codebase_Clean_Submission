Starting Surface Distance Metric Analysis job...
Job ID: 185907
Node: gpuvm18
Time: Thu 24 Jul 20:09:00 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Thu Jul 24 20:09:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   32C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-24 20:09:16.293157
Using device: cuda

Configuration:
  Embedding type: cosine_concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 200
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'cosine_concat'
Output dimension will be: 1537
GlobalDataLoader initialized:
  Embedding type: cosine_concat
  Output dimension: 1537
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating cosine_concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated cosine_concat embeddings: torch.Size([549367, 1537])
Generating embeddings for validation...
Generating cosine_concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9842, 1537])
Generating embeddings for test...
Generating cosine_concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9824, 1537])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1537])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1537])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1537])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1537
Updated model input_dim to: 1537
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
AttentionAutoencoder initialized:
  Input dim: 1537
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Attention Heads: 5
  Total parameters: 3,069,501
Model created with 3,069,501 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 100.0
Optimizer created: Adam (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 3,069,501
Starting training...
========================================
Starting Global Dataset Training...
============================================================

üåç Updating global dataset at epoch 1
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.116 ¬± 0.018
    Neg distances: 0.116 ¬± 0.018
    Separation ratio: 1.00x
    Gap: -0.193
    ‚ùå Poor global separation

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=35.4250 (C:2.2797, R:0.3315)
Batch  25/537: Loss=21.1231 (C:3.0754, R:0.1805)
Batch  50/537: Loss=13.4813 (C:3.1404, R:0.1034)
Batch  75/537: Loss=9.2034 (C:3.1547, R:0.0605)
Batch 100/537: Loss=6.9113 (C:3.1263, R:0.0378)
Batch 125/537: Loss=5.6704 (C:3.0397, R:0.0263)
Batch 150/537: Loss=4.9327 (C:2.9190, R:0.0201)
Batch 175/537: Loss=4.4566 (C:2.7914, R:0.0167)
Batch 200/537: Loss=4.1266 (C:2.6393, R:0.0149)
Batch 225/537: Loss=3.8556 (C:2.4875, R:0.0137)
Batch 250/537: Loss=3.6275 (C:2.3370, R:0.0129)
Batch 275/537: Loss=3.4267 (C:2.1864, R:0.0124)
Batch 300/537: Loss=3.2386 (C:2.0386, R:0.0120)
Batch 325/537: Loss=3.1114 (C:2.0073, R:0.0110)
Batch 350/537: Loss=3.0649 (C:2.0021, R:0.0106)
Batch 375/537: Loss=3.0395 (C:2.0017, R:0.0104)
Batch 400/537: Loss=3.0207 (C:2.0004, R:0.0102)
Batch 425/537: Loss=3.0164 (C:2.0005, R:0.0102)
Batch 450/537: Loss=3.0116 (C:1.9995, R:0.0101)
Batch 475/537: Loss=3.0087 (C:2.0008, R:0.0101)
Batch 500/537: Loss=3.0096 (C:2.0005, R:0.0101)
Batch 525/537: Loss=3.0055 (C:2.0003, R:0.0101)

============================================================
Epoch 1/200 completed in 34.9s
Train: Loss=5.9544 (C:2.4282, R:0.0353) Ratio=1.00x
Val:   Loss=2.9899 (C:2.0004, R:0.0099) Ratio=1.00x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.9899)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=3.0058 (C:2.0005, R:0.0101)
Batch  25/537: Loss=3.0033 (C:2.0006, R:0.0100)
Batch  50/537: Loss=3.0000 (C:2.0003, R:0.0100)
Batch  75/537: Loss=3.0011 (C:2.0003, R:0.0100)
Batch 100/537: Loss=3.0019 (C:2.0008, R:0.0100)
Batch 125/537: Loss=2.9980 (C:1.9995, R:0.0100)
Batch 150/537: Loss=2.9977 (C:2.0008, R:0.0100)
Batch 175/537: Loss=2.9970 (C:2.0008, R:0.0100)
Batch 200/537: Loss=2.9966 (C:2.0007, R:0.0100)
Batch 225/537: Loss=2.9979 (C:1.9994, R:0.0100)
Batch 250/537: Loss=2.9992 (C:2.0006, R:0.0100)
Batch 275/537: Loss=2.9975 (C:2.0004, R:0.0100)
Batch 300/537: Loss=2.9960 (C:2.0002, R:0.0100)
Batch 325/537: Loss=2.9912 (C:2.0001, R:0.0099)
Batch 350/537: Loss=2.9947 (C:2.0002, R:0.0099)
Batch 375/537: Loss=2.9944 (C:2.0003, R:0.0099)
Batch 400/537: Loss=2.9907 (C:1.9997, R:0.0099)
Batch 425/537: Loss=2.9909 (C:1.9979, R:0.0099)
Batch 450/537: Loss=2.9942 (C:1.9987, R:0.0100)
Batch 475/537: Loss=2.9933 (C:1.9959, R:0.0100)
Batch 500/537: Loss=2.9916 (C:1.9967, R:0.0099)
Batch 525/537: Loss=2.9912 (C:1.9940, R:0.0100)

============================================================
Epoch 2/200 completed in 23.7s
Train: Loss=2.9965 (C:1.9995, R:0.0100) Ratio=1.04x
Val:   Loss=2.9807 (C:1.9930, R:0.0099) Ratio=1.48x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.9807)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=2.9914 (C:1.9934, R:0.0100)
Batch  25/537: Loss=2.9919 (C:1.9931, R:0.0100)
Batch  50/537: Loss=2.9962 (C:1.9924, R:0.0100)
Batch  75/537: Loss=2.9889 (C:1.9904, R:0.0100)
Batch 100/537: Loss=2.9897 (C:1.9893, R:0.0100)
Batch 125/537: Loss=2.9900 (C:1.9897, R:0.0100)
Batch 150/537: Loss=2.9887 (C:1.9894, R:0.0100)
Batch 175/537: Loss=2.9937 (C:1.9874, R:0.0101)
Batch 200/537: Loss=2.9879 (C:1.9844, R:0.0100)
Batch 225/537: Loss=2.9856 (C:1.9855, R:0.0100)
Batch 250/537: Loss=2.9922 (C:1.9854, R:0.0101)
Batch 275/537: Loss=2.9900 (C:1.9844, R:0.0101)
Batch 300/537: Loss=2.9885 (C:1.9858, R:0.0100)
Batch 325/537: Loss=2.9870 (C:1.9846, R:0.0100)
Batch 350/537: Loss=2.9853 (C:1.9818, R:0.0100)
Batch 375/537: Loss=2.9802 (C:1.9788, R:0.0100)
Batch 400/537: Loss=2.9835 (C:1.9806, R:0.0100)
Batch 425/537: Loss=2.9819 (C:1.9777, R:0.0100)
Batch 450/537: Loss=2.9860 (C:1.9813, R:0.0100)
Batch 475/537: Loss=2.9823 (C:1.9778, R:0.0100)
Batch 500/537: Loss=2.9814 (C:1.9763, R:0.0101)
Batch 525/537: Loss=2.9826 (C:1.9781, R:0.0100)

============================================================
Epoch 3/200 completed in 24.2s
Train: Loss=2.9868 (C:1.9850, R:0.0100) Ratio=1.52x
Val:   Loss=2.9636 (C:1.9699, R:0.0099) Ratio=2.20x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.9636)
============================================================

üåç Updating global dataset at epoch 4
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.154 ¬± 0.136
    Neg distances: 0.346 ¬± 0.188
    Separation ratio: 2.25x
    Gap: -0.742
    ‚úÖ Good global separation

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=2.8301 (C:1.8259, R:0.0100)
Batch  25/537: Loss=2.8219 (C:1.8001, R:0.0102)
Batch  50/537: Loss=2.8127 (C:1.7938, R:0.0102)
Batch  75/537: Loss=2.8184 (C:1.7970, R:0.0102)
Batch 100/537: Loss=2.8119 (C:1.7976, R:0.0101)
Batch 125/537: Loss=2.8173 (C:1.7938, R:0.0102)
Batch 150/537: Loss=2.8010 (C:1.7766, R:0.0102)
Batch 175/537: Loss=2.7904 (C:1.7681, R:0.0102)
Batch 200/537: Loss=2.7882 (C:1.7677, R:0.0102)
Batch 225/537: Loss=2.7836 (C:1.7581, R:0.0103)
Batch 250/537: Loss=2.7875 (C:1.7622, R:0.0103)
Batch 275/537: Loss=2.7801 (C:1.7576, R:0.0102)
Batch 300/537: Loss=2.7699 (C:1.7534, R:0.0102)
Batch 325/537: Loss=2.7571 (C:1.7363, R:0.0102)
Batch 350/537: Loss=2.7504 (C:1.7323, R:0.0102)
Batch 375/537: Loss=2.7498 (C:1.7350, R:0.0101)
Batch 400/537: Loss=2.7570 (C:1.7417, R:0.0102)
Batch 425/537: Loss=2.7427 (C:1.7338, R:0.0101)
Batch 450/537: Loss=2.7475 (C:1.7352, R:0.0101)
Batch 475/537: Loss=2.7380 (C:1.7340, R:0.0100)
Batch 500/537: Loss=2.7353 (C:1.7309, R:0.0100)
Batch 525/537: Loss=2.7234 (C:1.7217, R:0.0100)

============================================================
Epoch 4/200 completed in 34.2s
Train: Loss=2.7785 (C:1.7619, R:0.0102) Ratio=2.00x
Val:   Loss=2.6991 (C:1.7058, R:0.0099) Ratio=2.69x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.6991)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=2.7259 (C:1.7226, R:0.0100)
Batch  25/537: Loss=2.7274 (C:1.7283, R:0.0100)
Batch  50/537: Loss=2.7224 (C:1.7208, R:0.0100)
Batch  75/537: Loss=2.7126 (C:1.7160, R:0.0100)
Batch 100/537: Loss=2.7177 (C:1.7195, R:0.0100)
Batch 125/537: Loss=2.7073 (C:1.7094, R:0.0100)
Batch 150/537: Loss=2.7186 (C:1.7204, R:0.0100)
Batch 175/537: Loss=2.7158 (C:1.7155, R:0.0100)
Batch 200/537: Loss=2.7222 (C:1.7263, R:0.0100)
Batch 225/537: Loss=2.7088 (C:1.7155, R:0.0099)
Batch 250/537: Loss=2.7123 (C:1.7157, R:0.0100)
Batch 275/537: Loss=2.6996 (C:1.7096, R:0.0099)
Batch 300/537: Loss=2.7179 (C:1.7263, R:0.0099)
Batch 325/537: Loss=2.7195 (C:1.7261, R:0.0099)
Batch 350/537: Loss=2.7197 (C:1.7289, R:0.0099)
Batch 375/537: Loss=2.7043 (C:1.7107, R:0.0099)
Batch 400/537: Loss=2.6925 (C:1.6992, R:0.0099)
Batch 425/537: Loss=2.7130 (C:1.7194, R:0.0099)
Batch 450/537: Loss=2.7056 (C:1.7135, R:0.0099)
Batch 475/537: Loss=2.6912 (C:1.7034, R:0.0099)
Batch 500/537: Loss=2.7042 (C:1.7126, R:0.0099)
Batch 525/537: Loss=2.6996 (C:1.7075, R:0.0099)

============================================================
Epoch 5/200 completed in 24.4s
Train: Loss=2.7115 (C:1.7175, R:0.0099) Ratio=2.30x
Val:   Loss=2.6754 (C:1.6937, R:0.0098) Ratio=2.80x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.6754)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=2.6961 (C:1.7031, R:0.0099)
Batch  25/537: Loss=2.6872 (C:1.6971, R:0.0099)
Batch  50/537: Loss=2.6934 (C:1.7026, R:0.0099)
Batch  75/537: Loss=2.7003 (C:1.7111, R:0.0099)
Batch 100/537: Loss=2.6987 (C:1.7099, R:0.0099)
Batch 125/537: Loss=2.7029 (C:1.7117, R:0.0099)
Batch 150/537: Loss=2.7038 (C:1.7107, R:0.0099)
Batch 175/537: Loss=2.7003 (C:1.7089, R:0.0099)
Batch 200/537: Loss=2.6967 (C:1.7075, R:0.0099)
Batch 225/537: Loss=2.6986 (C:1.7111, R:0.0099)
Batch 250/537: Loss=2.6938 (C:1.7037, R:0.0099)
Batch 275/537: Loss=2.7012 (C:1.7143, R:0.0099)
Batch 300/537: Loss=2.6906 (C:1.6991, R:0.0099)
Batch 325/537: Loss=2.7120 (C:1.7195, R:0.0099)
Batch 350/537: Loss=2.6860 (C:1.7024, R:0.0098)
Batch 375/537: Loss=2.6829 (C:1.6943, R:0.0099)
Batch 400/537: Loss=2.6903 (C:1.7033, R:0.0099)
Batch 425/537: Loss=2.6755 (C:1.6872, R:0.0099)
Batch 450/537: Loss=2.6937 (C:1.7059, R:0.0099)
Batch 475/537: Loss=2.6910 (C:1.6982, R:0.0099)
Batch 500/537: Loss=2.6792 (C:1.6935, R:0.0099)
Batch 525/537: Loss=2.6872 (C:1.6972, R:0.0099)

============================================================
Epoch 6/200 completed in 25.6s
Train: Loss=2.6932 (C:1.7042, R:0.0099) Ratio=2.48x
Val:   Loss=2.6683 (C:1.6896, R:0.0098) Ratio=2.90x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.6683)
============================================================

üåç Updating global dataset at epoch 7
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.493 ¬± 0.637
    Neg distances: 1.514 ¬± 0.854
    Separation ratio: 3.07x
    Gap: -2.678
    ‚úÖ Excellent global separation!

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=2.1488 (C:1.1639, R:0.0098)
Batch  25/537: Loss=2.1699 (C:1.1796, R:0.0099)
Batch  50/537: Loss=2.1630 (C:1.1755, R:0.0099)
Batch  75/537: Loss=2.1397 (C:1.1514, R:0.0099)
Batch 100/537: Loss=2.1465 (C:1.1560, R:0.0099)
Batch 125/537: Loss=2.2040 (C:1.2125, R:0.0099)
Batch 150/537: Loss=2.1230 (C:1.1343, R:0.0099)
Batch 175/537: Loss=2.1525 (C:1.1636, R:0.0099)
Batch 200/537: Loss=2.1653 (C:1.1767, R:0.0099)
Batch 225/537: Loss=2.1700 (C:1.1833, R:0.0099)
Batch 250/537: Loss=2.1561 (C:1.1659, R:0.0099)
Batch 275/537: Loss=2.1469 (C:1.1588, R:0.0099)
Batch 300/537: Loss=2.1683 (C:1.1790, R:0.0099)
Batch 325/537: Loss=2.1706 (C:1.1784, R:0.0099)
Batch 350/537: Loss=2.1569 (C:1.1717, R:0.0099)
Batch 375/537: Loss=2.1134 (C:1.1268, R:0.0099)
Batch 400/537: Loss=2.1420 (C:1.1510, R:0.0099)
Batch 425/537: Loss=2.1320 (C:1.1441, R:0.0099)
Batch 450/537: Loss=2.1562 (C:1.1699, R:0.0099)
Batch 475/537: Loss=2.1673 (C:1.1760, R:0.0099)
Batch 500/537: Loss=2.1444 (C:1.1535, R:0.0099)
Batch 525/537: Loss=2.1535 (C:1.1628, R:0.0099)

============================================================
Epoch 7/200 completed in 33.2s
Train: Loss=2.1484 (C:1.1591, R:0.0099) Ratio=2.85x
Val:   Loss=2.1364 (C:1.1549, R:0.0098) Ratio=2.98x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1364)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=2.1234 (C:1.1356, R:0.0099)
Batch  25/537: Loss=2.1208 (C:1.1331, R:0.0099)
Batch  50/537: Loss=2.1712 (C:1.1836, R:0.0099)
Batch  75/537: Loss=2.1283 (C:1.1396, R:0.0099)
Batch 100/537: Loss=2.1900 (C:1.1990, R:0.0099)
Batch 125/537: Loss=2.1105 (C:1.1250, R:0.0099)
Batch 150/537: Loss=2.1405 (C:1.1503, R:0.0099)
Batch 175/537: Loss=2.1295 (C:1.1398, R:0.0099)
Batch 200/537: Loss=2.1075 (C:1.1179, R:0.0099)
Batch 225/537: Loss=2.1119 (C:1.1275, R:0.0098)
Batch 250/537: Loss=2.1301 (C:1.1357, R:0.0099)
Batch 275/537: Loss=2.1241 (C:1.1347, R:0.0099)
Batch 300/537: Loss=2.0717 (C:1.0868, R:0.0098)
Batch 325/537: Loss=2.1398 (C:1.1519, R:0.0099)
Batch 350/537: Loss=2.1517 (C:1.1610, R:0.0099)
Batch 375/537: Loss=2.1209 (C:1.1304, R:0.0099)
Batch 400/537: Loss=2.1610 (C:1.1666, R:0.0099)
Batch 425/537: Loss=2.1212 (C:1.1343, R:0.0099)
Batch 450/537: Loss=2.1139 (C:1.1267, R:0.0099)
Batch 475/537: Loss=2.0806 (C:1.0906, R:0.0099)
Batch 500/537: Loss=2.0908 (C:1.1019, R:0.0099)
Batch 525/537: Loss=2.0956 (C:1.1091, R:0.0099)

============================================================
Epoch 8/200 completed in 24.4s
Train: Loss=2.1252 (C:1.1365, R:0.0099) Ratio=3.21x
Val:   Loss=2.1289 (C:1.1491, R:0.0098) Ratio=3.04x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1289)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=2.1339 (C:1.1451, R:0.0099)
Batch  25/537: Loss=2.0549 (C:1.0699, R:0.0098)
Batch  50/537: Loss=2.0891 (C:1.1038, R:0.0099)
Batch  75/537: Loss=2.0954 (C:1.1069, R:0.0099)
Batch 100/537: Loss=2.1198 (C:1.1281, R:0.0099)
Batch 125/537: Loss=2.1039 (C:1.1174, R:0.0099)
Batch 150/537: Loss=2.1413 (C:1.1506, R:0.0099)
Batch 175/537: Loss=2.1079 (C:1.1177, R:0.0099)
Batch 200/537: Loss=2.1346 (C:1.1477, R:0.0099)
Batch 225/537: Loss=2.0982 (C:1.1109, R:0.0099)
Batch 250/537: Loss=2.1006 (C:1.1140, R:0.0099)
Batch 275/537: Loss=2.1071 (C:1.1207, R:0.0099)
Batch 300/537: Loss=2.1138 (C:1.1273, R:0.0099)
Batch 325/537: Loss=2.1196 (C:1.1338, R:0.0099)
Batch 350/537: Loss=2.1088 (C:1.1192, R:0.0099)
Batch 375/537: Loss=2.1098 (C:1.1255, R:0.0098)
Batch 400/537: Loss=2.1133 (C:1.1248, R:0.0099)
Batch 425/537: Loss=2.1440 (C:1.1581, R:0.0099)
Batch 450/537: Loss=2.1105 (C:1.1243, R:0.0099)
Batch 475/537: Loss=2.1267 (C:1.1375, R:0.0099)
Batch 500/537: Loss=2.1252 (C:1.1367, R:0.0099)
Batch 525/537: Loss=2.0954 (C:1.1038, R:0.0099)

============================================================
Epoch 9/200 completed in 24.2s
Train: Loss=2.1103 (C:1.1230, R:0.0099) Ratio=3.42x
Val:   Loss=2.1237 (C:1.1457, R:0.0098) Ratio=3.13x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.1237)
============================================================

üåç Updating global dataset at epoch 10
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.407 ¬± 0.650
    Neg distances: 1.611 ¬± 0.888
    Separation ratio: 3.96x
    Gap: -2.622
    ‚úÖ Excellent global separation!

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=2.0358 (C:1.0526, R:0.0098)
Batch  25/537: Loss=1.9832 (C:0.9997, R:0.0098)
Batch  50/537: Loss=1.9957 (C:1.0077, R:0.0099)
Batch  75/537: Loss=2.0241 (C:1.0359, R:0.0099)
Batch 100/537: Loss=2.0508 (C:1.0664, R:0.0098)
Batch 125/537: Loss=2.0066 (C:1.0186, R:0.0099)
Batch 150/537: Loss=2.0117 (C:1.0270, R:0.0098)
Batch 175/537: Loss=2.0339 (C:1.0479, R:0.0099)
Batch 200/537: Loss=2.0130 (C:1.0204, R:0.0099)
Batch 225/537: Loss=1.9836 (C:0.9990, R:0.0098)
Batch 250/537: Loss=2.0227 (C:1.0334, R:0.0099)
Batch 275/537: Loss=2.0032 (C:1.0182, R:0.0099)
Batch 300/537: Loss=2.0467 (C:1.0592, R:0.0099)
Batch 325/537: Loss=1.9938 (C:1.0088, R:0.0099)
Batch 350/537: Loss=2.0099 (C:1.0259, R:0.0098)
Batch 375/537: Loss=2.0059 (C:1.0249, R:0.0098)
Batch 400/537: Loss=2.0172 (C:1.0260, R:0.0099)
Batch 425/537: Loss=2.0348 (C:1.0497, R:0.0099)
Batch 450/537: Loss=2.0061 (C:1.0208, R:0.0099)
Batch 475/537: Loss=1.9870 (C:0.9984, R:0.0099)
Batch 500/537: Loss=2.0431 (C:1.0538, R:0.0099)
Batch 525/537: Loss=2.0096 (C:1.0227, R:0.0099)

============================================================
Epoch 10/200 completed in 33.2s
Train: Loss=2.0168 (C:1.0302, R:0.0099) Ratio=3.58x
Val:   Loss=2.0360 (C:1.0588, R:0.0098) Ratio=3.16x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.0360)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=1.9836 (C:0.9995, R:0.0098)
Batch  25/537: Loss=2.0531 (C:1.0618, R:0.0099)
Batch  50/537: Loss=2.0193 (C:1.0313, R:0.0099)
Batch  75/537: Loss=2.0474 (C:1.0631, R:0.0098)
Batch 100/537: Loss=1.9904 (C:1.0058, R:0.0098)
Batch 125/537: Loss=2.0068 (C:1.0206, R:0.0099)
Batch 150/537: Loss=2.0383 (C:1.0539, R:0.0098)
Batch 175/537: Loss=1.9680 (C:0.9831, R:0.0098)
Batch 200/537: Loss=1.9869 (C:0.9998, R:0.0099)
Batch 225/537: Loss=2.0166 (C:1.0344, R:0.0098)
Batch 250/537: Loss=2.0203 (C:1.0331, R:0.0099)
Batch 275/537: Loss=1.9952 (C:1.0115, R:0.0098)
Batch 300/537: Loss=2.0241 (C:1.0390, R:0.0099)
Batch 325/537: Loss=2.0105 (C:1.0241, R:0.0099)
Batch 350/537: Loss=2.0060 (C:1.0246, R:0.0098)
Batch 375/537: Loss=1.9732 (C:0.9893, R:0.0098)
Batch 400/537: Loss=2.0221 (C:1.0380, R:0.0098)
Batch 425/537: Loss=2.0395 (C:1.0541, R:0.0099)
Batch 450/537: Loss=2.0304 (C:1.0445, R:0.0099)
Batch 475/537: Loss=2.0101 (C:1.0278, R:0.0098)
Batch 500/537: Loss=2.0701 (C:1.0839, R:0.0099)
Batch 525/537: Loss=2.0239 (C:1.0463, R:0.0098)

============================================================
Epoch 11/200 completed in 24.4s
Train: Loss=2.0061 (C:1.0225, R:0.0098) Ratio=3.62x
Val:   Loss=2.0314 (C:1.0588, R:0.0097) Ratio=3.17x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.0314)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=1.9939 (C:1.0111, R:0.0098)
Batch  25/537: Loss=1.9936 (C:1.0115, R:0.0098)
Batch  50/537: Loss=2.0185 (C:1.0401, R:0.0098)
Batch  75/537: Loss=2.0481 (C:1.0697, R:0.0098)
Batch 100/537: Loss=1.9640 (C:0.9830, R:0.0098)
Batch 125/537: Loss=2.0031 (C:1.0219, R:0.0098)
Batch 150/537: Loss=1.9752 (C:0.9922, R:0.0098)
Batch 175/537: Loss=2.0171 (C:1.0331, R:0.0098)
Batch 200/537: Loss=2.0130 (C:1.0315, R:0.0098)
Batch 225/537: Loss=2.0121 (C:1.0342, R:0.0098)
Batch 250/537: Loss=1.9681 (C:0.9869, R:0.0098)
Batch 275/537: Loss=1.9940 (C:1.0205, R:0.0097)
Batch 300/537: Loss=1.9550 (C:0.9809, R:0.0097)
Batch 325/537: Loss=1.9832 (C:1.0035, R:0.0098)
Batch 350/537: Loss=2.0154 (C:1.0370, R:0.0098)
Batch 375/537: Loss=2.0488 (C:1.0723, R:0.0098)
Batch 400/537: Loss=2.0090 (C:1.0272, R:0.0098)
Batch 425/537: Loss=1.9944 (C:1.0172, R:0.0098)
Batch 450/537: Loss=2.0379 (C:1.0599, R:0.0098)
Batch 475/537: Loss=2.0071 (C:1.0285, R:0.0098)
Batch 500/537: Loss=1.9967 (C:1.0150, R:0.0098)
Batch 525/537: Loss=1.9919 (C:1.0147, R:0.0098)

============================================================
Epoch 12/200 completed in 24.2s
Train: Loss=1.9964 (C:1.0174, R:0.0098) Ratio=3.71x
Val:   Loss=2.0301 (C:1.0612, R:0.0097) Ratio=3.18x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.0301)
============================================================

üåç Updating global dataset at epoch 13
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.385 ¬± 0.628
    Neg distances: 1.617 ¬± 0.885
    Separation ratio: 4.20x
    Gap: -2.630
    ‚úÖ Excellent global separation!

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=1.9698 (C:0.9945, R:0.0098)
Batch  25/537: Loss=1.9727 (C:0.9916, R:0.0098)
Batch  50/537: Loss=1.9974 (C:1.0197, R:0.0098)
Batch  75/537: Loss=1.9865 (C:1.0126, R:0.0097)
Batch 100/537: Loss=1.9718 (C:0.9946, R:0.0098)
Batch 125/537: Loss=1.9784 (C:1.0030, R:0.0098)
Batch 150/537: Loss=1.9519 (C:0.9747, R:0.0098)
Batch 175/537: Loss=1.9876 (C:1.0113, R:0.0098)
Batch 200/537: Loss=1.9771 (C:1.0083, R:0.0097)
Batch 225/537: Loss=1.9987 (C:1.0209, R:0.0098)
Batch 250/537: Loss=1.9908 (C:1.0176, R:0.0097)
Batch 275/537: Loss=1.9795 (C:1.0032, R:0.0098)
Batch 300/537: Loss=1.9509 (C:0.9741, R:0.0098)
Batch 325/537: Loss=1.9924 (C:1.0152, R:0.0098)
Batch 350/537: Loss=1.9283 (C:0.9550, R:0.0097)
Batch 375/537: Loss=1.9794 (C:1.0089, R:0.0097)
Batch 400/537: Loss=1.9871 (C:1.0129, R:0.0097)
Batch 425/537: Loss=1.9920 (C:1.0209, R:0.0097)
Batch 450/537: Loss=1.9554 (C:0.9881, R:0.0097)
Batch 475/537: Loss=1.9937 (C:1.0252, R:0.0097)
Batch 500/537: Loss=1.9635 (C:0.9912, R:0.0097)
Batch 525/537: Loss=1.9383 (C:0.9672, R:0.0097)

============================================================
Epoch 13/200 completed in 31.9s
Train: Loss=1.9704 (C:0.9968, R:0.0097) Ratio=3.79x
Val:   Loss=2.0043 (C:1.0415, R:0.0096) Ratio=3.15x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.0043)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=1.9735 (C:1.0007, R:0.0097)
Batch  25/537: Loss=1.9584 (C:0.9890, R:0.0097)
Batch  50/537: Loss=1.9596 (C:0.9860, R:0.0097)
Batch  75/537: Loss=1.9550 (C:0.9830, R:0.0097)
Batch 100/537: Loss=1.9683 (C:0.9930, R:0.0098)
Batch 125/537: Loss=1.9362 (C:0.9658, R:0.0097)
Batch 150/537: Loss=1.9563 (C:0.9854, R:0.0097)
Batch 175/537: Loss=1.9964 (C:1.0315, R:0.0096)
Batch 200/537: Loss=2.0218 (C:1.0520, R:0.0097)
Batch 225/537: Loss=1.9455 (C:0.9745, R:0.0097)
Batch 250/537: Loss=1.9465 (C:0.9758, R:0.0097)
Batch 275/537: Loss=1.9545 (C:0.9833, R:0.0097)
Batch 300/537: Loss=1.9811 (C:1.0086, R:0.0097)
Batch 325/537: Loss=1.9842 (C:1.0113, R:0.0097)
Batch 350/537: Loss=1.9556 (C:0.9883, R:0.0097)
Batch 375/537: Loss=1.9831 (C:1.0138, R:0.0097)
Batch 400/537: Loss=1.9726 (C:1.0035, R:0.0097)
Batch 425/537: Loss=1.9583 (C:0.9914, R:0.0097)
Batch 450/537: Loss=1.9679 (C:0.9998, R:0.0097)
Batch 475/537: Loss=1.9407 (C:0.9762, R:0.0096)
Batch 500/537: Loss=1.9935 (C:1.0214, R:0.0097)
Batch 525/537: Loss=1.9885 (C:1.0221, R:0.0097)

============================================================
Epoch 14/200 completed in 24.2s
Train: Loss=1.9624 (C:0.9926, R:0.0097) Ratio=3.84x
Val:   Loss=2.0025 (C:1.0426, R:0.0096) Ratio=3.16x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 2.0025)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=1.9876 (C:1.0197, R:0.0097)
Batch  25/537: Loss=1.9807 (C:1.0104, R:0.0097)
Batch  50/537: Loss=1.9438 (C:0.9760, R:0.0097)
Batch  75/537: Loss=1.9618 (C:0.9954, R:0.0097)
Batch 100/537: Loss=1.9265 (C:0.9580, R:0.0097)
Batch 125/537: Loss=1.9745 (C:1.0000, R:0.0097)
Batch 150/537: Loss=1.9733 (C:1.0075, R:0.0097)
Batch 175/537: Loss=1.9371 (C:0.9677, R:0.0097)
Batch 200/537: Loss=1.9277 (C:0.9639, R:0.0096)
Batch 225/537: Loss=1.9253 (C:0.9580, R:0.0097)
Batch 250/537: Loss=1.9722 (C:1.0024, R:0.0097)
Batch 275/537: Loss=1.9876 (C:1.0214, R:0.0097)
Batch 300/537: Loss=1.9265 (C:0.9585, R:0.0097)
Batch 325/537: Loss=1.9411 (C:0.9750, R:0.0097)
Batch 350/537: Loss=1.9170 (C:0.9554, R:0.0096)
Batch 375/537: Loss=1.9565 (C:0.9895, R:0.0097)
Batch 400/537: Loss=1.9887 (C:1.0212, R:0.0097)
Batch 425/537: Loss=1.9352 (C:0.9693, R:0.0097)
Batch 450/537: Loss=1.9476 (C:0.9820, R:0.0097)
Batch 475/537: Loss=1.9718 (C:1.0003, R:0.0097)
Batch 500/537: Loss=1.9590 (C:0.9950, R:0.0096)
Batch 525/537: Loss=1.9641 (C:0.9991, R:0.0097)

============================================================
Epoch 15/200 completed in 24.2s
Train: Loss=1.9554 (C:0.9883, R:0.0097) Ratio=3.95x
Val:   Loss=1.9931 (C:1.0355, R:0.0096) Ratio=3.13x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9931)
============================================================

üåç Updating global dataset at epoch 16
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.369 ¬± 0.617
    Neg distances: 1.642 ¬± 0.896
    Separation ratio: 4.46x
    Gap: -2.666
    ‚úÖ Excellent global separation!

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=1.9230 (C:0.9587, R:0.0096)
Batch  25/537: Loss=1.9106 (C:0.9424, R:0.0097)
Batch  50/537: Loss=1.9561 (C:0.9890, R:0.0097)
Batch  75/537: Loss=1.9232 (C:0.9568, R:0.0097)
Batch 100/537: Loss=1.9391 (C:0.9763, R:0.0096)
Batch 125/537: Loss=1.9746 (C:1.0141, R:0.0096)
Batch 150/537: Loss=1.9722 (C:1.0098, R:0.0096)
Batch 175/537: Loss=1.9223 (C:0.9627, R:0.0096)
Batch 200/537: Loss=1.9251 (C:0.9609, R:0.0096)
Batch 225/537: Loss=1.9446 (C:0.9774, R:0.0097)
Batch 250/537: Loss=1.9185 (C:0.9534, R:0.0097)
Batch 275/537: Loss=1.9127 (C:0.9523, R:0.0096)
Batch 300/537: Loss=1.9419 (C:0.9794, R:0.0096)
Batch 325/537: Loss=1.8557 (C:0.8929, R:0.0096)
Batch 350/537: Loss=1.9404 (C:0.9746, R:0.0097)
Batch 375/537: Loss=1.9260 (C:0.9565, R:0.0097)
Batch 400/537: Loss=1.8935 (C:0.9299, R:0.0096)
Batch 425/537: Loss=1.9170 (C:0.9572, R:0.0096)
Batch 450/537: Loss=1.9500 (C:0.9887, R:0.0096)
Batch 475/537: Loss=1.9413 (C:0.9813, R:0.0096)
Batch 500/537: Loss=1.9000 (C:0.9380, R:0.0096)
Batch 525/537: Loss=1.9437 (C:0.9785, R:0.0097)

============================================================
Epoch 16/200 completed in 32.1s
Train: Loss=1.9296 (C:0.9655, R:0.0096) Ratio=4.00x
Val:   Loss=1.9709 (C:1.0162, R:0.0095) Ratio=3.14x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9709)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=1.9528 (C:0.9914, R:0.0096)
Batch  25/537: Loss=1.9225 (C:0.9585, R:0.0096)
Batch  50/537: Loss=1.8924 (C:0.9331, R:0.0096)
Batch  75/537: Loss=1.9328 (C:0.9643, R:0.0097)
Batch 100/537: Loss=1.9135 (C:0.9521, R:0.0096)
Batch 125/537: Loss=1.9187 (C:0.9598, R:0.0096)
Batch 150/537: Loss=1.9155 (C:0.9535, R:0.0096)
Batch 175/537: Loss=1.9091 (C:0.9504, R:0.0096)
Batch 200/537: Loss=1.9362 (C:0.9737, R:0.0096)
Batch 225/537: Loss=1.9178 (C:0.9592, R:0.0096)
Batch 250/537: Loss=1.8898 (C:0.9314, R:0.0096)
Batch 275/537: Loss=1.9001 (C:0.9380, R:0.0096)
Batch 300/537: Loss=1.8785 (C:0.9109, R:0.0097)
Batch 325/537: Loss=1.9335 (C:0.9715, R:0.0096)
Batch 350/537: Loss=1.9452 (C:0.9832, R:0.0096)
Batch 375/537: Loss=1.8811 (C:0.9204, R:0.0096)
Batch 400/537: Loss=1.9510 (C:0.9845, R:0.0097)
Batch 425/537: Loss=1.9137 (C:0.9526, R:0.0096)
Batch 450/537: Loss=1.9037 (C:0.9408, R:0.0096)
Batch 475/537: Loss=1.9747 (C:1.0143, R:0.0096)
Batch 500/537: Loss=1.9420 (C:0.9786, R:0.0096)
Batch 525/537: Loss=1.9481 (C:0.9844, R:0.0096)

============================================================
Epoch 17/200 completed in 24.1s
Train: Loss=1.9251 (C:0.9628, R:0.0096) Ratio=4.03x
Val:   Loss=1.9711 (C:1.0188, R:0.0095) Ratio=3.18x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=1.9297 (C:0.9738, R:0.0096)
Batch  25/537: Loss=1.8948 (C:0.9349, R:0.0096)
Batch  50/537: Loss=1.9246 (C:0.9641, R:0.0096)
Batch  75/537: Loss=1.8960 (C:0.9352, R:0.0096)
Batch 100/537: Loss=1.8706 (C:0.9088, R:0.0096)
Batch 125/537: Loss=1.9422 (C:0.9812, R:0.0096)
Batch 150/537: Loss=1.9461 (C:0.9819, R:0.0096)
Batch 175/537: Loss=1.9244 (C:0.9621, R:0.0096)
Batch 200/537: Loss=1.9622 (C:1.0007, R:0.0096)
Batch 225/537: Loss=1.9192 (C:0.9597, R:0.0096)
Batch 250/537: Loss=1.9188 (C:0.9627, R:0.0096)
Batch 275/537: Loss=1.9453 (C:0.9863, R:0.0096)
Batch 300/537: Loss=1.9132 (C:0.9565, R:0.0096)
Batch 325/537: Loss=1.9318 (C:0.9707, R:0.0096)
Batch 350/537: Loss=1.9205 (C:0.9623, R:0.0096)
Batch 375/537: Loss=1.9080 (C:0.9506, R:0.0096)
Batch 400/537: Loss=1.9265 (C:0.9697, R:0.0096)
Batch 425/537: Loss=1.9567 (C:0.9971, R:0.0096)
Batch 450/537: Loss=1.9488 (C:0.9896, R:0.0096)
Batch 475/537: Loss=1.9823 (C:1.0191, R:0.0096)
Batch 500/537: Loss=1.9290 (C:0.9665, R:0.0096)
Batch 525/537: Loss=1.9266 (C:0.9680, R:0.0096)

============================================================
Epoch 18/200 completed in 24.3s
Train: Loss=1.9205 (C:0.9601, R:0.0096) Ratio=3.96x
Val:   Loss=1.9718 (C:1.0220, R:0.0095) Ratio=3.13x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 19
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.366 ¬± 0.628
    Neg distances: 1.662 ¬± 0.920
    Separation ratio: 4.54x
    Gap: -2.676
    ‚úÖ Excellent global separation!

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=1.9167 (C:0.9597, R:0.0096)
Batch  25/537: Loss=1.9073 (C:0.9456, R:0.0096)
Batch  50/537: Loss=1.9195 (C:0.9578, R:0.0096)
Batch  75/537: Loss=1.9300 (C:0.9716, R:0.0096)
Batch 100/537: Loss=1.9527 (C:0.9914, R:0.0096)
Batch 125/537: Loss=1.9084 (C:0.9471, R:0.0096)
Batch 150/537: Loss=1.9373 (C:0.9769, R:0.0096)
Batch 175/537: Loss=1.8892 (C:0.9309, R:0.0096)
Batch 200/537: Loss=1.8845 (C:0.9238, R:0.0096)
Batch 225/537: Loss=1.9444 (C:0.9914, R:0.0095)
Batch 250/537: Loss=1.9216 (C:0.9624, R:0.0096)
Batch 275/537: Loss=1.9644 (C:1.0113, R:0.0095)
Batch 300/537: Loss=1.9632 (C:1.0061, R:0.0096)
Batch 325/537: Loss=1.8984 (C:0.9424, R:0.0096)
Batch 350/537: Loss=1.9295 (C:0.9743, R:0.0096)
Batch 375/537: Loss=1.8830 (C:0.9214, R:0.0096)
Batch 400/537: Loss=1.9507 (C:0.9969, R:0.0095)
Batch 425/537: Loss=1.9168 (C:0.9539, R:0.0096)
Batch 450/537: Loss=1.9078 (C:0.9505, R:0.0096)
Batch 475/537: Loss=1.9238 (C:0.9617, R:0.0096)
Batch 500/537: Loss=1.9205 (C:0.9579, R:0.0096)
Batch 525/537: Loss=1.9304 (C:0.9751, R:0.0096)

============================================================
Epoch 19/200 completed in 32.1s
Train: Loss=1.9093 (C:0.9520, R:0.0096) Ratio=4.01x
Val:   Loss=1.9490 (C:1.0009, R:0.0095) Ratio=3.19x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9490)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=1.9195 (C:0.9614, R:0.0096)
Batch  25/537: Loss=1.8867 (C:0.9314, R:0.0096)
Batch  50/537: Loss=1.9082 (C:0.9496, R:0.0096)
Batch  75/537: Loss=1.8963 (C:0.9358, R:0.0096)
Batch 100/537: Loss=1.9023 (C:0.9481, R:0.0095)
Batch 125/537: Loss=1.8954 (C:0.9384, R:0.0096)
Batch 150/537: Loss=1.9408 (C:0.9839, R:0.0096)
Batch 175/537: Loss=1.9162 (C:0.9621, R:0.0095)
Batch 200/537: Loss=1.9109 (C:0.9596, R:0.0095)
Batch 225/537: Loss=1.8842 (C:0.9277, R:0.0096)
Batch 250/537: Loss=1.9621 (C:1.0057, R:0.0096)
Batch 275/537: Loss=1.9267 (C:0.9670, R:0.0096)
Batch 300/537: Loss=1.9319 (C:0.9806, R:0.0095)
Batch 325/537: Loss=1.9035 (C:0.9455, R:0.0096)
Batch 350/537: Loss=1.9617 (C:1.0056, R:0.0096)
Batch 375/537: Loss=1.9338 (C:0.9771, R:0.0096)
Batch 400/537: Loss=1.8810 (C:0.9246, R:0.0096)
Batch 425/537: Loss=1.8994 (C:0.9409, R:0.0096)
Batch 450/537: Loss=1.8888 (C:0.9337, R:0.0096)
Batch 475/537: Loss=1.8986 (C:0.9446, R:0.0095)
Batch 500/537: Loss=1.9095 (C:0.9525, R:0.0096)
Batch 525/537: Loss=1.8838 (C:0.9287, R:0.0096)

============================================================
Epoch 20/200 completed in 24.3s
Train: Loss=1.9051 (C:0.9485, R:0.0096) Ratio=4.14x
Val:   Loss=1.9535 (C:1.0063, R:0.0095) Ratio=3.22x
Reconstruction weight: 100.000
No improvement for 1 epochs
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=1.8910 (C:0.9395, R:0.0095)
Batch  25/537: Loss=1.9422 (C:0.9771, R:0.0097)
Batch  50/537: Loss=1.9053 (C:0.9471, R:0.0096)
Batch  75/537: Loss=1.9252 (C:0.9671, R:0.0096)
Batch 100/537: Loss=1.9016 (C:0.9492, R:0.0095)
Batch 125/537: Loss=1.8897 (C:0.9372, R:0.0095)
Batch 150/537: Loss=1.9224 (C:0.9681, R:0.0095)
Batch 175/537: Loss=1.9366 (C:0.9776, R:0.0096)
Batch 200/537: Loss=1.9225 (C:0.9615, R:0.0096)
Batch 225/537: Loss=1.8631 (C:0.9086, R:0.0095)
Batch 250/537: Loss=1.9063 (C:0.9489, R:0.0096)
Batch 275/537: Loss=1.8825 (C:0.9275, R:0.0095)
Batch 300/537: Loss=1.9199 (C:0.9692, R:0.0095)
Batch 325/537: Loss=1.9146 (C:0.9602, R:0.0095)
Batch 350/537: Loss=1.9140 (C:0.9496, R:0.0096)
Batch 375/537: Loss=1.8589 (C:0.9051, R:0.0095)
Batch 400/537: Loss=1.9546 (C:0.9982, R:0.0096)
Batch 425/537: Loss=1.9140 (C:0.9597, R:0.0095)
Batch 450/537: Loss=1.9076 (C:0.9552, R:0.0095)
Batch 475/537: Loss=1.9179 (C:0.9623, R:0.0096)
Batch 500/537: Loss=1.9384 (C:0.9784, R:0.0096)
Batch 525/537: Loss=1.8810 (C:0.9234, R:0.0096)

============================================================
Epoch 21/200 completed in 24.3s
Train: Loss=1.9015 (C:0.9460, R:0.0096) Ratio=4.15x
Val:   Loss=1.9624 (C:1.0154, R:0.0095) Ratio=3.19x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 22
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.326 ¬± 0.569
    Neg distances: 1.678 ¬± 0.928
    Separation ratio: 5.14x
    Gap: -2.717
    ‚úÖ Excellent global separation!

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=1.8387 (C:0.8859, R:0.0095)
Batch  25/537: Loss=1.8737 (C:0.9196, R:0.0095)
Batch  50/537: Loss=1.8787 (C:0.9260, R:0.0095)
Batch  75/537: Loss=1.8485 (C:0.8928, R:0.0096)
Batch 100/537: Loss=1.8270 (C:0.8685, R:0.0096)
Batch 125/537: Loss=1.8578 (C:0.9047, R:0.0095)
Batch 150/537: Loss=1.8707 (C:0.9183, R:0.0095)
Batch 175/537: Loss=1.8435 (C:0.8888, R:0.0095)
Batch 200/537: Loss=1.8736 (C:0.9173, R:0.0096)
Batch 225/537: Loss=1.8870 (C:0.9278, R:0.0096)
Batch 250/537: Loss=1.8767 (C:0.9263, R:0.0095)
Batch 275/537: Loss=1.8774 (C:0.9273, R:0.0095)
Batch 300/537: Loss=1.8213 (C:0.8652, R:0.0096)
Batch 325/537: Loss=1.8558 (C:0.9002, R:0.0096)
Batch 350/537: Loss=1.8563 (C:0.9031, R:0.0095)
Batch 375/537: Loss=1.8472 (C:0.8891, R:0.0096)
Batch 400/537: Loss=1.8491 (C:0.8950, R:0.0095)
Batch 425/537: Loss=1.8726 (C:0.9173, R:0.0096)
Batch 450/537: Loss=1.8638 (C:0.9089, R:0.0095)
Batch 475/537: Loss=1.8471 (C:0.8887, R:0.0096)
Batch 500/537: Loss=1.8443 (C:0.8963, R:0.0095)
Batch 525/537: Loss=1.8515 (C:0.8949, R:0.0096)

============================================================
Epoch 22/200 completed in 31.9s
Train: Loss=1.8703 (C:0.9158, R:0.0095) Ratio=4.37x
Val:   Loss=1.9288 (C:0.9838, R:0.0095) Ratio=3.20x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9288)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=1.8630 (C:0.9111, R:0.0095)
Batch  25/537: Loss=1.8783 (C:0.9252, R:0.0095)
Batch  50/537: Loss=1.8301 (C:0.8772, R:0.0095)
Batch  75/537: Loss=1.8697 (C:0.9141, R:0.0096)
Batch 100/537: Loss=1.8644 (C:0.9106, R:0.0095)
Batch 125/537: Loss=1.8777 (C:0.9238, R:0.0095)
Batch 150/537: Loss=1.9041 (C:0.9479, R:0.0096)
Batch 175/537: Loss=1.8580 (C:0.8985, R:0.0096)
Batch 200/537: Loss=1.8670 (C:0.9121, R:0.0095)
Batch 225/537: Loss=1.8818 (C:0.9246, R:0.0096)
Batch 250/537: Loss=1.8555 (C:0.9016, R:0.0095)
Batch 275/537: Loss=1.8864 (C:0.9331, R:0.0095)
Batch 300/537: Loss=1.8542 (C:0.9007, R:0.0095)
Batch 325/537: Loss=1.8419 (C:0.8870, R:0.0095)
Batch 350/537: Loss=1.8964 (C:0.9423, R:0.0095)
Batch 375/537: Loss=1.8644 (C:0.9096, R:0.0095)
Batch 400/537: Loss=1.8471 (C:0.8956, R:0.0095)
Batch 425/537: Loss=1.8733 (C:0.9209, R:0.0095)
Batch 450/537: Loss=1.8816 (C:0.9275, R:0.0095)
Batch 475/537: Loss=1.8647 (C:0.9136, R:0.0095)
Batch 500/537: Loss=1.8661 (C:0.9121, R:0.0095)
Batch 525/537: Loss=1.8621 (C:0.9061, R:0.0096)

============================================================
Epoch 23/200 completed in 24.1s
Train: Loss=1.8638 (C:0.9098, R:0.0095) Ratio=4.39x
Val:   Loss=1.9363 (C:0.9908, R:0.0095) Ratio=3.15x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=1.8800 (C:0.9264, R:0.0095)
Batch  25/537: Loss=1.8535 (C:0.9028, R:0.0095)
Batch  50/537: Loss=1.8681 (C:0.9153, R:0.0095)
Batch  75/537: Loss=1.8058 (C:0.8529, R:0.0095)
Batch 100/537: Loss=1.9025 (C:0.9467, R:0.0096)
Batch 125/537: Loss=1.8553 (C:0.8972, R:0.0096)
Batch 150/537: Loss=1.8634 (C:0.9098, R:0.0095)
Batch 175/537: Loss=1.8724 (C:0.9201, R:0.0095)
Batch 200/537: Loss=1.8638 (C:0.9085, R:0.0096)
Batch 225/537: Loss=1.8265 (C:0.8760, R:0.0095)
Batch 250/537: Loss=1.8528 (C:0.8993, R:0.0095)
Batch 275/537: Loss=1.8665 (C:0.9133, R:0.0095)
Batch 300/537: Loss=1.8303 (C:0.8762, R:0.0095)
Batch 325/537: Loss=1.8703 (C:0.9159, R:0.0095)
Batch 350/537: Loss=1.9144 (C:0.9538, R:0.0096)
Batch 375/537: Loss=1.8592 (C:0.9076, R:0.0095)
Batch 400/537: Loss=1.8410 (C:0.8906, R:0.0095)
Batch 425/537: Loss=1.8784 (C:0.9256, R:0.0095)
Batch 450/537: Loss=1.8566 (C:0.9000, R:0.0096)
Batch 475/537: Loss=1.8053 (C:0.8536, R:0.0095)
Batch 500/537: Loss=1.8930 (C:0.9382, R:0.0095)
Batch 525/537: Loss=1.8735 (C:0.9149, R:0.0096)

============================================================
Epoch 24/200 completed in 24.5s
Train: Loss=1.8629 (C:0.9094, R:0.0095) Ratio=4.41x
Val:   Loss=1.9362 (C:0.9920, R:0.0094) Ratio=3.24x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 25
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.324 ¬± 0.588
    Neg distances: 1.703 ¬± 0.935
    Separation ratio: 5.26x
    Gap: -2.714
    ‚úÖ Excellent global separation!

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=1.8471 (C:0.8917, R:0.0096)
Batch  25/537: Loss=1.8180 (C:0.8655, R:0.0095)
Batch  50/537: Loss=1.8330 (C:0.8733, R:0.0096)
Batch  75/537: Loss=1.8466 (C:0.8971, R:0.0095)
Batch 100/537: Loss=1.8225 (C:0.8677, R:0.0095)
Batch 125/537: Loss=1.8158 (C:0.8622, R:0.0095)
Batch 150/537: Loss=1.8632 (C:0.9096, R:0.0095)
Batch 175/537: Loss=1.8890 (C:0.9377, R:0.0095)
Batch 200/537: Loss=1.8669 (C:0.9155, R:0.0095)
Batch 225/537: Loss=1.8473 (C:0.8994, R:0.0095)
Batch 250/537: Loss=1.8677 (C:0.9149, R:0.0095)
Batch 275/537: Loss=1.8537 (C:0.8997, R:0.0095)
Batch 300/537: Loss=1.8424 (C:0.8947, R:0.0095)
Batch 325/537: Loss=1.8952 (C:0.9383, R:0.0096)
Batch 350/537: Loss=1.8385 (C:0.8927, R:0.0095)
Batch 375/537: Loss=1.8858 (C:0.9352, R:0.0095)
Batch 400/537: Loss=1.8642 (C:0.9061, R:0.0096)
Batch 425/537: Loss=1.8759 (C:0.9199, R:0.0096)
Batch 450/537: Loss=1.8633 (C:0.9085, R:0.0095)
Batch 475/537: Loss=1.8897 (C:0.9372, R:0.0095)
Batch 500/537: Loss=1.8411 (C:0.8905, R:0.0095)
Batch 525/537: Loss=1.8758 (C:0.9225, R:0.0095)

============================================================
Epoch 25/200 completed in 33.5s
Train: Loss=1.8512 (C:0.8983, R:0.0095) Ratio=4.39x
Val:   Loss=1.9140 (C:0.9698, R:0.0094) Ratio=3.23x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.9140)
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=1.8585 (C:0.9049, R:0.0095)
Batch  25/537: Loss=1.8482 (C:0.8899, R:0.0096)
Batch  50/537: Loss=1.8296 (C:0.8765, R:0.0095)
Batch  75/537: Loss=1.8242 (C:0.8739, R:0.0095)
Batch 100/537: Loss=1.8141 (C:0.8616, R:0.0095)
Batch 125/537: Loss=1.8340 (C:0.8864, R:0.0095)
Batch 150/537: Loss=1.8401 (C:0.8871, R:0.0095)
Batch 175/537: Loss=1.8313 (C:0.8781, R:0.0095)
Batch 200/537: Loss=1.8482 (C:0.8984, R:0.0095)
Batch 225/537: Loss=1.8316 (C:0.8851, R:0.0095)
Batch 250/537: Loss=1.8329 (C:0.8814, R:0.0095)
Batch 275/537: Loss=1.8610 (C:0.9054, R:0.0096)
Batch 300/537: Loss=1.8679 (C:0.9112, R:0.0096)
Batch 325/537: Loss=1.8360 (C:0.8809, R:0.0096)
Batch 350/537: Loss=1.8160 (C:0.8643, R:0.0095)
Batch 375/537: Loss=1.8398 (C:0.8911, R:0.0095)
Batch 400/537: Loss=1.8736 (C:0.9170, R:0.0096)
Batch 425/537: Loss=1.8043 (C:0.8558, R:0.0095)
Batch 450/537: Loss=1.8327 (C:0.8805, R:0.0095)
Batch 475/537: Loss=1.8614 (C:0.9090, R:0.0095)
Batch 500/537: Loss=1.8653 (C:0.9131, R:0.0095)
Batch 525/537: Loss=1.8639 (C:0.9108, R:0.0095)

============================================================
Epoch 26/200 completed in 24.6s
Train: Loss=1.8492 (C:0.8964, R:0.0095) Ratio=4.53x
Val:   Loss=1.9170 (C:0.9734, R:0.0094) Ratio=3.20x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=1.8104 (C:0.8577, R:0.0095)
Batch  25/537: Loss=1.8347 (C:0.8839, R:0.0095)
Batch  50/537: Loss=1.8428 (C:0.8900, R:0.0095)
Batch  75/537: Loss=1.8140 (C:0.8655, R:0.0095)
Batch 100/537: Loss=1.8327 (C:0.8824, R:0.0095)
Batch 125/537: Loss=1.8630 (C:0.9117, R:0.0095)
Batch 150/537: Loss=1.8600 (C:0.9107, R:0.0095)
Batch 175/537: Loss=1.8409 (C:0.8862, R:0.0095)
Batch 200/537: Loss=1.8298 (C:0.8788, R:0.0095)
Batch 225/537: Loss=1.8710 (C:0.9168, R:0.0095)
Batch 250/537: Loss=1.8005 (C:0.8496, R:0.0095)
Batch 275/537: Loss=1.8162 (C:0.8632, R:0.0095)
Batch 300/537: Loss=1.8137 (C:0.8641, R:0.0095)
Batch 325/537: Loss=1.8407 (C:0.8913, R:0.0095)
Batch 350/537: Loss=1.8933 (C:0.9392, R:0.0095)
Batch 375/537: Loss=1.8338 (C:0.8811, R:0.0095)
Batch 400/537: Loss=1.8444 (C:0.8973, R:0.0095)
Batch 425/537: Loss=1.8480 (C:0.8976, R:0.0095)
Batch 450/537: Loss=1.8465 (C:0.8985, R:0.0095)
Batch 475/537: Loss=1.8192 (C:0.8683, R:0.0095)
Batch 500/537: Loss=1.7943 (C:0.8418, R:0.0095)
Batch 525/537: Loss=1.8930 (C:0.9368, R:0.0096)

============================================================
Epoch 27/200 completed in 24.4s
Train: Loss=1.8455 (C:0.8934, R:0.0095) Ratio=4.59x
Val:   Loss=1.9164 (C:0.9731, R:0.0094) Ratio=3.29x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 28
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.312 ¬± 0.584
    Neg distances: 1.736 ¬± 0.934
    Separation ratio: 5.57x
    Gap: -2.744
    ‚úÖ Excellent global separation!

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=1.7660 (C:0.8197, R:0.0095)
Batch  25/537: Loss=1.8275 (C:0.8771, R:0.0095)
Batch  50/537: Loss=1.8063 (C:0.8517, R:0.0095)
Batch  75/537: Loss=1.7974 (C:0.8519, R:0.0095)
Batch 100/537: Loss=1.8164 (C:0.8674, R:0.0095)
Batch 125/537: Loss=1.8143 (C:0.8642, R:0.0095)
Batch 150/537: Loss=1.7922 (C:0.8389, R:0.0095)
Batch 175/537: Loss=1.8361 (C:0.8807, R:0.0096)
Batch 200/537: Loss=1.8528 (C:0.8984, R:0.0095)
Batch 225/537: Loss=1.8003 (C:0.8480, R:0.0095)
Batch 250/537: Loss=1.8266 (C:0.8752, R:0.0095)
Batch 275/537: Loss=1.8310 (C:0.8786, R:0.0095)
Batch 300/537: Loss=1.8065 (C:0.8560, R:0.0095)
Batch 325/537: Loss=1.8291 (C:0.8776, R:0.0095)
Batch 350/537: Loss=1.8282 (C:0.8738, R:0.0095)
Batch 375/537: Loss=1.8076 (C:0.8538, R:0.0095)
Batch 400/537: Loss=1.8461 (C:0.8926, R:0.0095)
Batch 425/537: Loss=1.7937 (C:0.8442, R:0.0095)
Batch 450/537: Loss=1.8429 (C:0.8966, R:0.0095)
Batch 475/537: Loss=1.8114 (C:0.8530, R:0.0096)
Batch 500/537: Loss=1.8258 (C:0.8761, R:0.0095)
Batch 525/537: Loss=1.8245 (C:0.8782, R:0.0095)

============================================================
Epoch 28/200 completed in 33.5s
Train: Loss=1.8218 (C:0.8699, R:0.0095) Ratio=4.63x
Val:   Loss=1.8858 (C:0.9423, R:0.0094) Ratio=3.21x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8858)
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=1.7975 (C:0.8466, R:0.0095)
Batch  25/537: Loss=1.7522 (C:0.8044, R:0.0095)
Batch  50/537: Loss=1.8011 (C:0.8487, R:0.0095)
Batch  75/537: Loss=1.7939 (C:0.8466, R:0.0095)
Batch 100/537: Loss=1.8077 (C:0.8551, R:0.0095)
Batch 125/537: Loss=1.8750 (C:0.9259, R:0.0095)
Batch 150/537: Loss=1.8037 (C:0.8500, R:0.0095)
Batch 175/537: Loss=1.8357 (C:0.8838, R:0.0095)
Batch 200/537: Loss=1.8333 (C:0.8825, R:0.0095)
Batch 225/537: Loss=1.8055 (C:0.8561, R:0.0095)
Batch 250/537: Loss=1.7870 (C:0.8348, R:0.0095)
Batch 275/537: Loss=1.8147 (C:0.8618, R:0.0095)
Batch 300/537: Loss=1.8294 (C:0.8818, R:0.0095)
Batch 325/537: Loss=1.8315 (C:0.8805, R:0.0095)
Batch 350/537: Loss=1.8540 (C:0.9003, R:0.0095)
Batch 375/537: Loss=1.8289 (C:0.8789, R:0.0095)
Batch 400/537: Loss=1.7947 (C:0.8481, R:0.0095)
Batch 425/537: Loss=1.8182 (C:0.8633, R:0.0095)
Batch 450/537: Loss=1.8534 (C:0.9024, R:0.0095)
Batch 475/537: Loss=1.8405 (C:0.8875, R:0.0095)
Batch 500/537: Loss=1.8200 (C:0.8697, R:0.0095)
Batch 525/537: Loss=1.8130 (C:0.8575, R:0.0096)

============================================================
Epoch 29/200 completed in 24.4s
Train: Loss=1.8195 (C:0.8681, R:0.0095) Ratio=4.63x
Val:   Loss=1.8953 (C:0.9525, R:0.0094) Ratio=3.22x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=1.8207 (C:0.8688, R:0.0095)
Batch  25/537: Loss=1.8313 (C:0.8763, R:0.0096)
Batch  50/537: Loss=1.8418 (C:0.8877, R:0.0095)
Batch  75/537: Loss=1.8108 (C:0.8623, R:0.0095)
Batch 100/537: Loss=1.8112 (C:0.8653, R:0.0095)
Batch 125/537: Loss=1.8434 (C:0.8874, R:0.0096)
Batch 150/537: Loss=1.8428 (C:0.8909, R:0.0095)
Batch 175/537: Loss=1.8302 (C:0.8767, R:0.0095)
Batch 200/537: Loss=1.7854 (C:0.8358, R:0.0095)
Batch 225/537: Loss=1.8152 (C:0.8631, R:0.0095)
Batch 250/537: Loss=1.8405 (C:0.8875, R:0.0095)
Batch 275/537: Loss=1.8313 (C:0.8793, R:0.0095)
Batch 300/537: Loss=1.8135 (C:0.8617, R:0.0095)
Batch 325/537: Loss=1.8255 (C:0.8724, R:0.0095)
Batch 350/537: Loss=1.8466 (C:0.8923, R:0.0095)
Batch 375/537: Loss=1.8088 (C:0.8586, R:0.0095)
Batch 400/537: Loss=1.8078 (C:0.8531, R:0.0095)
Batch 425/537: Loss=1.8000 (C:0.8493, R:0.0095)
Batch 450/537: Loss=1.8390 (C:0.8874, R:0.0095)
Batch 475/537: Loss=1.8053 (C:0.8565, R:0.0095)
Batch 500/537: Loss=1.8470 (C:0.8969, R:0.0095)
Batch 525/537: Loss=1.8074 (C:0.8499, R:0.0096)

============================================================
Epoch 30/200 completed in 24.9s
Train: Loss=1.8172 (C:0.8660, R:0.0095) Ratio=4.68x
Val:   Loss=1.8880 (C:0.9454, R:0.0094) Ratio=3.33x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 31
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.311 ¬± 0.597
    Neg distances: 1.732 ¬± 0.943
    Separation ratio: 5.56x
    Gap: -2.756
    ‚úÖ Excellent global separation!

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=1.8241 (C:0.8733, R:0.0095)
Batch  25/537: Loss=1.7940 (C:0.8459, R:0.0095)
Batch  50/537: Loss=1.8437 (C:0.8983, R:0.0095)
Batch  75/537: Loss=1.7720 (C:0.8243, R:0.0095)
Batch 100/537: Loss=1.8038 (C:0.8533, R:0.0095)
Batch 125/537: Loss=1.7920 (C:0.8394, R:0.0095)
Batch 150/537: Loss=1.8102 (C:0.8578, R:0.0095)
Batch 175/537: Loss=1.8300 (C:0.8797, R:0.0095)
Batch 200/537: Loss=1.8502 (C:0.8955, R:0.0095)
Batch 225/537: Loss=1.7904 (C:0.8445, R:0.0095)
Batch 250/537: Loss=1.7891 (C:0.8393, R:0.0095)
Batch 275/537: Loss=1.8562 (C:0.9035, R:0.0095)
Batch 300/537: Loss=1.7906 (C:0.8412, R:0.0095)
Batch 325/537: Loss=1.8061 (C:0.8582, R:0.0095)
Batch 350/537: Loss=1.8523 (C:0.8974, R:0.0095)
Batch 375/537: Loss=1.8316 (C:0.8829, R:0.0095)
Batch 400/537: Loss=1.8126 (C:0.8683, R:0.0094)
Batch 425/537: Loss=1.8312 (C:0.8896, R:0.0094)
Batch 450/537: Loss=1.8546 (C:0.9014, R:0.0095)
Batch 475/537: Loss=1.7869 (C:0.8397, R:0.0095)
Batch 500/537: Loss=1.7875 (C:0.8376, R:0.0095)
Batch 525/537: Loss=1.8295 (C:0.8796, R:0.0095)

============================================================
Epoch 31/200 completed in 33.8s
Train: Loss=1.8204 (C:0.8693, R:0.0095) Ratio=4.71x
Val:   Loss=1.8891 (C:0.9468, R:0.0094) Ratio=3.28x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 32 Training
----------------------------------------
Batch   0/537: Loss=1.8129 (C:0.8589, R:0.0095)
Batch  25/537: Loss=1.7988 (C:0.8484, R:0.0095)
Batch  50/537: Loss=1.8198 (C:0.8674, R:0.0095)
Batch  75/537: Loss=1.7878 (C:0.8396, R:0.0095)
Batch 100/537: Loss=1.8370 (C:0.8827, R:0.0095)
Batch 125/537: Loss=1.7762 (C:0.8249, R:0.0095)
Batch 150/537: Loss=1.8014 (C:0.8507, R:0.0095)
Batch 175/537: Loss=1.7937 (C:0.8491, R:0.0094)
Batch 200/537: Loss=1.8199 (C:0.8702, R:0.0095)
Batch 225/537: Loss=1.7958 (C:0.8434, R:0.0095)
Batch 250/537: Loss=1.7902 (C:0.8363, R:0.0095)
Batch 275/537: Loss=1.8595 (C:0.9108, R:0.0095)
Batch 300/537: Loss=1.8117 (C:0.8605, R:0.0095)
Batch 325/537: Loss=1.8178 (C:0.8667, R:0.0095)
Batch 350/537: Loss=1.8109 (C:0.8581, R:0.0095)
Batch 375/537: Loss=1.8080 (C:0.8613, R:0.0095)
Batch 400/537: Loss=1.8106 (C:0.8597, R:0.0095)
Batch 425/537: Loss=1.7898 (C:0.8437, R:0.0095)
Batch 450/537: Loss=1.8564 (C:0.9041, R:0.0095)
Batch 475/537: Loss=1.8145 (C:0.8652, R:0.0095)
Batch 500/537: Loss=1.8163 (C:0.8658, R:0.0095)
Batch 525/537: Loss=1.8255 (C:0.8728, R:0.0095)

============================================================
Epoch 32/200 completed in 24.4s
Train: Loss=1.8176 (C:0.8670, R:0.0095) Ratio=4.89x
Val:   Loss=1.8970 (C:0.9546, R:0.0094) Ratio=3.27x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 33 Training
----------------------------------------
Batch   0/537: Loss=1.8058 (C:0.8567, R:0.0095)
Batch  25/537: Loss=1.7725 (C:0.8236, R:0.0095)
Batch  50/537: Loss=1.8387 (C:0.8882, R:0.0095)
Batch  75/537: Loss=1.8513 (C:0.8990, R:0.0095)
Batch 100/537: Loss=1.8290 (C:0.8824, R:0.0095)
Batch 125/537: Loss=1.7772 (C:0.8252, R:0.0095)
Batch 150/537: Loss=1.8074 (C:0.8594, R:0.0095)
Batch 175/537: Loss=1.8397 (C:0.8900, R:0.0095)
Batch 200/537: Loss=1.8252 (C:0.8787, R:0.0095)
Batch 225/537: Loss=1.8349 (C:0.8819, R:0.0095)
Batch 250/537: Loss=1.8330 (C:0.8840, R:0.0095)
Batch 275/537: Loss=1.8451 (C:0.8989, R:0.0095)
Batch 300/537: Loss=1.8135 (C:0.8651, R:0.0095)
Batch 325/537: Loss=1.8061 (C:0.8512, R:0.0095)
Batch 350/537: Loss=1.8314 (C:0.8812, R:0.0095)
Batch 375/537: Loss=1.7921 (C:0.8413, R:0.0095)
Batch 400/537: Loss=1.8174 (C:0.8665, R:0.0095)
Batch 425/537: Loss=1.8586 (C:0.9054, R:0.0095)
Batch 450/537: Loss=1.8182 (C:0.8698, R:0.0095)
Batch 475/537: Loss=1.8265 (C:0.8752, R:0.0095)
Batch 500/537: Loss=1.7984 (C:0.8513, R:0.0095)
Batch 525/537: Loss=1.8311 (C:0.8767, R:0.0095)

============================================================
Epoch 33/200 completed in 24.6s
Train: Loss=1.8163 (C:0.8658, R:0.0095) Ratio=4.89x
Val:   Loss=1.8806 (C:0.9388, R:0.0094) Ratio=3.35x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8806)
============================================================

üåç Updating global dataset at epoch 34
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.299 ¬± 0.570
    Neg distances: 1.725 ¬± 0.940
    Separation ratio: 5.76x
    Gap: -2.776
    ‚úÖ Excellent global separation!

Epoch 34 Training
----------------------------------------
Batch   0/537: Loss=1.8178 (C:0.8674, R:0.0095)
Batch  25/537: Loss=1.8008 (C:0.8472, R:0.0095)
Batch  50/537: Loss=1.7885 (C:0.8393, R:0.0095)
Batch  75/537: Loss=1.8572 (C:0.9066, R:0.0095)
Batch 100/537: Loss=1.7698 (C:0.8160, R:0.0095)
Batch 125/537: Loss=1.7983 (C:0.8481, R:0.0095)
Batch 150/537: Loss=1.8111 (C:0.8618, R:0.0095)
Batch 175/537: Loss=1.8292 (C:0.8798, R:0.0095)
Batch 200/537: Loss=1.8554 (C:0.9029, R:0.0095)
Batch 225/537: Loss=1.8283 (C:0.8760, R:0.0095)
Batch 250/537: Loss=1.8128 (C:0.8616, R:0.0095)
Batch 275/537: Loss=1.8274 (C:0.8779, R:0.0095)
Batch 300/537: Loss=1.7938 (C:0.8482, R:0.0095)
Batch 325/537: Loss=1.8180 (C:0.8657, R:0.0095)
Batch 350/537: Loss=1.7803 (C:0.8322, R:0.0095)
Batch 375/537: Loss=1.7781 (C:0.8218, R:0.0096)
Batch 400/537: Loss=1.8250 (C:0.8759, R:0.0095)
Batch 425/537: Loss=1.8351 (C:0.8876, R:0.0095)
Batch 450/537: Loss=1.8066 (C:0.8587, R:0.0095)
Batch 475/537: Loss=1.8254 (C:0.8763, R:0.0095)
Batch 500/537: Loss=1.7987 (C:0.8525, R:0.0095)
Batch 525/537: Loss=1.8175 (C:0.8716, R:0.0095)

============================================================
Epoch 34/200 completed in 32.0s
Train: Loss=1.8108 (C:0.8605, R:0.0095) Ratio=4.74x
Val:   Loss=1.8946 (C:0.9533, R:0.0094) Ratio=3.23x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 35 Training
----------------------------------------
Batch   0/537: Loss=1.8090 (C:0.8574, R:0.0095)
Batch  25/537: Loss=1.8184 (C:0.8656, R:0.0095)
Batch  50/537: Loss=1.8168 (C:0.8650, R:0.0095)
Batch  75/537: Loss=1.7980 (C:0.8489, R:0.0095)
Batch 100/537: Loss=1.8114 (C:0.8618, R:0.0095)
Batch 125/537: Loss=1.8082 (C:0.8583, R:0.0095)
Batch 150/537: Loss=1.7919 (C:0.8458, R:0.0095)
Batch 175/537: Loss=1.8196 (C:0.8706, R:0.0095)
Batch 200/537: Loss=1.8092 (C:0.8615, R:0.0095)
Batch 225/537: Loss=1.8052 (C:0.8522, R:0.0095)
Batch 250/537: Loss=1.7883 (C:0.8400, R:0.0095)
Batch 275/537: Loss=1.7612 (C:0.8091, R:0.0095)
Batch 300/537: Loss=1.7828 (C:0.8343, R:0.0095)
Batch 325/537: Loss=1.8162 (C:0.8712, R:0.0094)
Batch 350/537: Loss=1.8094 (C:0.8588, R:0.0095)
Batch 375/537: Loss=1.8481 (C:0.9020, R:0.0095)
Batch 400/537: Loss=1.8157 (C:0.8634, R:0.0095)
Batch 425/537: Loss=1.8304 (C:0.8834, R:0.0095)
Batch 450/537: Loss=1.7876 (C:0.8443, R:0.0094)
Batch 475/537: Loss=1.8031 (C:0.8526, R:0.0095)
Batch 500/537: Loss=1.8324 (C:0.8850, R:0.0095)
Batch 525/537: Loss=1.8246 (C:0.8743, R:0.0095)

============================================================
Epoch 35/200 completed in 24.4s
Train: Loss=1.8065 (C:0.8567, R:0.0095) Ratio=4.83x
Val:   Loss=1.8922 (C:0.9507, R:0.0094) Ratio=3.29x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 36 Training
----------------------------------------
Batch   0/537: Loss=1.7875 (C:0.8394, R:0.0095)
Batch  25/537: Loss=1.8119 (C:0.8601, R:0.0095)
Batch  50/537: Loss=1.7852 (C:0.8337, R:0.0095)
Batch  75/537: Loss=1.7909 (C:0.8428, R:0.0095)
Batch 100/537: Loss=1.8042 (C:0.8516, R:0.0095)
Batch 125/537: Loss=1.8103 (C:0.8567, R:0.0095)
Batch 150/537: Loss=1.7833 (C:0.8358, R:0.0095)
Batch 175/537: Loss=1.8131 (C:0.8630, R:0.0095)
Batch 200/537: Loss=1.7690 (C:0.8174, R:0.0095)
Batch 225/537: Loss=1.7933 (C:0.8406, R:0.0095)
Batch 250/537: Loss=1.8468 (C:0.8963, R:0.0095)
Batch 275/537: Loss=1.8091 (C:0.8639, R:0.0095)
Batch 300/537: Loss=1.7782 (C:0.8289, R:0.0095)
Batch 325/537: Loss=1.7881 (C:0.8376, R:0.0095)
Batch 350/537: Loss=1.8096 (C:0.8552, R:0.0095)
Batch 375/537: Loss=1.8338 (C:0.8835, R:0.0095)
Batch 400/537: Loss=1.8138 (C:0.8638, R:0.0095)
Batch 425/537: Loss=1.8217 (C:0.8770, R:0.0094)
Batch 450/537: Loss=1.8088 (C:0.8607, R:0.0095)
Batch 475/537: Loss=1.7941 (C:0.8415, R:0.0095)
Batch 500/537: Loss=1.7784 (C:0.8266, R:0.0095)
Batch 525/537: Loss=1.7816 (C:0.8335, R:0.0095)

============================================================
Epoch 36/200 completed in 24.5s
Train: Loss=1.8039 (C:0.8544, R:0.0095) Ratio=4.92x
Val:   Loss=1.8881 (C:0.9468, R:0.0094) Ratio=3.28x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

üåç Updating global dataset at epoch 37
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.287 ¬± 0.572
    Neg distances: 1.759 ¬± 0.943
    Separation ratio: 6.12x
    Gap: -2.796
    ‚úÖ Excellent global separation!

Epoch 37 Training
----------------------------------------
Batch   0/537: Loss=1.7554 (C:0.8134, R:0.0094)
Batch  25/537: Loss=1.7876 (C:0.8416, R:0.0095)
Batch  50/537: Loss=1.7967 (C:0.8438, R:0.0095)
Batch  75/537: Loss=1.7936 (C:0.8439, R:0.0095)
Batch 100/537: Loss=1.7840 (C:0.8351, R:0.0095)
Batch 125/537: Loss=1.8035 (C:0.8614, R:0.0094)
Batch 150/537: Loss=1.8012 (C:0.8516, R:0.0095)
Batch 175/537: Loss=1.7758 (C:0.8253, R:0.0095)
Batch 200/537: Loss=1.7409 (C:0.7958, R:0.0095)
Batch 225/537: Loss=1.8081 (C:0.8628, R:0.0095)
Batch 250/537: Loss=1.7539 (C:0.8003, R:0.0095)
Batch 275/537: Loss=1.8050 (C:0.8548, R:0.0095)
Batch 300/537: Loss=1.7687 (C:0.8247, R:0.0094)
Batch 325/537: Loss=1.7947 (C:0.8472, R:0.0095)
Batch 350/537: Loss=1.7895 (C:0.8388, R:0.0095)
Batch 375/537: Loss=1.7804 (C:0.8320, R:0.0095)
Batch 400/537: Loss=1.7688 (C:0.8200, R:0.0095)
Batch 425/537: Loss=1.7808 (C:0.8305, R:0.0095)
Batch 450/537: Loss=1.7533 (C:0.8031, R:0.0095)
Batch 475/537: Loss=1.7571 (C:0.8059, R:0.0095)
Batch 500/537: Loss=1.8263 (C:0.8714, R:0.0095)
Batch 525/537: Loss=1.7872 (C:0.8366, R:0.0095)

============================================================
Epoch 37/200 completed in 32.6s
Train: Loss=1.7843 (C:0.8349, R:0.0095) Ratio=4.97x
Val:   Loss=1.8669 (C:0.9254, R:0.0094) Ratio=3.28x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8669)
============================================================

Epoch 38 Training
----------------------------------------
Batch   0/537: Loss=1.7598 (C:0.8100, R:0.0095)
Batch  25/537: Loss=1.7958 (C:0.8466, R:0.0095)
Batch  50/537: Loss=1.7874 (C:0.8399, R:0.0095)
Batch  75/537: Loss=1.7982 (C:0.8486, R:0.0095)
Batch 100/537: Loss=1.8044 (C:0.8536, R:0.0095)
Batch 125/537: Loss=1.7900 (C:0.8416, R:0.0095)
Batch 150/537: Loss=1.7810 (C:0.8328, R:0.0095)
Batch 175/537: Loss=1.8005 (C:0.8494, R:0.0095)
Batch 200/537: Loss=1.8140 (C:0.8634, R:0.0095)
Batch 225/537: Loss=1.7729 (C:0.8262, R:0.0095)
Batch 250/537: Loss=1.7489 (C:0.8018, R:0.0095)
Batch 275/537: Loss=1.8306 (C:0.8821, R:0.0095)
Batch 300/537: Loss=1.7941 (C:0.8446, R:0.0095)
Batch 325/537: Loss=1.7534 (C:0.8031, R:0.0095)
Batch 350/537: Loss=1.7972 (C:0.8451, R:0.0095)
Batch 375/537: Loss=1.7927 (C:0.8440, R:0.0095)
Batch 400/537: Loss=1.7643 (C:0.8137, R:0.0095)
Batch 425/537: Loss=1.7517 (C:0.7994, R:0.0095)
Batch 450/537: Loss=1.7946 (C:0.8440, R:0.0095)
Batch 475/537: Loss=1.8102 (C:0.8586, R:0.0095)
Batch 500/537: Loss=1.7925 (C:0.8488, R:0.0094)
Batch 525/537: Loss=1.7821 (C:0.8310, R:0.0095)

============================================================
Epoch 38/200 completed in 24.6s
Train: Loss=1.7819 (C:0.8327, R:0.0095) Ratio=4.99x
Val:   Loss=1.8681 (C:0.9266, R:0.0094) Ratio=3.33x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 39 Training
----------------------------------------
Batch   0/537: Loss=1.8035 (C:0.8483, R:0.0096)
Batch  25/537: Loss=1.7613 (C:0.8117, R:0.0095)
Batch  50/537: Loss=1.7924 (C:0.8405, R:0.0095)
Batch  75/537: Loss=1.8170 (C:0.8699, R:0.0095)
Batch 100/537: Loss=1.8255 (C:0.8718, R:0.0095)
Batch 125/537: Loss=1.7872 (C:0.8399, R:0.0095)
Batch 150/537: Loss=1.8070 (C:0.8543, R:0.0095)
Batch 175/537: Loss=1.7806 (C:0.8323, R:0.0095)
Batch 200/537: Loss=1.8159 (C:0.8645, R:0.0095)
Batch 225/537: Loss=1.7570 (C:0.8136, R:0.0094)
Batch 250/537: Loss=1.7832 (C:0.8351, R:0.0095)
Batch 275/537: Loss=1.7405 (C:0.7933, R:0.0095)
Batch 300/537: Loss=1.7330 (C:0.7922, R:0.0094)
Batch 325/537: Loss=1.7995 (C:0.8450, R:0.0095)
Batch 350/537: Loss=1.7721 (C:0.8200, R:0.0095)
Batch 375/537: Loss=1.7861 (C:0.8403, R:0.0095)
Batch 400/537: Loss=1.7698 (C:0.8222, R:0.0095)
Batch 425/537: Loss=1.7629 (C:0.8167, R:0.0095)
Batch 450/537: Loss=1.7669 (C:0.8170, R:0.0095)
Batch 475/537: Loss=1.7545 (C:0.8075, R:0.0095)
Batch 500/537: Loss=1.8205 (C:0.8723, R:0.0095)
Batch 525/537: Loss=1.7973 (C:0.8490, R:0.0095)

============================================================
Epoch 39/200 completed in 24.7s
Train: Loss=1.7802 (C:0.8310, R:0.0095) Ratio=5.14x
Val:   Loss=1.8733 (C:0.9315, R:0.0094) Ratio=3.28x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 40
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.288 ¬± 0.566
    Neg distances: 1.790 ¬± 0.944
    Separation ratio: 6.21x
    Gap: -2.812
    ‚úÖ Excellent global separation!

Epoch 40 Training
----------------------------------------
Batch   0/537: Loss=1.7352 (C:0.7839, R:0.0095)
Batch  25/537: Loss=1.7510 (C:0.7936, R:0.0096)
Batch  50/537: Loss=1.7654 (C:0.8145, R:0.0095)
Batch  75/537: Loss=1.7651 (C:0.8119, R:0.0095)
Batch 100/537: Loss=1.7690 (C:0.8222, R:0.0095)
Batch 125/537: Loss=1.7884 (C:0.8380, R:0.0095)
Batch 150/537: Loss=1.7619 (C:0.8100, R:0.0095)
Batch 175/537: Loss=1.7591 (C:0.8088, R:0.0095)
Batch 200/537: Loss=1.7886 (C:0.8365, R:0.0095)
Batch 225/537: Loss=1.7782 (C:0.8291, R:0.0095)
Batch 250/537: Loss=1.7396 (C:0.7867, R:0.0095)
Batch 275/537: Loss=1.7470 (C:0.8012, R:0.0095)
Batch 300/537: Loss=1.7563 (C:0.8064, R:0.0095)
Batch 325/537: Loss=1.7866 (C:0.8311, R:0.0096)
Batch 350/537: Loss=1.8102 (C:0.8584, R:0.0095)
Batch 375/537: Loss=1.7756 (C:0.8231, R:0.0095)
Batch 400/537: Loss=1.7634 (C:0.8171, R:0.0095)
Batch 425/537: Loss=1.7752 (C:0.8248, R:0.0095)
Batch 450/537: Loss=1.7333 (C:0.7905, R:0.0094)
Batch 475/537: Loss=1.7828 (C:0.8342, R:0.0095)
Batch 500/537: Loss=1.7686 (C:0.8230, R:0.0095)
Batch 525/537: Loss=1.7557 (C:0.8062, R:0.0095)

============================================================
Epoch 40/200 completed in 33.1s
Train: Loss=1.7667 (C:0.8174, R:0.0095) Ratio=5.12x
Val:   Loss=1.8533 (C:0.9127, R:0.0094) Ratio=3.32x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8533)
Checkpoint saved at epoch 40
============================================================

Epoch 41 Training
----------------------------------------
Batch   0/537: Loss=1.7498 (C:0.7960, R:0.0095)
Batch  25/537: Loss=1.7375 (C:0.7887, R:0.0095)
Batch  50/537: Loss=1.7273 (C:0.7776, R:0.0095)
Batch  75/537: Loss=1.7726 (C:0.8260, R:0.0095)
Batch 100/537: Loss=1.7385 (C:0.7895, R:0.0095)
Batch 125/537: Loss=1.7547 (C:0.8091, R:0.0095)
Batch 150/537: Loss=1.7540 (C:0.8061, R:0.0095)
Batch 175/537: Loss=1.7509 (C:0.8002, R:0.0095)
Batch 200/537: Loss=1.7269 (C:0.7700, R:0.0096)
Batch 225/537: Loss=1.7720 (C:0.8192, R:0.0095)
Batch 250/537: Loss=1.7500 (C:0.8016, R:0.0095)
Batch 275/537: Loss=1.7702 (C:0.8248, R:0.0095)
Batch 300/537: Loss=1.7564 (C:0.8108, R:0.0095)
Batch 325/537: Loss=1.7430 (C:0.7938, R:0.0095)
Batch 350/537: Loss=1.7856 (C:0.8370, R:0.0095)
Batch 375/537: Loss=1.7312 (C:0.7884, R:0.0094)
Batch 400/537: Loss=1.7558 (C:0.8089, R:0.0095)
Batch 425/537: Loss=1.7667 (C:0.8123, R:0.0095)
Batch 450/537: Loss=1.7665 (C:0.8185, R:0.0095)
Batch 475/537: Loss=1.7633 (C:0.8106, R:0.0095)
Batch 500/537: Loss=1.7579 (C:0.8052, R:0.0095)
Batch 525/537: Loss=1.7780 (C:0.8298, R:0.0095)

============================================================
Epoch 41/200 completed in 24.8s
Train: Loss=1.7650 (C:0.8147, R:0.0095) Ratio=5.40x
Val:   Loss=1.8579 (C:0.9151, R:0.0094) Ratio=3.26x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 42 Training
----------------------------------------
Batch   0/537: Loss=1.7562 (C:0.8055, R:0.0095)
Batch  25/537: Loss=1.7639 (C:0.8133, R:0.0095)
Batch  50/537: Loss=1.7431 (C:0.7928, R:0.0095)
Batch  75/537: Loss=1.7662 (C:0.8141, R:0.0095)
Batch 100/537: Loss=1.7617 (C:0.8163, R:0.0095)
Batch 125/537: Loss=1.7386 (C:0.7869, R:0.0095)
Batch 150/537: Loss=1.7385 (C:0.7906, R:0.0095)
Batch 175/537: Loss=1.7693 (C:0.8189, R:0.0095)
Batch 200/537: Loss=1.7621 (C:0.8173, R:0.0094)
Batch 225/537: Loss=1.7466 (C:0.7972, R:0.0095)
Batch 250/537: Loss=1.7486 (C:0.8034, R:0.0095)
Batch 275/537: Loss=1.7622 (C:0.8130, R:0.0095)
Batch 300/537: Loss=1.7442 (C:0.7907, R:0.0095)
Batch 325/537: Loss=1.7527 (C:0.8022, R:0.0095)
Batch 350/537: Loss=1.7289 (C:0.7797, R:0.0095)
Batch 375/537: Loss=1.7741 (C:0.8305, R:0.0094)
Batch 400/537: Loss=1.7814 (C:0.8299, R:0.0095)
Batch 425/537: Loss=1.7212 (C:0.7741, R:0.0095)
Batch 450/537: Loss=1.7548 (C:0.8111, R:0.0094)
Batch 475/537: Loss=1.7275 (C:0.7788, R:0.0095)
Batch 500/537: Loss=1.7868 (C:0.8343, R:0.0095)
Batch 525/537: Loss=1.7649 (C:0.8172, R:0.0095)

============================================================
Epoch 42/200 completed in 24.4s
Train: Loss=1.7621 (C:0.8125, R:0.0095) Ratio=5.33x
Val:   Loss=1.8650 (C:0.9234, R:0.0094) Ratio=3.26x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 43
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.283 ¬± 0.562
    Neg distances: 1.783 ¬± 0.947
    Separation ratio: 6.31x
    Gap: -2.830
    ‚úÖ Excellent global separation!

Epoch 43 Training
----------------------------------------
Batch   0/537: Loss=1.7215 (C:0.7703, R:0.0095)
Batch  25/537: Loss=1.7449 (C:0.7946, R:0.0095)
Batch  50/537: Loss=1.7504 (C:0.8019, R:0.0095)
Batch  75/537: Loss=1.8008 (C:0.8481, R:0.0095)
Batch 100/537: Loss=1.7667 (C:0.8171, R:0.0095)
Batch 125/537: Loss=1.7539 (C:0.8077, R:0.0095)
Batch 150/537: Loss=1.7896 (C:0.8395, R:0.0095)
Batch 175/537: Loss=1.7360 (C:0.7834, R:0.0095)
Batch 200/537: Loss=1.7516 (C:0.8055, R:0.0095)
Batch 225/537: Loss=1.7733 (C:0.8240, R:0.0095)
Batch 250/537: Loss=1.7695 (C:0.8178, R:0.0095)
Batch 275/537: Loss=1.7594 (C:0.8103, R:0.0095)
Batch 300/537: Loss=1.7965 (C:0.8510, R:0.0095)
Batch 325/537: Loss=1.7456 (C:0.7935, R:0.0095)
Batch 350/537: Loss=1.7552 (C:0.8008, R:0.0095)
Batch 375/537: Loss=1.7529 (C:0.8020, R:0.0095)
Batch 400/537: Loss=1.7603 (C:0.8079, R:0.0095)
Batch 425/537: Loss=1.7192 (C:0.7709, R:0.0095)
Batch 450/537: Loss=1.7769 (C:0.8278, R:0.0095)
Batch 475/537: Loss=1.7681 (C:0.8242, R:0.0094)
Batch 500/537: Loss=1.7166 (C:0.7716, R:0.0095)
Batch 525/537: Loss=1.7818 (C:0.8357, R:0.0095)

============================================================
Epoch 43/200 completed in 32.8s
Train: Loss=1.7602 (C:0.8108, R:0.0095) Ratio=5.27x
Val:   Loss=1.8571 (C:0.9157, R:0.0094) Ratio=3.35x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 44 Training
----------------------------------------
Batch   0/537: Loss=1.7482 (C:0.8019, R:0.0095)
Batch  25/537: Loss=1.7534 (C:0.8016, R:0.0095)
Batch  50/537: Loss=1.7610 (C:0.8078, R:0.0095)
Batch  75/537: Loss=1.7242 (C:0.7753, R:0.0095)
Batch 100/537: Loss=1.7640 (C:0.8150, R:0.0095)
Batch 125/537: Loss=1.7632 (C:0.8134, R:0.0095)
Batch 150/537: Loss=1.7575 (C:0.8104, R:0.0095)
Batch 175/537: Loss=1.7809 (C:0.8322, R:0.0095)
Batch 200/537: Loss=1.7532 (C:0.8027, R:0.0095)
Batch 225/537: Loss=1.7243 (C:0.7770, R:0.0095)
Batch 250/537: Loss=1.7975 (C:0.8447, R:0.0095)
Batch 275/537: Loss=1.7709 (C:0.8215, R:0.0095)
Batch 300/537: Loss=1.7424 (C:0.7982, R:0.0094)
Batch 325/537: Loss=1.7576 (C:0.8052, R:0.0095)
Batch 350/537: Loss=1.7525 (C:0.8064, R:0.0095)
Batch 375/537: Loss=1.7543 (C:0.8060, R:0.0095)
Batch 400/537: Loss=1.7338 (C:0.7867, R:0.0095)
Batch 425/537: Loss=1.7528 (C:0.8044, R:0.0095)
Batch 450/537: Loss=1.7655 (C:0.8162, R:0.0095)
Batch 475/537: Loss=1.7482 (C:0.8032, R:0.0094)
Batch 500/537: Loss=1.7539 (C:0.8046, R:0.0095)
Batch 525/537: Loss=1.7620 (C:0.8124, R:0.0095)

============================================================
Epoch 44/200 completed in 24.6s
Train: Loss=1.7562 (C:0.8068, R:0.0095) Ratio=5.32x
Val:   Loss=1.8526 (C:0.9116, R:0.0094) Ratio=3.35x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8526)
============================================================

Epoch 45 Training
----------------------------------------
Batch   0/537: Loss=1.7722 (C:0.8235, R:0.0095)
Batch  25/537: Loss=1.7369 (C:0.7875, R:0.0095)
Batch  50/537: Loss=1.7808 (C:0.8313, R:0.0095)
Batch  75/537: Loss=1.7141 (C:0.7638, R:0.0095)
Batch 100/537: Loss=1.8218 (C:0.8761, R:0.0095)
Batch 125/537: Loss=1.7666 (C:0.8158, R:0.0095)
Batch 150/537: Loss=1.7324 (C:0.7844, R:0.0095)
Batch 175/537: Loss=1.7538 (C:0.8065, R:0.0095)
Batch 200/537: Loss=1.7356 (C:0.7836, R:0.0095)
Batch 225/537: Loss=1.7425 (C:0.7896, R:0.0095)
Batch 250/537: Loss=1.7502 (C:0.8001, R:0.0095)
Batch 275/537: Loss=1.7480 (C:0.7980, R:0.0095)
Batch 300/537: Loss=1.7187 (C:0.7667, R:0.0095)
Batch 325/537: Loss=1.7488 (C:0.7998, R:0.0095)
Batch 350/537: Loss=1.7328 (C:0.7871, R:0.0095)
Batch 375/537: Loss=1.7466 (C:0.7997, R:0.0095)
Batch 400/537: Loss=1.7610 (C:0.8078, R:0.0095)
Batch 425/537: Loss=1.7918 (C:0.8425, R:0.0095)
Batch 450/537: Loss=1.7683 (C:0.8245, R:0.0094)
Batch 475/537: Loss=1.7798 (C:0.8326, R:0.0095)
Batch 500/537: Loss=1.7364 (C:0.7936, R:0.0094)
Batch 525/537: Loss=1.7901 (C:0.8446, R:0.0095)

============================================================
Epoch 45/200 completed in 24.4s
Train: Loss=1.7555 (C:0.8062, R:0.0095) Ratio=5.42x
Val:   Loss=1.8526 (C:0.9112, R:0.0094) Ratio=3.26x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8526)
============================================================

üåç Updating global dataset at epoch 46
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.276 ¬± 0.549
    Neg distances: 1.803 ¬± 0.943
    Separation ratio: 6.53x
    Gap: -2.849
    ‚úÖ Excellent global separation!

Epoch 46 Training
----------------------------------------
Batch   0/537: Loss=1.7615 (C:0.8136, R:0.0095)
Batch  25/537: Loss=1.7469 (C:0.7956, R:0.0095)
Batch  50/537: Loss=1.7177 (C:0.7653, R:0.0095)
Batch  75/537: Loss=1.7607 (C:0.8120, R:0.0095)
Batch 100/537: Loss=1.7530 (C:0.8008, R:0.0095)
Batch 125/537: Loss=1.7552 (C:0.8013, R:0.0095)
Batch 150/537: Loss=1.7875 (C:0.8402, R:0.0095)
Batch 175/537: Loss=1.7239 (C:0.7802, R:0.0094)
Batch 200/537: Loss=1.7624 (C:0.8110, R:0.0095)
Batch 225/537: Loss=1.7386 (C:0.7887, R:0.0095)
Batch 250/537: Loss=1.7616 (C:0.8140, R:0.0095)
Batch 275/537: Loss=1.7517 (C:0.8005, R:0.0095)
Batch 300/537: Loss=1.6936 (C:0.7413, R:0.0095)
Batch 325/537: Loss=1.7347 (C:0.7859, R:0.0095)
Batch 350/537: Loss=1.7251 (C:0.7755, R:0.0095)
Batch 375/537: Loss=1.7356 (C:0.7820, R:0.0095)
Batch 400/537: Loss=1.7259 (C:0.7779, R:0.0095)
Batch 425/537: Loss=1.7419 (C:0.7898, R:0.0095)
Batch 450/537: Loss=1.7396 (C:0.7948, R:0.0094)
Batch 475/537: Loss=1.7446 (C:0.7953, R:0.0095)
Batch 500/537: Loss=1.7300 (C:0.7807, R:0.0095)
Batch 525/537: Loss=1.7428 (C:0.7924, R:0.0095)

============================================================
Epoch 46/200 completed in 32.7s
Train: Loss=1.7412 (C:0.7919, R:0.0095) Ratio=5.52x
Val:   Loss=1.8409 (C:0.9005, R:0.0094) Ratio=3.39x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8409)
============================================================

Epoch 47 Training
----------------------------------------
Batch   0/537: Loss=1.7255 (C:0.7732, R:0.0095)
Batch  25/537: Loss=1.7669 (C:0.8220, R:0.0094)
Batch  50/537: Loss=1.7203 (C:0.7693, R:0.0095)
Batch  75/537: Loss=1.7353 (C:0.7903, R:0.0094)
Batch 100/537: Loss=1.7178 (C:0.7669, R:0.0095)
Batch 125/537: Loss=1.7771 (C:0.8250, R:0.0095)
Batch 150/537: Loss=1.7274 (C:0.7793, R:0.0095)
Batch 175/537: Loss=1.7541 (C:0.8038, R:0.0095)
Batch 200/537: Loss=1.7786 (C:0.8257, R:0.0095)
Batch 225/537: Loss=1.7875 (C:0.8392, R:0.0095)
Batch 250/537: Loss=1.7456 (C:0.7984, R:0.0095)
Batch 275/537: Loss=1.7536 (C:0.8019, R:0.0095)
Batch 300/537: Loss=1.7705 (C:0.8174, R:0.0095)
Batch 325/537: Loss=1.7295 (C:0.7809, R:0.0095)
Batch 350/537: Loss=1.7729 (C:0.8229, R:0.0095)
Batch 375/537: Loss=1.7147 (C:0.7703, R:0.0094)
Batch 400/537: Loss=1.7748 (C:0.8255, R:0.0095)
Batch 425/537: Loss=1.7439 (C:0.7997, R:0.0094)
Batch 450/537: Loss=1.7619 (C:0.8150, R:0.0095)
Batch 475/537: Loss=1.7468 (C:0.7998, R:0.0095)
Batch 500/537: Loss=1.7748 (C:0.8293, R:0.0095)
Batch 525/537: Loss=1.7422 (C:0.7920, R:0.0095)

============================================================
Epoch 47/200 completed in 24.0s
Train: Loss=1.7401 (C:0.7909, R:0.0095) Ratio=5.32x
Val:   Loss=1.8475 (C:0.9057, R:0.0094) Ratio=3.26x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 48 Training
----------------------------------------
Batch   0/537: Loss=1.7444 (C:0.8004, R:0.0094)
Batch  25/537: Loss=1.7493 (C:0.8041, R:0.0095)
Batch  50/537: Loss=1.7556 (C:0.8058, R:0.0095)
Batch  75/537: Loss=1.7324 (C:0.7876, R:0.0094)
Batch 100/537: Loss=1.7441 (C:0.7942, R:0.0095)
Batch 125/537: Loss=1.7421 (C:0.7896, R:0.0095)
Batch 150/537: Loss=1.7844 (C:0.8366, R:0.0095)
Batch 175/537: Loss=1.7671 (C:0.8214, R:0.0095)
Batch 200/537: Loss=1.6981 (C:0.7524, R:0.0095)
Batch 225/537: Loss=1.7333 (C:0.7843, R:0.0095)
Batch 250/537: Loss=1.7516 (C:0.8046, R:0.0095)
Batch 275/537: Loss=1.7413 (C:0.7948, R:0.0095)
Batch 300/537: Loss=1.7495 (C:0.8003, R:0.0095)
Batch 325/537: Loss=1.7568 (C:0.8082, R:0.0095)
Batch 350/537: Loss=1.7645 (C:0.8144, R:0.0095)
Batch 375/537: Loss=1.7191 (C:0.7692, R:0.0095)
Batch 400/537: Loss=1.7535 (C:0.8030, R:0.0095)
Batch 425/537: Loss=1.7392 (C:0.7928, R:0.0095)
Batch 450/537: Loss=1.7436 (C:0.7973, R:0.0095)
Batch 475/537: Loss=1.7211 (C:0.7693, R:0.0095)
Batch 500/537: Loss=1.7551 (C:0.8062, R:0.0095)
Batch 525/537: Loss=1.7471 (C:0.7999, R:0.0095)

============================================================
Epoch 48/200 completed in 24.4s
Train: Loss=1.7399 (C:0.7907, R:0.0095) Ratio=5.41x
Val:   Loss=1.8424 (C:0.9016, R:0.0094) Ratio=3.36x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 49
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.295 ¬± 0.604
    Neg distances: 1.798 ¬± 0.940
    Separation ratio: 6.10x
    Gap: -2.872
    ‚úÖ Excellent global separation!

Epoch 49 Training
----------------------------------------
Batch   0/537: Loss=1.7227 (C:0.7731, R:0.0095)
Batch  25/537: Loss=1.7267 (C:0.7781, R:0.0095)
Batch  50/537: Loss=1.7564 (C:0.8074, R:0.0095)
Batch  75/537: Loss=1.7412 (C:0.7919, R:0.0095)
Batch 100/537: Loss=1.7763 (C:0.8287, R:0.0095)
Batch 125/537: Loss=1.7557 (C:0.8092, R:0.0095)
Batch 150/537: Loss=1.7377 (C:0.7896, R:0.0095)
Batch 175/537: Loss=1.7474 (C:0.7980, R:0.0095)
Batch 200/537: Loss=1.7260 (C:0.7748, R:0.0095)
Batch 225/537: Loss=1.7394 (C:0.7897, R:0.0095)
Batch 250/537: Loss=1.7618 (C:0.8122, R:0.0095)
Batch 275/537: Loss=1.7735 (C:0.8236, R:0.0095)
Batch 300/537: Loss=1.7405 (C:0.7887, R:0.0095)
Batch 325/537: Loss=1.7591 (C:0.8077, R:0.0095)
Batch 350/537: Loss=1.7418 (C:0.7963, R:0.0095)
Batch 375/537: Loss=1.7649 (C:0.8163, R:0.0095)
Batch 400/537: Loss=1.7425 (C:0.7947, R:0.0095)
Batch 425/537: Loss=1.7694 (C:0.8153, R:0.0095)
Batch 450/537: Loss=1.7491 (C:0.7978, R:0.0095)
Batch 475/537: Loss=1.7253 (C:0.7809, R:0.0094)
Batch 500/537: Loss=1.7196 (C:0.7724, R:0.0095)
Batch 525/537: Loss=1.7849 (C:0.8364, R:0.0095)

============================================================
Epoch 49/200 completed in 33.0s
Train: Loss=1.7502 (C:0.8011, R:0.0095) Ratio=5.55x
Val:   Loss=1.8500 (C:0.9091, R:0.0094) Ratio=3.37x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 50 Training
----------------------------------------
Batch   0/537: Loss=1.7406 (C:0.7891, R:0.0095)
Batch  25/537: Loss=1.7611 (C:0.8134, R:0.0095)
Batch  50/537: Loss=1.7474 (C:0.7888, R:0.0096)
Batch  75/537: Loss=1.7264 (C:0.7776, R:0.0095)
Batch 100/537: Loss=1.7300 (C:0.7863, R:0.0094)
Batch 125/537: Loss=1.7392 (C:0.7872, R:0.0095)
Batch 150/537: Loss=1.7564 (C:0.8096, R:0.0095)
Batch 175/537: Loss=1.7480 (C:0.8019, R:0.0095)
Batch 200/537: Loss=1.7285 (C:0.7789, R:0.0095)
Batch 225/537: Loss=1.7731 (C:0.8252, R:0.0095)
Batch 250/537: Loss=1.7337 (C:0.7860, R:0.0095)
Batch 275/537: Loss=1.7822 (C:0.8297, R:0.0095)
Batch 300/537: Loss=1.7303 (C:0.7757, R:0.0095)
Batch 325/537: Loss=1.7345 (C:0.7875, R:0.0095)
Batch 350/537: Loss=1.7684 (C:0.8164, R:0.0095)
Batch 375/537: Loss=1.7600 (C:0.8090, R:0.0095)
Batch 400/537: Loss=1.7328 (C:0.7835, R:0.0095)
Batch 425/537: Loss=1.7801 (C:0.8299, R:0.0095)
Batch 450/537: Loss=1.7523 (C:0.8041, R:0.0095)
Batch 475/537: Loss=1.7283 (C:0.7829, R:0.0095)
Batch 500/537: Loss=1.7550 (C:0.8056, R:0.0095)
Batch 525/537: Loss=1.7288 (C:0.7828, R:0.0095)

============================================================
Epoch 50/200 completed in 24.6s
Train: Loss=1.7487 (C:0.7997, R:0.0095) Ratio=5.50x
Val:   Loss=1.8592 (C:0.9178, R:0.0094) Ratio=3.34x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 51 Training
----------------------------------------
Batch   0/537: Loss=1.7155 (C:0.7668, R:0.0095)
Batch  25/537: Loss=1.7539 (C:0.8044, R:0.0095)
Batch  50/537: Loss=1.6903 (C:0.7404, R:0.0095)
Batch  75/537: Loss=1.7511 (C:0.8017, R:0.0095)
Batch 100/537: Loss=1.7316 (C:0.7838, R:0.0095)
Batch 125/537: Loss=1.7452 (C:0.7939, R:0.0095)
Batch 150/537: Loss=1.7350 (C:0.7842, R:0.0095)
Batch 175/537: Loss=1.7209 (C:0.7722, R:0.0095)
Batch 200/537: Loss=1.7016 (C:0.7562, R:0.0095)
Batch 225/537: Loss=1.7700 (C:0.8218, R:0.0095)
Batch 250/537: Loss=1.7235 (C:0.7783, R:0.0095)
Batch 275/537: Loss=1.7407 (C:0.7888, R:0.0095)
Batch 300/537: Loss=1.7681 (C:0.8181, R:0.0095)
Batch 325/537: Loss=1.7798 (C:0.8266, R:0.0095)
Batch 350/537: Loss=1.7325 (C:0.7819, R:0.0095)
Batch 375/537: Loss=1.7650 (C:0.8143, R:0.0095)
Batch 400/537: Loss=1.7438 (C:0.8035, R:0.0094)
Batch 425/537: Loss=1.7441 (C:0.7915, R:0.0095)
Batch 450/537: Loss=1.7689 (C:0.8210, R:0.0095)
Batch 475/537: Loss=1.7323 (C:0.7836, R:0.0095)
Batch 500/537: Loss=1.7398 (C:0.7907, R:0.0095)
Batch 525/537: Loss=1.7340 (C:0.7841, R:0.0095)

============================================================
Epoch 51/200 completed in 24.3s
Train: Loss=1.7476 (C:0.7988, R:0.0095) Ratio=5.74x
Val:   Loss=1.8483 (C:0.9077, R:0.0094) Ratio=3.33x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 52
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.277 ¬± 0.568
    Neg distances: 1.839 ¬± 0.942
    Separation ratio: 6.64x
    Gap: -2.898
    ‚úÖ Excellent global separation!

Epoch 52 Training
----------------------------------------
Batch   0/537: Loss=1.7128 (C:0.7618, R:0.0095)
Batch  25/537: Loss=1.7258 (C:0.7739, R:0.0095)
Batch  50/537: Loss=1.7103 (C:0.7560, R:0.0095)
Batch  75/537: Loss=1.7080 (C:0.7572, R:0.0095)
Batch 100/537: Loss=1.6897 (C:0.7423, R:0.0095)
Batch 125/537: Loss=1.7450 (C:0.8001, R:0.0094)
Batch 150/537: Loss=1.7167 (C:0.7709, R:0.0095)
Batch 175/537: Loss=1.7289 (C:0.7801, R:0.0095)
Batch 200/537: Loss=1.7119 (C:0.7617, R:0.0095)
Batch 225/537: Loss=1.7225 (C:0.7737, R:0.0095)
Batch 250/537: Loss=1.7462 (C:0.7998, R:0.0095)
Batch 275/537: Loss=1.7063 (C:0.7543, R:0.0095)
Batch 300/537: Loss=1.7143 (C:0.7632, R:0.0095)
Batch 325/537: Loss=1.7364 (C:0.7892, R:0.0095)
Batch 350/537: Loss=1.7342 (C:0.7892, R:0.0094)
Batch 375/537: Loss=1.7400 (C:0.7902, R:0.0095)
Batch 400/537: Loss=1.7227 (C:0.7746, R:0.0095)
Batch 425/537: Loss=1.6702 (C:0.7219, R:0.0095)
Batch 450/537: Loss=1.6921 (C:0.7397, R:0.0095)
Batch 475/537: Loss=1.7580 (C:0.8094, R:0.0095)
Batch 500/537: Loss=1.7479 (C:0.7983, R:0.0095)
Batch 525/537: Loss=1.7071 (C:0.7552, R:0.0095)

============================================================
Epoch 52/200 completed in 31.6s
Train: Loss=1.7210 (C:0.7723, R:0.0095) Ratio=5.75x
Val:   Loss=1.8247 (C:0.8843, R:0.0094) Ratio=3.36x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8247)
============================================================

Epoch 53 Training
----------------------------------------
Batch   0/537: Loss=1.7079 (C:0.7568, R:0.0095)
Batch  25/537: Loss=1.7017 (C:0.7526, R:0.0095)
Batch  50/537: Loss=1.7533 (C:0.8046, R:0.0095)
Batch  75/537: Loss=1.7424 (C:0.7963, R:0.0095)
Batch 100/537: Loss=1.7165 (C:0.7640, R:0.0095)
Batch 125/537: Loss=1.6905 (C:0.7396, R:0.0095)
Batch 150/537: Loss=1.7264 (C:0.7795, R:0.0095)
Batch 175/537: Loss=1.7196 (C:0.7733, R:0.0095)
Batch 200/537: Loss=1.7376 (C:0.7881, R:0.0095)
Batch 225/537: Loss=1.6732 (C:0.7253, R:0.0095)
Batch 250/537: Loss=1.7158 (C:0.7661, R:0.0095)
Batch 275/537: Loss=1.7037 (C:0.7525, R:0.0095)
Batch 300/537: Loss=1.7209 (C:0.7728, R:0.0095)
Batch 325/537: Loss=1.7297 (C:0.7791, R:0.0095)
Batch 350/537: Loss=1.7185 (C:0.7730, R:0.0095)
Batch 375/537: Loss=1.7089 (C:0.7575, R:0.0095)
Batch 400/537: Loss=1.7270 (C:0.7769, R:0.0095)
Batch 425/537: Loss=1.7392 (C:0.7900, R:0.0095)
Batch 450/537: Loss=1.6969 (C:0.7520, R:0.0094)
Batch 475/537: Loss=1.7279 (C:0.7780, R:0.0095)
Batch 500/537: Loss=1.7515 (C:0.7986, R:0.0095)
Batch 525/537: Loss=1.6998 (C:0.7514, R:0.0095)

============================================================
Epoch 53/200 completed in 23.7s
Train: Loss=1.7205 (C:0.7720, R:0.0095) Ratio=5.62x
Val:   Loss=1.8322 (C:0.8914, R:0.0094) Ratio=3.33x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 54 Training
----------------------------------------
Batch   0/537: Loss=1.7515 (C:0.8009, R:0.0095)
Batch  25/537: Loss=1.7327 (C:0.7838, R:0.0095)
Batch  50/537: Loss=1.7158 (C:0.7690, R:0.0095)
Batch  75/537: Loss=1.7405 (C:0.7931, R:0.0095)
Batch 100/537: Loss=1.7093 (C:0.7604, R:0.0095)
Batch 125/537: Loss=1.7455 (C:0.7936, R:0.0095)
Batch 150/537: Loss=1.6968 (C:0.7470, R:0.0095)
Batch 175/537: Loss=1.7239 (C:0.7728, R:0.0095)
Batch 200/537: Loss=1.7549 (C:0.8044, R:0.0095)
Batch 225/537: Loss=1.7425 (C:0.7944, R:0.0095)
Batch 250/537: Loss=1.7089 (C:0.7586, R:0.0095)
Batch 275/537: Loss=1.7284 (C:0.7800, R:0.0095)
Batch 300/537: Loss=1.6895 (C:0.7432, R:0.0095)
Batch 325/537: Loss=1.7091 (C:0.7581, R:0.0095)
Batch 350/537: Loss=1.7132 (C:0.7674, R:0.0095)
Batch 375/537: Loss=1.7257 (C:0.7764, R:0.0095)
Batch 400/537: Loss=1.7030 (C:0.7580, R:0.0094)
Batch 425/537: Loss=1.7207 (C:0.7706, R:0.0095)
Batch 450/537: Loss=1.7177 (C:0.7740, R:0.0094)
Batch 475/537: Loss=1.6785 (C:0.7281, R:0.0095)
Batch 500/537: Loss=1.7448 (C:0.7963, R:0.0095)
Batch 525/537: Loss=1.7174 (C:0.7710, R:0.0095)

============================================================
Epoch 54/200 completed in 23.9s
Train: Loss=1.7193 (C:0.7710, R:0.0095) Ratio=5.53x
Val:   Loss=1.8387 (C:0.8980, R:0.0094) Ratio=3.34x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 55
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.280 ¬± 0.571
    Neg distances: 1.848 ¬± 0.956
    Separation ratio: 6.59x
    Gap: -2.931
    ‚úÖ Excellent global separation!

Epoch 55 Training
----------------------------------------
Batch   0/537: Loss=1.7394 (C:0.7896, R:0.0095)
Batch  25/537: Loss=1.7466 (C:0.8013, R:0.0095)
Batch  50/537: Loss=1.7592 (C:0.8082, R:0.0095)
Batch  75/537: Loss=1.7404 (C:0.7914, R:0.0095)
Batch 100/537: Loss=1.7128 (C:0.7636, R:0.0095)
Batch 125/537: Loss=1.6998 (C:0.7526, R:0.0095)
Batch 150/537: Loss=1.7112 (C:0.7632, R:0.0095)
Batch 175/537: Loss=1.7574 (C:0.8071, R:0.0095)
Batch 200/537: Loss=1.7088 (C:0.7648, R:0.0094)
Batch 225/537: Loss=1.7324 (C:0.7850, R:0.0095)
Batch 250/537: Loss=1.7076 (C:0.7604, R:0.0095)
Batch 275/537: Loss=1.7374 (C:0.7826, R:0.0095)
Batch 300/537: Loss=1.7469 (C:0.8012, R:0.0095)
Batch 325/537: Loss=1.7045 (C:0.7524, R:0.0095)
Batch 350/537: Loss=1.7527 (C:0.8084, R:0.0094)
Batch 375/537: Loss=1.7356 (C:0.7855, R:0.0095)
Batch 400/537: Loss=1.6870 (C:0.7349, R:0.0095)
Batch 425/537: Loss=1.6956 (C:0.7456, R:0.0095)
Batch 450/537: Loss=1.7302 (C:0.7792, R:0.0095)
Batch 475/537: Loss=1.7561 (C:0.8063, R:0.0095)
Batch 500/537: Loss=1.7253 (C:0.7737, R:0.0095)
Batch 525/537: Loss=1.7617 (C:0.8164, R:0.0095)

============================================================
Epoch 55/200 completed in 31.8s
Train: Loss=1.7208 (C:0.7728, R:0.0095) Ratio=5.42x
Val:   Loss=1.8230 (C:0.8833, R:0.0094) Ratio=3.34x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8230)
============================================================

Epoch 56 Training
----------------------------------------
Batch   0/537: Loss=1.7259 (C:0.7743, R:0.0095)
Batch  25/537: Loss=1.7142 (C:0.7672, R:0.0095)
Batch  50/537: Loss=1.7119 (C:0.7678, R:0.0094)
Batch  75/537: Loss=1.7123 (C:0.7649, R:0.0095)
Batch 100/537: Loss=1.7260 (C:0.7840, R:0.0094)
Batch 125/537: Loss=1.7239 (C:0.7752, R:0.0095)
Batch 150/537: Loss=1.7197 (C:0.7745, R:0.0095)
Batch 175/537: Loss=1.6884 (C:0.7387, R:0.0095)
Batch 200/537: Loss=1.7005 (C:0.7522, R:0.0095)
Batch 225/537: Loss=1.7260 (C:0.7847, R:0.0094)
Batch 250/537: Loss=1.6981 (C:0.7487, R:0.0095)
Batch 275/537: Loss=1.7288 (C:0.7835, R:0.0095)
Batch 300/537: Loss=1.7224 (C:0.7758, R:0.0095)
Batch 325/537: Loss=1.7377 (C:0.7885, R:0.0095)
Batch 350/537: Loss=1.7535 (C:0.8081, R:0.0095)
Batch 375/537: Loss=1.7313 (C:0.7850, R:0.0095)
Batch 400/537: Loss=1.7489 (C:0.8015, R:0.0095)
Batch 425/537: Loss=1.7371 (C:0.7845, R:0.0095)
Batch 450/537: Loss=1.7540 (C:0.8054, R:0.0095)
Batch 475/537: Loss=1.7119 (C:0.7617, R:0.0095)
Batch 500/537: Loss=1.7350 (C:0.7872, R:0.0095)
Batch 525/537: Loss=1.7383 (C:0.7900, R:0.0095)

============================================================
Epoch 56/200 completed in 24.1s
Train: Loss=1.7226 (C:0.7750, R:0.0095) Ratio=5.61x
Val:   Loss=1.8414 (C:0.9024, R:0.0094) Ratio=3.30x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 57 Training
----------------------------------------
Batch   0/537: Loss=1.7408 (C:0.7909, R:0.0095)
Batch  25/537: Loss=1.6805 (C:0.7310, R:0.0095)
Batch  50/537: Loss=1.7017 (C:0.7563, R:0.0095)
Batch  75/537: Loss=1.6996 (C:0.7495, R:0.0095)
Batch 100/537: Loss=1.7113 (C:0.7667, R:0.0094)
Batch 125/537: Loss=1.7088 (C:0.7617, R:0.0095)
Batch 150/537: Loss=1.7451 (C:0.7997, R:0.0095)
Batch 175/537: Loss=1.7286 (C:0.7806, R:0.0095)
Batch 200/537: Loss=1.7271 (C:0.7780, R:0.0095)
Batch 225/537: Loss=1.7201 (C:0.7706, R:0.0095)
Batch 250/537: Loss=1.7217 (C:0.7783, R:0.0094)
Batch 275/537: Loss=1.7157 (C:0.7665, R:0.0095)
Batch 300/537: Loss=1.7341 (C:0.7824, R:0.0095)
Batch 325/537: Loss=1.6950 (C:0.7481, R:0.0095)
Batch 350/537: Loss=1.7138 (C:0.7657, R:0.0095)
Batch 375/537: Loss=1.6931 (C:0.7374, R:0.0096)
Batch 400/537: Loss=1.7309 (C:0.7796, R:0.0095)
Batch 425/537: Loss=1.7390 (C:0.7924, R:0.0095)
Batch 450/537: Loss=1.7176 (C:0.7690, R:0.0095)
Batch 475/537: Loss=1.7372 (C:0.7880, R:0.0095)
Batch 500/537: Loss=1.6833 (C:0.7388, R:0.0094)
Batch 525/537: Loss=1.7249 (C:0.7785, R:0.0095)

============================================================
Epoch 57/200 completed in 23.9s
Train: Loss=1.7194 (C:0.7723, R:0.0095) Ratio=5.80x
Val:   Loss=1.8417 (C:0.9029, R:0.0094) Ratio=3.27x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 58
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.289 ¬± 0.584
    Neg distances: 1.873 ¬± 0.955
    Separation ratio: 6.48x
    Gap: -2.960
    ‚úÖ Excellent global separation!

Epoch 58 Training
----------------------------------------
Batch   0/537: Loss=1.7069 (C:0.7635, R:0.0094)
Batch  25/537: Loss=1.6869 (C:0.7395, R:0.0095)
Batch  50/537: Loss=1.7103 (C:0.7652, R:0.0095)
Batch  75/537: Loss=1.7262 (C:0.7758, R:0.0095)
Batch 100/537: Loss=1.7305 (C:0.7829, R:0.0095)
Batch 125/537: Loss=1.7448 (C:0.8015, R:0.0094)
Batch 150/537: Loss=1.6657 (C:0.7198, R:0.0095)
Batch 175/537: Loss=1.7357 (C:0.7884, R:0.0095)
Batch 200/537: Loss=1.7200 (C:0.7747, R:0.0095)
Batch 225/537: Loss=1.6929 (C:0.7460, R:0.0095)
Batch 250/537: Loss=1.7127 (C:0.7661, R:0.0095)
Batch 275/537: Loss=1.6780 (C:0.7326, R:0.0095)
Batch 300/537: Loss=1.7007 (C:0.7580, R:0.0094)
Batch 325/537: Loss=1.7216 (C:0.7784, R:0.0094)
Batch 350/537: Loss=1.7011 (C:0.7554, R:0.0095)
Batch 375/537: Loss=1.7089 (C:0.7610, R:0.0095)
Batch 400/537: Loss=1.6953 (C:0.7474, R:0.0095)
Batch 425/537: Loss=1.7107 (C:0.7678, R:0.0094)
Batch 450/537: Loss=1.6352 (C:0.6896, R:0.0095)
Batch 475/537: Loss=1.7193 (C:0.7771, R:0.0094)
Batch 500/537: Loss=1.7233 (C:0.7818, R:0.0094)
Batch 525/537: Loss=1.6999 (C:0.7533, R:0.0095)

============================================================
Epoch 58/200 completed in 31.8s
Train: Loss=1.7139 (C:0.7676, R:0.0095) Ratio=5.68x
Val:   Loss=1.8246 (C:0.8869, R:0.0094) Ratio=3.32x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 59 Training
----------------------------------------
Batch   0/537: Loss=1.7262 (C:0.7787, R:0.0095)
Batch  25/537: Loss=1.6879 (C:0.7487, R:0.0094)
Batch  50/537: Loss=1.7077 (C:0.7635, R:0.0094)
Batch  75/537: Loss=1.6814 (C:0.7362, R:0.0095)
Batch 100/537: Loss=1.7582 (C:0.8083, R:0.0095)
Batch 125/537: Loss=1.6901 (C:0.7422, R:0.0095)
Batch 150/537: Loss=1.7224 (C:0.7779, R:0.0094)
Batch 175/537: Loss=1.7311 (C:0.7881, R:0.0094)
Batch 200/537: Loss=1.6914 (C:0.7444, R:0.0095)
Batch 225/537: Loss=1.7036 (C:0.7614, R:0.0094)
Batch 250/537: Loss=1.6851 (C:0.7389, R:0.0095)
Batch 275/537: Loss=1.7262 (C:0.7804, R:0.0095)
Batch 300/537: Loss=1.6761 (C:0.7304, R:0.0095)
Batch 325/537: Loss=1.7238 (C:0.7826, R:0.0094)
Batch 350/537: Loss=1.6738 (C:0.7310, R:0.0094)
Batch 375/537: Loss=1.7120 (C:0.7634, R:0.0095)
Batch 400/537: Loss=1.7458 (C:0.8028, R:0.0094)
Batch 425/537: Loss=1.6973 (C:0.7545, R:0.0094)
Batch 450/537: Loss=1.7182 (C:0.7724, R:0.0095)
Batch 475/537: Loss=1.7027 (C:0.7583, R:0.0094)
Batch 500/537: Loss=1.6995 (C:0.7546, R:0.0094)
Batch 525/537: Loss=1.7034 (C:0.7579, R:0.0095)

============================================================
Epoch 59/200 completed in 24.0s
Train: Loss=1.7107 (C:0.7658, R:0.0094) Ratio=5.77x
Val:   Loss=1.8139 (C:0.8786, R:0.0094) Ratio=3.31x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8139)
============================================================

Epoch 60 Training
----------------------------------------
Batch   0/537: Loss=1.7213 (C:0.7779, R:0.0094)
Batch  25/537: Loss=1.7128 (C:0.7673, R:0.0095)
Batch  50/537: Loss=1.6937 (C:0.7487, R:0.0094)
Batch  75/537: Loss=1.6895 (C:0.7432, R:0.0095)
Batch 100/537: Loss=1.7137 (C:0.7705, R:0.0094)
Batch 125/537: Loss=1.6981 (C:0.7583, R:0.0094)
Batch 150/537: Loss=1.7024 (C:0.7593, R:0.0094)
Batch 175/537: Loss=1.6843 (C:0.7412, R:0.0094)
Batch 200/537: Loss=1.6743 (C:0.7334, R:0.0094)
Batch 225/537: Loss=1.7311 (C:0.7870, R:0.0094)
Batch 250/537: Loss=1.6787 (C:0.7368, R:0.0094)
Batch 275/537: Loss=1.6876 (C:0.7437, R:0.0094)
Batch 300/537: Loss=1.6991 (C:0.7591, R:0.0094)
Batch 325/537: Loss=1.7191 (C:0.7747, R:0.0094)
Batch 350/537: Loss=1.7064 (C:0.7702, R:0.0094)
Batch 375/537: Loss=1.6955 (C:0.7514, R:0.0094)
Batch 400/537: Loss=1.7187 (C:0.7768, R:0.0094)
Batch 425/537: Loss=1.7211 (C:0.7819, R:0.0094)
Batch 450/537: Loss=1.7165 (C:0.7765, R:0.0094)
Batch 475/537: Loss=1.6781 (C:0.7378, R:0.0094)
Batch 500/537: Loss=1.7075 (C:0.7656, R:0.0094)
Batch 525/537: Loss=1.6954 (C:0.7528, R:0.0094)

============================================================
Epoch 60/200 completed in 24.3s
Train: Loss=1.7077 (C:0.7653, R:0.0094) Ratio=5.69x
Val:   Loss=1.8203 (C:0.8876, R:0.0093) Ratio=3.30x
Reconstruction weight: 100.000
No improvement for 1 epochs
Checkpoint saved at epoch 60
============================================================

üåç Updating global dataset at epoch 61
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.292 ¬± 0.570
    Neg distances: 1.883 ¬± 0.943
    Separation ratio: 6.45x
    Gap: -2.994
    ‚úÖ Excellent global separation!

Epoch 61 Training
----------------------------------------
Batch   0/537: Loss=1.7064 (C:0.7572, R:0.0095)
Batch  25/537: Loss=1.7190 (C:0.7784, R:0.0094)
Batch  50/537: Loss=1.6915 (C:0.7534, R:0.0094)
Batch  75/537: Loss=1.6973 (C:0.7623, R:0.0094)
Batch 100/537: Loss=1.6944 (C:0.7533, R:0.0094)
Batch 125/537: Loss=1.6840 (C:0.7473, R:0.0094)
Batch 150/537: Loss=1.7055 (C:0.7670, R:0.0094)
Batch 175/537: Loss=1.6727 (C:0.7393, R:0.0093)
Batch 200/537: Loss=1.6965 (C:0.7582, R:0.0094)
Batch 225/537: Loss=1.7025 (C:0.7636, R:0.0094)
Batch 250/537: Loss=1.7013 (C:0.7671, R:0.0093)
Batch 275/537: Loss=1.6720 (C:0.7368, R:0.0094)
Batch 300/537: Loss=1.6659 (C:0.7240, R:0.0094)
Batch 325/537: Loss=1.6590 (C:0.7282, R:0.0093)
Batch 350/537: Loss=1.7211 (C:0.7858, R:0.0094)
Batch 375/537: Loss=1.7112 (C:0.7755, R:0.0094)
Batch 400/537: Loss=1.7225 (C:0.7825, R:0.0094)
Batch 425/537: Loss=1.6577 (C:0.7208, R:0.0094)
Batch 450/537: Loss=1.7248 (C:0.7919, R:0.0093)
Batch 475/537: Loss=1.6388 (C:0.7058, R:0.0093)
Batch 500/537: Loss=1.7118 (C:0.7733, R:0.0094)
Batch 525/537: Loss=1.6941 (C:0.7596, R:0.0093)

============================================================
Epoch 61/200 completed in 32.2s
Train: Loss=1.6985 (C:0.7616, R:0.0094) Ratio=5.46x
Val:   Loss=1.8090 (C:0.8841, R:0.0092) Ratio=3.21x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8090)
============================================================

Epoch 62 Training
----------------------------------------
Batch   0/537: Loss=1.6680 (C:0.7353, R:0.0093)
Batch  25/537: Loss=1.6798 (C:0.7449, R:0.0093)
Batch  50/537: Loss=1.6878 (C:0.7544, R:0.0093)
Batch  75/537: Loss=1.7114 (C:0.7777, R:0.0093)
Batch 100/537: Loss=1.6635 (C:0.7297, R:0.0093)
Batch 125/537: Loss=1.7201 (C:0.7886, R:0.0093)
Batch 150/537: Loss=1.7168 (C:0.7815, R:0.0094)
Batch 175/537: Loss=1.6684 (C:0.7347, R:0.0093)
Batch 200/537: Loss=1.7165 (C:0.7863, R:0.0093)
Batch 225/537: Loss=1.7103 (C:0.7774, R:0.0093)
Batch 250/537: Loss=1.6891 (C:0.7532, R:0.0094)
Batch 275/537: Loss=1.7140 (C:0.7816, R:0.0093)
Batch 300/537: Loss=1.6822 (C:0.7507, R:0.0093)
Batch 325/537: Loss=1.6768 (C:0.7406, R:0.0094)
Batch 350/537: Loss=1.6807 (C:0.7497, R:0.0093)
Batch 375/537: Loss=1.7184 (C:0.7896, R:0.0093)
Batch 400/537: Loss=1.7344 (C:0.8070, R:0.0093)
Batch 425/537: Loss=1.6220 (C:0.6923, R:0.0093)
Batch 450/537: Loss=1.7280 (C:0.7942, R:0.0093)
Batch 475/537: Loss=1.7249 (C:0.7991, R:0.0093)
Batch 500/537: Loss=1.6766 (C:0.7428, R:0.0093)
Batch 525/537: Loss=1.7006 (C:0.7739, R:0.0093)

============================================================
Epoch 62/200 completed in 24.2s
Train: Loss=1.6935 (C:0.7616, R:0.0093) Ratio=5.33x
Val:   Loss=1.8018 (C:0.8812, R:0.0092) Ratio=3.28x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8018)
============================================================

Epoch 63 Training
----------------------------------------
Batch   0/537: Loss=1.6847 (C:0.7488, R:0.0094)
Batch  25/537: Loss=1.7191 (C:0.7923, R:0.0093)
Batch  50/537: Loss=1.7259 (C:0.7911, R:0.0093)
Batch  75/537: Loss=1.6685 (C:0.7414, R:0.0093)
Batch 100/537: Loss=1.6476 (C:0.7156, R:0.0093)
Batch 125/537: Loss=1.6731 (C:0.7430, R:0.0093)
Batch 150/537: Loss=1.6658 (C:0.7401, R:0.0093)
Batch 175/537: Loss=1.6880 (C:0.7585, R:0.0093)
Batch 200/537: Loss=1.7246 (C:0.7958, R:0.0093)
Batch 225/537: Loss=1.7031 (C:0.7727, R:0.0093)
Batch 250/537: Loss=1.6710 (C:0.7388, R:0.0093)
Batch 275/537: Loss=1.6605 (C:0.7265, R:0.0093)
Batch 300/537: Loss=1.7053 (C:0.7794, R:0.0093)
Batch 325/537: Loss=1.7057 (C:0.7738, R:0.0093)
Batch 350/537: Loss=1.6771 (C:0.7507, R:0.0093)
Batch 375/537: Loss=1.7141 (C:0.7871, R:0.0093)
Batch 400/537: Loss=1.6693 (C:0.7423, R:0.0093)
Batch 425/537: Loss=1.6818 (C:0.7578, R:0.0092)
Batch 450/537: Loss=1.6999 (C:0.7727, R:0.0093)
Batch 475/537: Loss=1.7291 (C:0.8006, R:0.0093)
Batch 500/537: Loss=1.6833 (C:0.7540, R:0.0093)
Batch 525/537: Loss=1.7111 (C:0.7816, R:0.0093)

============================================================
Epoch 63/200 completed in 24.5s
Train: Loss=1.6882 (C:0.7600, R:0.0093) Ratio=5.44x
Val:   Loss=1.8017 (C:0.8844, R:0.0092) Ratio=3.22x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.8017)
============================================================

üåç Updating global dataset at epoch 64
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.299 ¬± 0.569
    Neg distances: 1.940 ¬± 0.945
    Separation ratio: 6.49x
    Gap: -3.049
    ‚úÖ Excellent global separation!

Epoch 64 Training
----------------------------------------
Batch   0/537: Loss=1.6577 (C:0.7309, R:0.0093)
Batch  25/537: Loss=1.6563 (C:0.7272, R:0.0093)
Batch  50/537: Loss=1.6742 (C:0.7495, R:0.0092)
Batch  75/537: Loss=1.6309 (C:0.7059, R:0.0093)
Batch 100/537: Loss=1.7007 (C:0.7779, R:0.0092)
Batch 125/537: Loss=1.6488 (C:0.7182, R:0.0093)
Batch 150/537: Loss=1.7008 (C:0.7755, R:0.0093)
Batch 175/537: Loss=1.6920 (C:0.7665, R:0.0093)
Batch 200/537: Loss=1.6827 (C:0.7570, R:0.0093)
Batch 225/537: Loss=1.6290 (C:0.7034, R:0.0093)
Batch 250/537: Loss=1.6994 (C:0.7759, R:0.0092)
Batch 275/537: Loss=1.6864 (C:0.7604, R:0.0093)
Batch 300/537: Loss=1.6623 (C:0.7343, R:0.0093)
Batch 325/537: Loss=1.6728 (C:0.7512, R:0.0092)
Batch 350/537: Loss=1.6740 (C:0.7464, R:0.0093)
Batch 375/537: Loss=1.7058 (C:0.7812, R:0.0092)
Batch 400/537: Loss=1.6500 (C:0.7225, R:0.0093)
Batch 425/537: Loss=1.6810 (C:0.7581, R:0.0092)
Batch 450/537: Loss=1.6773 (C:0.7505, R:0.0093)
Batch 475/537: Loss=1.6598 (C:0.7377, R:0.0092)
Batch 500/537: Loss=1.6957 (C:0.7683, R:0.0093)
Batch 525/537: Loss=1.7015 (C:0.7782, R:0.0092)

============================================================
Epoch 64/200 completed in 32.7s
Train: Loss=1.6708 (C:0.7460, R:0.0092) Ratio=5.39x
Val:   Loss=1.7910 (C:0.8766, R:0.0091) Ratio=3.20x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7910)
============================================================

Epoch 65 Training
----------------------------------------
Batch   0/537: Loss=1.6634 (C:0.7385, R:0.0092)
Batch  25/537: Loss=1.6918 (C:0.7671, R:0.0092)
Batch  50/537: Loss=1.6475 (C:0.7286, R:0.0092)
Batch  75/537: Loss=1.6604 (C:0.7355, R:0.0092)
Batch 100/537: Loss=1.6717 (C:0.7467, R:0.0092)
Batch 125/537: Loss=1.6589 (C:0.7326, R:0.0093)
Batch 150/537: Loss=1.6747 (C:0.7487, R:0.0093)
Batch 175/537: Loss=1.6810 (C:0.7577, R:0.0092)
Batch 200/537: Loss=1.6535 (C:0.7306, R:0.0092)
Batch 225/537: Loss=1.6813 (C:0.7546, R:0.0093)
Batch 250/537: Loss=1.6500 (C:0.7252, R:0.0092)
Batch 275/537: Loss=1.6772 (C:0.7546, R:0.0092)
Batch 300/537: Loss=1.6534 (C:0.7323, R:0.0092)
Batch 325/537: Loss=1.6658 (C:0.7416, R:0.0092)
Batch 350/537: Loss=1.6642 (C:0.7411, R:0.0092)
Batch 375/537: Loss=1.6674 (C:0.7438, R:0.0092)
Batch 400/537: Loss=1.6665 (C:0.7470, R:0.0092)
Batch 425/537: Loss=1.6715 (C:0.7571, R:0.0091)
Batch 450/537: Loss=1.6598 (C:0.7329, R:0.0093)
Batch 475/537: Loss=1.6606 (C:0.7377, R:0.0092)
Batch 500/537: Loss=1.6749 (C:0.7522, R:0.0092)
Batch 525/537: Loss=1.6787 (C:0.7606, R:0.0092)

============================================================
Epoch 65/200 completed in 24.5s
Train: Loss=1.6687 (C:0.7457, R:0.0092) Ratio=5.38x
Val:   Loss=1.7808 (C:0.8684, R:0.0091) Ratio=3.26x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7808)
============================================================

Epoch 66 Training
----------------------------------------
Batch   0/537: Loss=1.6945 (C:0.7704, R:0.0092)
Batch  25/537: Loss=1.6875 (C:0.7633, R:0.0092)
Batch  50/537: Loss=1.6340 (C:0.7126, R:0.0092)
Batch  75/537: Loss=1.6354 (C:0.7169, R:0.0092)
Batch 100/537: Loss=1.6895 (C:0.7610, R:0.0093)
Batch 125/537: Loss=1.6554 (C:0.7356, R:0.0092)
Batch 150/537: Loss=1.6527 (C:0.7296, R:0.0092)
Batch 175/537: Loss=1.6584 (C:0.7409, R:0.0092)
Batch 200/537: Loss=1.6474 (C:0.7250, R:0.0092)
Batch 225/537: Loss=1.7084 (C:0.7863, R:0.0092)
Batch 250/537: Loss=1.6626 (C:0.7398, R:0.0092)
Batch 275/537: Loss=1.6977 (C:0.7755, R:0.0092)
Batch 300/537: Loss=1.6667 (C:0.7466, R:0.0092)
Batch 325/537: Loss=1.7074 (C:0.7897, R:0.0092)
Batch 350/537: Loss=1.6612 (C:0.7326, R:0.0093)
Batch 375/537: Loss=1.6552 (C:0.7338, R:0.0092)
Batch 400/537: Loss=1.6852 (C:0.7604, R:0.0092)
Batch 425/537: Loss=1.6147 (C:0.6884, R:0.0093)
Batch 450/537: Loss=1.6912 (C:0.7715, R:0.0092)
Batch 475/537: Loss=1.6473 (C:0.7296, R:0.0092)
Batch 500/537: Loss=1.6456 (C:0.7238, R:0.0092)
Batch 525/537: Loss=1.6600 (C:0.7333, R:0.0093)

============================================================
Epoch 66/200 completed in 24.4s
Train: Loss=1.6665 (C:0.7450, R:0.0092) Ratio=5.47x
Val:   Loss=1.7777 (C:0.8660, R:0.0091) Ratio=3.27x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7777)
============================================================

üåç Updating global dataset at epoch 67
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.316 ¬± 0.592
    Neg distances: 1.961 ¬± 0.966
    Separation ratio: 6.20x
    Gap: -3.107
    ‚úÖ Excellent global separation!

Epoch 67 Training
----------------------------------------
Batch   0/537: Loss=1.6316 (C:0.7096, R:0.0092)
Batch  25/537: Loss=1.6548 (C:0.7336, R:0.0092)
Batch  50/537: Loss=1.6491 (C:0.7261, R:0.0092)
Batch  75/537: Loss=1.6722 (C:0.7531, R:0.0092)
Batch 100/537: Loss=1.7037 (C:0.7822, R:0.0092)
Batch 125/537: Loss=1.6538 (C:0.7320, R:0.0092)
Batch 150/537: Loss=1.6822 (C:0.7634, R:0.0092)
Batch 175/537: Loss=1.6544 (C:0.7324, R:0.0092)
Batch 200/537: Loss=1.6689 (C:0.7502, R:0.0092)
Batch 225/537: Loss=1.6725 (C:0.7519, R:0.0092)
Batch 250/537: Loss=1.6768 (C:0.7542, R:0.0092)
Batch 275/537: Loss=1.6456 (C:0.7281, R:0.0092)
Batch 300/537: Loss=1.6682 (C:0.7499, R:0.0092)
Batch 325/537: Loss=1.6611 (C:0.7420, R:0.0092)
Batch 350/537: Loss=1.6895 (C:0.7738, R:0.0092)
Batch 375/537: Loss=1.6449 (C:0.7252, R:0.0092)
Batch 400/537: Loss=1.6451 (C:0.7267, R:0.0092)
Batch 425/537: Loss=1.6257 (C:0.7071, R:0.0092)
Batch 450/537: Loss=1.6465 (C:0.7305, R:0.0092)
Batch 475/537: Loss=1.6429 (C:0.7210, R:0.0092)
Batch 500/537: Loss=1.6775 (C:0.7541, R:0.0092)
Batch 525/537: Loss=1.7385 (C:0.8179, R:0.0092)

============================================================
Epoch 67/200 completed in 32.2s
Train: Loss=1.6695 (C:0.7491, R:0.0092) Ratio=5.50x
Val:   Loss=1.7800 (C:0.8706, R:0.0091) Ratio=3.23x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 68 Training
----------------------------------------
Batch   0/537: Loss=1.6813 (C:0.7619, R:0.0092)
Batch  25/537: Loss=1.6763 (C:0.7587, R:0.0092)
Batch  50/537: Loss=1.6930 (C:0.7694, R:0.0092)
Batch  75/537: Loss=1.6328 (C:0.7076, R:0.0093)
Batch 100/537: Loss=1.6594 (C:0.7435, R:0.0092)
Batch 125/537: Loss=1.6222 (C:0.7098, R:0.0091)
Batch 150/537: Loss=1.6357 (C:0.7167, R:0.0092)
Batch 175/537: Loss=1.6610 (C:0.7396, R:0.0092)
Batch 200/537: Loss=1.6492 (C:0.7283, R:0.0092)
Batch 225/537: Loss=1.6353 (C:0.7159, R:0.0092)
Batch 250/537: Loss=1.6166 (C:0.7044, R:0.0091)
Batch 275/537: Loss=1.6587 (C:0.7413, R:0.0092)
Batch 300/537: Loss=1.6592 (C:0.7364, R:0.0092)
Batch 325/537: Loss=1.7018 (C:0.7806, R:0.0092)
Batch 350/537: Loss=1.6488 (C:0.7261, R:0.0092)
Batch 375/537: Loss=1.6751 (C:0.7557, R:0.0092)
Batch 400/537: Loss=1.7077 (C:0.7820, R:0.0093)
Batch 425/537: Loss=1.6698 (C:0.7571, R:0.0091)
Batch 450/537: Loss=1.6672 (C:0.7482, R:0.0092)
Batch 475/537: Loss=1.6629 (C:0.7472, R:0.0092)
Batch 500/537: Loss=1.6408 (C:0.7222, R:0.0092)
Batch 525/537: Loss=1.6816 (C:0.7597, R:0.0092)

============================================================
Epoch 68/200 completed in 24.3s
Train: Loss=1.6683 (C:0.7488, R:0.0092) Ratio=5.65x
Val:   Loss=1.7854 (C:0.8769, R:0.0091) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 69 Training
----------------------------------------
Batch   0/537: Loss=1.6298 (C:0.7087, R:0.0092)
Batch  25/537: Loss=1.6929 (C:0.7718, R:0.0092)
Batch  50/537: Loss=1.6840 (C:0.7616, R:0.0092)
Batch  75/537: Loss=1.6529 (C:0.7345, R:0.0092)
Batch 100/537: Loss=1.6334 (C:0.7148, R:0.0092)
Batch 125/537: Loss=1.7018 (C:0.7872, R:0.0091)
Batch 150/537: Loss=1.6722 (C:0.7592, R:0.0091)
Batch 175/537: Loss=1.6972 (C:0.7713, R:0.0093)
Batch 200/537: Loss=1.6616 (C:0.7441, R:0.0092)
Batch 225/537: Loss=1.6647 (C:0.7500, R:0.0091)
Batch 250/537: Loss=1.6556 (C:0.7330, R:0.0092)
Batch 275/537: Loss=1.6816 (C:0.7623, R:0.0092)
Batch 300/537: Loss=1.6767 (C:0.7577, R:0.0092)
Batch 325/537: Loss=1.6635 (C:0.7460, R:0.0092)
Batch 350/537: Loss=1.6375 (C:0.7184, R:0.0092)
Batch 375/537: Loss=1.6804 (C:0.7630, R:0.0092)
Batch 400/537: Loss=1.6559 (C:0.7379, R:0.0092)
Batch 425/537: Loss=1.6593 (C:0.7356, R:0.0092)
Batch 450/537: Loss=1.6918 (C:0.7773, R:0.0091)
Batch 475/537: Loss=1.6827 (C:0.7619, R:0.0092)
Batch 500/537: Loss=1.6704 (C:0.7523, R:0.0092)
Batch 525/537: Loss=1.6797 (C:0.7643, R:0.0092)

============================================================
Epoch 69/200 completed in 24.5s
Train: Loss=1.6683 (C:0.7494, R:0.0092) Ratio=5.59x
Val:   Loss=1.7860 (C:0.8785, R:0.0091) Ratio=3.22x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

üåç Updating global dataset at epoch 70
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.315 ¬± 0.586
    Neg distances: 1.990 ¬± 0.962
    Separation ratio: 6.31x
    Gap: -3.160
    ‚úÖ Excellent global separation!

Epoch 70 Training
----------------------------------------
Batch   0/537: Loss=1.6846 (C:0.7657, R:0.0092)
Batch  25/537: Loss=1.6319 (C:0.7124, R:0.0092)
Batch  50/537: Loss=1.6704 (C:0.7510, R:0.0092)
Batch  75/537: Loss=1.6790 (C:0.7591, R:0.0092)
Batch 100/537: Loss=1.6440 (C:0.7243, R:0.0092)
Batch 125/537: Loss=1.6338 (C:0.7166, R:0.0092)
Batch 150/537: Loss=1.6410 (C:0.7217, R:0.0092)
Batch 175/537: Loss=1.6653 (C:0.7451, R:0.0092)
Batch 200/537: Loss=1.6500 (C:0.7318, R:0.0092)
Batch 225/537: Loss=1.6300 (C:0.7163, R:0.0091)
Batch 250/537: Loss=1.6689 (C:0.7480, R:0.0092)
Batch 275/537: Loss=1.6624 (C:0.7429, R:0.0092)
Batch 300/537: Loss=1.6336 (C:0.7190, R:0.0091)
Batch 325/537: Loss=1.6407 (C:0.7236, R:0.0092)
Batch 350/537: Loss=1.6542 (C:0.7370, R:0.0092)
Batch 375/537: Loss=1.6375 (C:0.7151, R:0.0092)
Batch 400/537: Loss=1.6823 (C:0.7648, R:0.0092)
Batch 425/537: Loss=1.6440 (C:0.7264, R:0.0092)
Batch 450/537: Loss=1.6512 (C:0.7346, R:0.0092)
Batch 475/537: Loss=1.6625 (C:0.7404, R:0.0092)
Batch 500/537: Loss=1.6619 (C:0.7465, R:0.0092)
Batch 525/537: Loss=1.6376 (C:0.7165, R:0.0092)

============================================================
Epoch 70/200 completed in 33.1s
Train: Loss=1.6494 (C:0.7310, R:0.0092) Ratio=5.41x
Val:   Loss=1.7705 (C:0.8630, R:0.0091) Ratio=3.26x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7705)
============================================================

Epoch 71 Training
----------------------------------------
Batch   0/537: Loss=1.6363 (C:0.7215, R:0.0091)
Batch  25/537: Loss=1.6568 (C:0.7388, R:0.0092)
Batch  50/537: Loss=1.5920 (C:0.6737, R:0.0092)
Batch  75/537: Loss=1.6589 (C:0.7431, R:0.0092)
Batch 100/537: Loss=1.6229 (C:0.7073, R:0.0092)
Batch 125/537: Loss=1.6154 (C:0.6930, R:0.0092)
Batch 150/537: Loss=1.6838 (C:0.7624, R:0.0092)
Batch 175/537: Loss=1.6601 (C:0.7439, R:0.0092)
Batch 200/537: Loss=1.6308 (C:0.7121, R:0.0092)
Batch 225/537: Loss=1.6288 (C:0.7070, R:0.0092)
Batch 250/537: Loss=1.6757 (C:0.7563, R:0.0092)
Batch 275/537: Loss=1.6371 (C:0.7151, R:0.0092)
Batch 300/537: Loss=1.6410 (C:0.7253, R:0.0092)
Batch 325/537: Loss=1.6742 (C:0.7575, R:0.0092)
Batch 350/537: Loss=1.6504 (C:0.7320, R:0.0092)
Batch 375/537: Loss=1.6802 (C:0.7662, R:0.0091)
Batch 400/537: Loss=1.6560 (C:0.7423, R:0.0091)
Batch 425/537: Loss=1.6716 (C:0.7564, R:0.0092)
Batch 450/537: Loss=1.6660 (C:0.7527, R:0.0091)
Batch 475/537: Loss=1.6178 (C:0.7006, R:0.0092)
Batch 500/537: Loss=1.6173 (C:0.6933, R:0.0092)
Batch 525/537: Loss=1.6515 (C:0.7314, R:0.0092)

============================================================
Epoch 71/200 completed in 24.3s
Train: Loss=1.6478 (C:0.7299, R:0.0092) Ratio=5.58x
Val:   Loss=1.7704 (C:0.8628, R:0.0091) Ratio=3.28x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7704)
============================================================

Epoch 72 Training
----------------------------------------
Batch   0/537: Loss=1.6534 (C:0.7346, R:0.0092)
Batch  25/537: Loss=1.6292 (C:0.7129, R:0.0092)
Batch  50/537: Loss=1.6427 (C:0.7259, R:0.0092)
Batch  75/537: Loss=1.6437 (C:0.7257, R:0.0092)
Batch 100/537: Loss=1.6466 (C:0.7281, R:0.0092)
Batch 125/537: Loss=1.6309 (C:0.7075, R:0.0092)
Batch 150/537: Loss=1.6312 (C:0.7140, R:0.0092)
Batch 175/537: Loss=1.6557 (C:0.7348, R:0.0092)
Batch 200/537: Loss=1.6517 (C:0.7378, R:0.0091)
Batch 225/537: Loss=1.6416 (C:0.7219, R:0.0092)
Batch 250/537: Loss=1.6667 (C:0.7502, R:0.0092)
Batch 275/537: Loss=1.6671 (C:0.7503, R:0.0092)
Batch 300/537: Loss=1.6478 (C:0.7315, R:0.0092)
Batch 325/537: Loss=1.6943 (C:0.7747, R:0.0092)
Batch 350/537: Loss=1.6424 (C:0.7226, R:0.0092)
Batch 375/537: Loss=1.6496 (C:0.7304, R:0.0092)
Batch 400/537: Loss=1.6609 (C:0.7452, R:0.0092)
Batch 425/537: Loss=1.6647 (C:0.7468, R:0.0092)
Batch 450/537: Loss=1.6464 (C:0.7275, R:0.0092)
Batch 475/537: Loss=1.6468 (C:0.7253, R:0.0092)
Batch 500/537: Loss=1.6690 (C:0.7506, R:0.0092)
Batch 525/537: Loss=1.6547 (C:0.7363, R:0.0092)

============================================================
Epoch 72/200 completed in 24.3s
Train: Loss=1.6449 (C:0.7273, R:0.0092) Ratio=5.56x
Val:   Loss=1.7721 (C:0.8648, R:0.0091) Ratio=3.27x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 73
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.297 ¬± 0.568
    Neg distances: 2.037 ¬± 0.971
    Separation ratio: 6.86x
    Gap: -3.201
    ‚úÖ Excellent global separation!

Epoch 73 Training
----------------------------------------
Batch   0/537: Loss=1.6365 (C:0.7179, R:0.0092)
Batch  25/537: Loss=1.6553 (C:0.7383, R:0.0092)
Batch  50/537: Loss=1.5901 (C:0.6740, R:0.0092)
Batch  75/537: Loss=1.6118 (C:0.6915, R:0.0092)
Batch 100/537: Loss=1.6238 (C:0.7029, R:0.0092)
Batch 125/537: Loss=1.6137 (C:0.6987, R:0.0092)
Batch 150/537: Loss=1.6276 (C:0.7036, R:0.0092)
Batch 175/537: Loss=1.6469 (C:0.7291, R:0.0092)
Batch 200/537: Loss=1.6102 (C:0.6971, R:0.0091)
Batch 225/537: Loss=1.6066 (C:0.6918, R:0.0091)
Batch 250/537: Loss=1.6098 (C:0.6905, R:0.0092)
Batch 275/537: Loss=1.6396 (C:0.7259, R:0.0091)
Batch 300/537: Loss=1.6183 (C:0.7001, R:0.0092)
Batch 325/537: Loss=1.6267 (C:0.7090, R:0.0092)
Batch 350/537: Loss=1.6257 (C:0.7060, R:0.0092)
Batch 375/537: Loss=1.5867 (C:0.6738, R:0.0091)
Batch 400/537: Loss=1.6668 (C:0.7458, R:0.0092)
Batch 425/537: Loss=1.6231 (C:0.7079, R:0.0092)
Batch 450/537: Loss=1.6067 (C:0.6890, R:0.0092)
Batch 475/537: Loss=1.6304 (C:0.7129, R:0.0092)
Batch 500/537: Loss=1.6550 (C:0.7408, R:0.0091)
Batch 525/537: Loss=1.6310 (C:0.7118, R:0.0092)

============================================================
Epoch 73/200 completed in 32.5s
Train: Loss=1.6196 (C:0.7022, R:0.0092) Ratio=5.58x
Val:   Loss=1.7399 (C:0.8330, R:0.0091) Ratio=3.32x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7399)
============================================================

Epoch 74 Training
----------------------------------------
Batch   0/537: Loss=1.5852 (C:0.6699, R:0.0092)
Batch  25/537: Loss=1.5968 (C:0.6833, R:0.0091)
Batch  50/537: Loss=1.5849 (C:0.6678, R:0.0092)
Batch  75/537: Loss=1.6043 (C:0.6863, R:0.0092)
Batch 100/537: Loss=1.6470 (C:0.7281, R:0.0092)
Batch 125/537: Loss=1.6165 (C:0.6971, R:0.0092)
Batch 150/537: Loss=1.6398 (C:0.7201, R:0.0092)
Batch 175/537: Loss=1.6105 (C:0.6877, R:0.0092)
Batch 200/537: Loss=1.6165 (C:0.7012, R:0.0092)
Batch 225/537: Loss=1.6359 (C:0.7228, R:0.0091)
Batch 250/537: Loss=1.6174 (C:0.7021, R:0.0092)
Batch 275/537: Loss=1.6340 (C:0.7146, R:0.0092)
Batch 300/537: Loss=1.6431 (C:0.7311, R:0.0091)
Batch 325/537: Loss=1.6131 (C:0.6954, R:0.0092)
Batch 350/537: Loss=1.6230 (C:0.7057, R:0.0092)
Batch 375/537: Loss=1.6145 (C:0.6971, R:0.0092)
Batch 400/537: Loss=1.5972 (C:0.6836, R:0.0091)
Batch 425/537: Loss=1.5972 (C:0.6796, R:0.0092)
Batch 450/537: Loss=1.6083 (C:0.6911, R:0.0092)
Batch 475/537: Loss=1.6064 (C:0.6953, R:0.0091)
Batch 500/537: Loss=1.6131 (C:0.6958, R:0.0092)
Batch 525/537: Loss=1.6178 (C:0.7020, R:0.0092)

============================================================
Epoch 74/200 completed in 24.4s
Train: Loss=1.6194 (C:0.7022, R:0.0092) Ratio=5.83x
Val:   Loss=1.7368 (C:0.8310, R:0.0091) Ratio=3.28x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7368)
============================================================

Epoch 75 Training
----------------------------------------
Batch   0/537: Loss=1.6048 (C:0.6861, R:0.0092)
Batch  25/537: Loss=1.5888 (C:0.6786, R:0.0091)
Batch  50/537: Loss=1.5847 (C:0.6712, R:0.0091)
Batch  75/537: Loss=1.6355 (C:0.7180, R:0.0092)
Batch 100/537: Loss=1.6049 (C:0.6958, R:0.0091)
Batch 125/537: Loss=1.6117 (C:0.6895, R:0.0092)
Batch 150/537: Loss=1.6401 (C:0.7199, R:0.0092)
Batch 175/537: Loss=1.6101 (C:0.6942, R:0.0092)
Batch 200/537: Loss=1.5862 (C:0.6684, R:0.0092)
Batch 225/537: Loss=1.6004 (C:0.6820, R:0.0092)
Batch 250/537: Loss=1.6480 (C:0.7284, R:0.0092)
Batch 275/537: Loss=1.6175 (C:0.7015, R:0.0092)
Batch 300/537: Loss=1.6454 (C:0.7235, R:0.0092)
Batch 325/537: Loss=1.6233 (C:0.7052, R:0.0092)
Batch 350/537: Loss=1.5815 (C:0.6667, R:0.0091)
Batch 375/537: Loss=1.6116 (C:0.7033, R:0.0091)
Batch 400/537: Loss=1.6366 (C:0.7226, R:0.0091)
Batch 425/537: Loss=1.5899 (C:0.6704, R:0.0092)
Batch 450/537: Loss=1.6260 (C:0.7136, R:0.0091)
Batch 475/537: Loss=1.6199 (C:0.7106, R:0.0091)
Batch 500/537: Loss=1.6502 (C:0.7350, R:0.0092)
Batch 525/537: Loss=1.6108 (C:0.6898, R:0.0092)

============================================================
Epoch 75/200 completed in 24.5s
Train: Loss=1.6179 (C:0.7011, R:0.0092) Ratio=5.72x
Val:   Loss=1.7443 (C:0.8389, R:0.0091) Ratio=3.31x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 76
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.297 ¬± 0.593
    Neg distances: 2.080 ¬± 0.966
    Separation ratio: 7.00x
    Gap: -3.248
    ‚úÖ Excellent global separation!

Epoch 76 Training
----------------------------------------
Batch   0/537: Loss=1.6031 (C:0.6897, R:0.0091)
Batch  25/537: Loss=1.6023 (C:0.6822, R:0.0092)
Batch  50/537: Loss=1.5668 (C:0.6482, R:0.0092)
Batch  75/537: Loss=1.6174 (C:0.7027, R:0.0091)
Batch 100/537: Loss=1.6290 (C:0.7155, R:0.0091)
Batch 125/537: Loss=1.5472 (C:0.6383, R:0.0091)
Batch 150/537: Loss=1.5845 (C:0.6700, R:0.0091)
Batch 175/537: Loss=1.6411 (C:0.7271, R:0.0091)
Batch 200/537: Loss=1.6479 (C:0.7312, R:0.0092)
Batch 225/537: Loss=1.6169 (C:0.7002, R:0.0092)
Batch 250/537: Loss=1.5905 (C:0.6724, R:0.0092)
Batch 275/537: Loss=1.5407 (C:0.6250, R:0.0092)
Batch 300/537: Loss=1.6030 (C:0.6859, R:0.0092)
Batch 325/537: Loss=1.6121 (C:0.6930, R:0.0092)
Batch 350/537: Loss=1.5995 (C:0.6819, R:0.0092)
Batch 375/537: Loss=1.6333 (C:0.7121, R:0.0092)
Batch 400/537: Loss=1.5795 (C:0.6679, R:0.0091)
Batch 425/537: Loss=1.5785 (C:0.6584, R:0.0092)
Batch 450/537: Loss=1.5875 (C:0.6695, R:0.0092)
Batch 475/537: Loss=1.6515 (C:0.7329, R:0.0092)
Batch 500/537: Loss=1.5847 (C:0.6700, R:0.0091)
Batch 525/537: Loss=1.5725 (C:0.6533, R:0.0092)

============================================================
Epoch 76/200 completed in 33.0s
Train: Loss=1.6000 (C:0.6834, R:0.0092) Ratio=5.65x
Val:   Loss=1.7266 (C:0.8210, R:0.0091) Ratio=3.27x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7266)
============================================================

Epoch 77 Training
----------------------------------------
Batch   0/537: Loss=1.5978 (C:0.6805, R:0.0092)
Batch  25/537: Loss=1.6098 (C:0.6926, R:0.0092)
Batch  50/537: Loss=1.6108 (C:0.6908, R:0.0092)
Batch  75/537: Loss=1.6068 (C:0.6945, R:0.0091)
Batch 100/537: Loss=1.6101 (C:0.6899, R:0.0092)
Batch 125/537: Loss=1.5802 (C:0.6629, R:0.0092)
Batch 150/537: Loss=1.5791 (C:0.6602, R:0.0092)
Batch 175/537: Loss=1.6196 (C:0.6977, R:0.0092)
Batch 200/537: Loss=1.5674 (C:0.6488, R:0.0092)
Batch 225/537: Loss=1.5917 (C:0.6755, R:0.0092)
Batch 250/537: Loss=1.5931 (C:0.6657, R:0.0093)
Batch 275/537: Loss=1.5973 (C:0.6819, R:0.0092)
Batch 300/537: Loss=1.5439 (C:0.6280, R:0.0092)
Batch 325/537: Loss=1.6076 (C:0.6974, R:0.0091)
Batch 350/537: Loss=1.5908 (C:0.6757, R:0.0092)
Batch 375/537: Loss=1.5785 (C:0.6622, R:0.0092)
Batch 400/537: Loss=1.6091 (C:0.6952, R:0.0091)
Batch 425/537: Loss=1.6125 (C:0.6894, R:0.0092)
Batch 450/537: Loss=1.5669 (C:0.6521, R:0.0091)
Batch 475/537: Loss=1.5712 (C:0.6562, R:0.0092)
Batch 500/537: Loss=1.5821 (C:0.6681, R:0.0091)
Batch 525/537: Loss=1.5821 (C:0.6667, R:0.0092)

============================================================
Epoch 77/200 completed in 24.5s
Train: Loss=1.6005 (C:0.6841, R:0.0092) Ratio=5.83x
Val:   Loss=1.7340 (C:0.8288, R:0.0091) Ratio=3.26x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 78 Training
----------------------------------------
Batch   0/537: Loss=1.6037 (C:0.6839, R:0.0092)
Batch  25/537: Loss=1.6270 (C:0.7110, R:0.0092)
Batch  50/537: Loss=1.6221 (C:0.7087, R:0.0091)
Batch  75/537: Loss=1.6175 (C:0.7031, R:0.0091)
Batch 100/537: Loss=1.5919 (C:0.6765, R:0.0092)
Batch 125/537: Loss=1.6140 (C:0.6972, R:0.0092)
Batch 150/537: Loss=1.5887 (C:0.6781, R:0.0091)
Batch 175/537: Loss=1.5899 (C:0.6761, R:0.0091)
Batch 200/537: Loss=1.5789 (C:0.6579, R:0.0092)
Batch 225/537: Loss=1.6015 (C:0.6859, R:0.0092)
Batch 250/537: Loss=1.5702 (C:0.6492, R:0.0092)
Batch 275/537: Loss=1.6061 (C:0.6886, R:0.0092)
Batch 300/537: Loss=1.6047 (C:0.6882, R:0.0092)
Batch 325/537: Loss=1.6288 (C:0.7086, R:0.0092)
Batch 350/537: Loss=1.6332 (C:0.7152, R:0.0092)
Batch 375/537: Loss=1.5317 (C:0.6166, R:0.0092)
Batch 400/537: Loss=1.5855 (C:0.6709, R:0.0091)
Batch 425/537: Loss=1.5735 (C:0.6559, R:0.0092)
Batch 450/537: Loss=1.5531 (C:0.6332, R:0.0092)
Batch 475/537: Loss=1.5755 (C:0.6588, R:0.0092)
Batch 500/537: Loss=1.6519 (C:0.7316, R:0.0092)
Batch 525/537: Loss=1.6260 (C:0.7136, R:0.0091)

============================================================
Epoch 78/200 completed in 24.6s
Train: Loss=1.5979 (C:0.6818, R:0.0092) Ratio=5.66x
Val:   Loss=1.7281 (C:0.8226, R:0.0091) Ratio=3.29x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 79
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.310 ¬± 0.610
    Neg distances: 2.082 ¬± 0.976
    Separation ratio: 6.73x
    Gap: -3.291
    ‚úÖ Excellent global separation!

Epoch 79 Training
----------------------------------------
Batch   0/537: Loss=1.5948 (C:0.6768, R:0.0092)
Batch  25/537: Loss=1.6038 (C:0.6878, R:0.0092)
Batch  50/537: Loss=1.5842 (C:0.6704, R:0.0091)
Batch  75/537: Loss=1.5940 (C:0.6789, R:0.0092)
Batch 100/537: Loss=1.6048 (C:0.6866, R:0.0092)
Batch 125/537: Loss=1.5793 (C:0.6609, R:0.0092)
Batch 150/537: Loss=1.6147 (C:0.6943, R:0.0092)
Batch 175/537: Loss=1.5947 (C:0.6753, R:0.0092)
Batch 200/537: Loss=1.5897 (C:0.6753, R:0.0091)
Batch 225/537: Loss=1.6379 (C:0.7218, R:0.0092)
Batch 250/537: Loss=1.6117 (C:0.6945, R:0.0092)
Batch 275/537: Loss=1.5980 (C:0.6808, R:0.0092)
Batch 300/537: Loss=1.6047 (C:0.6917, R:0.0091)
Batch 325/537: Loss=1.6164 (C:0.7014, R:0.0091)
Batch 350/537: Loss=1.6170 (C:0.7024, R:0.0091)
Batch 375/537: Loss=1.5772 (C:0.6617, R:0.0092)
Batch 400/537: Loss=1.6081 (C:0.6928, R:0.0092)
Batch 425/537: Loss=1.6121 (C:0.6952, R:0.0092)
Batch 450/537: Loss=1.5988 (C:0.6774, R:0.0092)
Batch 475/537: Loss=1.5683 (C:0.6550, R:0.0091)
Batch 500/537: Loss=1.6269 (C:0.7087, R:0.0092)
Batch 525/537: Loss=1.6107 (C:0.6968, R:0.0091)

============================================================
Epoch 79/200 completed in 33.5s
Train: Loss=1.6005 (C:0.6847, R:0.0092) Ratio=5.70x
Val:   Loss=1.7401 (C:0.8361, R:0.0090) Ratio=3.32x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 80 Training
----------------------------------------
Batch   0/537: Loss=1.5732 (C:0.6579, R:0.0092)
Batch  25/537: Loss=1.5741 (C:0.6621, R:0.0091)
Batch  50/537: Loss=1.5612 (C:0.6430, R:0.0092)
Batch  75/537: Loss=1.5777 (C:0.6629, R:0.0091)
Batch 100/537: Loss=1.6056 (C:0.6943, R:0.0091)
Batch 125/537: Loss=1.6539 (C:0.7381, R:0.0092)
Batch 150/537: Loss=1.6410 (C:0.7239, R:0.0092)
Batch 175/537: Loss=1.6086 (C:0.6877, R:0.0092)
Batch 200/537: Loss=1.6604 (C:0.7457, R:0.0091)
Batch 225/537: Loss=1.6389 (C:0.7231, R:0.0092)
Batch 250/537: Loss=1.6119 (C:0.6945, R:0.0092)
Batch 275/537: Loss=1.6181 (C:0.6983, R:0.0092)
Batch 300/537: Loss=1.5522 (C:0.6389, R:0.0091)
Batch 325/537: Loss=1.6198 (C:0.6994, R:0.0092)
Batch 350/537: Loss=1.5577 (C:0.6420, R:0.0092)
Batch 375/537: Loss=1.6439 (C:0.7270, R:0.0092)
Batch 400/537: Loss=1.6159 (C:0.7023, R:0.0091)
Batch 425/537: Loss=1.6082 (C:0.6954, R:0.0091)
Batch 450/537: Loss=1.6031 (C:0.6897, R:0.0091)
Batch 475/537: Loss=1.5943 (C:0.6747, R:0.0092)
Batch 500/537: Loss=1.6073 (C:0.6891, R:0.0092)
Batch 525/537: Loss=1.6196 (C:0.7077, R:0.0091)

============================================================
Epoch 80/200 completed in 24.5s
Train: Loss=1.6009 (C:0.6853, R:0.0092) Ratio=5.55x
Val:   Loss=1.7306 (C:0.8257, R:0.0090) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 4 epochs
Checkpoint saved at epoch 80
============================================================

Epoch 81 Training
----------------------------------------
Batch   0/537: Loss=1.5961 (C:0.6799, R:0.0092)
Batch  25/537: Loss=1.6272 (C:0.7101, R:0.0092)
Batch  50/537: Loss=1.6378 (C:0.7229, R:0.0091)
Batch  75/537: Loss=1.6175 (C:0.7032, R:0.0091)
Batch 100/537: Loss=1.5867 (C:0.6671, R:0.0092)
Batch 125/537: Loss=1.6098 (C:0.6916, R:0.0092)
Batch 150/537: Loss=1.6092 (C:0.6887, R:0.0092)
Batch 175/537: Loss=1.6215 (C:0.7005, R:0.0092)
Batch 200/537: Loss=1.6256 (C:0.7052, R:0.0092)
Batch 225/537: Loss=1.6036 (C:0.6903, R:0.0091)
Batch 250/537: Loss=1.6121 (C:0.6941, R:0.0092)
Batch 275/537: Loss=1.6110 (C:0.6910, R:0.0092)
Batch 300/537: Loss=1.6173 (C:0.6942, R:0.0092)
Batch 325/537: Loss=1.6131 (C:0.7014, R:0.0091)
Batch 350/537: Loss=1.6334 (C:0.7190, R:0.0091)
Batch 375/537: Loss=1.5824 (C:0.6667, R:0.0092)
Batch 400/537: Loss=1.5778 (C:0.6610, R:0.0092)
Batch 425/537: Loss=1.5977 (C:0.6803, R:0.0092)
Batch 450/537: Loss=1.5912 (C:0.6801, R:0.0091)
Batch 475/537: Loss=1.5708 (C:0.6549, R:0.0092)
Batch 500/537: Loss=1.6131 (C:0.6942, R:0.0092)
Batch 525/537: Loss=1.6322 (C:0.7179, R:0.0091)

============================================================
Epoch 81/200 completed in 24.6s
Train: Loss=1.5994 (C:0.6841, R:0.0092) Ratio=5.64x
Val:   Loss=1.7160 (C:0.8113, R:0.0090) Ratio=3.30x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7160)
============================================================

üåç Updating global dataset at epoch 82
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.320 ¬± 0.620
    Neg distances: 2.107 ¬± 0.989
    Separation ratio: 6.58x
    Gap: -3.338
    ‚úÖ Excellent global separation!

Epoch 82 Training
----------------------------------------
Batch   0/537: Loss=1.6087 (C:0.6919, R:0.0092)
Batch  25/537: Loss=1.6155 (C:0.6985, R:0.0092)
Batch  50/537: Loss=1.6329 (C:0.7188, R:0.0091)
Batch  75/537: Loss=1.6059 (C:0.6873, R:0.0092)
Batch 100/537: Loss=1.5951 (C:0.6817, R:0.0091)
Batch 125/537: Loss=1.5697 (C:0.6560, R:0.0091)
Batch 150/537: Loss=1.6004 (C:0.6880, R:0.0091)
Batch 175/537: Loss=1.6025 (C:0.6895, R:0.0091)
Batch 200/537: Loss=1.5650 (C:0.6485, R:0.0092)
Batch 225/537: Loss=1.6189 (C:0.7041, R:0.0091)
Batch 250/537: Loss=1.5758 (C:0.6678, R:0.0091)
Batch 275/537: Loss=1.5867 (C:0.6717, R:0.0092)
Batch 300/537: Loss=1.5843 (C:0.6746, R:0.0091)
Batch 325/537: Loss=1.5785 (C:0.6661, R:0.0091)
Batch 350/537: Loss=1.6461 (C:0.7314, R:0.0091)
Batch 375/537: Loss=1.6311 (C:0.7200, R:0.0091)
Batch 400/537: Loss=1.6035 (C:0.6840, R:0.0092)
Batch 425/537: Loss=1.5892 (C:0.6765, R:0.0091)
Batch 450/537: Loss=1.6213 (C:0.7068, R:0.0091)
Batch 475/537: Loss=1.5959 (C:0.6774, R:0.0092)
Batch 500/537: Loss=1.5882 (C:0.6749, R:0.0091)
Batch 525/537: Loss=1.6106 (C:0.6918, R:0.0092)

============================================================
Epoch 82/200 completed in 32.9s
Train: Loss=1.5962 (C:0.6810, R:0.0092) Ratio=5.61x
Val:   Loss=1.7282 (C:0.8238, R:0.0090) Ratio=3.32x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 83 Training
----------------------------------------
Batch   0/537: Loss=1.5949 (C:0.6808, R:0.0091)
Batch  25/537: Loss=1.5786 (C:0.6597, R:0.0092)
Batch  50/537: Loss=1.6081 (C:0.6945, R:0.0091)
Batch  75/537: Loss=1.5838 (C:0.6705, R:0.0091)
Batch 100/537: Loss=1.5860 (C:0.6676, R:0.0092)
Batch 125/537: Loss=1.5757 (C:0.6612, R:0.0091)
Batch 150/537: Loss=1.5961 (C:0.6829, R:0.0091)
Batch 175/537: Loss=1.6102 (C:0.6927, R:0.0092)
Batch 200/537: Loss=1.5792 (C:0.6615, R:0.0092)
Batch 225/537: Loss=1.6363 (C:0.7240, R:0.0091)
Batch 250/537: Loss=1.6466 (C:0.7375, R:0.0091)
Batch 275/537: Loss=1.5564 (C:0.6376, R:0.0092)
Batch 300/537: Loss=1.5621 (C:0.6503, R:0.0091)
Batch 325/537: Loss=1.6041 (C:0.6897, R:0.0091)
Batch 350/537: Loss=1.5999 (C:0.6842, R:0.0092)
Batch 375/537: Loss=1.6231 (C:0.7088, R:0.0091)
Batch 400/537: Loss=1.6376 (C:0.7222, R:0.0092)
Batch 425/537: Loss=1.5939 (C:0.6790, R:0.0091)
Batch 450/537: Loss=1.5888 (C:0.6772, R:0.0091)
Batch 475/537: Loss=1.6188 (C:0.7005, R:0.0092)
Batch 500/537: Loss=1.5634 (C:0.6444, R:0.0092)
Batch 525/537: Loss=1.6130 (C:0.6999, R:0.0091)

============================================================
Epoch 83/200 completed in 24.9s
Train: Loss=1.5963 (C:0.6814, R:0.0091) Ratio=5.69x
Val:   Loss=1.7305 (C:0.8267, R:0.0090) Ratio=3.28x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 84 Training
----------------------------------------
Batch   0/537: Loss=1.5403 (C:0.6229, R:0.0092)
Batch  25/537: Loss=1.6019 (C:0.6803, R:0.0092)
Batch  50/537: Loss=1.6087 (C:0.6957, R:0.0091)
Batch  75/537: Loss=1.6091 (C:0.6888, R:0.0092)
Batch 100/537: Loss=1.6032 (C:0.6909, R:0.0091)
Batch 125/537: Loss=1.5883 (C:0.6771, R:0.0091)
Batch 150/537: Loss=1.6377 (C:0.7198, R:0.0092)
Batch 175/537: Loss=1.5678 (C:0.6555, R:0.0091)
Batch 200/537: Loss=1.6310 (C:0.7189, R:0.0091)
Batch 225/537: Loss=1.5817 (C:0.6677, R:0.0091)
Batch 250/537: Loss=1.6098 (C:0.6940, R:0.0092)
Batch 275/537: Loss=1.5882 (C:0.6720, R:0.0092)
Batch 300/537: Loss=1.5966 (C:0.6795, R:0.0092)
Batch 325/537: Loss=1.5983 (C:0.6843, R:0.0091)
Batch 350/537: Loss=1.5682 (C:0.6531, R:0.0092)
Batch 375/537: Loss=1.6104 (C:0.6897, R:0.0092)
Batch 400/537: Loss=1.5582 (C:0.6405, R:0.0092)
Batch 425/537: Loss=1.6501 (C:0.7331, R:0.0092)
Batch 450/537: Loss=1.5882 (C:0.6752, R:0.0091)
Batch 475/537: Loss=1.5721 (C:0.6605, R:0.0091)
Batch 500/537: Loss=1.5892 (C:0.6792, R:0.0091)
Batch 525/537: Loss=1.5797 (C:0.6677, R:0.0091)

============================================================
Epoch 84/200 completed in 25.0s
Train: Loss=1.5928 (C:0.6781, R:0.0091) Ratio=5.86x
Val:   Loss=1.7324 (C:0.8290, R:0.0090) Ratio=3.29x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

üåç Updating global dataset at epoch 85
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.322 ¬± 0.658
    Neg distances: 2.181 ¬± 1.003
    Separation ratio: 6.77x
    Gap: -3.386
    ‚úÖ Excellent global separation!

Epoch 85 Training
----------------------------------------
Batch   0/537: Loss=1.6022 (C:0.6816, R:0.0092)
Batch  25/537: Loss=1.5939 (C:0.6734, R:0.0092)
Batch  50/537: Loss=1.5479 (C:0.6373, R:0.0091)
Batch  75/537: Loss=1.5827 (C:0.6698, R:0.0091)
Batch 100/537: Loss=1.5109 (C:0.5967, R:0.0091)
Batch 125/537: Loss=1.5435 (C:0.6283, R:0.0092)
Batch 150/537: Loss=1.5358 (C:0.6234, R:0.0091)
Batch 175/537: Loss=1.5860 (C:0.6687, R:0.0092)
Batch 200/537: Loss=1.6134 (C:0.6966, R:0.0092)
Batch 225/537: Loss=1.6023 (C:0.6862, R:0.0092)
Batch 250/537: Loss=1.5800 (C:0.6661, R:0.0091)
Batch 275/537: Loss=1.5668 (C:0.6524, R:0.0091)
Batch 300/537: Loss=1.6052 (C:0.6907, R:0.0091)
Batch 325/537: Loss=1.5840 (C:0.6746, R:0.0091)
Batch 350/537: Loss=1.6010 (C:0.6858, R:0.0092)
Batch 375/537: Loss=1.6073 (C:0.6975, R:0.0091)
Batch 400/537: Loss=1.5905 (C:0.6749, R:0.0092)
Batch 425/537: Loss=1.5786 (C:0.6667, R:0.0091)
Batch 450/537: Loss=1.5672 (C:0.6498, R:0.0092)
Batch 475/537: Loss=1.5666 (C:0.6514, R:0.0092)
Batch 500/537: Loss=1.5795 (C:0.6624, R:0.0092)
Batch 525/537: Loss=1.5480 (C:0.6303, R:0.0092)

============================================================
Epoch 85/200 completed in 32.4s
Train: Loss=1.5724 (C:0.6579, R:0.0091) Ratio=5.75x
Val:   Loss=1.7074 (C:0.8041, R:0.0090) Ratio=3.29x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7074)
============================================================

Epoch 86 Training
----------------------------------------
Batch   0/537: Loss=1.5812 (C:0.6706, R:0.0091)
Batch  25/537: Loss=1.5813 (C:0.6665, R:0.0091)
Batch  50/537: Loss=1.5797 (C:0.6690, R:0.0091)
Batch  75/537: Loss=1.5797 (C:0.6692, R:0.0091)
Batch 100/537: Loss=1.5297 (C:0.6143, R:0.0092)
Batch 125/537: Loss=1.5927 (C:0.6818, R:0.0091)
Batch 150/537: Loss=1.5485 (C:0.6352, R:0.0091)
Batch 175/537: Loss=1.5598 (C:0.6527, R:0.0091)
Batch 200/537: Loss=1.5596 (C:0.6447, R:0.0091)
Batch 225/537: Loss=1.5752 (C:0.6631, R:0.0091)
Batch 250/537: Loss=1.5669 (C:0.6528, R:0.0091)
Batch 275/537: Loss=1.5710 (C:0.6596, R:0.0091)
Batch 300/537: Loss=1.5971 (C:0.6783, R:0.0092)
Batch 325/537: Loss=1.5597 (C:0.6507, R:0.0091)
Batch 350/537: Loss=1.5510 (C:0.6378, R:0.0091)
Batch 375/537: Loss=1.5465 (C:0.6359, R:0.0091)
Batch 400/537: Loss=1.5714 (C:0.6569, R:0.0091)
Batch 425/537: Loss=1.5673 (C:0.6559, R:0.0091)
Batch 450/537: Loss=1.5749 (C:0.6585, R:0.0092)
Batch 475/537: Loss=1.5955 (C:0.6802, R:0.0092)
Batch 500/537: Loss=1.6021 (C:0.6855, R:0.0092)
Batch 525/537: Loss=1.6198 (C:0.7032, R:0.0092)

============================================================
Epoch 86/200 completed in 24.5s
Train: Loss=1.5737 (C:0.6594, R:0.0091) Ratio=5.89x
Val:   Loss=1.7049 (C:0.8016, R:0.0090) Ratio=3.26x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7049)
============================================================

Epoch 87 Training
----------------------------------------
Batch   0/537: Loss=1.5785 (C:0.6637, R:0.0091)
Batch  25/537: Loss=1.5690 (C:0.6491, R:0.0092)
Batch  50/537: Loss=1.5490 (C:0.6348, R:0.0091)
Batch  75/537: Loss=1.5965 (C:0.6838, R:0.0091)
Batch 100/537: Loss=1.5702 (C:0.6557, R:0.0091)
Batch 125/537: Loss=1.5387 (C:0.6277, R:0.0091)
Batch 150/537: Loss=1.6015 (C:0.6825, R:0.0092)
Batch 175/537: Loss=1.6051 (C:0.6924, R:0.0091)
Batch 200/537: Loss=1.5681 (C:0.6553, R:0.0091)
Batch 225/537: Loss=1.5480 (C:0.6381, R:0.0091)
Batch 250/537: Loss=1.5912 (C:0.6834, R:0.0091)
Batch 275/537: Loss=1.5610 (C:0.6462, R:0.0091)
Batch 300/537: Loss=1.5579 (C:0.6478, R:0.0091)
Batch 325/537: Loss=1.5475 (C:0.6323, R:0.0092)
Batch 350/537: Loss=1.5791 (C:0.6671, R:0.0091)
Batch 375/537: Loss=1.5604 (C:0.6493, R:0.0091)
Batch 400/537: Loss=1.5873 (C:0.6725, R:0.0091)
Batch 425/537: Loss=1.5763 (C:0.6574, R:0.0092)
Batch 450/537: Loss=1.5966 (C:0.6837, R:0.0091)
Batch 475/537: Loss=1.6089 (C:0.6996, R:0.0091)
Batch 500/537: Loss=1.5516 (C:0.6354, R:0.0092)
Batch 525/537: Loss=1.5628 (C:0.6509, R:0.0091)

============================================================
Epoch 87/200 completed in 24.4s
Train: Loss=1.5726 (C:0.6586, R:0.0091) Ratio=5.78x
Val:   Loss=1.7006 (C:0.7977, R:0.0090) Ratio=3.29x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.7006)
============================================================

üåç Updating global dataset at epoch 88
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.320 ¬± 0.634
    Neg distances: 2.174 ¬± 1.006
    Separation ratio: 6.80x
    Gap: -3.440
    ‚úÖ Excellent global separation!

Epoch 88 Training
----------------------------------------
Batch   0/537: Loss=1.5683 (C:0.6516, R:0.0092)
Batch  25/537: Loss=1.5811 (C:0.6700, R:0.0091)
Batch  50/537: Loss=1.5879 (C:0.6672, R:0.0092)
Batch  75/537: Loss=1.5787 (C:0.6597, R:0.0092)
Batch 100/537: Loss=1.5516 (C:0.6409, R:0.0091)
Batch 125/537: Loss=1.5486 (C:0.6339, R:0.0091)
Batch 150/537: Loss=1.5479 (C:0.6413, R:0.0091)
Batch 175/537: Loss=1.5508 (C:0.6395, R:0.0091)
Batch 200/537: Loss=1.5722 (C:0.6616, R:0.0091)
Batch 225/537: Loss=1.5884 (C:0.6744, R:0.0091)
Batch 250/537: Loss=1.5877 (C:0.6730, R:0.0091)
Batch 275/537: Loss=1.5714 (C:0.6551, R:0.0092)
Batch 300/537: Loss=1.5243 (C:0.6137, R:0.0091)
Batch 325/537: Loss=1.5631 (C:0.6512, R:0.0091)
Batch 350/537: Loss=1.5519 (C:0.6351, R:0.0092)
Batch 375/537: Loss=1.5670 (C:0.6532, R:0.0091)
Batch 400/537: Loss=1.5517 (C:0.6381, R:0.0091)
Batch 425/537: Loss=1.5529 (C:0.6378, R:0.0092)
Batch 450/537: Loss=1.5867 (C:0.6770, R:0.0091)
Batch 475/537: Loss=1.5888 (C:0.6767, R:0.0091)
Batch 500/537: Loss=1.5640 (C:0.6520, R:0.0091)
Batch 525/537: Loss=1.5625 (C:0.6464, R:0.0092)

============================================================
Epoch 88/200 completed in 34.4s
Train: Loss=1.5671 (C:0.6533, R:0.0091) Ratio=5.69x
Val:   Loss=1.6900 (C:0.7878, R:0.0090) Ratio=3.33x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6900)
============================================================

Epoch 89 Training
----------------------------------------
Batch   0/537: Loss=1.5909 (C:0.6769, R:0.0091)
Batch  25/537: Loss=1.5616 (C:0.6470, R:0.0091)
Batch  50/537: Loss=1.5572 (C:0.6487, R:0.0091)
Batch  75/537: Loss=1.5180 (C:0.6073, R:0.0091)
Batch 100/537: Loss=1.6244 (C:0.7135, R:0.0091)
Batch 125/537: Loss=1.5854 (C:0.6727, R:0.0091)
Batch 150/537: Loss=1.5758 (C:0.6609, R:0.0091)
Batch 175/537: Loss=1.5558 (C:0.6424, R:0.0091)
Batch 200/537: Loss=1.5852 (C:0.6714, R:0.0091)
Batch 225/537: Loss=1.5454 (C:0.6373, R:0.0091)
Batch 250/537: Loss=1.5274 (C:0.6174, R:0.0091)
Batch 275/537: Loss=1.6215 (C:0.7083, R:0.0091)
Batch 300/537: Loss=1.5304 (C:0.6207, R:0.0091)
Batch 325/537: Loss=1.5656 (C:0.6583, R:0.0091)
Batch 350/537: Loss=1.5072 (C:0.5975, R:0.0091)
Batch 375/537: Loss=1.5987 (C:0.6822, R:0.0092)
Batch 400/537: Loss=1.5938 (C:0.6805, R:0.0091)
Batch 425/537: Loss=1.5420 (C:0.6248, R:0.0092)
Batch 450/537: Loss=1.5715 (C:0.6660, R:0.0091)
Batch 475/537: Loss=1.5711 (C:0.6575, R:0.0091)
Batch 500/537: Loss=1.5674 (C:0.6527, R:0.0091)
Batch 525/537: Loss=1.5968 (C:0.6739, R:0.0092)

============================================================
Epoch 89/200 completed in 25.5s
Train: Loss=1.5653 (C:0.6519, R:0.0091) Ratio=5.77x
Val:   Loss=1.7063 (C:0.8041, R:0.0090) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 90 Training
----------------------------------------
Batch   0/537: Loss=1.6091 (C:0.6965, R:0.0091)
Batch  25/537: Loss=1.5741 (C:0.6566, R:0.0092)
Batch  50/537: Loss=1.6118 (C:0.7010, R:0.0091)
Batch  75/537: Loss=1.5865 (C:0.6749, R:0.0091)
Batch 100/537: Loss=1.6052 (C:0.6890, R:0.0092)
Batch 125/537: Loss=1.5414 (C:0.6257, R:0.0092)
Batch 150/537: Loss=1.5396 (C:0.6285, R:0.0091)
Batch 175/537: Loss=1.5779 (C:0.6631, R:0.0091)
Batch 200/537: Loss=1.5640 (C:0.6501, R:0.0091)
Batch 225/537: Loss=1.5874 (C:0.6726, R:0.0091)
Batch 250/537: Loss=1.5510 (C:0.6355, R:0.0092)
Batch 275/537: Loss=1.5434 (C:0.6308, R:0.0091)
Batch 300/537: Loss=1.5647 (C:0.6534, R:0.0091)
Batch 325/537: Loss=1.5790 (C:0.6684, R:0.0091)
Batch 350/537: Loss=1.5444 (C:0.6324, R:0.0091)
Batch 375/537: Loss=1.5781 (C:0.6642, R:0.0091)
Batch 400/537: Loss=1.5665 (C:0.6534, R:0.0091)
Batch 425/537: Loss=1.5733 (C:0.6642, R:0.0091)
Batch 450/537: Loss=1.5702 (C:0.6546, R:0.0092)
Batch 475/537: Loss=1.5424 (C:0.6246, R:0.0092)
Batch 500/537: Loss=1.5839 (C:0.6775, R:0.0091)
Batch 525/537: Loss=1.5641 (C:0.6565, R:0.0091)

============================================================
Epoch 90/200 completed in 25.0s
Train: Loss=1.5657 (C:0.6528, R:0.0091) Ratio=5.70x
Val:   Loss=1.7092 (C:0.8077, R:0.0090) Ratio=3.33x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 91
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.328 ¬± 0.658
    Neg distances: 2.209 ¬± 1.019
    Separation ratio: 6.73x
    Gap: -3.490
    ‚úÖ Excellent global separation!

Epoch 91 Training
----------------------------------------
Batch   0/537: Loss=1.5505 (C:0.6427, R:0.0091)
Batch  25/537: Loss=1.5877 (C:0.6724, R:0.0092)
Batch  50/537: Loss=1.5306 (C:0.6168, R:0.0091)
Batch  75/537: Loss=1.5891 (C:0.6798, R:0.0091)
Batch 100/537: Loss=1.5522 (C:0.6386, R:0.0091)
Batch 125/537: Loss=1.5661 (C:0.6537, R:0.0091)
Batch 150/537: Loss=1.5639 (C:0.6531, R:0.0091)
Batch 175/537: Loss=1.5687 (C:0.6564, R:0.0091)
Batch 200/537: Loss=1.5692 (C:0.6551, R:0.0091)
Batch 225/537: Loss=1.6017 (C:0.6889, R:0.0091)
Batch 250/537: Loss=1.5375 (C:0.6286, R:0.0091)
Batch 275/537: Loss=1.5511 (C:0.6413, R:0.0091)
Batch 300/537: Loss=1.5338 (C:0.6218, R:0.0091)
Batch 325/537: Loss=1.5511 (C:0.6374, R:0.0091)
Batch 350/537: Loss=1.5766 (C:0.6633, R:0.0091)
Batch 375/537: Loss=1.5443 (C:0.6313, R:0.0091)
Batch 400/537: Loss=1.6055 (C:0.6890, R:0.0092)
Batch 425/537: Loss=1.5926 (C:0.6794, R:0.0091)
Batch 450/537: Loss=1.5670 (C:0.6571, R:0.0091)
Batch 475/537: Loss=1.5761 (C:0.6669, R:0.0091)
Batch 500/537: Loss=1.5388 (C:0.6251, R:0.0091)
Batch 525/537: Loss=1.5540 (C:0.6397, R:0.0091)

============================================================
Epoch 91/200 completed in 33.3s
Train: Loss=1.5589 (C:0.6466, R:0.0091) Ratio=5.62x
Val:   Loss=1.6932 (C:0.7924, R:0.0090) Ratio=3.34x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 92 Training
----------------------------------------
Batch   0/537: Loss=1.6043 (C:0.6914, R:0.0091)
Batch  25/537: Loss=1.5460 (C:0.6379, R:0.0091)
Batch  50/537: Loss=1.6065 (C:0.6857, R:0.0092)
Batch  75/537: Loss=1.5803 (C:0.6646, R:0.0092)
Batch 100/537: Loss=1.5923 (C:0.6806, R:0.0091)
Batch 125/537: Loss=1.5740 (C:0.6619, R:0.0091)
Batch 150/537: Loss=1.5593 (C:0.6477, R:0.0091)
Batch 175/537: Loss=1.5887 (C:0.6774, R:0.0091)
Batch 200/537: Loss=1.5577 (C:0.6457, R:0.0091)
Batch 225/537: Loss=1.5560 (C:0.6458, R:0.0091)
Batch 250/537: Loss=1.5596 (C:0.6478, R:0.0091)
Batch 275/537: Loss=1.5633 (C:0.6551, R:0.0091)
Batch 300/537: Loss=1.5421 (C:0.6265, R:0.0092)
Batch 325/537: Loss=1.6191 (C:0.7076, R:0.0091)
Batch 350/537: Loss=1.5699 (C:0.6648, R:0.0091)
Batch 375/537: Loss=1.5309 (C:0.6165, R:0.0091)
Batch 400/537: Loss=1.5180 (C:0.6060, R:0.0091)
Batch 425/537: Loss=1.5182 (C:0.6074, R:0.0091)
Batch 450/537: Loss=1.5653 (C:0.6520, R:0.0091)
Batch 475/537: Loss=1.5244 (C:0.6183, R:0.0091)
Batch 500/537: Loss=1.5334 (C:0.6242, R:0.0091)
Batch 525/537: Loss=1.5417 (C:0.6347, R:0.0091)

============================================================
Epoch 92/200 completed in 23.8s
Train: Loss=1.5558 (C:0.6446, R:0.0091) Ratio=5.71x
Val:   Loss=1.6999 (C:0.8002, R:0.0090) Ratio=3.28x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 93 Training
----------------------------------------
Batch   0/537: Loss=1.5620 (C:0.6514, R:0.0091)
Batch  25/537: Loss=1.5250 (C:0.6143, R:0.0091)
Batch  50/537: Loss=1.5780 (C:0.6718, R:0.0091)
Batch  75/537: Loss=1.5729 (C:0.6656, R:0.0091)
Batch 100/537: Loss=1.5967 (C:0.6874, R:0.0091)
Batch 125/537: Loss=1.5399 (C:0.6310, R:0.0091)
Batch 150/537: Loss=1.5960 (C:0.6889, R:0.0091)
Batch 175/537: Loss=1.5519 (C:0.6435, R:0.0091)
Batch 200/537: Loss=1.5745 (C:0.6638, R:0.0091)
Batch 225/537: Loss=1.5720 (C:0.6634, R:0.0091)
Batch 250/537: Loss=1.5493 (C:0.6380, R:0.0091)
Batch 275/537: Loss=1.5462 (C:0.6388, R:0.0091)
Batch 300/537: Loss=1.5768 (C:0.6673, R:0.0091)
Batch 325/537: Loss=1.5696 (C:0.6603, R:0.0091)
Batch 350/537: Loss=1.5727 (C:0.6630, R:0.0091)
Batch 375/537: Loss=1.6103 (C:0.7027, R:0.0091)
Batch 400/537: Loss=1.5726 (C:0.6623, R:0.0091)
Batch 425/537: Loss=1.5805 (C:0.6713, R:0.0091)
Batch 450/537: Loss=1.5483 (C:0.6374, R:0.0091)
Batch 475/537: Loss=1.5856 (C:0.6792, R:0.0091)
Batch 500/537: Loss=1.5265 (C:0.6178, R:0.0091)
Batch 525/537: Loss=1.5890 (C:0.6816, R:0.0091)

============================================================
Epoch 93/200 completed in 23.4s
Train: Loss=1.5550 (C:0.6454, R:0.0091) Ratio=5.62x
Val:   Loss=1.7010 (C:0.8031, R:0.0090) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 94
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.351 ¬± 0.678
    Neg distances: 2.236 ¬± 1.024
    Separation ratio: 6.37x
    Gap: -3.565
    ‚úÖ Excellent global separation!

Epoch 94 Training
----------------------------------------
Batch   0/537: Loss=1.5573 (C:0.6491, R:0.0091)
Batch  25/537: Loss=1.5568 (C:0.6452, R:0.0091)
Batch  50/537: Loss=1.5479 (C:0.6373, R:0.0091)
Batch  75/537: Loss=1.5208 (C:0.6086, R:0.0091)
Batch 100/537: Loss=1.5247 (C:0.6134, R:0.0091)
Batch 125/537: Loss=1.5308 (C:0.6206, R:0.0091)
Batch 150/537: Loss=1.5716 (C:0.6663, R:0.0091)
Batch 175/537: Loss=1.5375 (C:0.6299, R:0.0091)
Batch 200/537: Loss=1.5598 (C:0.6518, R:0.0091)
Batch 225/537: Loss=1.5232 (C:0.6171, R:0.0091)
Batch 250/537: Loss=1.5167 (C:0.6043, R:0.0091)
Batch 275/537: Loss=1.5426 (C:0.6401, R:0.0090)
Batch 300/537: Loss=1.5502 (C:0.6455, R:0.0090)
Batch 325/537: Loss=1.5784 (C:0.6733, R:0.0091)
Batch 350/537: Loss=1.5434 (C:0.6413, R:0.0090)
Batch 375/537: Loss=1.5610 (C:0.6501, R:0.0091)
Batch 400/537: Loss=1.5646 (C:0.6598, R:0.0090)
Batch 425/537: Loss=1.5719 (C:0.6698, R:0.0090)
Batch 450/537: Loss=1.5595 (C:0.6513, R:0.0091)
Batch 475/537: Loss=1.5424 (C:0.6331, R:0.0091)
Batch 500/537: Loss=1.5372 (C:0.6293, R:0.0091)
Batch 525/537: Loss=1.5707 (C:0.6628, R:0.0091)

============================================================
Epoch 94/200 completed in 30.9s
Train: Loss=1.5513 (C:0.6440, R:0.0091) Ratio=5.73x
Val:   Loss=1.6912 (C:0.7962, R:0.0090) Ratio=3.24x
Reconstruction weight: 100.000
No improvement for 6 epochs
============================================================

Epoch 95 Training
----------------------------------------
Batch   0/537: Loss=1.5213 (C:0.6198, R:0.0090)
Batch  25/537: Loss=1.5293 (C:0.6286, R:0.0090)
Batch  50/537: Loss=1.5099 (C:0.6053, R:0.0090)
Batch  75/537: Loss=1.5470 (C:0.6453, R:0.0090)
Batch 100/537: Loss=1.5728 (C:0.6668, R:0.0091)
Batch 125/537: Loss=1.5643 (C:0.6573, R:0.0091)
Batch 150/537: Loss=1.5639 (C:0.6541, R:0.0091)
Batch 175/537: Loss=1.5697 (C:0.6649, R:0.0090)
Batch 200/537: Loss=1.5388 (C:0.6328, R:0.0091)
Batch 225/537: Loss=1.6033 (C:0.6962, R:0.0091)
Batch 250/537: Loss=1.5515 (C:0.6469, R:0.0090)
Batch 275/537: Loss=1.5550 (C:0.6426, R:0.0091)
Batch 300/537: Loss=1.5466 (C:0.6436, R:0.0090)
Batch 325/537: Loss=1.5503 (C:0.6428, R:0.0091)
Batch 350/537: Loss=1.5932 (C:0.6819, R:0.0091)
Batch 375/537: Loss=1.5214 (C:0.6162, R:0.0091)
Batch 400/537: Loss=1.5925 (C:0.6836, R:0.0091)
Batch 425/537: Loss=1.5448 (C:0.6438, R:0.0090)
Batch 450/537: Loss=1.5302 (C:0.6266, R:0.0090)
Batch 475/537: Loss=1.5583 (C:0.6636, R:0.0089)
Batch 500/537: Loss=1.5486 (C:0.6472, R:0.0090)
Batch 525/537: Loss=1.5703 (C:0.6635, R:0.0091)

============================================================
Epoch 95/200 completed in 24.5s
Train: Loss=1.5501 (C:0.6452, R:0.0090) Ratio=5.62x
Val:   Loss=1.6985 (C:0.8066, R:0.0089) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 7 epochs
============================================================

Epoch 96 Training
----------------------------------------
Batch   0/537: Loss=1.5486 (C:0.6472, R:0.0090)
Batch  25/537: Loss=1.5341 (C:0.6285, R:0.0091)
Batch  50/537: Loss=1.5653 (C:0.6588, R:0.0091)
Batch  75/537: Loss=1.5394 (C:0.6379, R:0.0090)
Batch 100/537: Loss=1.5482 (C:0.6447, R:0.0090)
Batch 125/537: Loss=1.5171 (C:0.6122, R:0.0090)
Batch 150/537: Loss=1.5159 (C:0.6103, R:0.0091)
Batch 175/537: Loss=1.5360 (C:0.6357, R:0.0090)
Batch 200/537: Loss=1.5769 (C:0.6726, R:0.0090)
Batch 225/537: Loss=1.5062 (C:0.6100, R:0.0090)
Batch 250/537: Loss=1.5171 (C:0.6096, R:0.0091)
Batch 275/537: Loss=1.5266 (C:0.6235, R:0.0090)
Batch 300/537: Loss=1.5336 (C:0.6333, R:0.0090)
Batch 325/537: Loss=1.5526 (C:0.6532, R:0.0090)
Batch 350/537: Loss=1.5657 (C:0.6615, R:0.0090)
Batch 375/537: Loss=1.5149 (C:0.6146, R:0.0090)
Batch 400/537: Loss=1.5488 (C:0.6455, R:0.0090)
Batch 425/537: Loss=1.5692 (C:0.6651, R:0.0090)
Batch 450/537: Loss=1.5816 (C:0.6799, R:0.0090)
Batch 475/537: Loss=1.5400 (C:0.6376, R:0.0090)
Batch 500/537: Loss=1.5352 (C:0.6330, R:0.0090)
Batch 525/537: Loss=1.5616 (C:0.6631, R:0.0090)

============================================================
Epoch 96/200 completed in 24.2s
Train: Loss=1.5471 (C:0.6445, R:0.0090) Ratio=5.58x
Val:   Loss=1.6884 (C:0.7980, R:0.0089) Ratio=3.22x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6884)
============================================================

üåç Updating global dataset at epoch 97
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.346 ¬± 0.661
    Neg distances: 2.309 ¬± 1.037
    Separation ratio: 6.68x
    Gap: -3.643
    ‚úÖ Excellent global separation!

Epoch 97 Training
----------------------------------------
Batch   0/537: Loss=1.5271 (C:0.6285, R:0.0090)
Batch  25/537: Loss=1.5023 (C:0.6002, R:0.0090)
Batch  50/537: Loss=1.5227 (C:0.6221, R:0.0090)
Batch  75/537: Loss=1.5048 (C:0.5990, R:0.0091)
Batch 100/537: Loss=1.5064 (C:0.6120, R:0.0089)
Batch 125/537: Loss=1.4904 (C:0.5922, R:0.0090)
Batch 150/537: Loss=1.5278 (C:0.6259, R:0.0090)
Batch 175/537: Loss=1.4804 (C:0.5829, R:0.0090)
Batch 200/537: Loss=1.5255 (C:0.6192, R:0.0091)
Batch 225/537: Loss=1.5126 (C:0.6146, R:0.0090)
Batch 250/537: Loss=1.5335 (C:0.6320, R:0.0090)
Batch 275/537: Loss=1.5310 (C:0.6305, R:0.0090)
Batch 300/537: Loss=1.4520 (C:0.5534, R:0.0090)
Batch 325/537: Loss=1.5179 (C:0.6220, R:0.0090)
Batch 350/537: Loss=1.5023 (C:0.6001, R:0.0090)
Batch 375/537: Loss=1.5191 (C:0.6211, R:0.0090)
Batch 400/537: Loss=1.4959 (C:0.5961, R:0.0090)
Batch 425/537: Loss=1.5596 (C:0.6601, R:0.0090)
Batch 450/537: Loss=1.5217 (C:0.6228, R:0.0090)
Batch 475/537: Loss=1.5513 (C:0.6549, R:0.0090)
Batch 500/537: Loss=1.5017 (C:0.6000, R:0.0090)
Batch 525/537: Loss=1.4935 (C:0.5900, R:0.0090)

============================================================
Epoch 97/200 completed in 30.8s
Train: Loss=1.5198 (C:0.6197, R:0.0090) Ratio=5.70x
Val:   Loss=1.6612 (C:0.7728, R:0.0089) Ratio=3.21x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6612)
============================================================

Epoch 98 Training
----------------------------------------
Batch   0/537: Loss=1.4984 (C:0.5980, R:0.0090)
Batch  25/537: Loss=1.5522 (C:0.6540, R:0.0090)
Batch  50/537: Loss=1.5497 (C:0.6483, R:0.0090)
Batch  75/537: Loss=1.5725 (C:0.6662, R:0.0091)
Batch 100/537: Loss=1.5243 (C:0.6282, R:0.0090)
Batch 125/537: Loss=1.5267 (C:0.6293, R:0.0090)
Batch 150/537: Loss=1.5326 (C:0.6350, R:0.0090)
Batch 175/537: Loss=1.5115 (C:0.6153, R:0.0090)
Batch 200/537: Loss=1.5469 (C:0.6517, R:0.0090)
Batch 225/537: Loss=1.5253 (C:0.6295, R:0.0090)
Batch 250/537: Loss=1.5277 (C:0.6292, R:0.0090)
Batch 275/537: Loss=1.4615 (C:0.5679, R:0.0089)
Batch 300/537: Loss=1.5346 (C:0.6353, R:0.0090)
Batch 325/537: Loss=1.5510 (C:0.6556, R:0.0090)
Batch 350/537: Loss=1.4986 (C:0.6018, R:0.0090)
Batch 375/537: Loss=1.5254 (C:0.6278, R:0.0090)
Batch 400/537: Loss=1.5787 (C:0.6764, R:0.0090)
Batch 425/537: Loss=1.5246 (C:0.6260, R:0.0090)
Batch 450/537: Loss=1.5221 (C:0.6211, R:0.0090)
Batch 475/537: Loss=1.5411 (C:0.6425, R:0.0090)
Batch 500/537: Loss=1.5217 (C:0.6231, R:0.0090)
Batch 525/537: Loss=1.5538 (C:0.6552, R:0.0090)

============================================================
Epoch 98/200 completed in 23.9s
Train: Loss=1.5185 (C:0.6203, R:0.0090) Ratio=5.49x
Val:   Loss=1.6578 (C:0.7710, R:0.0089) Ratio=3.27x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6578)
============================================================

Epoch 99 Training
----------------------------------------
Batch   0/537: Loss=1.4878 (C:0.5887, R:0.0090)
Batch  25/537: Loss=1.5167 (C:0.6248, R:0.0089)
Batch  50/537: Loss=1.5519 (C:0.6475, R:0.0090)
Batch  75/537: Loss=1.5212 (C:0.6220, R:0.0090)
Batch 100/537: Loss=1.5150 (C:0.6178, R:0.0090)
Batch 125/537: Loss=1.5381 (C:0.6450, R:0.0089)
Batch 150/537: Loss=1.5509 (C:0.6481, R:0.0090)
Batch 175/537: Loss=1.4801 (C:0.5843, R:0.0090)
Batch 200/537: Loss=1.4873 (C:0.5915, R:0.0090)
Batch 225/537: Loss=1.4981 (C:0.6010, R:0.0090)
Batch 250/537: Loss=1.5334 (C:0.6373, R:0.0090)
Batch 275/537: Loss=1.5126 (C:0.6111, R:0.0090)
Batch 300/537: Loss=1.5128 (C:0.6198, R:0.0089)
Batch 325/537: Loss=1.4945 (C:0.5975, R:0.0090)
Batch 350/537: Loss=1.5061 (C:0.6108, R:0.0090)
Batch 375/537: Loss=1.5337 (C:0.6366, R:0.0090)
Batch 400/537: Loss=1.5322 (C:0.6341, R:0.0090)
Batch 425/537: Loss=1.5644 (C:0.6641, R:0.0090)
Batch 450/537: Loss=1.5358 (C:0.6353, R:0.0090)
Batch 475/537: Loss=1.5449 (C:0.6467, R:0.0090)
Batch 500/537: Loss=1.5224 (C:0.6256, R:0.0090)
Batch 525/537: Loss=1.5157 (C:0.6164, R:0.0090)

============================================================
Epoch 99/200 completed in 24.4s
Train: Loss=1.5148 (C:0.6178, R:0.0090) Ratio=5.56x
Val:   Loss=1.6581 (C:0.7738, R:0.0088) Ratio=3.24x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 100
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.358 ¬± 0.687
    Neg distances: 2.341 ¬± 1.050
    Separation ratio: 6.53x
    Gap: -3.687
    ‚úÖ Excellent global separation!

Epoch 100 Training
----------------------------------------
Batch   0/537: Loss=1.5506 (C:0.6562, R:0.0089)
Batch  25/537: Loss=1.5141 (C:0.6188, R:0.0090)
Batch  50/537: Loss=1.5008 (C:0.6110, R:0.0089)
Batch  75/537: Loss=1.5602 (C:0.6676, R:0.0089)
Batch 100/537: Loss=1.4830 (C:0.5824, R:0.0090)
Batch 125/537: Loss=1.5047 (C:0.6103, R:0.0089)
Batch 150/537: Loss=1.4994 (C:0.6024, R:0.0090)
Batch 175/537: Loss=1.5021 (C:0.6066, R:0.0090)
Batch 200/537: Loss=1.5224 (C:0.6273, R:0.0090)
Batch 225/537: Loss=1.5311 (C:0.6314, R:0.0090)
Batch 250/537: Loss=1.5267 (C:0.6327, R:0.0089)
Batch 275/537: Loss=1.5603 (C:0.6624, R:0.0090)
Batch 300/537: Loss=1.4961 (C:0.6008, R:0.0090)
Batch 325/537: Loss=1.5012 (C:0.6025, R:0.0090)
Batch 350/537: Loss=1.5046 (C:0.6082, R:0.0090)
Batch 375/537: Loss=1.4990 (C:0.6008, R:0.0090)
Batch 400/537: Loss=1.4357 (C:0.5386, R:0.0090)
Batch 425/537: Loss=1.5168 (C:0.6256, R:0.0089)
Batch 450/537: Loss=1.5025 (C:0.6101, R:0.0089)
Batch 475/537: Loss=1.5020 (C:0.6022, R:0.0090)
Batch 500/537: Loss=1.5275 (C:0.6301, R:0.0090)
Batch 525/537: Loss=1.5098 (C:0.6141, R:0.0090)

============================================================
Epoch 100/200 completed in 31.6s
Train: Loss=1.5101 (C:0.6143, R:0.0090) Ratio=5.66x
Val:   Loss=1.6591 (C:0.7754, R:0.0088) Ratio=3.26x
Reconstruction weight: 100.000
No improvement for 2 epochs
Checkpoint saved at epoch 100
============================================================

Epoch 101 Training
----------------------------------------
Batch   0/537: Loss=1.4797 (C:0.5863, R:0.0089)
Batch  25/537: Loss=1.5034 (C:0.6137, R:0.0089)
Batch  50/537: Loss=1.4932 (C:0.5969, R:0.0090)
Batch  75/537: Loss=1.4865 (C:0.5899, R:0.0090)
Batch 100/537: Loss=1.5311 (C:0.6344, R:0.0090)
Batch 125/537: Loss=1.5193 (C:0.6228, R:0.0090)
Batch 150/537: Loss=1.5376 (C:0.6447, R:0.0089)
Batch 175/537: Loss=1.4827 (C:0.5878, R:0.0089)
Batch 200/537: Loss=1.4830 (C:0.5906, R:0.0089)
Batch 225/537: Loss=1.5109 (C:0.6176, R:0.0089)
Batch 250/537: Loss=1.5087 (C:0.6130, R:0.0090)
Batch 275/537: Loss=1.5356 (C:0.6347, R:0.0090)
Batch 300/537: Loss=1.4955 (C:0.5961, R:0.0090)
Batch 325/537: Loss=1.5315 (C:0.6361, R:0.0090)
Batch 350/537: Loss=1.5178 (C:0.6235, R:0.0089)
Batch 375/537: Loss=1.5187 (C:0.6226, R:0.0090)
Batch 400/537: Loss=1.4729 (C:0.5815, R:0.0089)
Batch 425/537: Loss=1.4979 (C:0.6008, R:0.0090)
Batch 450/537: Loss=1.5059 (C:0.6159, R:0.0089)
Batch 475/537: Loss=1.5502 (C:0.6563, R:0.0089)
Batch 500/537: Loss=1.5013 (C:0.6062, R:0.0090)
Batch 525/537: Loss=1.5349 (C:0.6409, R:0.0089)

============================================================
Epoch 101/200 completed in 24.2s
Train: Loss=1.5104 (C:0.6156, R:0.0089) Ratio=5.40x
Val:   Loss=1.6524 (C:0.7691, R:0.0088) Ratio=3.21x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6524)
============================================================

Epoch 102 Training
----------------------------------------
Batch   0/537: Loss=1.5344 (C:0.6384, R:0.0090)
Batch  25/537: Loss=1.5189 (C:0.6268, R:0.0089)
Batch  50/537: Loss=1.5338 (C:0.6435, R:0.0089)
Batch  75/537: Loss=1.4883 (C:0.5908, R:0.0090)
Batch 100/537: Loss=1.5323 (C:0.6404, R:0.0089)
Batch 125/537: Loss=1.5191 (C:0.6317, R:0.0089)
Batch 150/537: Loss=1.5195 (C:0.6278, R:0.0089)
Batch 175/537: Loss=1.5092 (C:0.6159, R:0.0089)
Batch 200/537: Loss=1.4692 (C:0.5754, R:0.0089)
Batch 225/537: Loss=1.4906 (C:0.5981, R:0.0089)
Batch 250/537: Loss=1.4965 (C:0.6030, R:0.0089)
Batch 275/537: Loss=1.4929 (C:0.5992, R:0.0089)
Batch 300/537: Loss=1.5146 (C:0.6191, R:0.0090)
Batch 325/537: Loss=1.5191 (C:0.6215, R:0.0090)
Batch 350/537: Loss=1.5220 (C:0.6295, R:0.0089)
Batch 375/537: Loss=1.4442 (C:0.5499, R:0.0089)
Batch 400/537: Loss=1.5112 (C:0.6199, R:0.0089)
Batch 425/537: Loss=1.5319 (C:0.6401, R:0.0089)
Batch 450/537: Loss=1.5091 (C:0.6153, R:0.0089)
Batch 475/537: Loss=1.5474 (C:0.6483, R:0.0090)
Batch 500/537: Loss=1.4966 (C:0.6018, R:0.0089)
Batch 525/537: Loss=1.5147 (C:0.6202, R:0.0089)

============================================================
Epoch 102/200 completed in 23.5s
Train: Loss=1.5065 (C:0.6124, R:0.0089) Ratio=5.62x
Val:   Loss=1.6500 (C:0.7684, R:0.0088) Ratio=3.35x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6500)
============================================================

üåç Updating global dataset at epoch 103
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.362 ¬± 0.713
    Neg distances: 2.393 ¬± 1.059
    Separation ratio: 6.61x
    Gap: -3.737
    ‚úÖ Excellent global separation!

Epoch 103 Training
----------------------------------------
Batch   0/537: Loss=1.4827 (C:0.5921, R:0.0089)
Batch  25/537: Loss=1.4871 (C:0.5920, R:0.0090)
Batch  50/537: Loss=1.5054 (C:0.6103, R:0.0090)
Batch  75/537: Loss=1.5009 (C:0.6044, R:0.0090)
Batch 100/537: Loss=1.4768 (C:0.5857, R:0.0089)
Batch 125/537: Loss=1.4883 (C:0.5928, R:0.0090)
Batch 150/537: Loss=1.4916 (C:0.6002, R:0.0089)
Batch 175/537: Loss=1.4802 (C:0.5871, R:0.0089)
Batch 200/537: Loss=1.5009 (C:0.6024, R:0.0090)
Batch 225/537: Loss=1.5111 (C:0.6192, R:0.0089)
Batch 250/537: Loss=1.4544 (C:0.5621, R:0.0089)
Batch 275/537: Loss=1.4987 (C:0.6088, R:0.0089)
Batch 300/537: Loss=1.4930 (C:0.5969, R:0.0090)
Batch 325/537: Loss=1.4442 (C:0.5539, R:0.0089)
Batch 350/537: Loss=1.4390 (C:0.5468, R:0.0089)
Batch 375/537: Loss=1.5264 (C:0.6317, R:0.0089)
Batch 400/537: Loss=1.4806 (C:0.5838, R:0.0090)
Batch 425/537: Loss=1.5388 (C:0.6427, R:0.0090)
Batch 450/537: Loss=1.5492 (C:0.6529, R:0.0090)
Batch 475/537: Loss=1.5229 (C:0.6301, R:0.0089)
Batch 500/537: Loss=1.4994 (C:0.6050, R:0.0089)
Batch 525/537: Loss=1.4711 (C:0.5772, R:0.0089)

============================================================
Epoch 103/200 completed in 30.8s
Train: Loss=1.4903 (C:0.5969, R:0.0089) Ratio=5.72x
Val:   Loss=1.6276 (C:0.7457, R:0.0088) Ratio=3.31x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6276)
============================================================

Epoch 104 Training
----------------------------------------
Batch   0/537: Loss=1.4871 (C:0.5940, R:0.0089)
Batch  25/537: Loss=1.4979 (C:0.6053, R:0.0089)
Batch  50/537: Loss=1.4893 (C:0.5969, R:0.0089)
Batch  75/537: Loss=1.4399 (C:0.5488, R:0.0089)
Batch 100/537: Loss=1.4710 (C:0.5767, R:0.0089)
Batch 125/537: Loss=1.4903 (C:0.5966, R:0.0089)
Batch 150/537: Loss=1.5337 (C:0.6364, R:0.0090)
Batch 175/537: Loss=1.5246 (C:0.6317, R:0.0089)
Batch 200/537: Loss=1.5094 (C:0.6142, R:0.0090)
Batch 225/537: Loss=1.4958 (C:0.6020, R:0.0089)
Batch 250/537: Loss=1.5134 (C:0.6172, R:0.0090)
Batch 275/537: Loss=1.4821 (C:0.5915, R:0.0089)
Batch 300/537: Loss=1.5108 (C:0.6213, R:0.0089)
Batch 325/537: Loss=1.4880 (C:0.5961, R:0.0089)
Batch 350/537: Loss=1.5496 (C:0.6565, R:0.0089)
Batch 375/537: Loss=1.5244 (C:0.6272, R:0.0090)
Batch 400/537: Loss=1.5062 (C:0.6098, R:0.0090)
Batch 425/537: Loss=1.4383 (C:0.5448, R:0.0089)
Batch 450/537: Loss=1.4892 (C:0.6026, R:0.0089)
Batch 475/537: Loss=1.4898 (C:0.5923, R:0.0090)
Batch 500/537: Loss=1.5125 (C:0.6185, R:0.0089)
Batch 525/537: Loss=1.4746 (C:0.5819, R:0.0089)

============================================================
Epoch 104/200 completed in 23.8s
Train: Loss=1.4893 (C:0.5966, R:0.0089) Ratio=5.58x
Val:   Loss=1.6399 (C:0.7590, R:0.0088) Ratio=3.22x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 105 Training
----------------------------------------
Batch   0/537: Loss=1.5160 (C:0.6253, R:0.0089)
Batch  25/537: Loss=1.4786 (C:0.5849, R:0.0089)
Batch  50/537: Loss=1.4517 (C:0.5580, R:0.0089)
Batch  75/537: Loss=1.5003 (C:0.6107, R:0.0089)
Batch 100/537: Loss=1.5213 (C:0.6276, R:0.0089)
Batch 125/537: Loss=1.4747 (C:0.5836, R:0.0089)
Batch 150/537: Loss=1.4778 (C:0.5851, R:0.0089)
Batch 175/537: Loss=1.4569 (C:0.5656, R:0.0089)
Batch 200/537: Loss=1.5110 (C:0.6178, R:0.0089)
Batch 225/537: Loss=1.5193 (C:0.6290, R:0.0089)
Batch 250/537: Loss=1.4723 (C:0.5832, R:0.0089)
Batch 275/537: Loss=1.4351 (C:0.5437, R:0.0089)
Batch 300/537: Loss=1.5222 (C:0.6329, R:0.0089)
Batch 325/537: Loss=1.4508 (C:0.5573, R:0.0089)
Batch 350/537: Loss=1.4595 (C:0.5735, R:0.0089)
Batch 375/537: Loss=1.4885 (C:0.6015, R:0.0089)
Batch 400/537: Loss=1.5048 (C:0.6086, R:0.0090)
Batch 425/537: Loss=1.4929 (C:0.6073, R:0.0089)
Batch 450/537: Loss=1.4823 (C:0.5908, R:0.0089)
Batch 475/537: Loss=1.4818 (C:0.5889, R:0.0089)
Batch 500/537: Loss=1.4825 (C:0.5915, R:0.0089)
Batch 525/537: Loss=1.5125 (C:0.6227, R:0.0089)

============================================================
Epoch 105/200 completed in 23.8s
Train: Loss=1.4890 (C:0.5966, R:0.0089) Ratio=5.71x
Val:   Loss=1.6330 (C:0.7515, R:0.0088) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 106
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.374 ¬± 0.719
    Neg distances: 2.398 ¬± 1.077
    Separation ratio: 6.41x
    Gap: -3.778
    ‚úÖ Excellent global separation!

Epoch 106 Training
----------------------------------------
Batch   0/537: Loss=1.5219 (C:0.6279, R:0.0089)
Batch  25/537: Loss=1.4800 (C:0.5935, R:0.0089)
Batch  50/537: Loss=1.5002 (C:0.6047, R:0.0090)
Batch  75/537: Loss=1.4683 (C:0.5779, R:0.0089)
Batch 100/537: Loss=1.4827 (C:0.5860, R:0.0090)
Batch 125/537: Loss=1.4952 (C:0.6076, R:0.0089)
Batch 150/537: Loss=1.5277 (C:0.6387, R:0.0089)
Batch 175/537: Loss=1.4663 (C:0.5739, R:0.0089)
Batch 200/537: Loss=1.5305 (C:0.6438, R:0.0089)
Batch 225/537: Loss=1.4888 (C:0.6026, R:0.0089)
Batch 250/537: Loss=1.4585 (C:0.5704, R:0.0089)
Batch 275/537: Loss=1.4977 (C:0.6047, R:0.0089)
Batch 300/537: Loss=1.4859 (C:0.5963, R:0.0089)
Batch 325/537: Loss=1.4650 (C:0.5714, R:0.0089)
Batch 350/537: Loss=1.4865 (C:0.5969, R:0.0089)
Batch 375/537: Loss=1.4945 (C:0.6063, R:0.0089)
Batch 400/537: Loss=1.5215 (C:0.6272, R:0.0089)
Batch 425/537: Loss=1.4925 (C:0.5989, R:0.0089)
Batch 450/537: Loss=1.5046 (C:0.6135, R:0.0089)
Batch 475/537: Loss=1.5383 (C:0.6424, R:0.0090)
Batch 500/537: Loss=1.4841 (C:0.5944, R:0.0089)
Batch 525/537: Loss=1.4724 (C:0.5747, R:0.0090)

============================================================
Epoch 106/200 completed in 30.4s
Train: Loss=1.4925 (C:0.6005, R:0.0089) Ratio=5.66x
Val:   Loss=1.6368 (C:0.7568, R:0.0088) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 107 Training
----------------------------------------
Batch   0/537: Loss=1.5058 (C:0.6175, R:0.0089)
Batch  25/537: Loss=1.4601 (C:0.5705, R:0.0089)
Batch  50/537: Loss=1.4932 (C:0.6020, R:0.0089)
Batch  75/537: Loss=1.4702 (C:0.5786, R:0.0089)
Batch 100/537: Loss=1.4838 (C:0.5930, R:0.0089)
Batch 125/537: Loss=1.4913 (C:0.5973, R:0.0089)
Batch 150/537: Loss=1.4984 (C:0.6029, R:0.0090)
Batch 175/537: Loss=1.5279 (C:0.6373, R:0.0089)
Batch 200/537: Loss=1.5219 (C:0.6292, R:0.0089)
Batch 225/537: Loss=1.5128 (C:0.6243, R:0.0089)
Batch 250/537: Loss=1.5114 (C:0.6158, R:0.0090)
Batch 275/537: Loss=1.4684 (C:0.5773, R:0.0089)
Batch 300/537: Loss=1.4783 (C:0.5865, R:0.0089)
Batch 325/537: Loss=1.5072 (C:0.6154, R:0.0089)
Batch 350/537: Loss=1.4683 (C:0.5809, R:0.0089)
Batch 375/537: Loss=1.5009 (C:0.6086, R:0.0089)
Batch 400/537: Loss=1.5336 (C:0.6425, R:0.0089)
Batch 425/537: Loss=1.4791 (C:0.5862, R:0.0089)
Batch 450/537: Loss=1.4847 (C:0.5953, R:0.0089)
Batch 475/537: Loss=1.4627 (C:0.5717, R:0.0089)
Batch 500/537: Loss=1.5156 (C:0.6193, R:0.0090)
Batch 525/537: Loss=1.5132 (C:0.6164, R:0.0090)

============================================================
Epoch 107/200 completed in 23.1s
Train: Loss=1.4916 (C:0.6001, R:0.0089) Ratio=5.62x
Val:   Loss=1.6320 (C:0.7519, R:0.0088) Ratio=3.26x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

Epoch 108 Training
----------------------------------------
Batch   0/537: Loss=1.4954 (C:0.6065, R:0.0089)
Batch  25/537: Loss=1.4830 (C:0.5911, R:0.0089)
Batch  50/537: Loss=1.4894 (C:0.6012, R:0.0089)
Batch  75/537: Loss=1.5092 (C:0.6116, R:0.0090)
Batch 100/537: Loss=1.4788 (C:0.5852, R:0.0089)
Batch 125/537: Loss=1.4592 (C:0.5650, R:0.0089)
Batch 150/537: Loss=1.4599 (C:0.5643, R:0.0090)
Batch 175/537: Loss=1.5056 (C:0.6135, R:0.0089)
Batch 200/537: Loss=1.5551 (C:0.6636, R:0.0089)
Batch 225/537: Loss=1.5229 (C:0.6325, R:0.0089)
Batch 250/537: Loss=1.4714 (C:0.5792, R:0.0089)
Batch 275/537: Loss=1.5100 (C:0.6206, R:0.0089)
Batch 300/537: Loss=1.4803 (C:0.5914, R:0.0089)
Batch 325/537: Loss=1.4885 (C:0.6017, R:0.0089)
Batch 350/537: Loss=1.5075 (C:0.6161, R:0.0089)
Batch 375/537: Loss=1.4619 (C:0.5745, R:0.0089)
Batch 400/537: Loss=1.5019 (C:0.6124, R:0.0089)
Batch 425/537: Loss=1.4934 (C:0.6001, R:0.0089)
Batch 450/537: Loss=1.4467 (C:0.5552, R:0.0089)
Batch 475/537: Loss=1.5112 (C:0.6190, R:0.0089)
Batch 500/537: Loss=1.5192 (C:0.6290, R:0.0089)
Batch 525/537: Loss=1.4900 (C:0.5962, R:0.0089)

============================================================
Epoch 108/200 completed in 23.3s
Train: Loss=1.4904 (C:0.5991, R:0.0089) Ratio=5.63x
Val:   Loss=1.6299 (C:0.7506, R:0.0088) Ratio=3.33x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

üåç Updating global dataset at epoch 109
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.360 ¬± 0.703
    Neg distances: 2.456 ¬± 1.086
    Separation ratio: 6.82x
    Gap: -3.827
    ‚úÖ Excellent global separation!

Epoch 109 Training
----------------------------------------
Batch   0/537: Loss=1.4592 (C:0.5651, R:0.0089)
Batch  25/537: Loss=1.4620 (C:0.5708, R:0.0089)
Batch  50/537: Loss=1.4941 (C:0.6006, R:0.0089)
Batch  75/537: Loss=1.4744 (C:0.5871, R:0.0089)
Batch 100/537: Loss=1.4666 (C:0.5756, R:0.0089)
Batch 125/537: Loss=1.4677 (C:0.5774, R:0.0089)
Batch 150/537: Loss=1.4717 (C:0.5834, R:0.0089)
Batch 175/537: Loss=1.4582 (C:0.5678, R:0.0089)
Batch 200/537: Loss=1.4859 (C:0.5964, R:0.0089)
Batch 225/537: Loss=1.4658 (C:0.5737, R:0.0089)
Batch 250/537: Loss=1.4297 (C:0.5388, R:0.0089)
Batch 275/537: Loss=1.5137 (C:0.6186, R:0.0090)
Batch 300/537: Loss=1.4905 (C:0.6027, R:0.0089)
Batch 325/537: Loss=1.4515 (C:0.5596, R:0.0089)
Batch 350/537: Loss=1.4674 (C:0.5792, R:0.0089)
Batch 375/537: Loss=1.5056 (C:0.6128, R:0.0089)
Batch 400/537: Loss=1.4361 (C:0.5514, R:0.0088)
Batch 425/537: Loss=1.5153 (C:0.6272, R:0.0089)
Batch 450/537: Loss=1.4776 (C:0.5856, R:0.0089)
Batch 475/537: Loss=1.4854 (C:0.5964, R:0.0089)
Batch 500/537: Loss=1.5277 (C:0.6358, R:0.0089)
Batch 525/537: Loss=1.4592 (C:0.5676, R:0.0089)

============================================================
Epoch 109/200 completed in 30.6s
Train: Loss=1.4640 (C:0.5730, R:0.0089) Ratio=5.32x
Val:   Loss=1.6076 (C:0.7284, R:0.0088) Ratio=3.27x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.6076)
============================================================

Epoch 110 Training
----------------------------------------
Batch   0/537: Loss=1.4266 (C:0.5332, R:0.0089)
Batch  25/537: Loss=1.4397 (C:0.5495, R:0.0089)
Batch  50/537: Loss=1.4427 (C:0.5558, R:0.0089)
Batch  75/537: Loss=1.4491 (C:0.5644, R:0.0088)
Batch 100/537: Loss=1.4818 (C:0.5904, R:0.0089)
Batch 125/537: Loss=1.4484 (C:0.5590, R:0.0089)
Batch 150/537: Loss=1.4558 (C:0.5640, R:0.0089)
Batch 175/537: Loss=1.4720 (C:0.5856, R:0.0089)
Batch 200/537: Loss=1.4311 (C:0.5444, R:0.0089)
Batch 225/537: Loss=1.4658 (C:0.5757, R:0.0089)
Batch 250/537: Loss=1.4290 (C:0.5332, R:0.0090)
Batch 275/537: Loss=1.4735 (C:0.5818, R:0.0089)
Batch 300/537: Loss=1.5141 (C:0.6236, R:0.0089)
Batch 325/537: Loss=1.5056 (C:0.6122, R:0.0089)
Batch 350/537: Loss=1.4906 (C:0.6014, R:0.0089)
Batch 375/537: Loss=1.4487 (C:0.5539, R:0.0089)
Batch 400/537: Loss=1.4716 (C:0.5823, R:0.0089)
Batch 425/537: Loss=1.4536 (C:0.5626, R:0.0089)
Batch 450/537: Loss=1.4731 (C:0.5794, R:0.0089)
Batch 475/537: Loss=1.4900 (C:0.6058, R:0.0088)
Batch 500/537: Loss=1.4742 (C:0.5870, R:0.0089)
Batch 525/537: Loss=1.4769 (C:0.5835, R:0.0089)

============================================================
Epoch 110/200 completed in 23.3s
Train: Loss=1.4649 (C:0.5742, R:0.0089) Ratio=5.58x
Val:   Loss=1.6101 (C:0.7308, R:0.0088) Ratio=3.26x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

Epoch 111 Training
----------------------------------------
Batch   0/537: Loss=1.4597 (C:0.5670, R:0.0089)
Batch  25/537: Loss=1.4643 (C:0.5691, R:0.0090)
Batch  50/537: Loss=1.4453 (C:0.5582, R:0.0089)
Batch  75/537: Loss=1.4546 (C:0.5675, R:0.0089)
Batch 100/537: Loss=1.4762 (C:0.5848, R:0.0089)
Batch 125/537: Loss=1.4665 (C:0.5773, R:0.0089)
Batch 150/537: Loss=1.4829 (C:0.5909, R:0.0089)
Batch 175/537: Loss=1.4706 (C:0.5819, R:0.0089)
Batch 200/537: Loss=1.4250 (C:0.5318, R:0.0089)
Batch 225/537: Loss=1.4837 (C:0.5962, R:0.0089)
Batch 250/537: Loss=1.4576 (C:0.5728, R:0.0088)
Batch 275/537: Loss=1.4317 (C:0.5411, R:0.0089)
Batch 300/537: Loss=1.4561 (C:0.5702, R:0.0089)
Batch 325/537: Loss=1.5071 (C:0.6226, R:0.0088)
Batch 350/537: Loss=1.4586 (C:0.5669, R:0.0089)
Batch 375/537: Loss=1.4275 (C:0.5423, R:0.0089)
Batch 400/537: Loss=1.4796 (C:0.5918, R:0.0089)
Batch 425/537: Loss=1.4468 (C:0.5527, R:0.0089)
Batch 450/537: Loss=1.4594 (C:0.5681, R:0.0089)
Batch 475/537: Loss=1.5151 (C:0.6233, R:0.0089)
Batch 500/537: Loss=1.4527 (C:0.5611, R:0.0089)
Batch 525/537: Loss=1.4806 (C:0.5949, R:0.0089)

============================================================
Epoch 111/200 completed in 23.2s
Train: Loss=1.4619 (C:0.5714, R:0.0089) Ratio=5.45x
Val:   Loss=1.6172 (C:0.7388, R:0.0088) Ratio=3.27x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

üåç Updating global dataset at epoch 112
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.359 ¬± 0.718
    Neg distances: 2.494 ¬± 1.095
    Separation ratio: 6.96x
    Gap: -3.887
    ‚úÖ Excellent global separation!

Epoch 112 Training
----------------------------------------
Batch   0/537: Loss=1.3966 (C:0.5036, R:0.0089)
Batch  25/537: Loss=1.4249 (C:0.5339, R:0.0089)
Batch  50/537: Loss=1.3879 (C:0.5039, R:0.0088)
Batch  75/537: Loss=1.3986 (C:0.5140, R:0.0088)
Batch 100/537: Loss=1.4542 (C:0.5618, R:0.0089)
Batch 125/537: Loss=1.4602 (C:0.5697, R:0.0089)
Batch 150/537: Loss=1.4242 (C:0.5352, R:0.0089)
Batch 175/537: Loss=1.4690 (C:0.5743, R:0.0089)
Batch 200/537: Loss=1.4576 (C:0.5643, R:0.0089)
Batch 225/537: Loss=1.4005 (C:0.5089, R:0.0089)
Batch 250/537: Loss=1.4688 (C:0.5789, R:0.0089)
Batch 275/537: Loss=1.4566 (C:0.5721, R:0.0088)
Batch 300/537: Loss=1.4451 (C:0.5571, R:0.0089)
Batch 325/537: Loss=1.4715 (C:0.5789, R:0.0089)
Batch 350/537: Loss=1.4888 (C:0.5956, R:0.0089)
Batch 375/537: Loss=1.4473 (C:0.5540, R:0.0089)
Batch 400/537: Loss=1.4072 (C:0.5151, R:0.0089)
Batch 425/537: Loss=1.4601 (C:0.5677, R:0.0089)
Batch 450/537: Loss=1.4634 (C:0.5755, R:0.0089)
Batch 475/537: Loss=1.4457 (C:0.5568, R:0.0089)
Batch 500/537: Loss=1.4244 (C:0.5380, R:0.0089)
Batch 525/537: Loss=1.4717 (C:0.5805, R:0.0089)

============================================================
Epoch 112/200 completed in 30.2s
Train: Loss=1.4420 (C:0.5516, R:0.0089) Ratio=5.65x
Val:   Loss=1.5862 (C:0.7074, R:0.0088) Ratio=3.31x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5862)
============================================================

Epoch 113 Training
----------------------------------------
Batch   0/537: Loss=1.4431 (C:0.5498, R:0.0089)
Batch  25/537: Loss=1.4822 (C:0.5893, R:0.0089)
Batch  50/537: Loss=1.4346 (C:0.5443, R:0.0089)
Batch  75/537: Loss=1.4656 (C:0.5812, R:0.0088)
Batch 100/537: Loss=1.4391 (C:0.5493, R:0.0089)
Batch 125/537: Loss=1.4730 (C:0.5789, R:0.0089)
Batch 150/537: Loss=1.4273 (C:0.5325, R:0.0089)
Batch 175/537: Loss=1.4663 (C:0.5752, R:0.0089)
Batch 200/537: Loss=1.4609 (C:0.5711, R:0.0089)
Batch 225/537: Loss=1.4156 (C:0.5251, R:0.0089)
Batch 250/537: Loss=1.4086 (C:0.5221, R:0.0089)
Batch 275/537: Loss=1.4276 (C:0.5390, R:0.0089)
Batch 300/537: Loss=1.4784 (C:0.5853, R:0.0089)
Batch 325/537: Loss=1.4824 (C:0.5926, R:0.0089)
Batch 350/537: Loss=1.4788 (C:0.5916, R:0.0089)
Batch 375/537: Loss=1.4164 (C:0.5264, R:0.0089)
Batch 400/537: Loss=1.4678 (C:0.5758, R:0.0089)
Batch 425/537: Loss=1.4451 (C:0.5491, R:0.0090)
Batch 450/537: Loss=1.4255 (C:0.5359, R:0.0089)
Batch 475/537: Loss=1.4299 (C:0.5411, R:0.0089)
Batch 500/537: Loss=1.4271 (C:0.5390, R:0.0089)
Batch 525/537: Loss=1.4279 (C:0.5397, R:0.0089)

============================================================
Epoch 113/200 completed in 23.3s
Train: Loss=1.4418 (C:0.5517, R:0.0089) Ratio=5.61x
Val:   Loss=1.5836 (C:0.7043, R:0.0088) Ratio=3.32x
Reconstruction weight: 100.000
‚úÖ New best model saved (Val Loss: 1.5836)
============================================================

Epoch 114 Training
----------------------------------------
Batch   0/537: Loss=1.4103 (C:0.5212, R:0.0089)
Batch  25/537: Loss=1.4573 (C:0.5676, R:0.0089)
Batch  50/537: Loss=1.4819 (C:0.5968, R:0.0089)
Batch  75/537: Loss=1.4324 (C:0.5416, R:0.0089)
Batch 100/537: Loss=1.4490 (C:0.5615, R:0.0089)
Batch 125/537: Loss=1.4311 (C:0.5389, R:0.0089)
Batch 150/537: Loss=1.4109 (C:0.5200, R:0.0089)
Batch 175/537: Loss=1.4230 (C:0.5311, R:0.0089)
Batch 200/537: Loss=1.4774 (C:0.5851, R:0.0089)
Batch 225/537: Loss=1.4442 (C:0.5543, R:0.0089)
Batch 250/537: Loss=1.4520 (C:0.5617, R:0.0089)
Batch 275/537: Loss=1.4356 (C:0.5457, R:0.0089)
Batch 300/537: Loss=1.4553 (C:0.5647, R:0.0089)
Batch 325/537: Loss=1.4283 (C:0.5376, R:0.0089)
Batch 350/537: Loss=1.4641 (C:0.5817, R:0.0088)
Batch 375/537: Loss=1.4626 (C:0.5748, R:0.0089)
Batch 400/537: Loss=1.4448 (C:0.5522, R:0.0089)
Batch 425/537: Loss=1.4037 (C:0.5107, R:0.0089)
Batch 450/537: Loss=1.4715 (C:0.5794, R:0.0089)
Batch 475/537: Loss=1.4054 (C:0.5162, R:0.0089)
Batch 500/537: Loss=1.4169 (C:0.5293, R:0.0089)
Batch 525/537: Loss=1.4484 (C:0.5622, R:0.0089)

============================================================
Epoch 114/200 completed in 23.2s
Train: Loss=1.4406 (C:0.5507, R:0.0089) Ratio=5.63x
Val:   Loss=1.5922 (C:0.7141, R:0.0088) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 1 epochs
============================================================

üåç Updating global dataset at epoch 115
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.382 ¬± 0.763
    Neg distances: 2.515 ¬± 1.115
    Separation ratio: 6.58x
    Gap: -3.921
    ‚úÖ Excellent global separation!

Epoch 115 Training
----------------------------------------
Batch   0/537: Loss=1.4421 (C:0.5513, R:0.0089)
Batch  25/537: Loss=1.4057 (C:0.5203, R:0.0089)
Batch  50/537: Loss=1.4074 (C:0.5153, R:0.0089)
Batch  75/537: Loss=1.4906 (C:0.6034, R:0.0089)
Batch 100/537: Loss=1.4610 (C:0.5739, R:0.0089)
Batch 125/537: Loss=1.4556 (C:0.5639, R:0.0089)
Batch 150/537: Loss=1.4830 (C:0.5896, R:0.0089)
Batch 175/537: Loss=1.3977 (C:0.5081, R:0.0089)
Batch 200/537: Loss=1.4634 (C:0.5752, R:0.0089)
Batch 225/537: Loss=1.4418 (C:0.5527, R:0.0089)
Batch 250/537: Loss=1.4112 (C:0.5233, R:0.0089)
Batch 275/537: Loss=1.4178 (C:0.5339, R:0.0088)
Batch 300/537: Loss=1.4592 (C:0.5717, R:0.0089)
Batch 325/537: Loss=1.4348 (C:0.5436, R:0.0089)
Batch 350/537: Loss=1.4911 (C:0.6016, R:0.0089)
Batch 375/537: Loss=1.4418 (C:0.5532, R:0.0089)
Batch 400/537: Loss=1.4239 (C:0.5297, R:0.0089)
Batch 425/537: Loss=1.4664 (C:0.5815, R:0.0088)
Batch 450/537: Loss=1.4147 (C:0.5209, R:0.0089)
Batch 475/537: Loss=1.4389 (C:0.5473, R:0.0089)
Batch 500/537: Loss=1.4392 (C:0.5427, R:0.0090)
Batch 525/537: Loss=1.4210 (C:0.5372, R:0.0088)

============================================================
Epoch 115/200 completed in 30.8s
Train: Loss=1.4489 (C:0.5591, R:0.0089) Ratio=5.77x
Val:   Loss=1.6011 (C:0.7230, R:0.0088) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 2 epochs
============================================================

Epoch 116 Training
----------------------------------------
Batch   0/537: Loss=1.4560 (C:0.5636, R:0.0089)
Batch  25/537: Loss=1.4692 (C:0.5804, R:0.0089)
Batch  50/537: Loss=1.3913 (C:0.5020, R:0.0089)
Batch  75/537: Loss=1.4950 (C:0.6073, R:0.0089)
Batch 100/537: Loss=1.4580 (C:0.5694, R:0.0089)
Batch 125/537: Loss=1.4337 (C:0.5452, R:0.0089)
Batch 150/537: Loss=1.4058 (C:0.5205, R:0.0089)
Batch 175/537: Loss=1.4479 (C:0.5576, R:0.0089)
Batch 200/537: Loss=1.4380 (C:0.5538, R:0.0088)
Batch 225/537: Loss=1.4689 (C:0.5771, R:0.0089)
Batch 250/537: Loss=1.4407 (C:0.5534, R:0.0089)
Batch 275/537: Loss=1.4237 (C:0.5355, R:0.0089)
Batch 300/537: Loss=1.4543 (C:0.5637, R:0.0089)
Batch 325/537: Loss=1.4635 (C:0.5714, R:0.0089)
Batch 350/537: Loss=1.4123 (C:0.5227, R:0.0089)
Batch 375/537: Loss=1.4660 (C:0.5800, R:0.0089)
Batch 400/537: Loss=1.4279 (C:0.5402, R:0.0089)
Batch 425/537: Loss=1.4773 (C:0.5892, R:0.0089)
Batch 450/537: Loss=1.4467 (C:0.5599, R:0.0089)
Batch 475/537: Loss=1.4379 (C:0.5487, R:0.0089)
Batch 500/537: Loss=1.4742 (C:0.5857, R:0.0089)
Batch 525/537: Loss=1.4972 (C:0.6082, R:0.0089)

============================================================
Epoch 116/200 completed in 23.6s
Train: Loss=1.4538 (C:0.5643, R:0.0089) Ratio=5.61x
Val:   Loss=1.6055 (C:0.7275, R:0.0088) Ratio=3.22x
Reconstruction weight: 100.000
No improvement for 3 epochs
============================================================

Epoch 117 Training
----------------------------------------
Batch   0/537: Loss=1.4735 (C:0.5845, R:0.0089)
Batch  25/537: Loss=1.4732 (C:0.5863, R:0.0089)
Batch  50/537: Loss=1.4661 (C:0.5783, R:0.0089)
Batch  75/537: Loss=1.4618 (C:0.5733, R:0.0089)
Batch 100/537: Loss=1.4456 (C:0.5491, R:0.0090)
Batch 125/537: Loss=1.4391 (C:0.5493, R:0.0089)
Batch 150/537: Loss=1.4455 (C:0.5614, R:0.0088)
Batch 175/537: Loss=1.4662 (C:0.5751, R:0.0089)
Batch 200/537: Loss=1.4478 (C:0.5569, R:0.0089)
Batch 225/537: Loss=1.4522 (C:0.5615, R:0.0089)
Batch 250/537: Loss=1.4656 (C:0.5720, R:0.0089)
Batch 275/537: Loss=1.4138 (C:0.5283, R:0.0089)
Batch 300/537: Loss=1.4632 (C:0.5704, R:0.0089)
Batch 325/537: Loss=1.4457 (C:0.5570, R:0.0089)
Batch 350/537: Loss=1.4820 (C:0.5944, R:0.0089)
Batch 375/537: Loss=1.4631 (C:0.5722, R:0.0089)
Batch 400/537: Loss=1.4137 (C:0.5261, R:0.0089)
Batch 425/537: Loss=1.4891 (C:0.5971, R:0.0089)
Batch 450/537: Loss=1.4801 (C:0.5943, R:0.0089)
Batch 475/537: Loss=1.4263 (C:0.5399, R:0.0089)
Batch 500/537: Loss=1.4333 (C:0.5379, R:0.0090)
Batch 525/537: Loss=1.4728 (C:0.5825, R:0.0089)

============================================================
Epoch 117/200 completed in 23.5s
Train: Loss=1.4509 (C:0.5616, R:0.0089) Ratio=5.50x
Val:   Loss=1.6025 (C:0.7247, R:0.0088) Ratio=3.19x
Reconstruction weight: 100.000
No improvement for 4 epochs
============================================================

üåç Updating global dataset at epoch 118
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.391 ¬± 0.779
    Neg distances: 2.480 ¬± 1.108
    Separation ratio: 6.35x
    Gap: -3.920
    ‚úÖ Excellent global separation!

Epoch 118 Training
----------------------------------------
Batch   0/537: Loss=1.4249 (C:0.5318, R:0.0089)
Batch  25/537: Loss=1.4168 (C:0.5325, R:0.0088)
Batch  50/537: Loss=1.4696 (C:0.5778, R:0.0089)
Batch  75/537: Loss=1.4019 (C:0.5103, R:0.0089)
Batch 100/537: Loss=1.4472 (C:0.5635, R:0.0088)
Batch 125/537: Loss=1.4843 (C:0.5946, R:0.0089)
Batch 150/537: Loss=1.4511 (C:0.5661, R:0.0089)
Batch 175/537: Loss=1.4228 (C:0.5353, R:0.0089)
Batch 200/537: Loss=1.3971 (C:0.5050, R:0.0089)
Batch 225/537: Loss=1.4496 (C:0.5592, R:0.0089)
Batch 250/537: Loss=1.4035 (C:0.5156, R:0.0089)
Batch 275/537: Loss=1.4291 (C:0.5432, R:0.0089)
Batch 300/537: Loss=1.4705 (C:0.5825, R:0.0089)
Batch 325/537: Loss=1.4608 (C:0.5730, R:0.0089)
Batch 350/537: Loss=1.4859 (C:0.5941, R:0.0089)
Batch 375/537: Loss=1.4600 (C:0.5711, R:0.0089)
Batch 400/537: Loss=1.3888 (C:0.5017, R:0.0089)
Batch 425/537: Loss=1.4732 (C:0.5817, R:0.0089)
Batch 450/537: Loss=1.4360 (C:0.5521, R:0.0088)
Batch 475/537: Loss=1.4767 (C:0.5882, R:0.0089)
Batch 500/537: Loss=1.4855 (C:0.5978, R:0.0089)
Batch 525/537: Loss=1.4968 (C:0.6036, R:0.0089)

============================================================
Epoch 118/200 completed in 31.3s
Train: Loss=1.4580 (C:0.5688, R:0.0089) Ratio=5.65x
Val:   Loss=1.6091 (C:0.7314, R:0.0088) Ratio=3.20x
Reconstruction weight: 100.000
No improvement for 5 epochs
============================================================

Epoch 119 Training
----------------------------------------
Batch   0/537: Loss=1.4246 (C:0.5347, R:0.0089)
Batch  25/537: Loss=1.4422 (C:0.5520, R:0.0089)
Batch  50/537: Loss=1.4643 (C:0.5775, R:0.0089)
Batch  75/537: Loss=1.4495 (C:0.5613, R:0.0089)
Batch 100/537: Loss=1.4459 (C:0.5514, R:0.0089)
Batch 125/537: Loss=1.4699 (C:0.5867, R:0.0088)
Batch 150/537: Loss=1.4750 (C:0.5882, R:0.0089)
Batch 175/537: Loss=1.4295 (C:0.5372, R:0.0089)
Batch 200/537: Loss=1.4363 (C:0.5476, R:0.0089)
Batch 225/537: Loss=1.4198 (C:0.5344, R:0.0089)
Batch 250/537: Loss=1.4902 (C:0.5997, R:0.0089)
Batch 275/537: Loss=1.4426 (C:0.5540, R:0.0089)
Batch 300/537: Loss=1.4661 (C:0.5811, R:0.0088)
Batch 325/537: Loss=1.4636 (C:0.5773, R:0.0089)
Batch 350/537: Loss=1.4821 (C:0.5943, R:0.0089)
Batch 375/537: Loss=1.4879 (C:0.5998, R:0.0089)
Batch 400/537: Loss=1.4668 (C:0.5738, R:0.0089)
Batch 425/537: Loss=1.4136 (C:0.5208, R:0.0089)
Batch 450/537: Loss=1.5061 (C:0.6162, R:0.0089)
Batch 475/537: Loss=1.4872 (C:0.6044, R:0.0088)
Batch 500/537: Loss=1.4550 (C:0.5698, R:0.0089)
Batch 525/537: Loss=1.4708 (C:0.5771, R:0.0089)

============================================================
Epoch 119/200 completed in 23.6s
Train: Loss=1.4558 (C:0.5670, R:0.0089) Ratio=5.59x
Val:   Loss=1.6098 (C:0.7331, R:0.0088) Ratio=3.22x
Reconstruction weight: 100.000
No improvement for 6 epochs
============================================================

Epoch 120 Training
----------------------------------------
Batch   0/537: Loss=1.4950 (C:0.6053, R:0.0089)
Batch  25/537: Loss=1.4801 (C:0.5872, R:0.0089)
Batch  50/537: Loss=1.4350 (C:0.5441, R:0.0089)
Batch  75/537: Loss=1.4638 (C:0.5777, R:0.0089)
Batch 100/537: Loss=1.4423 (C:0.5510, R:0.0089)
Batch 125/537: Loss=1.4634 (C:0.5767, R:0.0089)
Batch 150/537: Loss=1.4340 (C:0.5499, R:0.0088)
Batch 175/537: Loss=1.4222 (C:0.5355, R:0.0089)
Batch 200/537: Loss=1.4558 (C:0.5676, R:0.0089)
Batch 225/537: Loss=1.4291 (C:0.5401, R:0.0089)
Batch 250/537: Loss=1.4504 (C:0.5545, R:0.0090)
Batch 275/537: Loss=1.4524 (C:0.5712, R:0.0088)
Batch 300/537: Loss=1.4435 (C:0.5514, R:0.0089)
Batch 325/537: Loss=1.4128 (C:0.5276, R:0.0089)
Batch 350/537: Loss=1.4155 (C:0.5318, R:0.0088)
Batch 375/537: Loss=1.4186 (C:0.5311, R:0.0089)
Batch 400/537: Loss=1.4300 (C:0.5443, R:0.0089)
Batch 425/537: Loss=1.4100 (C:0.5209, R:0.0089)
Batch 450/537: Loss=1.4317 (C:0.5379, R:0.0089)
Batch 475/537: Loss=1.4852 (C:0.5970, R:0.0089)
Batch 500/537: Loss=1.4257 (C:0.5382, R:0.0089)
Batch 525/537: Loss=1.5261 (C:0.6414, R:0.0088)

============================================================
Epoch 120/200 completed in 23.3s
Train: Loss=1.4519 (C:0.5633, R:0.0089) Ratio=5.61x
Val:   Loss=1.6023 (C:0.7261, R:0.0088) Ratio=3.32x
Reconstruction weight: 100.000
No improvement for 7 epochs
Checkpoint saved at epoch 120
============================================================

üåç Updating global dataset at epoch 121
üåç Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.368 ¬± 0.743
    Neg distances: 2.509 ¬± 1.103
    Separation ratio: 6.82x
    Gap: -3.912
    ‚úÖ Excellent global separation!

Epoch 121 Training
----------------------------------------
Batch   0/537: Loss=1.4762 (C:0.5871, R:0.0089)
Batch  25/537: Loss=1.3810 (C:0.4917, R:0.0089)
Batch  50/537: Loss=1.3847 (C:0.4996, R:0.0089)
Batch  75/537: Loss=1.4426 (C:0.5472, R:0.0090)
Batch 100/537: Loss=1.4796 (C:0.5917, R:0.0089)
Batch 125/537: Loss=1.4743 (C:0.5848, R:0.0089)
Batch 150/537: Loss=1.4186 (C:0.5278, R:0.0089)
Batch 175/537: Loss=1.4262 (C:0.5393, R:0.0089)
Batch 200/537: Loss=1.4078 (C:0.5227, R:0.0089)
Batch 225/537: Loss=1.4319 (C:0.5421, R:0.0089)
Batch 250/537: Loss=1.4082 (C:0.5217, R:0.0089)
Batch 275/537: Loss=1.4248 (C:0.5370, R:0.0089)
Batch 300/537: Loss=1.4438 (C:0.5493, R:0.0089)
Batch 325/537: Loss=1.4151 (C:0.5239, R:0.0089)
Batch 350/537: Loss=1.4076 (C:0.5163, R:0.0089)
Batch 375/537: Loss=1.4081 (C:0.5182, R:0.0089)
Batch 400/537: Loss=1.4030 (C:0.5197, R:0.0088)
Batch 425/537: Loss=1.4200 (C:0.5270, R:0.0089)
Batch 450/537: Loss=1.4726 (C:0.5852, R:0.0089)
Batch 475/537: Loss=1.4723 (C:0.5848, R:0.0089)
Batch 500/537: Loss=1.4285 (C:0.5403, R:0.0089)
Batch 525/537: Loss=1.4473 (C:0.5540, R:0.0089)

============================================================
Epoch 121/200 completed in 31.7s
Train: Loss=1.4339 (C:0.5456, R:0.0089) Ratio=5.66x
Val:   Loss=1.6024 (C:0.7258, R:0.0088) Ratio=3.25x
Reconstruction weight: 100.000
No improvement for 8 epochs

Early stopping triggered after 121 epochs
Best model was at epoch 113 with Val Loss: 1.5836

Global Dataset Training Completed!
Best epoch: 113
Best validation loss: 1.5836
Final separation ratios: Train=5.66x, Val=3.25x
Training completed!
Creating loss plots...
Loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/plots/global_cosine_test_attention_training_losses.png
Loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/plots/global_cosine_test_attention_training_losses.png
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.5129
  Adjusted Rand Score: 0.5654
  Clustering Accuracy: 0.8306
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.8298
  Per-class F1: [0.8462159109246409, 0.7755705138729273, 0.8675889328063241]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.008756
Evaluating separation quality...
Separation Results:
  Positive distances: 0.704 ¬± 1.034
  Negative distances: 2.331 ¬± 1.253
  Separation ratio: 3.31x
  Gap: -3.919
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.5129
  Clustering Accuracy: 0.8306
  Adjusted Rand Score: 0.5654

Classification Performance:
  Accuracy: 0.8298

Separation Quality:
  Separation Ratio: 3.31x
  Gap: -3.919
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.008756
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/results/evaluation_results_20250724_210418.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/results/evaluation_results_20250724_210418.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_cosine_test_attention_20250724_200916/final_results.json

Key Results:
  Separation ratio: 3.31x
  Perfect separation: False
  Classification accuracy: 0.8298

Analysis completed with exit code: 0
Time: Thu 24 Jul 21:04:20 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
