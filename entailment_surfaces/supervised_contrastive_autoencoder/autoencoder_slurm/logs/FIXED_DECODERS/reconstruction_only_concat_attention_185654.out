Starting Surface Distance Metric Analysis job...
Job ID: 185654
Node: gpuvm13
Time: Thu 24 Jul 12:06:40 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Thu Jul 24 12:06:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

GLOBAL DATASET CONTRASTIVE AUTOENCODER PIPELINE
============================================================
Start time: 2025-07-24 12:06:49.011533
Using device: cuda

Configuration:
  Embedding type: concat
  Batch size: 1020
  Global update frequency: 3 epochs
  Training epochs: 200
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Creating model and trainer...
========================================
AttentionAutoencoder initialized:
  Input dim: 1536
  Latent dim: 100
  Hidden dims: []
  Attention Heads: 5
  Total parameters: 349,436
Model created with 349,436 parameters
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 0.0
  Base reconstruction weight: 1.0
Optimizer created: Adam (lr=0.0001)
GlobalDatasetTrainer initialized on device: cuda
Model parameters: 349,436
Starting training...
========================================
Starting Global Dataset Training...
============================================================

Epoch 1 Training
----------------------------------------
Batch   0/537: Loss=0.3445 (C:13.2264, R:0.3445)
Batch  25/537: Loss=0.1900 (C:8.5941, R:0.1900)
Batch  50/537: Loss=0.0767 (C:4.7808, R:0.0767)
Batch  75/537: Loss=0.0357 (C:3.3225, R:0.0357)
Batch 100/537: Loss=0.0229 (C:2.7299, R:0.0229)
Batch 125/537: Loss=0.0185 (C:2.4606, R:0.0185)
Batch 150/537: Loss=0.0156 (C:2.2139, R:0.0156)
Batch 175/537: Loss=0.0148 (C:2.1574, R:0.0148)
Batch 200/537: Loss=0.0139 (C:2.1062, R:0.0139)
Batch 225/537: Loss=0.0130 (C:2.0550, R:0.0130)
Batch 250/537: Loss=0.0124 (C:2.0273, R:0.0124)
Batch 275/537: Loss=0.0120 (C:2.0179, R:0.0120)
Batch 300/537: Loss=0.0117 (C:2.0135, R:0.0117)
Batch 325/537: Loss=0.0119 (C:2.0517, R:0.0119)
Batch 350/537: Loss=0.0115 (C:2.0305, R:0.0115)
Batch 375/537: Loss=0.0110 (C:2.0000, R:0.0110)
Batch 400/537: Loss=0.0110 (C:2.0154, R:0.0110)
Batch 425/537: Loss=0.0107 (C:2.0336, R:0.0107)
Batch 450/537: Loss=0.0104 (C:1.9986, R:0.0104)
Batch 475/537: Loss=0.0104 (C:2.0005, R:0.0104)
Batch 500/537: Loss=0.0104 (C:2.0037, R:0.0104)
Batch 525/537: Loss=0.0103 (C:1.9990, R:0.0103)

============================================================
Epoch 1/200 completed in 9.7s
Train: Loss=0.0328 (C:2.8559, R:0.0328) Ratio=1.00x
Val:   Loss=0.0099 (C:1.9982, R:0.0099) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0099)
============================================================

Epoch 2 Training
----------------------------------------
Batch   0/537: Loss=0.0103 (C:2.0017, R:0.0103)
Batch  25/537: Loss=0.0102 (C:1.9991, R:0.0102)
Batch  50/537: Loss=0.0101 (C:1.9995, R:0.0101)
Batch  75/537: Loss=0.0101 (C:2.0320, R:0.0101)
Batch 100/537: Loss=0.0100 (C:2.0018, R:0.0100)
Batch 125/537: Loss=0.0100 (C:1.9987, R:0.0100)
Batch 150/537: Loss=0.0099 (C:1.9982, R:0.0099)
Batch 175/537: Loss=0.0099 (C:2.0332, R:0.0099)
Batch 200/537: Loss=0.0100 (C:2.0016, R:0.0100)
Batch 225/537: Loss=0.0099 (C:2.0004, R:0.0099)
Batch 250/537: Loss=0.0098 (C:1.9989, R:0.0098)
Batch 275/537: Loss=0.0098 (C:2.0079, R:0.0098)
Batch 300/537: Loss=0.0097 (C:1.9983, R:0.0097)
Batch 325/537: Loss=0.0097 (C:1.9980, R:0.0097)
Batch 350/537: Loss=0.0097 (C:2.0002, R:0.0097)
Batch 375/537: Loss=0.0096 (C:1.9982, R:0.0096)
Batch 400/537: Loss=0.0098 (C:2.0247, R:0.0098)
Batch 425/537: Loss=0.0096 (C:1.9982, R:0.0096)
Batch 450/537: Loss=0.0096 (C:2.0001, R:0.0096)
Batch 475/537: Loss=0.0095 (C:1.9984, R:0.0095)
Batch 500/537: Loss=0.0095 (C:1.9985, R:0.0095)
Batch 525/537: Loss=0.0095 (C:1.9996, R:0.0095)

============================================================
Epoch 2/200 completed in 9.1s
Train: Loss=0.0098 (C:2.0027, R:0.0098) Ratio=1.00x
Val:   Loss=0.0093 (C:1.9981, R:0.0093) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0093)
============================================================

Epoch 3 Training
----------------------------------------
Batch   0/537: Loss=0.0095 (C:2.0048, R:0.0095)
Batch  25/537: Loss=0.0095 (C:1.9998, R:0.0095)
Batch  50/537: Loss=0.0097 (C:2.0217, R:0.0097)
Batch  75/537: Loss=0.0094 (C:1.9993, R:0.0094)
Batch 100/537: Loss=0.0093 (C:1.9978, R:0.0093)
Batch 125/537: Loss=0.0093 (C:1.9987, R:0.0093)
Batch 150/537: Loss=0.0093 (C:1.9991, R:0.0093)
Batch 175/537: Loss=0.0093 (C:1.9992, R:0.0093)
Batch 200/537: Loss=0.0092 (C:1.9982, R:0.0092)
Batch 225/537: Loss=0.0092 (C:2.0001, R:0.0092)
Batch 250/537: Loss=0.0092 (C:1.9986, R:0.0092)
Batch 275/537: Loss=0.0092 (C:2.0032, R:0.0092)
Batch 300/537: Loss=0.0091 (C:1.9976, R:0.0091)
Batch 325/537: Loss=0.0091 (C:1.9978, R:0.0091)
Batch 350/537: Loss=0.0091 (C:1.9985, R:0.0091)
Batch 375/537: Loss=0.0090 (C:1.9985, R:0.0090)
Batch 400/537: Loss=0.0090 (C:1.9999, R:0.0090)
Batch 425/537: Loss=0.0089 (C:1.9981, R:0.0089)
Batch 450/537: Loss=0.0089 (C:2.0020, R:0.0089)
Batch 475/537: Loss=0.0089 (C:2.0000, R:0.0089)
Batch 500/537: Loss=0.0088 (C:1.9996, R:0.0088)
Batch 525/537: Loss=0.0088 (C:2.0004, R:0.0088)

============================================================
Epoch 3/200 completed in 9.2s
Train: Loss=0.0092 (C:2.0017, R:0.0092) Ratio=1.00x
Val:   Loss=0.0087 (C:1.9967, R:0.0087) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0087)
============================================================

Epoch 4 Training
----------------------------------------
Batch   0/537: Loss=0.0089 (C:1.9997, R:0.0089)
Batch  25/537: Loss=0.0087 (C:1.9986, R:0.0087)
Batch  50/537: Loss=0.0087 (C:1.9973, R:0.0087)
Batch  75/537: Loss=0.0087 (C:2.0003, R:0.0087)
Batch 100/537: Loss=0.0087 (C:1.9977, R:0.0087)
Batch 125/537: Loss=0.0087 (C:2.0003, R:0.0087)
Batch 150/537: Loss=0.0086 (C:2.0028, R:0.0086)
Batch 175/537: Loss=0.0086 (C:2.0004, R:0.0086)
Batch 200/537: Loss=0.0086 (C:1.9972, R:0.0086)
Batch 225/537: Loss=0.0085 (C:2.0010, R:0.0085)
Batch 250/537: Loss=0.0086 (C:2.0006, R:0.0086)
Batch 275/537: Loss=0.0085 (C:2.0088, R:0.0085)
Batch 300/537: Loss=0.0084 (C:2.0033, R:0.0084)
Batch 325/537: Loss=0.0084 (C:2.0046, R:0.0084)
Batch 350/537: Loss=0.0084 (C:1.9982, R:0.0084)
Batch 375/537: Loss=0.0084 (C:2.0014, R:0.0084)
Batch 400/537: Loss=0.0084 (C:2.0042, R:0.0084)
Batch 425/537: Loss=0.0083 (C:2.0047, R:0.0083)
Batch 450/537: Loss=0.0084 (C:2.0407, R:0.0084)
Batch 475/537: Loss=0.0082 (C:2.0013, R:0.0082)
Batch 500/537: Loss=0.0082 (C:2.0055, R:0.0082)
Batch 525/537: Loss=0.0082 (C:2.0062, R:0.0082)

============================================================
Epoch 4/200 completed in 11.1s
Train: Loss=0.0085 (C:2.0037, R:0.0085) Ratio=1.00x
Val:   Loss=0.0080 (C:1.9946, R:0.0080) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0080)
============================================================

Epoch 5 Training
----------------------------------------
Batch   0/537: Loss=0.0082 (C:2.0071, R:0.0082)
Batch  25/537: Loss=0.0082 (C:2.0052, R:0.0082)
Batch  50/537: Loss=0.0081 (C:2.0093, R:0.0081)
Batch  75/537: Loss=0.0081 (C:2.0103, R:0.0081)
Batch 100/537: Loss=0.0081 (C:2.0073, R:0.0081)
Batch 125/537: Loss=0.0080 (C:2.0169, R:0.0080)
Batch 150/537: Loss=0.0080 (C:2.0059, R:0.0080)
Batch 175/537: Loss=0.0079 (C:2.0070, R:0.0079)
Batch 200/537: Loss=0.0079 (C:2.0140, R:0.0079)
Batch 225/537: Loss=0.0080 (C:2.0138, R:0.0080)
Batch 250/537: Loss=0.0079 (C:2.0226, R:0.0079)
Batch 275/537: Loss=0.0079 (C:2.0244, R:0.0079)
Batch 300/537: Loss=0.0078 (C:2.0231, R:0.0078)
Batch 325/537: Loss=0.0079 (C:2.0555, R:0.0079)
Batch 350/537: Loss=0.0078 (C:2.0339, R:0.0078)
Batch 375/537: Loss=0.0078 (C:2.0565, R:0.0078)
Batch 400/537: Loss=0.0077 (C:2.0236, R:0.0077)
Batch 425/537: Loss=0.0077 (C:2.0338, R:0.0077)
Batch 450/537: Loss=0.0076 (C:2.0274, R:0.0076)
Batch 475/537: Loss=0.0076 (C:2.0424, R:0.0076)
Batch 500/537: Loss=0.0076 (C:2.0439, R:0.0076)
Batch 525/537: Loss=0.0076 (C:2.0375, R:0.0076)

============================================================
Epoch 5/200 completed in 9.9s
Train: Loss=0.0079 (C:2.0213, R:0.0079) Ratio=1.01x
Val:   Loss=0.0074 (C:2.0053, R:0.0074) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0074)
============================================================

Epoch 6 Training
----------------------------------------
Batch   0/537: Loss=0.0076 (C:2.0444, R:0.0076)
Batch  25/537: Loss=0.0075 (C:2.0538, R:0.0075)
Batch  50/537: Loss=0.0075 (C:2.0635, R:0.0075)
Batch  75/537: Loss=0.0075 (C:2.0635, R:0.0075)
Batch 100/537: Loss=0.0075 (C:2.0424, R:0.0075)
Batch 125/537: Loss=0.0074 (C:2.0599, R:0.0074)
Batch 150/537: Loss=0.0074 (C:2.0718, R:0.0074)
Batch 175/537: Loss=0.0074 (C:2.0698, R:0.0074)
Batch 200/537: Loss=0.0073 (C:2.0670, R:0.0073)
Batch 225/537: Loss=0.0073 (C:2.0669, R:0.0073)
Batch 250/537: Loss=0.0073 (C:2.0832, R:0.0073)
Batch 275/537: Loss=0.0073 (C:2.0860, R:0.0073)
Batch 300/537: Loss=0.0072 (C:2.1091, R:0.0072)
Batch 325/537: Loss=0.0072 (C:2.1151, R:0.0072)
Batch 350/537: Loss=0.0072 (C:2.1183, R:0.0072)
Batch 375/537: Loss=0.0071 (C:2.1061, R:0.0071)
Batch 400/537: Loss=0.0071 (C:2.1280, R:0.0071)
Batch 425/537: Loss=0.0070 (C:2.1022, R:0.0070)
Batch 450/537: Loss=0.0071 (C:2.1205, R:0.0071)
Batch 475/537: Loss=0.0071 (C:2.1520, R:0.0071)
Batch 500/537: Loss=0.0071 (C:2.1787, R:0.0071)
Batch 525/537: Loss=0.0071 (C:2.1826, R:0.0071)

============================================================
Epoch 6/200 completed in 8.9s
Train: Loss=0.0073 (C:2.0970, R:0.0073) Ratio=1.01x
Val:   Loss=0.0069 (C:2.0990, R:0.0069) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0069)
============================================================

Epoch 7 Training
----------------------------------------
Batch   0/537: Loss=0.0070 (C:2.1829, R:0.0070)
Batch  25/537: Loss=0.0069 (C:2.1638, R:0.0069)
Batch  50/537: Loss=0.0070 (C:2.2097, R:0.0070)
Batch  75/537: Loss=0.0069 (C:2.2237, R:0.0069)
Batch 100/537: Loss=0.0069 (C:2.2440, R:0.0069)
Batch 125/537: Loss=0.0069 (C:2.2182, R:0.0069)
Batch 150/537: Loss=0.0068 (C:2.2404, R:0.0068)
Batch 175/537: Loss=0.0068 (C:2.2420, R:0.0068)
Batch 200/537: Loss=0.0068 (C:2.2449, R:0.0068)
Batch 225/537: Loss=0.0067 (C:2.2517, R:0.0067)
Batch 250/537: Loss=0.0067 (C:2.3093, R:0.0067)
Batch 275/537: Loss=0.0067 (C:2.3232, R:0.0067)
Batch 300/537: Loss=0.0067 (C:2.3056, R:0.0067)
Batch 325/537: Loss=0.0067 (C:2.3271, R:0.0067)
Batch 350/537: Loss=0.0066 (C:2.3603, R:0.0066)
Batch 375/537: Loss=0.0066 (C:2.3798, R:0.0066)
Batch 400/537: Loss=0.0066 (C:2.3702, R:0.0066)
Batch 425/537: Loss=0.0066 (C:2.3782, R:0.0066)
Batch 450/537: Loss=0.0066 (C:2.4332, R:0.0066)
Batch 475/537: Loss=0.0065 (C:2.4706, R:0.0065)
Batch 500/537: Loss=0.0065 (C:2.4707, R:0.0065)
Batch 525/537: Loss=0.0065 (C:2.4643, R:0.0065)

============================================================
Epoch 7/200 completed in 9.5s
Train: Loss=0.0067 (C:2.3119, R:0.0067) Ratio=1.01x
Val:   Loss=0.0064 (C:2.3942, R:0.0064) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0064)
============================================================

Epoch 8 Training
----------------------------------------
Batch   0/537: Loss=0.0064 (C:2.5053, R:0.0064)
Batch  25/537: Loss=0.0065 (C:2.5090, R:0.0065)
Batch  50/537: Loss=0.0064 (C:2.5344, R:0.0064)
Batch  75/537: Loss=0.0064 (C:2.5455, R:0.0064)
Batch 100/537: Loss=0.0064 (C:2.5565, R:0.0064)
Batch 125/537: Loss=0.0063 (C:2.5838, R:0.0063)
Batch 150/537: Loss=0.0063 (C:2.5937, R:0.0063)
Batch 175/537: Loss=0.0063 (C:2.6149, R:0.0063)
Batch 200/537: Loss=0.0063 (C:2.6508, R:0.0063)
Batch 225/537: Loss=0.0063 (C:2.7188, R:0.0063)
Batch 250/537: Loss=0.0062 (C:2.7075, R:0.0062)
Batch 275/537: Loss=0.0063 (C:2.7625, R:0.0063)
Batch 300/537: Loss=0.0061 (C:2.7724, R:0.0061)
Batch 325/537: Loss=0.0062 (C:2.7923, R:0.0062)
Batch 350/537: Loss=0.0061 (C:2.7812, R:0.0061)
Batch 375/537: Loss=0.0061 (C:2.8149, R:0.0061)
Batch 400/537: Loss=0.0061 (C:2.8665, R:0.0061)
Batch 425/537: Loss=0.0061 (C:2.8461, R:0.0061)
Batch 450/537: Loss=0.0061 (C:2.9002, R:0.0061)
Batch 475/537: Loss=0.0060 (C:2.9386, R:0.0060)
Batch 500/537: Loss=0.0060 (C:2.9686, R:0.0060)
Batch 525/537: Loss=0.0059 (C:2.9635, R:0.0059)

============================================================
Epoch 8/200 completed in 9.5s
Train: Loss=0.0062 (C:2.7265, R:0.0062) Ratio=1.01x
Val:   Loss=0.0059 (C:2.9244, R:0.0059) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0059)
============================================================

Epoch 9 Training
----------------------------------------
Batch   0/537: Loss=0.0060 (C:3.0161, R:0.0060)
Batch  25/537: Loss=0.0060 (C:2.9966, R:0.0060)
Batch  50/537: Loss=0.0059 (C:3.0523, R:0.0059)
Batch  75/537: Loss=0.0058 (C:3.1297, R:0.0058)
Batch 100/537: Loss=0.0059 (C:3.1144, R:0.0059)
Batch 125/537: Loss=0.0059 (C:3.1514, R:0.0059)
Batch 150/537: Loss=0.0058 (C:3.2152, R:0.0058)
Batch 175/537: Loss=0.0058 (C:3.1995, R:0.0058)
Batch 200/537: Loss=0.0058 (C:3.2396, R:0.0058)
Batch 225/537: Loss=0.0058 (C:3.2912, R:0.0058)
Batch 250/537: Loss=0.0057 (C:3.2978, R:0.0057)
Batch 275/537: Loss=0.0058 (C:3.2997, R:0.0058)
Batch 300/537: Loss=0.0057 (C:3.3407, R:0.0057)
Batch 325/537: Loss=0.0056 (C:3.4098, R:0.0056)
Batch 350/537: Loss=0.0057 (C:3.4237, R:0.0057)
Batch 375/537: Loss=0.0057 (C:3.4447, R:0.0057)
Batch 400/537: Loss=0.0056 (C:3.4988, R:0.0056)
Batch 425/537: Loss=0.0056 (C:3.4974, R:0.0056)
Batch 450/537: Loss=0.0056 (C:3.5427, R:0.0056)
Batch 475/537: Loss=0.0056 (C:3.5723, R:0.0056)
Batch 500/537: Loss=0.0055 (C:3.6392, R:0.0055)
Batch 525/537: Loss=0.0055 (C:3.6594, R:0.0055)

============================================================
Epoch 9/200 completed in 8.8s
Train: Loss=0.0057 (C:3.3313, R:0.0057) Ratio=1.01x
Val:   Loss=0.0055 (C:3.6266, R:0.0055) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0055)
============================================================

Epoch 10 Training
----------------------------------------
Batch   0/537: Loss=0.0056 (C:3.6779, R:0.0056)
Batch  25/537: Loss=0.0054 (C:3.7294, R:0.0054)
Batch  50/537: Loss=0.0054 (C:3.7817, R:0.0054)
Batch  75/537: Loss=0.0054 (C:3.8042, R:0.0054)
Batch 100/537: Loss=0.0054 (C:3.8308, R:0.0054)
Batch 125/537: Loss=0.0054 (C:3.8519, R:0.0054)
Batch 150/537: Loss=0.0054 (C:3.9033, R:0.0054)
Batch 175/537: Loss=0.0053 (C:3.9678, R:0.0053)
Batch 200/537: Loss=0.0053 (C:4.0040, R:0.0053)
Batch 225/537: Loss=0.0053 (C:4.0239, R:0.0053)
Batch 250/537: Loss=0.0053 (C:4.0780, R:0.0053)
Batch 275/537: Loss=0.0053 (C:4.0929, R:0.0053)
Batch 300/537: Loss=0.0053 (C:4.1373, R:0.0053)
Batch 325/537: Loss=0.0052 (C:4.1505, R:0.0052)
Batch 350/537: Loss=0.0052 (C:4.1900, R:0.0052)
Batch 375/537: Loss=0.0052 (C:4.2535, R:0.0052)
Batch 400/537: Loss=0.0053 (C:4.2223, R:0.0053)
Batch 425/537: Loss=0.0052 (C:4.2821, R:0.0052)
Batch 450/537: Loss=0.0052 (C:4.3338, R:0.0052)
Batch 475/537: Loss=0.0051 (C:4.3896, R:0.0051)
Batch 500/537: Loss=0.0051 (C:4.3936, R:0.0051)
Batch 525/537: Loss=0.0051 (C:4.4276, R:0.0051)

============================================================
Epoch 10/200 completed in 8.8s
Train: Loss=0.0053 (C:4.0729, R:0.0053) Ratio=1.01x
Val:   Loss=0.0051 (C:4.4003, R:0.0051) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0051)
============================================================

Epoch 11 Training
----------------------------------------
Batch   0/537: Loss=0.0051 (C:4.4586, R:0.0051)
Batch  25/537: Loss=0.0051 (C:4.4865, R:0.0051)
Batch  50/537: Loss=0.0050 (C:4.5535, R:0.0050)
Batch  75/537: Loss=0.0050 (C:4.5808, R:0.0050)
Batch 100/537: Loss=0.0050 (C:4.6067, R:0.0050)
Batch 125/537: Loss=0.0050 (C:4.6399, R:0.0050)
Batch 150/537: Loss=0.0050 (C:4.6512, R:0.0050)
Batch 175/537: Loss=0.0050 (C:4.7116, R:0.0050)
Batch 200/537: Loss=0.0050 (C:4.7180, R:0.0050)
Batch 225/537: Loss=0.0050 (C:4.7675, R:0.0050)
Batch 250/537: Loss=0.0049 (C:4.8078, R:0.0049)
Batch 275/537: Loss=0.0050 (C:4.7950, R:0.0050)
Batch 300/537: Loss=0.0049 (C:4.8545, R:0.0049)
Batch 325/537: Loss=0.0049 (C:4.8775, R:0.0049)
Batch 350/537: Loss=0.0049 (C:4.9322, R:0.0049)
Batch 375/537: Loss=0.0048 (C:5.0169, R:0.0048)
Batch 400/537: Loss=0.0048 (C:5.0035, R:0.0048)
Batch 425/537: Loss=0.0048 (C:5.0354, R:0.0048)
Batch 450/537: Loss=0.0048 (C:5.0309, R:0.0048)
Batch 475/537: Loss=0.0048 (C:5.0574, R:0.0048)
Batch 500/537: Loss=0.0048 (C:5.0800, R:0.0048)
Batch 525/537: Loss=0.0048 (C:5.1345, R:0.0048)

============================================================
Epoch 11/200 completed in 8.8s
Train: Loss=0.0049 (C:4.8150, R:0.0049) Ratio=1.01x
Val:   Loss=0.0048 (C:5.1019, R:0.0048) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0048)
============================================================

Epoch 12 Training
----------------------------------------
Batch   0/537: Loss=0.0048 (C:5.1471, R:0.0048)
Batch  25/537: Loss=0.0048 (C:5.1554, R:0.0048)
Batch  50/537: Loss=0.0048 (C:5.1963, R:0.0048)
Batch  75/537: Loss=0.0047 (C:5.2480, R:0.0047)
Batch 100/537: Loss=0.0047 (C:5.2607, R:0.0047)
Batch 125/537: Loss=0.0047 (C:5.2820, R:0.0047)
Batch 150/537: Loss=0.0047 (C:5.3469, R:0.0047)
Batch 175/537: Loss=0.0046 (C:5.3621, R:0.0046)
Batch 200/537: Loss=0.0046 (C:5.3972, R:0.0046)
Batch 225/537: Loss=0.0047 (C:5.4285, R:0.0047)
Batch 250/537: Loss=0.0046 (C:5.4712, R:0.0046)
Batch 275/537: Loss=0.0046 (C:5.4886, R:0.0046)
Batch 300/537: Loss=0.0046 (C:5.4825, R:0.0046)
Batch 325/537: Loss=0.0046 (C:5.5229, R:0.0046)
Batch 350/537: Loss=0.0046 (C:5.5611, R:0.0046)
Batch 375/537: Loss=0.0046 (C:5.5793, R:0.0046)
Batch 400/537: Loss=0.0045 (C:5.6575, R:0.0045)
Batch 425/537: Loss=0.0045 (C:5.6429, R:0.0045)
Batch 450/537: Loss=0.0045 (C:5.6705, R:0.0045)
Batch 475/537: Loss=0.0044 (C:5.7375, R:0.0044)
Batch 500/537: Loss=0.0044 (C:5.7950, R:0.0044)
Batch 525/537: Loss=0.0045 (C:5.7983, R:0.0045)

============================================================
Epoch 12/200 completed in 9.0s
Train: Loss=0.0046 (C:5.4765, R:0.0046) Ratio=1.01x
Val:   Loss=0.0045 (C:5.7333, R:0.0045) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0045)
============================================================

Epoch 13 Training
----------------------------------------
Batch   0/537: Loss=0.0044 (C:5.8134, R:0.0044)
Batch  25/537: Loss=0.0044 (C:5.8112, R:0.0044)
Batch  50/537: Loss=0.0044 (C:5.8441, R:0.0044)
Batch  75/537: Loss=0.0045 (C:5.8560, R:0.0045)
Batch 100/537: Loss=0.0044 (C:5.9021, R:0.0044)
Batch 125/537: Loss=0.0044 (C:5.9218, R:0.0044)
Batch 150/537: Loss=0.0044 (C:5.9128, R:0.0044)
Batch 175/537: Loss=0.0043 (C:5.9916, R:0.0043)
Batch 200/537: Loss=0.0043 (C:6.0456, R:0.0043)
Batch 225/537: Loss=0.0044 (C:6.0502, R:0.0044)
Batch 250/537: Loss=0.0043 (C:6.0857, R:0.0043)
Batch 275/537: Loss=0.0043 (C:6.0814, R:0.0043)
Batch 300/537: Loss=0.0043 (C:6.1016, R:0.0043)
Batch 325/537: Loss=0.0043 (C:6.1739, R:0.0043)
Batch 350/537: Loss=0.0043 (C:6.1758, R:0.0043)
Batch 375/537: Loss=0.0043 (C:6.2187, R:0.0043)
Batch 400/537: Loss=0.0042 (C:6.2616, R:0.0042)
Batch 425/537: Loss=0.0043 (C:6.2888, R:0.0043)
Batch 450/537: Loss=0.0043 (C:6.2791, R:0.0043)
Batch 475/537: Loss=0.0042 (C:6.3341, R:0.0042)
Batch 500/537: Loss=0.0043 (C:6.3268, R:0.0043)
Batch 525/537: Loss=0.0042 (C:6.3952, R:0.0042)

============================================================
Epoch 13/200 completed in 9.3s
Train: Loss=0.0043 (C:6.0911, R:0.0043) Ratio=1.01x
Val:   Loss=0.0042 (C:6.3359, R:0.0042) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0042)
============================================================

Epoch 14 Training
----------------------------------------
Batch   0/537: Loss=0.0042 (C:6.4148, R:0.0042)
Batch  25/537: Loss=0.0042 (C:6.4076, R:0.0042)
Batch  50/537: Loss=0.0042 (C:6.4419, R:0.0042)
Batch  75/537: Loss=0.0041 (C:6.4892, R:0.0041)
Batch 100/537: Loss=0.0042 (C:6.4967, R:0.0042)
Batch 125/537: Loss=0.0041 (C:6.5453, R:0.0041)
Batch 150/537: Loss=0.0042 (C:6.5817, R:0.0042)
Batch 175/537: Loss=0.0041 (C:6.5899, R:0.0041)
Batch 200/537: Loss=0.0041 (C:6.6176, R:0.0041)
Batch 225/537: Loss=0.0041 (C:6.6584, R:0.0041)
Batch 250/537: Loss=0.0041 (C:6.6797, R:0.0041)
Batch 275/537: Loss=0.0041 (C:6.6927, R:0.0041)
Batch 300/537: Loss=0.0041 (C:6.7449, R:0.0041)
Batch 325/537: Loss=0.0041 (C:6.7784, R:0.0041)
Batch 350/537: Loss=0.0041 (C:6.8191, R:0.0041)
Batch 375/537: Loss=0.0041 (C:6.8150, R:0.0041)
Batch 400/537: Loss=0.0040 (C:6.8555, R:0.0040)
Batch 425/537: Loss=0.0041 (C:6.8704, R:0.0041)
Batch 450/537: Loss=0.0040 (C:6.9329, R:0.0040)
Batch 475/537: Loss=0.0040 (C:6.9445, R:0.0040)
Batch 500/537: Loss=0.0040 (C:7.0002, R:0.0040)
Batch 525/537: Loss=0.0040 (C:7.0329, R:0.0040)

============================================================
Epoch 14/200 completed in 9.4s
Train: Loss=0.0041 (C:6.7057, R:0.0041) Ratio=1.01x
Val:   Loss=0.0040 (C:6.9708, R:0.0040) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0040)
============================================================

Epoch 15 Training
----------------------------------------
Batch   0/537: Loss=0.0039 (C:7.0335, R:0.0039)
Batch  25/537: Loss=0.0040 (C:7.0412, R:0.0040)
Batch  50/537: Loss=0.0040 (C:7.0725, R:0.0040)
Batch  75/537: Loss=0.0040 (C:7.0951, R:0.0040)
Batch 100/537: Loss=0.0039 (C:7.1428, R:0.0039)
Batch 125/537: Loss=0.0039 (C:7.1957, R:0.0039)
Batch 150/537: Loss=0.0039 (C:7.2445, R:0.0039)
Batch 175/537: Loss=0.0040 (C:7.2428, R:0.0040)
Batch 200/537: Loss=0.0039 (C:7.2610, R:0.0039)
Batch 225/537: Loss=0.0039 (C:7.2828, R:0.0039)
Batch 250/537: Loss=0.0039 (C:7.3135, R:0.0039)
Batch 275/537: Loss=0.0039 (C:7.3307, R:0.0039)
Batch 300/537: Loss=0.0039 (C:7.3733, R:0.0039)
Batch 325/537: Loss=0.0039 (C:7.4233, R:0.0039)
Batch 350/537: Loss=0.0039 (C:7.4460, R:0.0039)
Batch 375/537: Loss=0.0039 (C:7.5064, R:0.0039)
Batch 400/537: Loss=0.0039 (C:7.4871, R:0.0039)
Batch 425/537: Loss=0.0039 (C:7.5323, R:0.0039)
Batch 450/537: Loss=0.0039 (C:7.5774, R:0.0039)
Batch 475/537: Loss=0.0038 (C:7.6225, R:0.0038)
Batch 500/537: Loss=0.0039 (C:7.6198, R:0.0039)
Batch 525/537: Loss=0.0038 (C:7.6556, R:0.0038)

============================================================
Epoch 15/200 completed in 9.0s
Train: Loss=0.0039 (C:7.3520, R:0.0039) Ratio=1.01x
Val:   Loss=0.0039 (C:7.6229, R:0.0039) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0039)
============================================================

Epoch 16 Training
----------------------------------------
Batch   0/537: Loss=0.0038 (C:7.7098, R:0.0038)
Batch  25/537: Loss=0.0038 (C:7.7200, R:0.0038)
Batch  50/537: Loss=0.0039 (C:7.7381, R:0.0039)
Batch  75/537: Loss=0.0038 (C:7.7801, R:0.0038)
Batch 100/537: Loss=0.0038 (C:7.8345, R:0.0038)
Batch 125/537: Loss=0.0038 (C:7.8473, R:0.0038)
Batch 150/537: Loss=0.0038 (C:7.8798, R:0.0038)
Batch 175/537: Loss=0.0038 (C:7.8988, R:0.0038)
Batch 200/537: Loss=0.0038 (C:7.9221, R:0.0038)
Batch 225/537: Loss=0.0038 (C:7.9857, R:0.0038)
Batch 250/537: Loss=0.0038 (C:7.9849, R:0.0038)
Batch 275/537: Loss=0.0038 (C:8.0467, R:0.0038)
Batch 300/537: Loss=0.0038 (C:8.0904, R:0.0038)
Batch 325/537: Loss=0.0038 (C:8.0992, R:0.0038)
Batch 350/537: Loss=0.0038 (C:8.1358, R:0.0038)
Batch 375/537: Loss=0.0038 (C:8.1637, R:0.0038)
Batch 400/537: Loss=0.0038 (C:8.1990, R:0.0038)
Batch 425/537: Loss=0.0037 (C:8.2153, R:0.0037)
Batch 450/537: Loss=0.0038 (C:8.2561, R:0.0038)
Batch 475/537: Loss=0.0038 (C:8.2930, R:0.0038)
Batch 500/537: Loss=0.0037 (C:8.3418, R:0.0037)
Batch 525/537: Loss=0.0038 (C:8.3562, R:0.0038)

============================================================
Epoch 16/200 completed in 9.0s
Train: Loss=0.0038 (C:8.0337, R:0.0038) Ratio=1.01x
Val:   Loss=0.0038 (C:8.3371, R:0.0038) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0038)
============================================================

Epoch 17 Training
----------------------------------------
Batch   0/537: Loss=0.0037 (C:8.3993, R:0.0037)
Batch  25/537: Loss=0.0037 (C:8.4228, R:0.0037)
Batch  50/537: Loss=0.0037 (C:8.4475, R:0.0037)
Batch  75/537: Loss=0.0037 (C:8.4889, R:0.0037)
Batch 100/537: Loss=0.0037 (C:8.5253, R:0.0037)
Batch 125/537: Loss=0.0037 (C:8.5546, R:0.0037)
Batch 150/537: Loss=0.0037 (C:8.5764, R:0.0037)
Batch 175/537: Loss=0.0037 (C:8.6172, R:0.0037)
Batch 200/537: Loss=0.0037 (C:8.6480, R:0.0037)
Batch 225/537: Loss=0.0037 (C:8.7073, R:0.0037)
Batch 250/537: Loss=0.0038 (C:8.7123, R:0.0038)
Batch 275/537: Loss=0.0038 (C:8.7408, R:0.0038)
Batch 300/537: Loss=0.0037 (C:8.7678, R:0.0037)
Batch 325/537: Loss=0.0037 (C:8.8518, R:0.0037)
Batch 350/537: Loss=0.0037 (C:8.8527, R:0.0037)
Batch 375/537: Loss=0.0037 (C:8.8770, R:0.0037)
Batch 400/537: Loss=0.0037 (C:8.9330, R:0.0037)
Batch 425/537: Loss=0.0037 (C:8.9657, R:0.0037)
Batch 450/537: Loss=0.0037 (C:8.9992, R:0.0037)
Batch 475/537: Loss=0.0037 (C:9.0057, R:0.0037)
Batch 500/537: Loss=0.0037 (C:9.0703, R:0.0037)
Batch 525/537: Loss=0.0037 (C:9.1180, R:0.0037)

============================================================
Epoch 17/200 completed in 9.2s
Train: Loss=0.0037 (C:8.7514, R:0.0037) Ratio=1.01x
Val:   Loss=0.0037 (C:9.0741, R:0.0037) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0037)
============================================================

Epoch 18 Training
----------------------------------------
Batch   0/537: Loss=0.0037 (C:9.1321, R:0.0037)
Batch  25/537: Loss=0.0037 (C:9.1656, R:0.0037)
Batch  50/537: Loss=0.0036 (C:9.2077, R:0.0036)
Batch  75/537: Loss=0.0037 (C:9.2431, R:0.0037)
Batch 100/537: Loss=0.0037 (C:9.2664, R:0.0037)
Batch 125/537: Loss=0.0037 (C:9.3083, R:0.0037)
Batch 150/537: Loss=0.0037 (C:9.3372, R:0.0037)
Batch 175/537: Loss=0.0037 (C:9.3392, R:0.0037)
Batch 200/537: Loss=0.0037 (C:9.4108, R:0.0037)
Batch 225/537: Loss=0.0037 (C:9.4519, R:0.0037)
Batch 250/537: Loss=0.0036 (C:9.4873, R:0.0036)
Batch 275/537: Loss=0.0037 (C:9.5102, R:0.0037)
Batch 300/537: Loss=0.0036 (C:9.5618, R:0.0036)
Batch 325/537: Loss=0.0036 (C:9.6137, R:0.0036)
Batch 350/537: Loss=0.0036 (C:9.6584, R:0.0036)
Batch 375/537: Loss=0.0037 (C:9.6618, R:0.0037)
Batch 400/537: Loss=0.0037 (C:9.6894, R:0.0037)
Batch 425/537: Loss=0.0037 (C:9.7226, R:0.0037)
Batch 450/537: Loss=0.0037 (C:9.7575, R:0.0037)
Batch 475/537: Loss=0.0037 (C:9.7612, R:0.0037)
Batch 500/537: Loss=0.0037 (C:9.8293, R:0.0037)
Batch 525/537: Loss=0.0037 (C:9.8397, R:0.0037)

============================================================
Epoch 18/200 completed in 9.7s
Train: Loss=0.0037 (C:9.5009, R:0.0037) Ratio=1.01x
Val:   Loss=0.0037 (C:9.8126, R:0.0037) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0037)
============================================================

Epoch 19 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:9.8672, R:0.0036)
Batch  25/537: Loss=0.0037 (C:9.8574, R:0.0037)
Batch  50/537: Loss=0.0036 (C:9.9326, R:0.0036)
Batch  75/537: Loss=0.0037 (C:9.9329, R:0.0037)
Batch 100/537: Loss=0.0037 (C:9.9641, R:0.0037)
Batch 125/537: Loss=0.0036 (C:9.9577, R:0.0036)
Batch 150/537: Loss=0.0037 (C:9.9876, R:0.0037)
Batch 175/537: Loss=0.0037 (C:10.0048, R:0.0037)
Batch 200/537: Loss=0.0037 (C:10.0031, R:0.0037)
Batch 225/537: Loss=0.0036 (C:10.0334, R:0.0036)
Batch 250/537: Loss=0.0036 (C:10.0415, R:0.0036)
Batch 275/537: Loss=0.0036 (C:10.0519, R:0.0036)
Batch 300/537: Loss=0.0036 (C:10.0651, R:0.0036)
Batch 325/537: Loss=0.0037 (C:10.0560, R:0.0037)
Batch 350/537: Loss=0.0036 (C:10.0511, R:0.0036)
Batch 375/537: Loss=0.0036 (C:10.0740, R:0.0036)
Batch 400/537: Loss=0.0036 (C:10.0835, R:0.0036)
Batch 425/537: Loss=0.0037 (C:10.0683, R:0.0037)
Batch 450/537: Loss=0.0036 (C:10.0668, R:0.0036)
Batch 475/537: Loss=0.0036 (C:10.0726, R:0.0036)
Batch 500/537: Loss=0.0037 (C:10.0515, R:0.0037)
Batch 525/537: Loss=0.0036 (C:10.0851, R:0.0036)

============================================================
Epoch 19/200 completed in 9.6s
Train: Loss=0.0036 (C:10.0158, R:0.0036) Ratio=1.01x
Val:   Loss=0.0036 (C:10.0422, R:0.0036) Ratio=1.01x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0036)
============================================================

Epoch 20 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:10.0610, R:0.0036)
Batch  25/537: Loss=0.0037 (C:10.0605, R:0.0037)
Batch  50/537: Loss=0.0036 (C:10.0410, R:0.0036)
Batch  75/537: Loss=0.0036 (C:10.0514, R:0.0036)
Batch 100/537: Loss=0.0036 (C:10.0425, R:0.0036)
Batch 125/537: Loss=0.0037 (C:10.0268, R:0.0037)
Batch 150/537: Loss=0.0036 (C:10.0121, R:0.0036)
Batch 175/537: Loss=0.0036 (C:10.0054, R:0.0036)
Batch 200/537: Loss=0.0037 (C:10.0090, R:0.0037)
Batch 225/537: Loss=0.0036 (C:9.9936, R:0.0036)
Batch 250/537: Loss=0.0036 (C:9.9792, R:0.0036)
Batch 275/537: Loss=0.0036 (C:9.9719, R:0.0036)
Batch 300/537: Loss=0.0037 (C:9.9633, R:0.0037)
Batch 325/537: Loss=0.0037 (C:9.9385, R:0.0037)
Batch 350/537: Loss=0.0036 (C:9.9247, R:0.0036)
Batch 375/537: Loss=0.0037 (C:9.9239, R:0.0037)
Batch 400/537: Loss=0.0036 (C:9.9075, R:0.0036)
Batch 425/537: Loss=0.0036 (C:9.8876, R:0.0036)
Batch 450/537: Loss=0.0036 (C:9.8848, R:0.0036)
Batch 475/537: Loss=0.0036 (C:9.8735, R:0.0036)
Batch 500/537: Loss=0.0036 (C:9.8511, R:0.0036)
Batch 525/537: Loss=0.0037 (C:9.8337, R:0.0037)

============================================================
Epoch 20/200 completed in 9.4s
Train: Loss=0.0036 (C:9.9628, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:9.8217, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0036)
Checkpoint saved at epoch 20
============================================================

Epoch 21 Training
----------------------------------------
Batch   0/537: Loss=0.0037 (C:9.8320, R:0.0037)
Batch  25/537: Loss=0.0036 (C:9.8049, R:0.0036)
Batch  50/537: Loss=0.0037 (C:9.8068, R:0.0037)
Batch  75/537: Loss=0.0036 (C:9.7780, R:0.0036)
Batch 100/537: Loss=0.0036 (C:9.7609, R:0.0036)
Batch 125/537: Loss=0.0036 (C:9.7522, R:0.0036)
Batch 150/537: Loss=0.0037 (C:9.7364, R:0.0037)
Batch 175/537: Loss=0.0036 (C:9.7171, R:0.0036)
Batch 200/537: Loss=0.0036 (C:9.6997, R:0.0036)
Batch 225/537: Loss=0.0037 (C:9.6730, R:0.0037)
Batch 250/537: Loss=0.0036 (C:9.6753, R:0.0036)
Batch 275/537: Loss=0.0036 (C:9.6588, R:0.0036)
Batch 300/537: Loss=0.0037 (C:9.6461, R:0.0037)
Batch 325/537: Loss=0.0036 (C:9.6233, R:0.0036)
Batch 350/537: Loss=0.0036 (C:9.6006, R:0.0036)
Batch 375/537: Loss=0.0036 (C:9.5856, R:0.0036)
Batch 400/537: Loss=0.0036 (C:9.5730, R:0.0036)
Batch 425/537: Loss=0.0037 (C:9.5411, R:0.0037)
Batch 450/537: Loss=0.0037 (C:9.5233, R:0.0037)
Batch 475/537: Loss=0.0036 (C:9.5128, R:0.0036)
Batch 500/537: Loss=0.0036 (C:9.5126, R:0.0036)
Batch 525/537: Loss=0.0036 (C:9.4665, R:0.0036)

============================================================
Epoch 21/200 completed in 9.4s
Train: Loss=0.0036 (C:9.6582, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:9.4661, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0036)
============================================================

Epoch 22 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:9.4626, R:0.0036)
Batch  25/537: Loss=0.0037 (C:9.4618, R:0.0037)
Batch  50/537: Loss=0.0036 (C:9.4336, R:0.0036)
Batch  75/537: Loss=0.0036 (C:9.4239, R:0.0036)
Batch 100/537: Loss=0.0036 (C:9.4061, R:0.0036)
Batch 125/537: Loss=0.0036 (C:9.3888, R:0.0036)
Batch 150/537: Loss=0.0036 (C:9.3674, R:0.0036)
Batch 175/537: Loss=0.0036 (C:9.3523, R:0.0036)
Batch 200/537: Loss=0.0036 (C:9.3324, R:0.0036)
Batch 225/537: Loss=0.0036 (C:9.3096, R:0.0036)
Batch 250/537: Loss=0.0036 (C:9.2947, R:0.0036)
Batch 275/537: Loss=0.0036 (C:9.2819, R:0.0036)
Batch 300/537: Loss=0.0037 (C:9.2783, R:0.0037)
Batch 325/537: Loss=0.0036 (C:9.2444, R:0.0036)
Batch 350/537: Loss=0.0036 (C:9.2281, R:0.0036)
Batch 375/537: Loss=0.0036 (C:9.2173, R:0.0036)
Batch 400/537: Loss=0.0036 (C:9.1895, R:0.0036)
Batch 425/537: Loss=0.0037 (C:9.1686, R:0.0037)
Batch 450/537: Loss=0.0036 (C:9.1612, R:0.0036)
Batch 475/537: Loss=0.0036 (C:9.1321, R:0.0036)
Batch 500/537: Loss=0.0036 (C:9.1174, R:0.0036)
Batch 525/537: Loss=0.0036 (C:9.1057, R:0.0036)

============================================================
Epoch 22/200 completed in 9.0s
Train: Loss=0.0036 (C:9.2868, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:9.0872, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0036)
============================================================

Epoch 23 Training
----------------------------------------
Batch   0/537: Loss=0.0037 (C:9.0926, R:0.0037)
Batch  25/537: Loss=0.0036 (C:9.0674, R:0.0036)
Batch  50/537: Loss=0.0036 (C:9.0564, R:0.0036)
Batch  75/537: Loss=0.0036 (C:9.0377, R:0.0036)
Batch 100/537: Loss=0.0036 (C:9.0280, R:0.0036)
Batch 125/537: Loss=0.0036 (C:9.0050, R:0.0036)
Batch 150/537: Loss=0.0036 (C:8.9974, R:0.0036)
Batch 175/537: Loss=0.0036 (C:8.9717, R:0.0036)
Batch 200/537: Loss=0.0036 (C:8.9590, R:0.0036)
Batch 225/537: Loss=0.0036 (C:8.9362, R:0.0036)
Batch 250/537: Loss=0.0036 (C:8.9126, R:0.0036)
Batch 275/537: Loss=0.0036 (C:8.9031, R:0.0036)
Batch 300/537: Loss=0.0036 (C:8.8829, R:0.0036)
Batch 325/537: Loss=0.0036 (C:8.8621, R:0.0036)
Batch 350/537: Loss=0.0036 (C:8.8414, R:0.0036)
Batch 375/537: Loss=0.0036 (C:8.8323, R:0.0036)
Batch 400/537: Loss=0.0036 (C:8.8071, R:0.0036)
Batch 425/537: Loss=0.0036 (C:8.7858, R:0.0036)
Batch 450/537: Loss=0.0036 (C:8.7645, R:0.0036)
Batch 475/537: Loss=0.0036 (C:8.7555, R:0.0036)
Batch 500/537: Loss=0.0036 (C:8.7325, R:0.0036)
Batch 525/537: Loss=0.0036 (C:8.7265, R:0.0036)

============================================================
Epoch 23/200 completed in 9.0s
Train: Loss=0.0036 (C:8.9024, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:8.7096, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
✅ New best model saved (Val Loss: 0.0036)
============================================================

Epoch 24 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:8.7162, R:0.0036)
Batch  25/537: Loss=0.0036 (C:8.7020, R:0.0036)
Batch  50/537: Loss=0.0036 (C:8.6821, R:0.0036)
Batch  75/537: Loss=0.0037 (C:8.6588, R:0.0037)
Batch 100/537: Loss=0.0036 (C:8.6471, R:0.0036)
Batch 125/537: Loss=0.0036 (C:8.6203, R:0.0036)
Batch 150/537: Loss=0.0036 (C:8.6124, R:0.0036)
Batch 175/537: Loss=0.0037 (C:8.5897, R:0.0037)
Batch 200/537: Loss=0.0036 (C:8.5834, R:0.0036)
Batch 225/537: Loss=0.0036 (C:8.5602, R:0.0036)
Batch 250/537: Loss=0.0036 (C:8.5404, R:0.0036)
Batch 275/537: Loss=0.0036 (C:8.5221, R:0.0036)
Batch 300/537: Loss=0.0036 (C:8.5132, R:0.0036)
Batch 325/537: Loss=0.0036 (C:8.4946, R:0.0036)
Batch 350/537: Loss=0.0036 (C:8.4707, R:0.0036)
Batch 375/537: Loss=0.0036 (C:8.4672, R:0.0036)
Batch 400/537: Loss=0.0036 (C:8.4519, R:0.0036)
Batch 425/537: Loss=0.0036 (C:8.4301, R:0.0036)
Batch 450/537: Loss=0.0036 (C:8.4189, R:0.0036)
Batch 475/537: Loss=0.0036 (C:8.4010, R:0.0036)
Batch 500/537: Loss=0.0036 (C:8.3805, R:0.0036)
Batch 525/537: Loss=0.0036 (C:8.3692, R:0.0036)

============================================================
Epoch 24/200 completed in 9.1s
Train: Loss=0.0036 (C:8.5356, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:8.3598, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 1 epochs
============================================================

Epoch 25 Training
----------------------------------------
Batch   0/537: Loss=0.0037 (C:8.3672, R:0.0037)
Batch  25/537: Loss=0.0036 (C:8.3460, R:0.0036)
Batch  50/537: Loss=0.0036 (C:8.3355, R:0.0036)
Batch  75/537: Loss=0.0036 (C:8.3260, R:0.0036)
Batch 100/537: Loss=0.0036 (C:8.3024, R:0.0036)
Batch 125/537: Loss=0.0037 (C:8.2862, R:0.0037)
Batch 150/537: Loss=0.0036 (C:8.2768, R:0.0036)
Batch 175/537: Loss=0.0036 (C:8.2539, R:0.0036)
Batch 200/537: Loss=0.0036 (C:8.2434, R:0.0036)
Batch 225/537: Loss=0.0036 (C:8.2280, R:0.0036)
Batch 250/537: Loss=0.0036 (C:8.2153, R:0.0036)
Batch 275/537: Loss=0.0036 (C:8.2097, R:0.0036)
Batch 300/537: Loss=0.0037 (C:8.1816, R:0.0037)
Batch 325/537: Loss=0.0036 (C:8.1764, R:0.0036)
Batch 350/537: Loss=0.0036 (C:8.1703, R:0.0036)
Batch 375/537: Loss=0.0036 (C:8.1479, R:0.0036)
Batch 400/537: Loss=0.0036 (C:8.1381, R:0.0036)
Batch 425/537: Loss=0.0036 (C:8.1245, R:0.0036)
Batch 450/537: Loss=0.0036 (C:8.1142, R:0.0036)
Batch 475/537: Loss=0.0036 (C:8.0952, R:0.0036)
Batch 500/537: Loss=0.0037 (C:8.0822, R:0.0037)
Batch 525/537: Loss=0.0036 (C:8.0805, R:0.0036)

============================================================
Epoch 25/200 completed in 9.5s
Train: Loss=0.0036 (C:8.2112, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:8.0614, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 2 epochs
============================================================

Epoch 26 Training
----------------------------------------
Batch   0/537: Loss=0.0037 (C:8.0637, R:0.0037)
Batch  25/537: Loss=0.0036 (C:8.0539, R:0.0036)
Batch  50/537: Loss=0.0036 (C:8.0405, R:0.0036)
Batch  75/537: Loss=0.0036 (C:8.0273, R:0.0036)
Batch 100/537: Loss=0.0037 (C:8.0229, R:0.0037)
Batch 125/537: Loss=0.0037 (C:7.9977, R:0.0037)
Batch 150/537: Loss=0.0037 (C:7.9972, R:0.0037)
Batch 175/537: Loss=0.0036 (C:7.9794, R:0.0036)
Batch 200/537: Loss=0.0036 (C:7.9731, R:0.0036)
Batch 225/537: Loss=0.0037 (C:7.9643, R:0.0037)
Batch 250/537: Loss=0.0036 (C:7.9553, R:0.0036)
Batch 275/537: Loss=0.0036 (C:7.9428, R:0.0036)
Batch 300/537: Loss=0.0036 (C:7.9282, R:0.0036)
Batch 325/537: Loss=0.0037 (C:7.9142, R:0.0037)
Batch 350/537: Loss=0.0036 (C:7.9016, R:0.0036)
Batch 375/537: Loss=0.0036 (C:7.9001, R:0.0036)
Batch 400/537: Loss=0.0036 (C:7.8899, R:0.0036)
Batch 425/537: Loss=0.0036 (C:7.8731, R:0.0036)
Batch 450/537: Loss=0.0037 (C:7.8639, R:0.0037)
Batch 475/537: Loss=0.0036 (C:7.8490, R:0.0036)
Batch 500/537: Loss=0.0036 (C:7.8447, R:0.0036)
Batch 525/537: Loss=0.0036 (C:7.8407, R:0.0036)

============================================================
Epoch 26/200 completed in 9.5s
Train: Loss=0.0036 (C:7.9442, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:7.8261, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 3 epochs
============================================================

Epoch 27 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:7.8236, R:0.0036)
Batch  25/537: Loss=0.0036 (C:7.8222, R:0.0036)
Batch  50/537: Loss=0.0036 (C:7.8140, R:0.0036)
Batch  75/537: Loss=0.0036 (C:7.8017, R:0.0036)
Batch 100/537: Loss=0.0036 (C:7.7945, R:0.0036)
Batch 125/537: Loss=0.0036 (C:7.7862, R:0.0036)
Batch 150/537: Loss=0.0036 (C:7.7743, R:0.0036)
Batch 175/537: Loss=0.0036 (C:7.7659, R:0.0036)
Batch 200/537: Loss=0.0036 (C:7.7592, R:0.0036)
Batch 225/537: Loss=0.0037 (C:7.7500, R:0.0037)
Batch 250/537: Loss=0.0036 (C:7.7399, R:0.0036)
Batch 275/537: Loss=0.0037 (C:7.7404, R:0.0037)
Batch 300/537: Loss=0.0036 (C:7.7333, R:0.0036)
Batch 325/537: Loss=0.0036 (C:7.7253, R:0.0036)
Batch 350/537: Loss=0.0036 (C:7.7192, R:0.0036)
Batch 375/537: Loss=0.0036 (C:7.7007, R:0.0036)
Batch 400/537: Loss=0.0036 (C:7.6989, R:0.0036)
Batch 425/537: Loss=0.0036 (C:7.6870, R:0.0036)
Batch 450/537: Loss=0.0036 (C:7.6825, R:0.0036)
Batch 475/537: Loss=0.0036 (C:7.6752, R:0.0036)
Batch 500/537: Loss=0.0036 (C:7.6730, R:0.0036)
Batch 525/537: Loss=0.0036 (C:7.6652, R:0.0036)

============================================================
Epoch 27/200 completed in 9.5s
Train: Loss=0.0036 (C:7.7406, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:7.6533, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 4 epochs
============================================================

Epoch 28 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:7.6655, R:0.0036)
Batch  25/537: Loss=0.0036 (C:7.6516, R:0.0036)
Batch  50/537: Loss=0.0036 (C:7.6491, R:0.0036)
Batch  75/537: Loss=0.0036 (C:7.6502, R:0.0036)
Batch 100/537: Loss=0.0036 (C:7.6281, R:0.0036)
Batch 125/537: Loss=0.0037 (C:7.6227, R:0.0037)
Batch 150/537: Loss=0.0036 (C:7.6235, R:0.0036)
Batch 175/537: Loss=0.0036 (C:7.6085, R:0.0036)
Batch 200/537: Loss=0.0036 (C:7.6046, R:0.0036)
Batch 225/537: Loss=0.0036 (C:7.6071, R:0.0036)
Batch 250/537: Loss=0.0036 (C:7.5978, R:0.0036)
Batch 275/537: Loss=0.0036 (C:7.5852, R:0.0036)
Batch 300/537: Loss=0.0036 (C:7.5884, R:0.0036)
Batch 325/537: Loss=0.0036 (C:7.5872, R:0.0036)
Batch 350/537: Loss=0.0036 (C:7.5712, R:0.0036)
Batch 375/537: Loss=0.0037 (C:7.5679, R:0.0037)
Batch 400/537: Loss=0.0037 (C:7.5666, R:0.0037)
Batch 425/537: Loss=0.0036 (C:7.5617, R:0.0036)
Batch 450/537: Loss=0.0036 (C:7.5583, R:0.0036)
Batch 475/537: Loss=0.0036 (C:7.5529, R:0.0036)
Batch 500/537: Loss=0.0036 (C:7.5493, R:0.0036)
Batch 525/537: Loss=0.0036 (C:7.5466, R:0.0036)

============================================================
Epoch 28/200 completed in 9.3s
Train: Loss=0.0036 (C:7.5968, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:7.5356, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 5 epochs
============================================================

Epoch 29 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:7.5442, R:0.0036)
Batch  25/537: Loss=0.0036 (C:7.5350, R:0.0036)
Batch  50/537: Loss=0.0036 (C:7.5387, R:0.0036)
Batch  75/537: Loss=0.0036 (C:7.5383, R:0.0036)
Batch 100/537: Loss=0.0036 (C:7.5193, R:0.0036)
Batch 125/537: Loss=0.0037 (C:7.5191, R:0.0037)
Batch 150/537: Loss=0.0036 (C:7.5201, R:0.0036)
Batch 175/537: Loss=0.0036 (C:7.5209, R:0.0036)
Batch 200/537: Loss=0.0036 (C:7.5100, R:0.0036)
Batch 225/537: Loss=0.0036 (C:7.5102, R:0.0036)
Batch 250/537: Loss=0.0036 (C:7.5036, R:0.0036)
Batch 275/537: Loss=0.0036 (C:7.4973, R:0.0036)
Batch 300/537: Loss=0.0036 (C:7.5035, R:0.0036)
Batch 325/537: Loss=0.0036 (C:7.4900, R:0.0036)
Batch 350/537: Loss=0.0036 (C:7.4941, R:0.0036)
Batch 375/537: Loss=0.0037 (C:7.4835, R:0.0037)
Batch 400/537: Loss=0.0036 (C:7.4835, R:0.0036)
Batch 425/537: Loss=0.0036 (C:7.4839, R:0.0036)
Batch 450/537: Loss=0.0036 (C:7.4810, R:0.0036)
Batch 475/537: Loss=0.0037 (C:7.4754, R:0.0037)
Batch 500/537: Loss=0.0036 (C:7.4770, R:0.0036)
Batch 525/537: Loss=0.0036 (C:7.4679, R:0.0036)

============================================================
Epoch 29/200 completed in 9.1s
Train: Loss=0.0036 (C:7.5045, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:7.4658, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 6 epochs
============================================================

Epoch 30 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:7.4758, R:0.0036)
Batch  25/537: Loss=0.0036 (C:7.4754, R:0.0036)
Batch  50/537: Loss=0.0036 (C:7.4687, R:0.0036)
Batch  75/537: Loss=0.0036 (C:7.4685, R:0.0036)
Batch 100/537: Loss=0.0036 (C:7.4600, R:0.0036)
Batch 125/537: Loss=0.0037 (C:7.4601, R:0.0037)
Batch 150/537: Loss=0.0036 (C:7.4570, R:0.0036)
Batch 175/537: Loss=0.0036 (C:7.4547, R:0.0036)
Batch 200/537: Loss=0.0036 (C:7.4604, R:0.0036)
Batch 225/537: Loss=0.0036 (C:7.4515, R:0.0036)
Batch 250/537: Loss=0.0036 (C:7.4504, R:0.0036)
Batch 275/537: Loss=0.0036 (C:7.4500, R:0.0036)
Batch 300/537: Loss=0.0037 (C:7.4457, R:0.0037)
Batch 325/537: Loss=0.0036 (C:7.4539, R:0.0036)
Batch 350/537: Loss=0.0036 (C:7.4443, R:0.0036)
Batch 375/537: Loss=0.0036 (C:7.4388, R:0.0036)
Batch 400/537: Loss=0.0036 (C:7.4367, R:0.0036)
Batch 425/537: Loss=0.0036 (C:7.4376, R:0.0036)
Batch 450/537: Loss=0.0036 (C:7.4326, R:0.0036)
Batch 475/537: Loss=0.0037 (C:7.4384, R:0.0037)
Batch 500/537: Loss=0.0037 (C:7.4327, R:0.0037)
Batch 525/537: Loss=0.0037 (C:7.4330, R:0.0037)

============================================================
Epoch 30/200 completed in 9.0s
Train: Loss=0.0036 (C:7.4500, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:7.4244, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 7 epochs
============================================================

Epoch 31 Training
----------------------------------------
Batch   0/537: Loss=0.0036 (C:7.4365, R:0.0036)
Batch  25/537: Loss=0.0036 (C:7.4340, R:0.0036)
Batch  50/537: Loss=0.0036 (C:7.4324, R:0.0036)
Batch  75/537: Loss=0.0036 (C:7.4325, R:0.0036)
Batch 100/537: Loss=0.0036 (C:7.4294, R:0.0036)
Batch 125/537: Loss=0.0037 (C:7.4165, R:0.0037)
Batch 150/537: Loss=0.0036 (C:7.4190, R:0.0036)
Batch 175/537: Loss=0.0036 (C:7.4183, R:0.0036)
Batch 200/537: Loss=0.0037 (C:7.4250, R:0.0037)
Batch 225/537: Loss=0.0036 (C:7.4206, R:0.0036)
Batch 250/537: Loss=0.0036 (C:7.4206, R:0.0036)
Batch 275/537: Loss=0.0036 (C:7.4239, R:0.0036)
Batch 300/537: Loss=0.0036 (C:7.4145, R:0.0036)
Batch 325/537: Loss=0.0036 (C:7.4196, R:0.0036)
Batch 350/537: Loss=0.0036 (C:7.4202, R:0.0036)
Batch 375/537: Loss=0.0037 (C:7.4208, R:0.0037)
Batch 400/537: Loss=0.0036 (C:7.4112, R:0.0036)
Batch 425/537: Loss=0.0036 (C:7.4184, R:0.0036)
Batch 450/537: Loss=0.0037 (C:7.4176, R:0.0037)
Batch 475/537: Loss=0.0036 (C:7.4193, R:0.0036)
Batch 500/537: Loss=0.0036 (C:7.4166, R:0.0036)
Batch 525/537: Loss=0.0036 (C:7.4074, R:0.0036)

============================================================
Epoch 31/200 completed in 9.5s
Train: Loss=0.0036 (C:7.4209, R:0.0036) Ratio=1.00x
Val:   Loss=0.0036 (C:7.4031, R:0.0036) Ratio=1.00x
Reconstruction weight: 1.000
No improvement for 8 epochs

Early stopping triggered after 31 epochs
Best model was at epoch 23 with Val Loss: 0.0036

Global Dataset Training Completed!
Best epoch: 23
Best validation loss: 0.0036
Final separation ratios: Train=1.00x, Val=1.00x
Training completed!
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 100])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.0011
  Adjusted Rand Score: 0.0005
  Clustering Accuracy: 0.3479
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 100])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 100])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.5266
  Per-class F1: [0.5804827822903558, 0.4076506955177744, 0.5590817015530047]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.003613
Evaluating separation quality...
Separation Results:
  Positive distances: 8.707 ± 0.545
  Negative distances: 8.742 ± 0.523
  Separation ratio: 1.00x
  Gap: -10.216
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.0011
  Clustering Accuracy: 0.3479
  Adjusted Rand Score: 0.0005

Classification Performance:
  Accuracy: 0.5266

Separation Quality:
  Separation Ratio: 1.00x
  Gap: -10.216
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.003613
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649/results/evaluation_results_20250724_121202.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649/results/evaluation_results_20250724_121202.json
Saving final experiment results...
Final results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649/final_results.json

============================================================
PIPELINE COMPLETED SUCCESSFULLY!
============================================================
Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649
Best model: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649/checkpoints/best_model.pt
Final results: entailment_surfaces/supervised_contrastive_autoencoder/experiments/global_concat_pure_reconstruction_20250724_120649/final_results.json

Key Results:
  Separation ratio: 1.00x
  Perfect separation: False
  Classification accuracy: 0.5266

Analysis completed with exit code: 0
Time: Thu 24 Jul 12:12:03 BST 2025

=== ANALYSIS SUCCESSFUL ===
Autoencoder pipeline successful!


Job finished.
