Starting Surface Distance Metric Analysis job...
Job ID: 186243
Node: gpuvm17
Time: Fri 25 Jul 13:08:43 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Fri Jul 25 13:08:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   32C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
AttentionAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Attention Heads: 5
  Total parameters: 3,068,401
FullDatasetContrastiveLoss initialized:
  Positive Margin: 2.0
  Negative Margin: 10.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 100.0
MoorTopologicalLoss Initialized: Using 0-dimensional persistence pairings (MST edges).
No prototypes being used for topological loss - whole dataset instead.
TopologicalTrainer initialized on device: cuda
Model parameters: 3,068,401
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.2
  Reconstruction weight: 100.0

======================================================================
ğŸ§  TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

ğŸŒ Updating global dataset at epoch 1
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.098 Â± 0.016
    Neg distances: 0.098 Â± 0.016
    Separation ratio: 1.00x
    Gap: -0.176
    âŒ Poor global separation

============================================================
EPOCH 1 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=48.5612 (C:8.7701, R:0.3589, T:19.4955(w:0.200)âš ï¸)
Batch  25/537: Loss=14.8709 (C:8.6736, R:0.0375, T:12.2511(w:0.200)âš ï¸)
Batch  50/537: Loss=13.9709 (C:8.6735, R:0.0309, T:11.0480(w:0.200)âš ï¸)
Batch  75/537: Loss=13.6161 (C:8.6743, R:0.0302, T:9.6072(w:0.200)ğŸš€)
Batch 100/537: Loss=13.2257 (C:8.6712, R:0.0288, T:8.3549(w:0.200)ğŸš€)
Batch 125/537: Loss=12.6628 (C:8.6725, R:0.0251, T:7.4190(w:0.200)ğŸš€)
Batch 150/537: Loss=12.0662 (C:8.6744, R:0.0222, T:5.8661(w:0.200)ğŸš€)
Batch 175/537: Loss=11.5860 (C:8.6681, R:0.0189, T:5.1307(w:0.200)ğŸš€)
Batch 200/537: Loss=11.1410 (C:8.6671, R:0.0159, T:4.4211(w:0.200)ğŸš€)
Batch 225/537: Loss=10.8715 (C:8.6696, R:0.0141, T:3.9434(w:0.200)ğŸš€)
Batch 250/537: Loss=10.6725 (C:8.6666, R:0.0126, T:3.7522(w:0.200)ğŸš€)
Batch 275/537: Loss=10.5290 (C:8.6689, R:0.0117, T:3.4591(w:0.200)ğŸš€)
Batch 300/537: Loss=10.4065 (C:8.6606, R:0.0109, T:3.2641(w:0.200)ğŸš€)
Batch 325/537: Loss=10.3165 (C:8.6592, R:0.0104, T:3.0768(w:0.200)ğŸš€)
Batch 350/537: Loss=10.2721 (C:8.6605, R:0.0101, T:3.0278(w:0.200)ğŸš€)
Batch 375/537: Loss=10.2159 (C:8.6593, R:0.0098, T:2.8821(w:0.200)ğŸš€)
Batch 400/537: Loss=10.1785 (C:8.6574, R:0.0095, T:2.8312(w:0.200)ğŸš€)
Batch 425/537: Loss=10.1392 (C:8.6556, R:0.0093, T:2.7512(w:0.200)ğŸš€)
Batch 450/537: Loss=10.0961 (C:8.6571, R:0.0092, T:2.6024(w:0.200)ğŸš€)
Batch 475/537: Loss=10.0878 (C:8.6565, R:0.0090, T:2.6390(w:0.200)ğŸš€)
Batch 500/537: Loss=10.0492 (C:8.6644, R:0.0089, T:2.4860(w:0.200)ğŸš€)
Batch 525/537: Loss=10.0274 (C:8.6546, R:0.0088, T:2.4777(w:0.200)ğŸš€)
ğŸ‰ MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 5.0753
ğŸ“ˆ New best topological loss: 5.0753

ğŸ“Š EPOCH 1 TRAINING SUMMARY:
  Total Loss: 11.6680
  Contrastive: 8.6649
  Reconstruction: 0.0199
  Topological: 5.0753 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.5348
  Contrastive: 8.6520
  Reconstruction: 0.0079
  Topological: 5.4601 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 1/300 COMPLETE (56.9s)
Train Loss: 11.6680 (C:8.6649, R:0.0199, T:5.0753)
Val Loss:   10.5348 (C:8.6520, R:0.0079, T:5.4601)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=10.0399 (C:8.6571, R:0.0088, T:2.5183(w:0.200)ğŸš€)
Batch  25/537: Loss=10.0157 (C:8.6542, R:0.0088, T:2.4235(w:0.200)ğŸš€)
Batch  50/537: Loss=9.9660 (C:8.6586, R:0.0086, T:2.2453(w:0.200)ğŸš€)
Batch  75/537: Loss=9.9833 (C:8.6599, R:0.0086, T:2.3080(w:0.200)ğŸš€)
Batch 100/537: Loss=9.9617 (C:8.6578, R:0.0086, T:2.2403(w:0.200)ğŸš€)
Batch 125/537: Loss=9.9652 (C:8.6579, R:0.0085, T:2.2868(w:0.200)ğŸš€)
Batch 150/537: Loss=9.9338 (C:8.6511, R:0.0084, T:2.2270(w:0.200)ğŸš€)
Batch 175/537: Loss=9.9382 (C:8.6545, R:0.0084, T:2.2188(w:0.200)ğŸš€)
Batch 200/537: Loss=9.9128 (C:8.6475, R:0.0083, T:2.1559(w:0.200)ğŸš€)
Batch 225/537: Loss=9.9132 (C:8.6516, R:0.0084, T:2.1315(w:0.200)ğŸš€)
Batch 250/537: Loss=9.9145 (C:8.6519, R:0.0084, T:2.1358(w:0.200)ğŸš€)
Batch 275/537: Loss=9.8873 (C:8.6577, R:0.0083, T:2.0156(w:0.200)ğŸš€)
Batch 300/537: Loss=9.9028 (C:8.6591, R:0.0083, T:2.0827(w:0.200)ğŸš€)
Batch 325/537: Loss=9.8745 (C:8.6549, R:0.0082, T:1.9789(w:0.200)ğŸš€)
Batch 350/537: Loss=9.8689 (C:8.6398, R:0.0082, T:2.0338(w:0.200)ğŸš€)
Batch 375/537: Loss=9.8650 (C:8.6499, R:0.0081, T:2.0109(w:0.200)ğŸš€)
Batch 400/537: Loss=9.8561 (C:8.6574, R:0.0081, T:1.9230(w:0.200)ğŸš€)
Batch 425/537: Loss=9.8585 (C:8.6480, R:0.0082, T:1.9660(w:0.200)ğŸš€)
Batch 450/537: Loss=9.9008 (C:8.6593, R:0.0081, T:2.1401(w:0.200)ğŸš€)
Batch 475/537: Loss=9.8399 (C:8.6560, R:0.0081, T:1.8613(w:0.200)ğŸš€)
Batch 500/537: Loss=9.8592 (C:8.6537, R:0.0081, T:1.9700(w:0.200)ğŸš€)
Batch 525/537: Loss=9.8471 (C:8.6522, R:0.0081, T:1.9195(w:0.200)ğŸš€)
ğŸ“ˆ New best topological loss: 2.0944

ğŸ“Š EPOCH 2 TRAINING SUMMARY:
  Total Loss: 9.9030
  Contrastive: 8.6526
  Reconstruction: 0.0083
  Topological: 2.0944 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.1505
  Contrastive: 8.6399
  Reconstruction: 0.0076
  Topological: 3.7610 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 2/300 COMPLETE (48.2s)
Train Loss: 9.9030 (C:8.6526, R:0.0083, T:2.0944)
Val Loss:   10.1505 (C:8.6399, R:0.0076, T:3.7610)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=9.8242 (C:8.6302, R:0.0081, T:1.9392(w:0.200)ğŸš€)
Batch  25/537: Loss=9.8112 (C:8.6415, R:0.0080, T:1.8384(w:0.200)ğŸš€)
Batch  50/537: Loss=9.8035 (C:8.6426, R:0.0080, T:1.8045(w:0.200)ğŸš€)
Batch  75/537: Loss=9.7835 (C:8.6367, R:0.0080, T:1.7376(w:0.200)ğŸš€)
Batch 100/537: Loss=9.8227 (C:8.6552, R:0.0080, T:1.8593(w:0.200)ğŸš€)
Batch 125/537: Loss=9.8168 (C:8.6550, R:0.0080, T:1.7930(w:0.200)ğŸš€)
Batch 150/537: Loss=9.8361 (C:8.6591, R:0.0080, T:1.8728(w:0.200)ğŸš€)
Batch 175/537: Loss=9.8092 (C:8.6443, R:0.0080, T:1.8273(w:0.200)ğŸš€)
Batch 200/537: Loss=9.7959 (C:8.6434, R:0.0080, T:1.7858(w:0.200)ğŸš€)
Batch 225/537: Loss=9.7743 (C:8.6336, R:0.0079, T:1.7312(w:0.200)ğŸš€)
Batch 250/537: Loss=9.7848 (C:8.6543, R:0.0079, T:1.6935(w:0.200)ğŸš€)
Batch 275/537: Loss=9.7620 (C:8.6308, R:0.0079, T:1.7037(w:0.200)ğŸš€)
Batch 300/537: Loss=9.7718 (C:8.6350, R:0.0079, T:1.7226(w:0.200)ğŸš€)
Batch 325/537: Loss=9.7963 (C:8.6368, R:0.0079, T:1.8356(w:0.200)ğŸš€)
Batch 350/537: Loss=9.7808 (C:8.6335, R:0.0079, T:1.7842(w:0.200)ğŸš€)
Batch 375/537: Loss=9.7624 (C:8.6206, R:0.0079, T:1.7543(w:0.200)ğŸš€)
Batch 400/537: Loss=9.7833 (C:8.6456, R:0.0079, T:1.7460(w:0.200)ğŸš€)
Batch 425/537: Loss=9.7504 (C:8.6438, R:0.0078, T:1.6279(w:0.200)ğŸš€)
Batch 450/537: Loss=9.7464 (C:8.6155, R:0.0079, T:1.7154(w:0.200)ğŸš€)
Batch 475/537: Loss=9.7594 (C:8.6347, R:0.0079, T:1.6952(w:0.200)ğŸš€)
Batch 500/537: Loss=9.7727 (C:8.6488, R:0.0079, T:1.6845(w:0.200)ğŸš€)
Batch 525/537: Loss=9.7746 (C:8.6493, R:0.0078, T:1.7050(w:0.200)ğŸš€)
ğŸ“ˆ New best topological loss: 1.7720

ğŸ“Š EPOCH 3 TRAINING SUMMARY:
  Total Loss: 9.7905
  Contrastive: 8.6418
  Reconstruction: 0.0079
  Topological: 1.7720 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.0144
  Contrastive: 8.6243
  Reconstruction: 0.0074
  Topological: 3.2547 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 3/300 COMPLETE (47.1s)
Train Loss: 9.7905 (C:8.6418, R:0.0079, T:1.7720)
Val Loss:   10.0144 (C:8.6243, R:0.0074, T:3.2547)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 4
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 6.397 Â± 1.025
    Neg distances: 6.672 Â± 1.049
    Separation ratio: 1.04x
    Gap: -9.964
    âŒ Poor global separation

============================================================
EPOCH 4 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=8.8633 (C:7.7608, R:0.0078, T:1.6160(w:0.200)ğŸš€)
Batch  25/537: Loss=8.8644 (C:7.6570, R:0.0079, T:2.0687(w:0.200)ğŸš€)
Batch  50/537: Loss=8.8398 (C:7.6767, R:0.0079, T:1.8874(w:0.200)ğŸš€)
Batch  75/537: Loss=8.8661 (C:7.6591, R:0.0079, T:2.1020(w:0.200)ğŸš€)
Batch 100/537: Loss=8.7874 (C:7.6123, R:0.0079, T:1.9448(w:0.200)ğŸš€)
Batch 125/537: Loss=8.8313 (C:7.6327, R:0.0079, T:2.0373(w:0.200)ğŸš€)
Batch 150/537: Loss=8.8314 (C:7.6303, R:0.0079, T:2.0605(w:0.200)ğŸš€)
Batch 175/537: Loss=8.8073 (C:7.6266, R:0.0078, T:1.9881(w:0.200)ğŸš€)
Batch 200/537: Loss=8.8044 (C:7.6103, R:0.0079, T:2.0192(w:0.200)ğŸš€)
Batch 225/537: Loss=8.7827 (C:7.6049, R:0.0078, T:1.9803(w:0.200)ğŸš€)
Batch 250/537: Loss=8.7833 (C:7.5826, R:0.0078, T:2.0882(w:0.200)ğŸš€)
Batch 275/537: Loss=8.8269 (C:7.5933, R:0.0079, T:2.2144(w:0.200)ğŸš€)
Batch 300/537: Loss=8.7915 (C:7.5957, R:0.0080, T:2.0009(w:0.200)ğŸš€)
Batch 325/537: Loss=8.7855 (C:7.5742, R:0.0079, T:2.1010(w:0.200)ğŸš€)
Batch 350/537: Loss=8.7606 (C:7.5807, R:0.0079, T:1.9685(w:0.200)ğŸš€)
Batch 375/537: Loss=8.7973 (C:7.6001, R:0.0078, T:2.0711(w:0.200)ğŸš€)
Batch 400/537: Loss=8.7789 (C:7.5848, R:0.0078, T:2.0730(w:0.200)ğŸš€)
Batch 425/537: Loss=8.7447 (C:7.5562, R:0.0079, T:1.9897(w:0.200)ğŸš€)
Batch 450/537: Loss=8.7814 (C:7.5637, R:0.0079, T:2.1483(w:0.200)ğŸš€)
Batch 475/537: Loss=8.7880 (C:7.5491, R:0.0078, T:2.2792(w:0.200)ğŸš€)
Batch 500/537: Loss=8.7250 (C:7.5274, R:0.0078, T:2.0678(w:0.200)ğŸš€)
Batch 525/537: Loss=8.7380 (C:7.5462, R:0.0078, T:2.0552(w:0.200)ğŸš€)

ğŸ“Š EPOCH 4 TRAINING SUMMARY:
  Total Loss: 8.8034
  Contrastive: 7.6035
  Reconstruction: 0.0079
  Topological: 2.0629 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.9867
  Contrastive: 7.4874
  Reconstruction: 0.0074
  Topological: 3.8144 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 4/300 COMPLETE (54.0s)
Train Loss: 8.8034 (C:7.6035, R:0.0079, T:2.0629)
Val Loss:   8.9867 (C:7.4874, R:0.0074, T:3.8144)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=8.7366 (C:7.5397, R:0.0078, T:2.0727(w:0.200)ğŸš€)
Batch  25/537: Loss=8.7148 (C:7.5311, R:0.0079, T:1.9785(w:0.200)ğŸš€)
Batch  50/537: Loss=8.7519 (C:7.5653, R:0.0079, T:1.9913(w:0.200)ğŸš€)
Batch  75/537: Loss=8.7643 (C:7.5399, R:0.0078, T:2.2129(w:0.200)ğŸš€)
Batch 100/537: Loss=8.7456 (C:7.5178, R:0.0078, T:2.2578(w:0.200)ğŸš€)
Batch 125/537: Loss=8.7153 (C:7.5398, R:0.0078, T:1.9878(w:0.200)ğŸš€)
Batch 150/537: Loss=8.7588 (C:7.5299, R:0.0078, T:2.2238(w:0.200)ğŸš€)
Batch 175/537: Loss=8.7592 (C:7.5405, R:0.0078, T:2.2069(w:0.200)ğŸš€)
Batch 200/537: Loss=8.7830 (C:7.5510, R:0.0079, T:2.2293(w:0.200)ğŸš€)
Batch 225/537: Loss=8.7179 (C:7.5144, R:0.0078, T:2.1272(w:0.200)ğŸš€)
Batch 250/537: Loss=8.7064 (C:7.5349, R:0.0078, T:1.9648(w:0.200)ğŸš€)
Batch 275/537: Loss=8.7336 (C:7.5315, R:0.0078, T:2.1116(w:0.200)ğŸš€)
Batch 300/537: Loss=8.7003 (C:7.5074, R:0.0078, T:2.0505(w:0.200)ğŸš€)
Batch 325/537: Loss=8.6893 (C:7.5363, R:0.0078, T:1.8846(w:0.200)ğŸš€)
Batch 350/537: Loss=8.7656 (C:7.5411, R:0.0078, T:2.2358(w:0.200)ğŸš€)
Batch 375/537: Loss=8.7015 (C:7.5006, R:0.0078, T:2.1010(w:0.200)ğŸš€)
Batch 400/537: Loss=8.7309 (C:7.5274, R:0.0078, T:2.1214(w:0.200)ğŸš€)
Batch 425/537: Loss=8.6965 (C:7.5059, R:0.0078, T:2.0669(w:0.200)ğŸš€)
Batch 450/537: Loss=8.7233 (C:7.5398, R:0.0078, T:2.0299(w:0.200)ğŸš€)
Batch 475/537: Loss=8.6241 (C:7.4393, R:0.0077, T:2.0495(w:0.200)ğŸš€)
Batch 500/537: Loss=8.6644 (C:7.5030, R:0.0078, T:1.9162(w:0.200)ğŸš€)
Batch 525/537: Loss=8.7684 (C:7.5250, R:0.0077, T:2.3552(w:0.200)ğŸš€)

ğŸ“Š EPOCH 5 TRAINING SUMMARY:
  Total Loss: 8.7252
  Contrastive: 7.5251
  Reconstruction: 0.0078
  Topological: 2.1059 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.9830
  Contrastive: 7.4404
  Reconstruction: 0.0073
  Topological: 4.0792 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 5/300 COMPLETE (46.8s)
Train Loss: 8.7252 (C:7.5251, R:0.0078, T:2.1059)
Val Loss:   8.9830 (C:7.4404, R:0.0073, T:4.0792)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=8.6508 (C:7.4672, R:0.0077, T:2.0494(w:0.200)ğŸš€)
Batch  25/537: Loss=8.6646 (C:7.4644, R:0.0077, T:2.1469(w:0.200)ğŸš€)
Batch  50/537: Loss=8.6767 (C:7.4861, R:0.0077, T:2.1196(w:0.200)ğŸš€)
Batch  75/537: Loss=8.6343 (C:7.4421, R:0.0077, T:2.1258(w:0.200)ğŸš€)
Batch 100/537: Loss=8.7440 (C:7.5092, R:0.0077, T:2.3056(w:0.200)ğŸš€)
Batch 125/537: Loss=8.6720 (C:7.4782, R:0.0077, T:2.0990(w:0.200)ğŸš€)
Batch 150/537: Loss=8.6848 (C:7.5161, R:0.0077, T:1.9844(w:0.200)ğŸš€)
Batch 175/537: Loss=8.6530 (C:7.4592, R:0.0078, T:2.0875(w:0.200)ğŸš€)
Batch 200/537: Loss=8.6475 (C:7.4741, R:0.0077, T:2.0006(w:0.200)ğŸš€)
Batch 225/537: Loss=8.7206 (C:7.5139, R:0.0077, T:2.1786(w:0.200)ğŸš€)
Batch 250/537: Loss=8.6494 (C:7.4692, R:0.0077, T:2.0617(w:0.200)ğŸš€)
Batch 275/537: Loss=8.6564 (C:7.5031, R:0.0077, T:1.9186(w:0.200)ğŸš€)
Batch 300/537: Loss=8.7022 (C:7.4906, R:0.0077, T:2.2079(w:0.200)ğŸš€)
Batch 325/537: Loss=8.6871 (C:7.4769, R:0.0078, T:2.1606(w:0.200)ğŸš€)
Batch 350/537: Loss=8.6654 (C:7.4707, R:0.0078, T:2.0921(w:0.200)ğŸš€)
Batch 375/537: Loss=8.6401 (C:7.4917, R:0.0077, T:1.8741(w:0.200)ğŸš€)
Batch 400/537: Loss=8.6779 (C:7.4826, R:0.0077, T:2.1220(w:0.200)ğŸš€)
Batch 425/537: Loss=8.6692 (C:7.4887, R:0.0077, T:2.0313(w:0.200)ğŸš€)
Batch 450/537: Loss=8.6780 (C:7.4677, R:0.0077, T:2.1769(w:0.200)ğŸš€)
Batch 475/537: Loss=8.6659 (C:7.4733, R:0.0077, T:2.1063(w:0.200)ğŸš€)
Batch 500/537: Loss=8.6710 (C:7.4728, R:0.0077, T:2.1402(w:0.200)ğŸš€)
Batch 525/537: Loss=8.6339 (C:7.4510, R:0.0077, T:2.0783(w:0.200)ğŸš€)

ğŸ“Š EPOCH 6 TRAINING SUMMARY:
  Total Loss: 8.6765
  Contrastive: 7.4830
  Reconstruction: 0.0077
  Topological: 2.1066 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.9393
  Contrastive: 7.4429
  Reconstruction: 0.0072
  Topological: 3.8963 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 6/300 COMPLETE (47.9s)
Train Loss: 8.6765 (C:7.4830, R:0.0077, T:2.1066)
Val Loss:   8.9393 (C:7.4429, R:0.0072, T:3.8963)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 7
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.831 Â± 0.999
    Neg distances: 7.072 Â± 1.464
    Separation ratio: 1.21x
    Gap: -11.140
    âŒ Poor global separation

============================================================
EPOCH 7 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.9873 (C:6.8346, R:0.0077, T:1.9286(w:0.200)ğŸš€)
Batch  25/537: Loss=8.0362 (C:6.7682, R:0.0077, T:2.4695(w:0.200)ğŸš€)
Batch  50/537: Loss=7.9451 (C:6.6911, R:0.0077, T:2.4363(w:0.200)ğŸš€)
Batch  75/537: Loss=7.9737 (C:6.7214, R:0.0077, T:2.3871(w:0.200)ğŸš€)
Batch 100/537: Loss=8.0130 (C:6.7589, R:0.0078, T:2.3830(w:0.200)ğŸš€)
Batch 125/537: Loss=7.9776 (C:6.7086, R:0.0078, T:2.4639(w:0.200)ğŸš€)
Batch 150/537: Loss=7.9690 (C:6.6969, R:0.0077, T:2.4938(w:0.200)ğŸš€)
Batch 175/537: Loss=8.0177 (C:6.6844, R:0.0078, T:2.7830(w:0.200)ğŸš€)
Batch 200/537: Loss=8.0628 (C:6.7443, R:0.0078, T:2.7120(w:0.200)ğŸš€)
Batch 225/537: Loss=8.0133 (C:6.6824, R:0.0078, T:2.7637(w:0.200)ğŸš€)
Batch 250/537: Loss=8.0501 (C:6.7779, R:0.0078, T:2.4845(w:0.200)ğŸš€)
Batch 275/537: Loss=8.0062 (C:6.7514, R:0.0078, T:2.3950(w:0.200)ğŸš€)
Batch 300/537: Loss=7.9891 (C:6.6791, R:0.0078, T:2.6488(w:0.200)ğŸš€)
Batch 325/537: Loss=8.0017 (C:6.7387, R:0.0077, T:2.4606(w:0.200)ğŸš€)
Batch 350/537: Loss=8.0423 (C:6.7379, R:0.0077, T:2.6605(w:0.200)ğŸš€)
Batch 375/537: Loss=7.9746 (C:6.7196, R:0.0078, T:2.3814(w:0.200)ğŸš€)
Batch 400/537: Loss=7.9929 (C:6.6896, R:0.0077, T:2.6701(w:0.200)ğŸš€)
Batch 425/537: Loss=8.0483 (C:6.7478, R:0.0077, T:2.6314(w:0.200)ğŸš€)
Batch 450/537: Loss=7.9659 (C:6.6586, R:0.0077, T:2.6775(w:0.200)ğŸš€)
Batch 475/537: Loss=8.0426 (C:6.7805, R:0.0077, T:2.4625(w:0.200)ğŸš€)
Batch 500/537: Loss=8.0097 (C:6.7383, R:0.0077, T:2.5067(w:0.200)ğŸš€)
Batch 525/537: Loss=7.9852 (C:6.7152, R:0.0077, T:2.5094(w:0.200)ğŸš€)

ğŸ“Š EPOCH 7 TRAINING SUMMARY:
  Total Loss: 8.0066
  Contrastive: 6.7265
  Reconstruction: 0.0077
  Topological: 2.5296 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.2310
  Contrastive: 6.6004
  Reconstruction: 0.0072
  Topological: 4.5666 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 7/300 COMPLETE (54.2s)
Train Loss: 8.0066 (C:6.7265, R:0.0077, T:2.5296)
Val Loss:   8.2310 (C:6.6004, R:0.0072, T:4.5666)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.8945 (C:6.6321, R:0.0078, T:2.4309(w:0.200)ğŸš€)
Batch  25/537: Loss=7.9353 (C:6.6655, R:0.0078, T:2.4608(w:0.200)ğŸš€)
Batch  50/537: Loss=7.9966 (C:6.6527, R:0.0078, T:2.8098(w:0.200)ğŸš€)
Batch  75/537: Loss=7.9279 (C:6.6452, R:0.0077, T:2.5411(w:0.200)ğŸš€)
Batch 100/537: Loss=7.8655 (C:6.5938, R:0.0078, T:2.4811(w:0.200)ğŸš€)
Batch 125/537: Loss=7.9508 (C:6.6889, R:0.0077, T:2.4364(w:0.200)ğŸš€)
Batch 150/537: Loss=7.9373 (C:6.6769, R:0.0077, T:2.4276(w:0.200)ğŸš€)
Batch 175/537: Loss=7.9641 (C:6.7025, R:0.0077, T:2.4644(w:0.200)ğŸš€)
Batch 200/537: Loss=7.9284 (C:6.6547, R:0.0077, T:2.4985(w:0.200)ğŸš€)
Batch 225/537: Loss=7.9319 (C:6.6165, R:0.0077, T:2.7210(w:0.200)ğŸš€)
Batch 250/537: Loss=7.9484 (C:6.7170, R:0.0077, T:2.2844(w:0.200)ğŸš€)
Batch 275/537: Loss=7.9409 (C:6.6883, R:0.0077, T:2.4041(w:0.200)ğŸš€)
Batch 300/537: Loss=7.9386 (C:6.6853, R:0.0077, T:2.3977(w:0.200)ğŸš€)
Batch 325/537: Loss=7.9317 (C:6.6888, R:0.0077, T:2.3527(w:0.200)ğŸš€)
Batch 350/537: Loss=7.9177 (C:6.6720, R:0.0076, T:2.4092(w:0.200)ğŸš€)
Batch 375/537: Loss=7.9040 (C:6.6614, R:0.0077, T:2.3826(w:0.200)ğŸš€)
Batch 400/537: Loss=7.8634 (C:6.6163, R:0.0077, T:2.3683(w:0.200)ğŸš€)
Batch 425/537: Loss=7.8562 (C:6.6107, R:0.0077, T:2.4002(w:0.200)ğŸš€)
Batch 450/537: Loss=7.9774 (C:6.7042, R:0.0077, T:2.5219(w:0.200)ğŸš€)
Batch 475/537: Loss=7.9739 (C:6.6945, R:0.0078, T:2.5173(w:0.200)ğŸš€)
Batch 500/537: Loss=7.9449 (C:6.7338, R:0.0077, T:2.2137(w:0.200)ğŸš€)
Batch 525/537: Loss=7.9240 (C:6.6740, R:0.0078, T:2.3603(w:0.200)ğŸš€)

ğŸ“Š EPOCH 8 TRAINING SUMMARY:
  Total Loss: 7.9494
  Contrastive: 6.6733
  Reconstruction: 0.0077
  Topological: 2.5220 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.2110
  Contrastive: 6.5578
  Reconstruction: 0.0072
  Topological: 4.6851 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 8/300 COMPLETE (47.4s)
Train Loss: 7.9494 (C:6.6733, R:0.0077, T:2.5220)
Val Loss:   8.2110 (C:6.5578, R:0.0072, T:4.6851)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.8891 (C:6.5903, R:0.0078, T:2.6062(w:0.200)ğŸš€)
Batch  25/537: Loss=7.8723 (C:6.6369, R:0.0077, T:2.3175(w:0.200)ğŸš€)
Batch  50/537: Loss=7.8532 (C:6.5743, R:0.0077, T:2.5597(w:0.200)ğŸš€)
Batch  75/537: Loss=7.8679 (C:6.6202, R:0.0077, T:2.3729(w:0.200)ğŸš€)
Batch 100/537: Loss=7.9123 (C:6.6274, R:0.0077, T:2.5740(w:0.200)ğŸš€)
Batch 125/537: Loss=7.9449 (C:6.6338, R:0.0078, T:2.6787(w:0.200)ğŸš€)
Batch 150/537: Loss=7.9519 (C:6.6341, R:0.0077, T:2.7229(w:0.200)ğŸš€)
Batch 175/537: Loss=7.9743 (C:6.6614, R:0.0077, T:2.6932(w:0.200)ğŸš€)
Batch 200/537: Loss=7.9422 (C:6.6266, R:0.0077, T:2.7147(w:0.200)ğŸš€)
Batch 225/537: Loss=7.9337 (C:6.6717, R:0.0077, T:2.4697(w:0.200)ğŸš€)
Batch 250/537: Loss=8.0294 (C:6.6895, R:0.0077, T:2.8317(w:0.200)ğŸš€)
Batch 275/537: Loss=7.9836 (C:6.6489, R:0.0077, T:2.8227(w:0.200)ğŸš€)
Batch 300/537: Loss=7.9363 (C:6.6630, R:0.0077, T:2.5142(w:0.200)ğŸš€)
Batch 325/537: Loss=7.9259 (C:6.6522, R:0.0078, T:2.4928(w:0.200)ğŸš€)
Batch 350/537: Loss=7.8523 (C:6.6161, R:0.0077, T:2.3332(w:0.200)ğŸš€)
Batch 375/537: Loss=7.8542 (C:6.6290, R:0.0077, T:2.2838(w:0.200)ğŸš€)
Batch 400/537: Loss=7.8645 (C:6.6164, R:0.0077, T:2.3775(w:0.200)ğŸš€)
Batch 425/537: Loss=7.8987 (C:6.6291, R:0.0077, T:2.5165(w:0.200)ğŸš€)
Batch 450/537: Loss=7.9714 (C:6.6723, R:0.0077, T:2.6323(w:0.200)ğŸš€)
Batch 475/537: Loss=7.8744 (C:6.6375, R:0.0077, T:2.3540(w:0.200)ğŸš€)
Batch 500/537: Loss=7.8421 (C:6.5965, R:0.0077, T:2.3766(w:0.200)ğŸš€)
Batch 525/537: Loss=7.9049 (C:6.6075, R:0.0077, T:2.6475(w:0.200)ğŸš€)

ğŸ“Š EPOCH 9 TRAINING SUMMARY:
  Total Loss: 7.9065
  Contrastive: 6.6355
  Reconstruction: 0.0077
  Topological: 2.5064 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.2745
  Contrastive: 6.5196
  Reconstruction: 0.0071
  Topological: 5.2204 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 9/300 COMPLETE (49.0s)
Train Loss: 7.9065 (C:6.6355, R:0.0077, T:2.5064)
Val Loss:   8.2745 (C:6.5196, R:0.0071, T:5.2204)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 10
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.437 Â± 1.156
    Neg distances: 7.650 Â± 2.048
    Separation ratio: 1.41x
    Gap: -11.436
    âŒ Poor global separation

============================================================
EPOCH 10 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3042 (C:6.0360, R:0.0076, T:2.5163(w:0.200)ğŸš€)
Batch  25/537: Loss=7.3549 (C:6.0839, R:0.0077, T:2.5036(w:0.200)ğŸš€)
Batch  50/537: Loss=7.3944 (C:6.0794, R:0.0077, T:2.7117(w:0.200)ğŸš€)
Batch  75/537: Loss=7.2416 (C:5.9208, R:0.0077, T:2.7485(w:0.200)ğŸš€)
Batch 100/537: Loss=7.4430 (C:6.0975, R:0.0077, T:2.8592(w:0.200)ğŸš€)
Batch 125/537: Loss=7.3219 (C:6.0484, R:0.0077, T:2.5178(w:0.200)ğŸš€)
Batch 150/537: Loss=7.4283 (C:6.1271, R:0.0077, T:2.6429(w:0.200)ğŸš€)
Batch 175/537: Loss=7.4291 (C:6.1082, R:0.0077, T:2.7504(w:0.200)ğŸš€)
Batch 200/537: Loss=7.3346 (C:6.0562, R:0.0077, T:2.5507(w:0.200)ğŸš€)
Batch 225/537: Loss=7.3677 (C:6.1239, R:0.0078, T:2.3420(w:0.200)ğŸš€)
Batch 250/537: Loss=7.3882 (C:6.0382, R:0.0077, T:2.8996(w:0.200)ğŸš€)
Batch 275/537: Loss=7.3932 (C:6.0652, R:0.0078, T:2.7629(w:0.200)ğŸš€)
Batch 300/537: Loss=7.3899 (C:6.0917, R:0.0077, T:2.6217(w:0.200)ğŸš€)
Batch 325/537: Loss=7.2542 (C:5.9771, R:0.0077, T:2.5395(w:0.200)ğŸš€)
Batch 350/537: Loss=7.3768 (C:6.0637, R:0.0077, T:2.7390(w:0.200)ğŸš€)
Batch 375/537: Loss=7.4070 (C:6.1035, R:0.0077, T:2.6670(w:0.200)ğŸš€)
Batch 400/537: Loss=7.4325 (C:6.0699, R:0.0077, T:2.9429(w:0.200)ğŸš€)
Batch 425/537: Loss=7.3625 (C:6.0595, R:0.0077, T:2.6736(w:0.200)ğŸš€)
Batch 450/537: Loss=7.3683 (C:6.0670, R:0.0077, T:2.6732(w:0.200)ğŸš€)
Batch 475/537: Loss=7.4908 (C:6.0952, R:0.0077, T:3.1154(w:0.200)ğŸš€)
Batch 500/537: Loss=7.3907 (C:6.1019, R:0.0076, T:2.6236(w:0.200)ğŸš€)
Batch 525/537: Loss=7.4004 (C:6.0721, R:0.0077, T:2.7828(w:0.200)ğŸš€)

ğŸ“Š EPOCH 10 TRAINING SUMMARY:
  Total Loss: 7.3645
  Contrastive: 6.0617
  Reconstruction: 0.0077
  Topological: 2.6666 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.7805
  Contrastive: 5.9345
  Reconstruction: 0.0071
  Topological: 5.6825 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 10/300 COMPLETE (55.3s)
Train Loss: 7.3645 (C:6.0617, R:0.0077, T:2.6666)
Val Loss:   7.7805 (C:5.9345, R:0.0071, T:5.6825)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2689 (C:5.9760, R:0.0077, T:2.5970(w:0.200)ğŸš€)
Batch  25/537: Loss=7.2533 (C:5.9497, R:0.0077, T:2.6912(w:0.200)ğŸš€)
Batch  50/537: Loss=7.3011 (C:5.9979, R:0.0077, T:2.6831(w:0.200)ğŸš€)
Batch  75/537: Loss=7.2805 (C:6.0231, R:0.0077, T:2.4381(w:0.200)ğŸš€)
Batch 100/537: Loss=7.2175 (C:5.9530, R:0.0077, T:2.4848(w:0.200)ğŸš€)
Batch 125/537: Loss=7.3521 (C:6.0255, R:0.0076, T:2.8131(w:0.200)ğŸš€)
Batch 150/537: Loss=7.2148 (C:5.9498, R:0.0076, T:2.5014(w:0.200)ğŸš€)
Batch 175/537: Loss=7.3312 (C:6.0792, R:0.0077, T:2.4003(w:0.200)ğŸš€)
Batch 200/537: Loss=7.3052 (C:6.0205, R:0.0077, T:2.5920(w:0.200)ğŸš€)
Batch 225/537: Loss=7.3844 (C:6.0300, R:0.0077, T:2.9376(w:0.200)ğŸš€)
Batch 250/537: Loss=7.3799 (C:6.0765, R:0.0077, T:2.6764(w:0.200)ğŸš€)
Batch 275/537: Loss=7.3257 (C:6.0443, R:0.0076, T:2.5848(w:0.200)ğŸš€)
Batch 300/537: Loss=7.3778 (C:6.0568, R:0.0076, T:2.7862(w:0.200)ğŸš€)
Batch 325/537: Loss=7.3889 (C:6.0851, R:0.0076, T:2.7005(w:0.200)ğŸš€)
Batch 350/537: Loss=7.3492 (C:6.0652, R:0.0077, T:2.5754(w:0.200)ğŸš€)
Batch 375/537: Loss=7.2830 (C:5.9679, R:0.0076, T:2.7509(w:0.200)ğŸš€)
Batch 400/537: Loss=7.3281 (C:6.0055, R:0.0076, T:2.8220(w:0.200)ğŸš€)
Batch 425/537: Loss=7.3022 (C:5.9882, R:0.0077, T:2.7444(w:0.200)ğŸš€)
Batch 450/537: Loss=7.2570 (C:5.9907, R:0.0076, T:2.5132(w:0.200)ğŸš€)
Batch 475/537: Loss=7.2936 (C:6.0633, R:0.0077, T:2.3122(w:0.200)ğŸš€)
Batch 500/537: Loss=7.3164 (C:6.0157, R:0.0077, T:2.6694(w:0.200)ğŸš€)
Batch 525/537: Loss=7.3126 (C:5.9962, R:0.0076, T:2.7652(w:0.200)ğŸš€)

ğŸ“Š EPOCH 11 TRAINING SUMMARY:
  Total Loss: 7.3228
  Contrastive: 6.0254
  Reconstruction: 0.0077
  Topological: 2.6516 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.7706
  Contrastive: 5.9274
  Reconstruction: 0.0070
  Topological: 5.6918 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 11/300 COMPLETE (48.0s)
Train Loss: 7.3228 (C:6.0254, R:0.0077, T:2.6516)
Val Loss:   7.7706 (C:5.9274, R:0.0070, T:5.6918)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 537 | Topological Weight: 0.2200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3312 (C:6.0153, R:0.0076, T:2.5240(w:0.220)ğŸš€)
Batch  25/537: Loss=7.2140 (C:5.8971, R:0.0077, T:2.4912(w:0.220)ğŸš€)
Batch  50/537: Loss=7.3075 (C:5.9815, R:0.0077, T:2.5475(w:0.220)ğŸš€)
Batch  75/537: Loss=7.2847 (C:6.0024, R:0.0076, T:2.3605(w:0.220)ğŸš€)
Batch 100/537: Loss=7.2831 (C:5.9347, R:0.0077, T:2.6299(w:0.220)ğŸš€)
Batch 125/537: Loss=7.3924 (C:6.0562, R:0.0077, T:2.5927(w:0.220)ğŸš€)
Batch 150/537: Loss=7.2908 (C:5.9988, R:0.0076, T:2.4156(w:0.220)ğŸš€)
Batch 175/537: Loss=7.3195 (C:6.0299, R:0.0076, T:2.3898(w:0.220)ğŸš€)
Batch 200/537: Loss=7.3351 (C:6.0264, R:0.0077, T:2.4627(w:0.220)ğŸš€)
Batch 225/537: Loss=7.3961 (C:6.0465, R:0.0077, T:2.6444(w:0.220)ğŸš€)
Batch 250/537: Loss=7.3777 (C:6.0467, R:0.0077, T:2.5566(w:0.220)ğŸš€)
Batch 275/537: Loss=7.3671 (C:6.0451, R:0.0076, T:2.5392(w:0.220)ğŸš€)
Batch 300/537: Loss=7.3622 (C:6.0256, R:0.0076, T:2.6174(w:0.220)ğŸš€)
Batch 325/537: Loss=7.3988 (C:6.0919, R:0.0076, T:2.4748(w:0.220)ğŸš€)
Batch 350/537: Loss=7.2311 (C:5.9209, R:0.0077, T:2.4759(w:0.220)ğŸš€)
Batch 375/537: Loss=7.3187 (C:6.0059, R:0.0077, T:2.4854(w:0.220)ğŸš€)
Batch 400/537: Loss=7.3843 (C:6.0572, R:0.0076, T:2.5729(w:0.220)ğŸš€)
Batch 425/537: Loss=7.3759 (C:5.9679, R:0.0077, T:2.9135(w:0.220)ğŸš€)
Batch 450/537: Loss=7.3800 (C:5.9905, R:0.0076, T:2.8526(w:0.220)ğŸš€)
Batch 475/537: Loss=7.3488 (C:6.0485, R:0.0076, T:2.4613(w:0.220)ğŸš€)
Batch 500/537: Loss=7.3064 (C:6.0102, R:0.0076, T:2.4286(w:0.220)ğŸš€)
Batch 525/537: Loss=7.4239 (C:6.0966, R:0.0076, T:2.5671(w:0.220)ğŸš€)

ğŸ“Š EPOCH 12 TRAINING SUMMARY:
  Total Loss: 7.3321
  Contrastive: 6.0166
  Reconstruction: 0.0076
  Topological: 2.5053 (weight: 0.220)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.8349
  Contrastive: 5.9504
  Reconstruction: 0.0070
  Topological: 5.3834 (weight: 0.220)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 12/300 COMPLETE (47.8s)
Train Loss: 7.3321 (C:6.0166, R:0.0076, T:2.5053)
Val Loss:   7.8349 (C:5.9504, R:0.0070, T:5.3834)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 13
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.201 Â± 1.097
    Neg distances: 7.634 Â± 2.054
    Separation ratio: 1.47x
    Gap: -11.581
    âŒ Poor global separation

============================================================
EPOCH 13 | Batches: 537 | Topological Weight: 0.2400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1234 (C:5.8424, R:0.0076, T:2.1575(w:0.240)ğŸš€)
Batch  25/537: Loss=7.1656 (C:5.7709, R:0.0076, T:2.6571(w:0.240)ğŸš€)
Batch  50/537: Loss=7.2230 (C:5.8983, R:0.0076, T:2.3421(w:0.240)ğŸš€)
Batch  75/537: Loss=7.1640 (C:5.8406, R:0.0077, T:2.3106(w:0.240)ğŸš€)
Batch 100/537: Loss=7.2254 (C:5.8499, R:0.0077, T:2.5127(w:0.240)ğŸš€)
Batch 125/537: Loss=7.2869 (C:5.9305, R:0.0076, T:2.4708(w:0.240)ğŸš€)
Batch 150/537: Loss=7.1126 (C:5.8291, R:0.0076, T:2.1755(w:0.240)ğŸš€)
Batch 175/537: Loss=7.1696 (C:5.8360, R:0.0076, T:2.3810(w:0.240)ğŸš€)
Batch 200/537: Loss=7.1642 (C:5.8310, R:0.0077, T:2.3597(w:0.240)ğŸš€)
Batch 225/537: Loss=7.3222 (C:5.8798, R:0.0076, T:2.8247(w:0.240)ğŸš€)
Batch 250/537: Loss=7.2756 (C:5.9048, R:0.0076, T:2.5434(w:0.240)ğŸš€)
Batch 275/537: Loss=7.2446 (C:5.8936, R:0.0076, T:2.4464(w:0.240)ğŸš€)
Batch 300/537: Loss=7.2313 (C:5.8990, R:0.0077, T:2.3631(w:0.240)ğŸš€)
Batch 325/537: Loss=7.2446 (C:5.9352, R:0.0076, T:2.2929(w:0.240)ğŸš€)
Batch 350/537: Loss=7.2876 (C:5.8759, R:0.0076, T:2.7051(w:0.240)ğŸš€)
Batch 375/537: Loss=7.2459 (C:5.9279, R:0.0076, T:2.3435(w:0.240)ğŸš€)
Batch 400/537: Loss=7.1912 (C:5.9132, R:0.0076, T:2.1525(w:0.240)ğŸš€)
Batch 425/537: Loss=7.2069 (C:5.8707, R:0.0076, T:2.3940(w:0.240)ğŸš€)
Batch 450/537: Loss=7.1771 (C:5.8551, R:0.0076, T:2.3553(w:0.240)ğŸš€)
Batch 475/537: Loss=7.2422 (C:5.9134, R:0.0076, T:2.3619(w:0.240)ğŸš€)
Batch 500/537: Loss=7.2594 (C:5.9122, R:0.0076, T:2.4386(w:0.240)ğŸš€)
Batch 525/537: Loss=7.2105 (C:5.8688, R:0.0076, T:2.4192(w:0.240)ğŸš€)

ğŸ“Š EPOCH 13 TRAINING SUMMARY:
  Total Loss: 7.2172
  Contrastive: 5.8711
  Reconstruction: 0.0076
  Topological: 2.4331 (weight: 0.240)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.8000
  Contrastive: 5.8218
  Reconstruction: 0.0070
  Topological: 5.3351 (weight: 0.240)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 13/300 COMPLETE (56.5s)
Train Loss: 7.2172 (C:5.8711, R:0.0076, T:2.4331)
Val Loss:   7.8000 (C:5.8218, R:0.0070, T:5.3351)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 537 | Topological Weight: 0.2600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2212 (C:5.8480, R:0.0076, T:2.3634(w:0.260)ğŸš€)
Batch  25/537: Loss=7.1301 (C:5.7490, R:0.0076, T:2.3923(w:0.260)ğŸš€)
Batch  50/537: Loss=7.2442 (C:5.8266, R:0.0076, T:2.5197(w:0.260)ğŸš€)
Batch  75/537: Loss=7.1672 (C:5.8248, R:0.0076, T:2.2449(w:0.260)ğŸš€)
Batch 100/537: Loss=7.2309 (C:5.8820, R:0.0077, T:2.2289(w:0.260)ğŸš€)
Batch 125/537: Loss=7.1909 (C:5.7991, R:0.0076, T:2.4376(w:0.260)ğŸš€)
Batch 150/537: Loss=7.2136 (C:5.8443, R:0.0076, T:2.3469(w:0.260)ğŸš€)
Batch 175/537: Loss=7.2184 (C:5.8129, R:0.0076, T:2.4787(w:0.260)ğŸš€)
Batch 200/537: Loss=7.1781 (C:5.8461, R:0.0076, T:2.2082(w:0.260)ğŸš€)
Batch 225/537: Loss=7.2866 (C:5.9133, R:0.0077, T:2.3350(w:0.260)ğŸš€)
Batch 250/537: Loss=7.2553 (C:5.8964, R:0.0075, T:2.3256(w:0.260)ğŸš€)
Batch 275/537: Loss=7.2493 (C:5.8793, R:0.0075, T:2.3795(w:0.260)ğŸš€)
Batch 300/537: Loss=7.2476 (C:5.8928, R:0.0076, T:2.2782(w:0.260)ğŸš€)
Batch 325/537: Loss=7.2027 (C:5.8847, R:0.0075, T:2.1707(w:0.260)ğŸš€)
Batch 350/537: Loss=7.2470 (C:5.9387, R:0.0076, T:2.1142(w:0.260)ğŸš€)
Batch 375/537: Loss=7.3327 (C:5.9319, R:0.0076, T:2.4761(w:0.260)ğŸš€)
Batch 400/537: Loss=7.2471 (C:5.8858, R:0.0077, T:2.2915(w:0.260)ğŸš€)
Batch 425/537: Loss=7.3246 (C:5.9384, R:0.0076, T:2.4229(w:0.260)ğŸš€)
Batch 450/537: Loss=7.3205 (C:5.9039, R:0.0076, T:2.5303(w:0.260)ğŸš€)
Batch 475/537: Loss=7.2466 (C:5.8592, R:0.0076, T:2.4239(w:0.260)ğŸš€)
Batch 500/537: Loss=7.2973 (C:5.9062, R:0.0076, T:2.4224(w:0.260)ğŸš€)
Batch 525/537: Loss=7.1689 (C:5.8703, R:0.0075, T:2.0922(w:0.260)ğŸš€)

ğŸ“Š EPOCH 14 TRAINING SUMMARY:
  Total Loss: 7.2279
  Contrastive: 5.8652
  Reconstruction: 0.0076
  Topological: 2.3199 (weight: 0.260)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.8517
  Contrastive: 5.7368
  Reconstruction: 0.0069
  Topological: 5.4655 (weight: 0.260)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 14/300 COMPLETE (49.9s)
Train Loss: 7.2279 (C:5.8652, R:0.0076, T:2.3199)
Val Loss:   7.8517 (C:5.7368, R:0.0069, T:5.4655)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 537 | Topological Weight: 0.2800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1201 (C:5.7653, R:0.0076, T:2.1217(w:0.280)ğŸš€)
Batch  25/537: Loss=7.1754 (C:5.8113, R:0.0076, T:2.1649(w:0.280)ğŸš€)
Batch  50/537: Loss=7.3202 (C:5.9101, R:0.0076, T:2.3226(w:0.280)ğŸš€)
Batch  75/537: Loss=7.2876 (C:5.9152, R:0.0076, T:2.2046(w:0.280)ğŸš€)
Batch 100/537: Loss=7.2447 (C:5.8692, R:0.0076, T:2.2078(w:0.280)ğŸš€)
Batch 125/537: Loss=7.2388 (C:5.8548, R:0.0076, T:2.2261(w:0.280)ğŸš€)
Batch 150/537: Loss=7.1953 (C:5.8120, R:0.0076, T:2.2357(w:0.280)ğŸš€)
Batch 175/537: Loss=7.2656 (C:5.8726, R:0.0076, T:2.2562(w:0.280)ğŸš€)
Batch 200/537: Loss=7.2441 (C:5.8519, R:0.0076, T:2.2613(w:0.280)ğŸš€)
Batch 225/537: Loss=7.2210 (C:5.8628, R:0.0076, T:2.1429(w:0.280)ğŸš€)
Batch 250/537: Loss=7.2655 (C:5.8267, R:0.0075, T:2.4449(w:0.280)ğŸš€)
Batch 275/537: Loss=7.2174 (C:5.8414, R:0.0076, T:2.2074(w:0.280)ğŸš€)
Batch 300/537: Loss=7.3465 (C:5.9509, R:0.0076, T:2.2812(w:0.280)ğŸš€)
Batch 325/537: Loss=7.3011 (C:5.9038, R:0.0075, T:2.3093(w:0.280)ğŸš€)
Batch 350/537: Loss=7.2539 (C:5.9275, R:0.0075, T:2.0455(w:0.280)ğŸš€)
Batch 375/537: Loss=7.0867 (C:5.7282, R:0.0076, T:2.1504(w:0.280)ğŸš€)
Batch 400/537: Loss=7.2769 (C:5.9071, R:0.0075, T:2.1978(w:0.280)ğŸš€)
Batch 425/537: Loss=7.2743 (C:5.8541, R:0.0076, T:2.3697(w:0.280)ğŸš€)
Batch 450/537: Loss=7.2418 (C:5.8738, R:0.0076, T:2.1848(w:0.280)ğŸš€)
Batch 475/537: Loss=7.2967 (C:5.8627, R:0.0076, T:2.4250(w:0.280)ğŸš€)
Batch 500/537: Loss=7.2954 (C:5.9231, R:0.0075, T:2.2215(w:0.280)ğŸš€)
Batch 525/537: Loss=7.2350 (C:5.8369, R:0.0075, T:2.3102(w:0.280)ğŸš€)

ğŸ“Š EPOCH 15 TRAINING SUMMARY:
  Total Loss: 7.2377
  Contrastive: 5.8619
  Reconstruction: 0.0076
  Topological: 2.2128 (weight: 0.280)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.9767
  Contrastive: 5.8032
  Reconstruction: 0.0069
  Topological: 5.3009 (weight: 0.280)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 15/300 COMPLETE (50.3s)
Train Loss: 7.2377 (C:5.8619, R:0.0076, T:2.2128)
Val Loss:   7.9767 (C:5.8032, R:0.0069, T:5.3009)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 16
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.145 Â± 0.998
    Neg distances: 7.664 Â± 2.046
    Separation ratio: 1.49x
    Gap: -11.561
    âŒ Poor global separation

============================================================
EPOCH 16 | Batches: 537 | Topological Weight: 0.3000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1234 (C:5.7465, R:0.0076, T:2.0676(w:0.300)ğŸš€)
Batch  25/537: Loss=7.1578 (C:5.7712, R:0.0076, T:2.0815(w:0.300)ğŸš€)
Batch  50/537: Loss=7.2856 (C:5.7821, R:0.0076, T:2.4877(w:0.300)ğŸš€)
Batch  75/537: Loss=7.2787 (C:5.8207, R:0.0076, T:2.3225(w:0.300)ğŸš€)
Batch 100/537: Loss=7.2698 (C:5.8382, R:0.0075, T:2.2608(w:0.300)ğŸš€)
Batch 125/537: Loss=7.1565 (C:5.7856, R:0.0076, T:2.0381(w:0.300)ğŸš€)
Batch 150/537: Loss=7.1411 (C:5.7661, R:0.0075, T:2.0764(w:0.300)ğŸš€)
Batch 175/537: Loss=7.0247 (C:5.7287, R:0.0076, T:1.7915(w:0.300)ğŸš€)
Batch 200/537: Loss=7.2332 (C:5.8477, R:0.0076, T:2.0962(w:0.300)ğŸš€)
Batch 225/537: Loss=7.2096 (C:5.8277, R:0.0076, T:2.0870(w:0.300)ğŸš€)
Batch 250/537: Loss=7.2868 (C:5.8245, R:0.0076, T:2.3552(w:0.300)ğŸš€)
Batch 275/537: Loss=7.3345 (C:5.8236, R:0.0075, T:2.5229(w:0.300)ğŸš€)
Batch 300/537: Loss=7.1704 (C:5.7958, R:0.0076, T:2.0609(w:0.300)ğŸš€)
Batch 325/537: Loss=7.1601 (C:5.7756, R:0.0076, T:2.0931(w:0.300)ğŸš€)
Batch 350/537: Loss=7.2903 (C:5.8559, R:0.0075, T:2.2737(w:0.300)ğŸš€)
Batch 375/537: Loss=7.1753 (C:5.8059, R:0.0076, T:2.0235(w:0.300)ğŸš€)
Batch 400/537: Loss=7.1780 (C:5.8792, R:0.0075, T:1.8169(w:0.300)ğŸš€)
Batch 425/537: Loss=7.1737 (C:5.7476, R:0.0076, T:2.2215(w:0.300)ğŸš€)
Batch 450/537: Loss=7.2194 (C:5.8009, R:0.0075, T:2.2289(w:0.300)ğŸš€)
Batch 475/537: Loss=7.0930 (C:5.8109, R:0.0075, T:1.7698(w:0.300)ğŸš€)
Batch 500/537: Loss=7.1641 (C:5.7469, R:0.0076, T:2.2009(w:0.300)ğŸš€)
Batch 525/537: Loss=7.1984 (C:5.8144, R:0.0076, T:2.0911(w:0.300)ğŸš€)

ğŸ“Š EPOCH 16 TRAINING SUMMARY:
  Total Loss: 7.1935
  Contrastive: 5.7988
  Reconstruction: 0.0075
  Topological: 2.1336 (weight: 0.300)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.9662
  Contrastive: 5.7400
  Reconstruction: 0.0069
  Topological: 5.1264 (weight: 0.300)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 16/300 COMPLETE (57.4s)
Train Loss: 7.1935 (C:5.7988, R:0.0075, T:2.1336)
Val Loss:   7.9662 (C:5.7400, R:0.0069, T:5.1264)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 537 | Topological Weight: 0.3200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0221 (C:5.6947, R:0.0075, T:1.7928(w:0.320)ğŸš€)
Batch  25/537: Loss=7.2633 (C:5.7628, R:0.0075, T:2.3495(w:0.320)ğŸš€)
Batch  50/537: Loss=7.0965 (C:5.7278, R:0.0075, T:1.9420(w:0.320)ğŸš€)
Batch  75/537: Loss=7.2846 (C:5.8430, R:0.0076, T:2.1159(w:0.320)ğŸš€)
Batch 100/537: Loss=7.2139 (C:5.7475, R:0.0076, T:2.2187(w:0.320)ğŸš€)
Batch 125/537: Loss=7.1816 (C:5.8048, R:0.0076, T:1.9419(w:0.320)ğŸš€)
Batch 150/537: Loss=7.1714 (C:5.7754, R:0.0075, T:2.0064(w:0.320)ğŸš€)
Batch 175/537: Loss=7.2255 (C:5.7761, R:0.0075, T:2.1948(w:0.320)ğŸš€)
Batch 200/537: Loss=7.2209 (C:5.7798, R:0.0075, T:2.1505(w:0.320)ğŸš€)
Batch 225/537: Loss=7.1917 (C:5.7823, R:0.0075, T:2.0526(w:0.320)ğŸš€)
Batch 250/537: Loss=7.2206 (C:5.8080, R:0.0075, T:2.0575(w:0.320)ğŸš€)
Batch 275/537: Loss=7.2580 (C:5.8203, R:0.0075, T:2.1515(w:0.320)ğŸš€)
Batch 300/537: Loss=7.2194 (C:5.8041, R:0.0074, T:2.1005(w:0.320)ğŸš€)
Batch 325/537: Loss=7.1987 (C:5.7945, R:0.0075, T:2.0359(w:0.320)ğŸš€)
Batch 350/537: Loss=7.2516 (C:5.8106, R:0.0075, T:2.1579(w:0.320)ğŸš€)
Batch 375/537: Loss=7.1949 (C:5.7763, R:0.0076, T:2.0668(w:0.320)ğŸš€)
Batch 400/537: Loss=7.2711 (C:5.7589, R:0.0076, T:2.3590(w:0.320)ğŸš€)
Batch 425/537: Loss=7.2198 (C:5.8204, R:0.0075, T:2.0168(w:0.320)ğŸš€)
Batch 450/537: Loss=7.1436 (C:5.7593, R:0.0075, T:1.9709(w:0.320)ğŸš€)
Batch 475/537: Loss=7.1445 (C:5.7728, R:0.0075, T:1.9439(w:0.320)ğŸš€)
Batch 500/537: Loss=7.2280 (C:5.8306, R:0.0075, T:2.0169(w:0.320)ğŸš€)
Batch 525/537: Loss=7.1957 (C:5.8406, R:0.0075, T:1.8984(w:0.320)ğŸš€)

ğŸ“Š EPOCH 17 TRAINING SUMMARY:
  Total Loss: 7.2024
  Contrastive: 5.7936
  Reconstruction: 0.0075
  Topological: 2.0493 (weight: 0.320)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.0523
  Contrastive: 5.7458
  Reconstruction: 0.0069
  Topological: 5.0667 (weight: 0.320)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 17/300 COMPLETE (49.8s)
Train Loss: 7.2024 (C:5.7936, R:0.0075, T:2.0493)
Val Loss:   8.0523 (C:5.7458, R:0.0069, T:5.0667)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 537 | Topological Weight: 0.3400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1696 (C:5.7964, R:0.0075, T:1.8319(w:0.340)ğŸš€)
Batch  25/537: Loss=7.1898 (C:5.7579, R:0.0075, T:2.0075(w:0.340)ğŸš€)
Batch  50/537: Loss=7.1705 (C:5.7535, R:0.0075, T:1.9556(w:0.340)ğŸš€)
Batch  75/537: Loss=7.1230 (C:5.7062, R:0.0076, T:1.9370(w:0.340)ğŸš€)
Batch 100/537: Loss=7.1768 (C:5.7731, R:0.0075, T:1.9125(w:0.340)ğŸš€)
Batch 125/537: Loss=7.3640 (C:5.8805, R:0.0075, T:2.1468(w:0.340)ğŸš€)
Batch 150/537: Loss=7.1763 (C:5.7985, R:0.0075, T:1.8361(w:0.340)ğŸš€)
Batch 175/537: Loss=7.2840 (C:5.8424, R:0.0076, T:2.0134(w:0.340)ğŸš€)
Batch 200/537: Loss=7.1302 (C:5.7082, R:0.0075, T:1.9697(w:0.340)ğŸš€)
Batch 225/537: Loss=7.2634 (C:5.8231, R:0.0075, T:2.0171(w:0.340)ğŸš€)
Batch 250/537: Loss=7.2275 (C:5.8610, R:0.0075, T:1.8106(w:0.340)ğŸš€)
Batch 275/537: Loss=7.1907 (C:5.8004, R:0.0075, T:1.8716(w:0.340)ğŸš€)
Batch 300/537: Loss=7.1997 (C:5.7907, R:0.0075, T:1.9264(w:0.340)ğŸš€)
Batch 325/537: Loss=7.2778 (C:5.7872, R:0.0075, T:2.1750(w:0.340)ğŸš€)
Batch 350/537: Loss=7.0889 (C:5.6989, R:0.0075, T:1.8801(w:0.340)ğŸš€)
Batch 375/537: Loss=7.1406 (C:5.7613, R:0.0075, T:1.8419(w:0.340)ğŸš€)
Batch 400/537: Loss=7.1604 (C:5.7573, R:0.0075, T:1.9081(w:0.340)ğŸš€)
Batch 425/537: Loss=7.2611 (C:5.7859, R:0.0075, T:2.1293(w:0.340)ğŸš€)
Batch 450/537: Loss=7.2615 (C:5.7556, R:0.0075, T:2.2217(w:0.340)ğŸš€)
Batch 475/537: Loss=7.2912 (C:5.8620, R:0.0075, T:1.9954(w:0.340)ğŸš€)
Batch 500/537: Loss=7.1828 (C:5.8257, R:0.0075, T:1.7888(w:0.340)ğŸš€)
Batch 525/537: Loss=7.2386 (C:5.8567, R:0.0075, T:1.8626(w:0.340)ğŸš€)

ğŸ“Š EPOCH 18 TRAINING SUMMARY:
  Total Loss: 7.2136
  Contrastive: 5.7913
  Reconstruction: 0.0075
  Topological: 1.9724 (weight: 0.340)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.0718
  Contrastive: 5.7593
  Reconstruction: 0.0068
  Topological: 4.7917 (weight: 0.340)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 18/300 COMPLETE (49.8s)
Train Loss: 7.2136 (C:5.7913, R:0.0075, T:1.9724)
Val Loss:   8.0718 (C:5.7593, R:0.0068, T:4.7917)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 19
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.291 Â± 0.982
    Neg distances: 7.899 Â± 2.102
    Separation ratio: 1.49x
    Gap: -11.074
    âŒ Poor global separation

============================================================
EPOCH 19 | Batches: 537 | Topological Weight: 0.3600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3529 (C:5.8868, R:0.0075, T:1.9838(w:0.360)ğŸš€)
Batch  25/537: Loss=7.1124 (C:5.7092, R:0.0075, T:1.8119(w:0.360)ğŸš€)
Batch  50/537: Loss=7.2410 (C:5.7898, R:0.0075, T:1.9385(w:0.360)ğŸš€)
Batch  75/537: Loss=7.2212 (C:5.8092, R:0.0075, T:1.8388(w:0.360)ğŸš€)
Batch 100/537: Loss=7.1830 (C:5.7627, R:0.0075, T:1.8580(w:0.360)ğŸš€)
Batch 125/537: Loss=7.1473 (C:5.7567, R:0.0075, T:1.7877(w:0.360)ğŸš€)
Batch 150/537: Loss=7.2445 (C:5.7697, R:0.0075, T:2.0039(w:0.360)ğŸš€)
Batch 175/537: Loss=7.0882 (C:5.7145, R:0.0075, T:1.7355(w:0.360)ğŸš€)
Batch 200/537: Loss=7.1316 (C:5.7351, R:0.0075, T:1.7915(w:0.360)ğŸš€)
Batch 225/537: Loss=7.0862 (C:5.6734, R:0.0075, T:1.8351(w:0.360)ğŸš€)
Batch 250/537: Loss=7.1939 (C:5.7561, R:0.0075, T:1.9177(w:0.360)ğŸš€)
Batch 275/537: Loss=7.1728 (C:5.7422, R:0.0075, T:1.8782(w:0.360)ğŸš€)
Batch 300/537: Loss=7.1813 (C:5.7496, R:0.0075, T:1.8854(w:0.360)ğŸš€)
Batch 325/537: Loss=7.1042 (C:5.7461, R:0.0075, T:1.6978(w:0.360)ğŸš€)
Batch 350/537: Loss=7.2394 (C:5.7866, R:0.0075, T:1.9535(w:0.360)ğŸš€)
Batch 375/537: Loss=7.3140 (C:5.7816, R:0.0076, T:2.1530(w:0.360)ğŸš€)
Batch 400/537: Loss=7.1474 (C:5.7839, R:0.0075, T:1.7095(w:0.360)ğŸš€)
Batch 425/537: Loss=7.2661 (C:5.7724, R:0.0075, T:2.0689(w:0.360)ğŸš€)
Batch 450/537: Loss=7.2086 (C:5.7258, R:0.0075, T:2.0397(w:0.360)ğŸš€)
Batch 475/537: Loss=7.1497 (C:5.8156, R:0.0075, T:1.6242(w:0.360)ğŸš€)
Batch 500/537: Loss=7.1555 (C:5.7362, R:0.0075, T:1.8575(w:0.360)ğŸš€)
Batch 525/537: Loss=7.2353 (C:5.8184, R:0.0074, T:1.8704(w:0.360)ğŸš€)

ğŸ“Š EPOCH 19 TRAINING SUMMARY:
  Total Loss: 7.2054
  Contrastive: 5.7725
  Reconstruction: 0.0075
  Topological: 1.8961 (weight: 0.360)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.2152
  Contrastive: 5.7187
  Reconstruction: 0.0068
  Topological: 5.0379 (weight: 0.360)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 19/300 COMPLETE (57.3s)
Train Loss: 7.2054 (C:5.7725, R:0.0075, T:1.8961)
Val Loss:   8.2152 (C:5.7187, R:0.0068, T:5.0379)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 537 | Topological Weight: 0.3800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1802 (C:5.7874, R:0.0075, T:1.6935(w:0.380)ğŸš€)
Batch  25/537: Loss=7.1405 (C:5.7123, R:0.0075, T:1.7865(w:0.380)ğŸš€)
Batch  50/537: Loss=7.1921 (C:5.7511, R:0.0074, T:1.8329(w:0.380)ğŸš€)
Batch  75/537: Loss=7.2987 (C:5.8387, R:0.0075, T:1.8815(w:0.380)ğŸš€)
Batch 100/537: Loss=7.1191 (C:5.7466, R:0.0075, T:1.6401(w:0.380)ğŸš€)
Batch 125/537: Loss=7.1395 (C:5.7376, R:0.0075, T:1.7234(w:0.380)ğŸš€)
Batch 150/537: Loss=7.0356 (C:5.6421, R:0.0075, T:1.6863(w:0.380)ğŸš€)
Batch 175/537: Loss=7.1559 (C:5.7889, R:0.0075, T:1.6134(w:0.380)ğŸš€)
Batch 200/537: Loss=7.2276 (C:5.7438, R:0.0075, T:1.9284(w:0.380)ğŸš€)
Batch 225/537: Loss=7.1476 (C:5.7161, R:0.0075, T:1.8002(w:0.380)ğŸš€)
Batch 250/537: Loss=7.2314 (C:5.7331, R:0.0075, T:1.9568(w:0.380)ğŸš€)
Batch 275/537: Loss=7.2303 (C:5.7096, R:0.0075, T:2.0342(w:0.380)ğŸš€)
Batch 300/537: Loss=7.1591 (C:5.7346, R:0.0075, T:1.7807(w:0.380)ğŸš€)
Batch 325/537: Loss=7.2060 (C:5.8012, R:0.0075, T:1.7224(w:0.380)ğŸš€)
Batch 350/537: Loss=7.1709 (C:5.7234, R:0.0075, T:1.8436(w:0.380)ğŸš€)
Batch 375/537: Loss=7.1878 (C:5.7432, R:0.0075, T:1.8259(w:0.380)ğŸš€)
Batch 400/537: Loss=7.2288 (C:5.7772, R:0.0075, T:1.8509(w:0.380)ğŸš€)
Batch 425/537: Loss=7.1782 (C:5.7707, R:0.0075, T:1.7326(w:0.380)ğŸš€)
Batch 450/537: Loss=7.2051 (C:5.8131, R:0.0074, T:1.7070(w:0.380)ğŸš€)
Batch 475/537: Loss=7.2445 (C:5.8085, R:0.0075, T:1.8047(w:0.380)ğŸš€)
Batch 500/537: Loss=7.1202 (C:5.6433, R:0.0075, T:1.9117(w:0.380)ğŸš€)
Batch 525/537: Loss=7.1549 (C:5.7773, R:0.0074, T:1.6689(w:0.380)ğŸš€)

ğŸ“Š EPOCH 20 TRAINING SUMMARY:
  Total Loss: 7.2091
  Contrastive: 5.7643
  Reconstruction: 0.0075
  Topological: 1.8316 (weight: 0.380)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.2652
  Contrastive: 5.7048
  Reconstruction: 0.0068
  Topological: 4.9451 (weight: 0.380)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 20/300 COMPLETE (49.0s)
Train Loss: 7.2091 (C:5.7643, R:0.0075, T:1.8316)
Val Loss:   8.2652 (C:5.7048, R:0.0068, T:4.9451)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 537 | Topological Weight: 0.4000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1440 (C:5.7471, R:0.0075, T:1.6170(w:0.400)ğŸš€)
Batch  25/537: Loss=7.0977 (C:5.6703, R:0.0075, T:1.7016(w:0.400)ğŸš€)
Batch  50/537: Loss=7.1405 (C:5.6974, R:0.0074, T:1.7545(w:0.400)ğŸš€)
Batch  75/537: Loss=7.3307 (C:5.8249, R:0.0075, T:1.8870(w:0.400)ğŸš€)
Batch 100/537: Loss=7.2769 (C:5.7161, R:0.0075, T:2.0251(w:0.400)ğŸš€)
Batch 125/537: Loss=7.1967 (C:5.7404, R:0.0075, T:1.7552(w:0.400)ğŸš€)
Batch 150/537: Loss=7.1846 (C:5.7658, R:0.0075, T:1.6759(w:0.400)ğŸš€)
Batch 175/537: Loss=7.2095 (C:5.7480, R:0.0075, T:1.7895(w:0.400)ğŸš€)
Batch 200/537: Loss=7.1838 (C:5.6972, R:0.0075, T:1.8311(w:0.400)ğŸš€)
Batch 225/537: Loss=7.2889 (C:5.8176, R:0.0075, T:1.8084(w:0.400)ğŸš€)
Batch 250/537: Loss=7.1684 (C:5.6787, R:0.0075, T:1.8537(w:0.400)ğŸš€)
Batch 275/537: Loss=7.2326 (C:5.7965, R:0.0075, T:1.7198(w:0.400)ğŸš€)
Batch 300/537: Loss=7.2750 (C:5.7992, R:0.0075, T:1.8205(w:0.400)ğŸš€)
Batch 325/537: Loss=7.2189 (C:5.7892, R:0.0075, T:1.7048(w:0.400)ğŸš€)
Batch 350/537: Loss=7.1171 (C:5.6678, R:0.0075, T:1.7558(w:0.400)ğŸš€)
Batch 375/537: Loss=7.1756 (C:5.7625, R:0.0075, T:1.6695(w:0.400)ğŸš€)
Batch 400/537: Loss=7.1115 (C:5.7013, R:0.0075, T:1.6598(w:0.400)ğŸš€)
Batch 425/537: Loss=7.2009 (C:5.7802, R:0.0075, T:1.6801(w:0.400)ğŸš€)
Batch 450/537: Loss=7.1839 (C:5.7430, R:0.0075, T:1.7203(w:0.400)ğŸš€)
Batch 475/537: Loss=7.3166 (C:5.8636, R:0.0074, T:1.7769(w:0.400)ğŸš€)
Batch 500/537: Loss=7.2204 (C:5.7288, R:0.0075, T:1.8605(w:0.400)ğŸš€)
Batch 525/537: Loss=7.1676 (C:5.7748, R:0.0075, T:1.6118(w:0.400)ğŸš€)

ğŸ“Š EPOCH 21 TRAINING SUMMARY:
  Total Loss: 7.2199
  Contrastive: 5.7623
  Reconstruction: 0.0075
  Topological: 1.7733 (weight: 0.400)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.4335
  Contrastive: 5.6961
  Reconstruction: 0.0068
  Topological: 5.1430 (weight: 0.400)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 21/300 COMPLETE (49.2s)
Train Loss: 7.2199 (C:5.7623, R:0.0075, T:1.7733)
Val Loss:   8.4335 (C:5.6961, R:0.0068, T:5.1430)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 22
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.122 Â± 0.967
    Neg distances: 7.849 Â± 2.215
    Separation ratio: 1.53x
    Gap: -11.452
    âš ï¸  Moderate global separation

============================================================
EPOCH 22 | Batches: 537 | Topological Weight: 0.4200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1400 (C:5.6773, R:0.0075, T:1.7034(w:0.420)ğŸš€)
Batch  25/537: Loss=7.0969 (C:5.5973, R:0.0075, T:1.7864(w:0.420)ğŸš€)
Batch  50/537: Loss=7.0860 (C:5.5482, R:0.0075, T:1.8752(w:0.420)ğŸš€)
Batch  75/537: Loss=7.0681 (C:5.6342, R:0.0075, T:1.6217(w:0.420)ğŸš€)
Batch 100/537: Loss=7.0630 (C:5.6530, R:0.0075, T:1.5671(w:0.420)ğŸš€)
Batch 125/537: Loss=7.2022 (C:5.6640, R:0.0074, T:1.8991(w:0.420)ğŸš€)
Batch 150/537: Loss=7.1337 (C:5.6894, R:0.0075, T:1.6559(w:0.420)ğŸš€)
Batch 175/537: Loss=7.1242 (C:5.6370, R:0.0075, T:1.7667(w:0.420)ğŸš€)
Batch 200/537: Loss=7.1184 (C:5.6378, R:0.0074, T:1.7522(w:0.420)ğŸš€)
Batch 225/537: Loss=7.1787 (C:5.6641, R:0.0075, T:1.8268(w:0.420)ğŸš€)
Batch 250/537: Loss=7.1150 (C:5.6918, R:0.0075, T:1.6049(w:0.420)ğŸš€)
Batch 275/537: Loss=7.3552 (C:5.7502, R:0.0075, T:2.0323(w:0.420)ğŸš€)
Batch 300/537: Loss=7.1799 (C:5.6722, R:0.0075, T:1.7949(w:0.420)ğŸš€)
Batch 325/537: Loss=7.1235 (C:5.7094, R:0.0074, T:1.5940(w:0.420)ğŸš€)
Batch 350/537: Loss=7.1983 (C:5.7343, R:0.0075, T:1.7093(w:0.420)ğŸš€)
Batch 375/537: Loss=7.0818 (C:5.6900, R:0.0074, T:1.5428(w:0.420)ğŸš€)
Batch 400/537: Loss=7.0976 (C:5.7074, R:0.0075, T:1.5260(w:0.420)ğŸš€)
Batch 425/537: Loss=7.1099 (C:5.7398, R:0.0074, T:1.4885(w:0.420)ğŸš€)
Batch 450/537: Loss=7.1314 (C:5.7236, R:0.0075, T:1.5698(w:0.420)ğŸš€)
Batch 475/537: Loss=7.2277 (C:5.7483, R:0.0075, T:1.7394(w:0.420)ğŸš€)
Batch 500/537: Loss=7.1423 (C:5.7449, R:0.0074, T:1.5562(w:0.420)ğŸš€)
Batch 525/537: Loss=7.2272 (C:5.7686, R:0.0075, T:1.6901(w:0.420)ğŸš€)
ğŸ“ˆ New best topological loss: 1.7319

ğŸ“Š EPOCH 22 TRAINING SUMMARY:
  Total Loss: 7.1700
  Contrastive: 5.6946
  Reconstruction: 0.0075
  Topological: 1.7319 (weight: 0.420)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.4819
  Contrastive: 5.6361
  Reconstruction: 0.0068
  Topological: 5.1572 (weight: 0.420)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 22/300 COMPLETE (56.7s)
Train Loss: 7.1700 (C:5.6946, R:0.0075, T:1.7319)
Val Loss:   8.4819 (C:5.6361, R:0.0068, T:5.1572)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 23 | Batches: 537 | Topological Weight: 0.4400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2218 (C:5.7014, R:0.0075, T:1.7461(w:0.440)ğŸš€)
Batch  25/537: Loss=7.2140 (C:5.7329, R:0.0075, T:1.6644(w:0.440)ğŸš€)
Batch  50/537: Loss=7.0878 (C:5.5929, R:0.0075, T:1.6960(w:0.440)ğŸš€)
Batch  75/537: Loss=7.1431 (C:5.7097, R:0.0075, T:1.5610(w:0.440)ğŸš€)
Batch 100/537: Loss=7.0493 (C:5.5934, R:0.0075, T:1.6038(w:0.440)ğŸš€)
Batch 125/537: Loss=7.1630 (C:5.7083, R:0.0075, T:1.6066(w:0.440)ğŸš€)
Batch 150/537: Loss=7.1421 (C:5.6896, R:0.0074, T:1.6119(w:0.440)ğŸš€)
Batch 175/537: Loss=7.2174 (C:5.6661, R:0.0075, T:1.8206(w:0.440)ğŸš€)
Batch 200/537: Loss=7.1533 (C:5.6519, R:0.0075, T:1.7031(w:0.440)ğŸš€)
Batch 225/537: Loss=7.0962 (C:5.6609, R:0.0075, T:1.5579(w:0.440)ğŸš€)
Batch 250/537: Loss=7.1109 (C:5.6735, R:0.0075, T:1.5728(w:0.440)ğŸš€)
Batch 275/537: Loss=7.1646 (C:5.7437, R:0.0074, T:1.5377(w:0.440)ğŸš€)
Batch 300/537: Loss=7.1878 (C:5.6607, R:0.0075, T:1.7764(w:0.440)ğŸš€)
Batch 325/537: Loss=7.0921 (C:5.6920, R:0.0075, T:1.4818(w:0.440)ğŸš€)
Batch 350/537: Loss=7.1670 (C:5.6408, R:0.0074, T:1.7778(w:0.440)ğŸš€)
Batch 375/537: Loss=7.1896 (C:5.7203, R:0.0074, T:1.6502(w:0.440)ğŸš€)
Batch 400/537: Loss=7.0771 (C:5.6728, R:0.0074, T:1.5059(w:0.440)ğŸš€)
Batch 425/537: Loss=7.1741 (C:5.6648, R:0.0075, T:1.7235(w:0.440)ğŸš€)
Batch 450/537: Loss=7.2633 (C:5.7521, R:0.0075, T:1.7233(w:0.440)ğŸš€)
Batch 475/537: Loss=7.1471 (C:5.6496, R:0.0075, T:1.7095(w:0.440)ğŸš€)
Batch 500/537: Loss=7.0802 (C:5.6799, R:0.0075, T:1.4758(w:0.440)ğŸš€)
Batch 525/537: Loss=7.1959 (C:5.7396, R:0.0075, T:1.6139(w:0.440)ğŸš€)
ğŸ“ˆ New best topological loss: 1.6816

ğŸ“Š EPOCH 23 TRAINING SUMMARY:
  Total Loss: 7.1798
  Contrastive: 5.6924
  Reconstruction: 0.0075
  Topological: 1.6816 (weight: 0.440)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.4378
  Contrastive: 5.6733
  Reconstruction: 0.0068
  Topological: 4.7395 (weight: 0.440)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 23/300 COMPLETE (49.6s)
Train Loss: 7.1798 (C:5.6924, R:0.0075, T:1.6816)
Val Loss:   8.4378 (C:5.6733, R:0.0068, T:4.7395)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 24 | Batches: 537 | Topological Weight: 0.4600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2426 (C:5.8037, R:0.0075, T:1.5072(w:0.460)ğŸš€)
Batch  25/537: Loss=7.1891 (C:5.6508, R:0.0075, T:1.7130(w:0.460)ğŸš€)
Batch  50/537: Loss=7.1755 (C:5.6559, R:0.0075, T:1.6759(w:0.460)ğŸš€)
Batch  75/537: Loss=7.2088 (C:5.7170, R:0.0074, T:1.6260(w:0.460)ğŸš€)
Batch 100/537: Loss=7.2449 (C:5.7027, R:0.0075, T:1.7283(w:0.460)ğŸš€)
Batch 125/537: Loss=7.1964 (C:5.6251, R:0.0075, T:1.7948(w:0.460)ğŸš€)
Batch 150/537: Loss=7.1420 (C:5.6841, R:0.0075, T:1.5304(w:0.460)ğŸš€)
Batch 175/537: Loss=7.2389 (C:5.7521, R:0.0075, T:1.6100(w:0.460)ğŸš€)
Batch 200/537: Loss=7.2471 (C:5.6630, R:0.0074, T:1.8308(w:0.460)ğŸš€)
Batch 225/537: Loss=7.1325 (C:5.7562, R:0.0074, T:1.3748(w:0.460)ğŸš€)
Batch 250/537: Loss=7.2184 (C:5.7031, R:0.0075, T:1.6694(w:0.460)ğŸš€)
Batch 275/537: Loss=7.2241 (C:5.7170, R:0.0075, T:1.6502(w:0.460)ğŸš€)
Batch 300/537: Loss=7.2262 (C:5.6709, R:0.0074, T:1.7656(w:0.460)ğŸš€)
Batch 325/537: Loss=7.1987 (C:5.7696, R:0.0074, T:1.4885(w:0.460)ğŸš€)
Batch 350/537: Loss=7.2589 (C:5.6830, R:0.0075, T:1.8032(w:0.460)ğŸš€)
Batch 375/537: Loss=7.2418 (C:5.8323, R:0.0074, T:1.4445(w:0.460)ğŸš€)
Batch 400/537: Loss=7.2291 (C:5.6777, R:0.0074, T:1.7689(w:0.460)ğŸš€)
Batch 425/537: Loss=7.1495 (C:5.6507, R:0.0075, T:1.6306(w:0.460)ğŸš€)
Batch 450/537: Loss=7.1143 (C:5.5876, R:0.0075, T:1.6874(w:0.460)ğŸš€)
Batch 475/537: Loss=7.3040 (C:5.7640, R:0.0075, T:1.7279(w:0.460)ğŸš€)
Batch 500/537: Loss=7.3335 (C:5.7344, R:0.0075, T:1.8564(w:0.460)ğŸš€)
Batch 525/537: Loss=7.0991 (C:5.6584, R:0.0074, T:1.5209(w:0.460)ğŸš€)
ğŸ“ˆ New best topological loss: 1.6392

ğŸ“Š EPOCH 24 TRAINING SUMMARY:
  Total Loss: 7.1903
  Contrastive: 5.6901
  Reconstruction: 0.0075
  Topological: 1.6392 (weight: 0.460)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.5101
  Contrastive: 5.6409
  Reconstruction: 0.0068
  Topological: 4.7653 (weight: 0.460)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 24/300 COMPLETE (50.3s)
Train Loss: 7.1903 (C:5.6901, R:0.0075, T:1.6392)
Val Loss:   8.5101 (C:5.6409, R:0.0068, T:4.7653)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 25
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.252 Â± 0.941
    Neg distances: 8.153 Â± 2.294
    Separation ratio: 1.55x
    Gap: -11.855
    âš ï¸  Moderate global separation

============================================================
EPOCH 25 | Batches: 537 | Topological Weight: 0.4800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2268 (C:5.7296, R:0.0074, T:1.5726(w:0.480)ğŸš€)
Batch  25/537: Loss=7.1070 (C:5.5519, R:0.0075, T:1.6863(w:0.480)ğŸš€)
Batch  50/537: Loss=7.1839 (C:5.6823, R:0.0075, T:1.5698(w:0.480)ğŸš€)
Batch  75/537: Loss=7.1228 (C:5.7022, R:0.0075, T:1.3953(w:0.480)ğŸš€)
Batch 100/537: Loss=7.1625 (C:5.6708, R:0.0075, T:1.5539(w:0.480)ğŸš€)
Batch 125/537: Loss=7.0956 (C:5.6576, R:0.0074, T:1.4454(w:0.480)ğŸš€)
Batch 150/537: Loss=7.2331 (C:5.6931, R:0.0074, T:1.6617(w:0.480)ğŸš€)
Batch 175/537: Loss=7.2348 (C:5.6577, R:0.0074, T:1.7343(w:0.480)ğŸš€)
Batch 200/537: Loss=7.2594 (C:5.6204, R:0.0074, T:1.8656(w:0.480)ğŸš€)
Batch 225/537: Loss=7.1683 (C:5.6445, R:0.0075, T:1.6101(w:0.480)ğŸš€)
Batch 250/537: Loss=7.1352 (C:5.6242, R:0.0075, T:1.5837(w:0.480)ğŸš€)
Batch 275/537: Loss=7.2047 (C:5.6609, R:0.0075, T:1.6620(w:0.480)ğŸš€)
Batch 300/537: Loss=7.2133 (C:5.7150, R:0.0075, T:1.5677(w:0.480)ğŸš€)
Batch 325/537: Loss=7.1039 (C:5.6276, R:0.0075, T:1.5206(w:0.480)ğŸš€)
Batch 350/537: Loss=7.1634 (C:5.6788, R:0.0074, T:1.5488(w:0.480)ğŸš€)
Batch 375/537: Loss=7.1736 (C:5.6346, R:0.0074, T:1.6606(w:0.480)ğŸš€)
Batch 400/537: Loss=7.1713 (C:5.6338, R:0.0075, T:1.6440(w:0.480)ğŸš€)
Batch 425/537: Loss=7.2269 (C:5.6615, R:0.0075, T:1.7016(w:0.480)ğŸš€)
Batch 450/537: Loss=7.1159 (C:5.6265, R:0.0074, T:1.5527(w:0.480)ğŸš€)
Batch 475/537: Loss=7.2475 (C:5.7100, R:0.0074, T:1.6524(w:0.480)ğŸš€)
Batch 500/537: Loss=7.2648 (C:5.6812, R:0.0075, T:1.7414(w:0.480)ğŸš€)
Batch 525/537: Loss=7.3169 (C:5.7382, R:0.0075, T:1.7306(w:0.480)ğŸš€)
ğŸ“ˆ New best topological loss: 1.5995

ğŸ“Š EPOCH 25 TRAINING SUMMARY:
  Total Loss: 7.1528
  Contrastive: 5.6394
  Reconstruction: 0.0075
  Topological: 1.5995 (weight: 0.480)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.7081
  Contrastive: 5.5980
  Reconstruction: 0.0068
  Topological: 5.0712 (weight: 0.480)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 25/300 COMPLETE (59.0s)
Train Loss: 7.1528 (C:5.6394, R:0.0075, T:1.5995)
Val Loss:   8.7081 (C:5.5980, R:0.0068, T:5.0712)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 26 | Batches: 537 | Topological Weight: 0.5000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0321 (C:5.5518, R:0.0074, T:1.4759(w:0.500)ğŸš€)
Batch  25/537: Loss=7.0098 (C:5.5435, R:0.0074, T:1.4467(w:0.500)ğŸš€)
Batch  50/537: Loss=7.0528 (C:5.5536, R:0.0075, T:1.5075(w:0.500)ğŸš€)
Batch  75/537: Loss=7.0139 (C:5.5705, R:0.0075, T:1.3963(w:0.500)ğŸš€)
Batch 100/537: Loss=7.2702 (C:5.7120, R:0.0075, T:1.6259(w:0.500)ğŸš€)
Batch 125/537: Loss=7.0971 (C:5.5772, R:0.0075, T:1.5469(w:0.500)ğŸš€)
Batch 150/537: Loss=7.2123 (C:5.6757, R:0.0075, T:1.5806(w:0.500)ğŸš€)
Batch 175/537: Loss=7.1932 (C:5.6040, R:0.0075, T:1.6869(w:0.500)ğŸš€)
Batch 200/537: Loss=7.1422 (C:5.6337, R:0.0075, T:1.5259(w:0.500)ğŸš€)
Batch 225/537: Loss=7.2665 (C:5.7148, R:0.0074, T:1.6223(w:0.500)ğŸš€)
Batch 250/537: Loss=7.1878 (C:5.6828, R:0.0074, T:1.5236(w:0.500)ğŸš€)
Batch 275/537: Loss=7.2118 (C:5.6416, R:0.0074, T:1.6630(w:0.500)ğŸš€)
Batch 300/537: Loss=7.1079 (C:5.6102, R:0.0075, T:1.4945(w:0.500)ğŸš€)
Batch 325/537: Loss=7.1125 (C:5.6297, R:0.0074, T:1.4812(w:0.500)ğŸš€)
Batch 350/537: Loss=7.1194 (C:5.6227, R:0.0075, T:1.4943(w:0.500)ğŸš€)
Batch 375/537: Loss=7.2435 (C:5.6783, R:0.0074, T:1.6412(w:0.500)ğŸš€)
Batch 400/537: Loss=7.0759 (C:5.6322, R:0.0075, T:1.3971(w:0.500)ğŸš€)
Batch 425/537: Loss=7.2791 (C:5.7056, R:0.0075, T:1.6471(w:0.500)ğŸš€)
Batch 450/537: Loss=7.1622 (C:5.6207, R:0.0075, T:1.5854(w:0.500)ğŸš€)
Batch 475/537: Loss=7.0436 (C:5.5921, R:0.0074, T:1.4255(w:0.500)ğŸš€)
Batch 500/537: Loss=7.3539 (C:5.6930, R:0.0074, T:1.8324(w:0.500)ğŸš€)
Batch 525/537: Loss=7.2287 (C:5.6610, R:0.0074, T:1.6466(w:0.500)ğŸš€)
ğŸ“ˆ New best topological loss: 1.5614

ğŸ“Š EPOCH 26 TRAINING SUMMARY:
  Total Loss: 7.1619
  Contrastive: 5.6365
  Reconstruction: 0.0074
  Topological: 1.5614 (weight: 0.500)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.5754
  Contrastive: 5.6467
  Reconstruction: 0.0067
  Topological: 4.5082 (weight: 0.500)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 26/300 COMPLETE (50.3s)
Train Loss: 7.1619 (C:5.6365, R:0.0074, T:1.5614)
Val Loss:   8.5754 (C:5.6467, R:0.0067, T:4.5082)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 27 | Batches: 537 | Topological Weight: 0.5200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0742 (C:5.6794, R:0.0074, T:1.2598(w:0.520)ğŸš€)
Batch  25/537: Loss=7.1816 (C:5.5883, R:0.0074, T:1.6331(w:0.520)ğŸš€)
Batch  50/537: Loss=7.1774 (C:5.5374, R:0.0074, T:1.7223(w:0.520)ğŸš€)
Batch  75/537: Loss=7.1185 (C:5.6042, R:0.0074, T:1.4810(w:0.520)ğŸš€)
Batch 100/537: Loss=7.1357 (C:5.6087, R:0.0074, T:1.5051(w:0.520)ğŸš€)
Batch 125/537: Loss=7.0010 (C:5.4750, R:0.0074, T:1.5105(w:0.520)ğŸš€)
Batch 150/537: Loss=7.2632 (C:5.6835, R:0.0074, T:1.6109(w:0.520)ğŸš€)
Batch 175/537: Loss=7.1693 (C:5.6329, R:0.0074, T:1.5259(w:0.520)ğŸš€)
Batch 200/537: Loss=7.0126 (C:5.5685, R:0.0075, T:1.3414(w:0.520)ğŸš€)
Batch 225/537: Loss=7.3372 (C:5.7240, R:0.0074, T:1.6729(w:0.520)ğŸš€)
Batch 250/537: Loss=7.1967 (C:5.6492, R:0.0074, T:1.5514(w:0.520)ğŸš€)
Batch 275/537: Loss=7.1043 (C:5.5760, R:0.0074, T:1.5180(w:0.520)ğŸš€)
Batch 300/537: Loss=7.0709 (C:5.6583, R:0.0074, T:1.2986(w:0.520)ğŸš€)
Batch 325/537: Loss=7.0612 (C:5.5843, R:0.0074, T:1.4135(w:0.520)ğŸš€)
Batch 350/537: Loss=7.2026 (C:5.6608, R:0.0075, T:1.5280(w:0.520)ğŸš€)
Batch 375/537: Loss=7.1213 (C:5.6440, R:0.0074, T:1.4154(w:0.520)ğŸš€)
Batch 400/537: Loss=7.2234 (C:5.6289, R:0.0074, T:1.6385(w:0.520)ğŸš€)
Batch 425/537: Loss=7.1166 (C:5.6139, R:0.0075, T:1.4557(w:0.520)ğŸš€)
Batch 450/537: Loss=7.1848 (C:5.6323, R:0.0074, T:1.5564(w:0.520)ğŸš€)
Batch 475/537: Loss=7.1457 (C:5.6583, R:0.0074, T:1.4282(w:0.520)ğŸš€)
Batch 500/537: Loss=7.1021 (C:5.6777, R:0.0074, T:1.3248(w:0.520)ğŸš€)
Batch 525/537: Loss=7.1419 (C:5.6008, R:0.0074, T:1.5358(w:0.520)ğŸš€)
ğŸ“ˆ New best topological loss: 1.5283

ğŸ“Š EPOCH 27 TRAINING SUMMARY:
  Total Loss: 7.1679
  Contrastive: 5.6288
  Reconstruction: 0.0074
  Topological: 1.5283 (weight: 0.520)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.7836
  Contrastive: 5.6133
  Reconstruction: 0.0068
  Topological: 4.7981 (weight: 0.520)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 27/300 COMPLETE (48.6s)
Train Loss: 7.1679 (C:5.6288, R:0.0074, T:1.5283)
Val Loss:   8.7836 (C:5.6133, R:0.0068, T:4.7981)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 28
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.278 Â± 0.901
    Neg distances: 8.211 Â± 2.298
    Separation ratio: 1.56x
    Gap: -11.924
    âš ï¸  Moderate global separation

============================================================
EPOCH 28 | Batches: 537 | Topological Weight: 0.5400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0306 (C:5.5483, R:0.0074, T:1.3726(w:0.540)ğŸš€)
Batch  25/537: Loss=7.1209 (C:5.5736, R:0.0074, T:1.4942(w:0.540)ğŸš€)
Batch  50/537: Loss=7.1406 (C:5.5626, R:0.0075, T:1.5361(w:0.540)ğŸš€)
Batch  75/537: Loss=7.2338 (C:5.6263, R:0.0075, T:1.5956(w:0.540)ğŸš€)
Batch 100/537: Loss=7.1271 (C:5.5653, R:0.0074, T:1.5132(w:0.540)ğŸš€)
Batch 125/537: Loss=7.0377 (C:5.5703, R:0.0074, T:1.3501(w:0.540)ğŸš€)
Batch 150/537: Loss=7.2073 (C:5.6188, R:0.0075, T:1.5521(w:0.540)ğŸš€)
Batch 175/537: Loss=7.0949 (C:5.6179, R:0.0074, T:1.3616(w:0.540)ğŸš€)
Batch 200/537: Loss=7.2413 (C:5.6064, R:0.0075, T:1.6479(w:0.540)ğŸš€)
Batch 225/537: Loss=7.2638 (C:5.6597, R:0.0075, T:1.5898(w:0.540)ğŸš€)
Batch 250/537: Loss=7.3157 (C:5.6512, R:0.0075, T:1.6946(w:0.540)ğŸš€)
Batch 275/537: Loss=7.1004 (C:5.5732, R:0.0074, T:1.4549(w:0.540)ğŸš€)
Batch 300/537: Loss=7.2902 (C:5.6651, R:0.0074, T:1.6326(w:0.540)ğŸš€)
Batch 325/537: Loss=7.3382 (C:5.6677, R:0.0075, T:1.7120(w:0.540)ğŸš€)
Batch 350/537: Loss=7.0965 (C:5.5890, R:0.0074, T:1.4146(w:0.540)ğŸš€)
Batch 375/537: Loss=7.0781 (C:5.6203, R:0.0075, T:1.3141(w:0.540)ğŸš€)
Batch 400/537: Loss=7.1709 (C:5.6618, R:0.0074, T:1.4218(w:0.540)ğŸš€)
Batch 425/537: Loss=7.2412 (C:5.6488, R:0.0074, T:1.5749(w:0.540)ğŸš€)
Batch 450/537: Loss=7.2209 (C:5.6495, R:0.0074, T:1.5393(w:0.540)ğŸš€)
Batch 475/537: Loss=7.1805 (C:5.6497, R:0.0074, T:1.4623(w:0.540)ğŸš€)
Batch 500/537: Loss=7.1741 (C:5.6262, R:0.0074, T:1.4911(w:0.540)ğŸš€)
Batch 525/537: Loss=7.1666 (C:5.7024, R:0.0074, T:1.3350(w:0.540)ğŸš€)
ğŸ“ˆ New best topological loss: 1.4942

ğŸ“Š EPOCH 28 TRAINING SUMMARY:
  Total Loss: 7.1702
  Contrastive: 5.6187
  Reconstruction: 0.0074
  Topological: 1.4942 (weight: 0.540)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.8837
  Contrastive: 5.5614
  Reconstruction: 0.0067
  Topological: 4.9047 (weight: 0.540)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 28/300 COMPLETE (56.0s)
Train Loss: 7.1702 (C:5.6187, R:0.0074, T:1.4942)
Val Loss:   8.8837 (C:5.5614, R:0.0067, T:4.9047)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 29 | Batches: 537 | Topological Weight: 0.5600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1581 (C:5.6289, R:0.0075, T:1.3976(w:0.560)ğŸš€)
Batch  25/537: Loss=7.2237 (C:5.6271, R:0.0075, T:1.5143(w:0.560)ğŸš€)
Batch  50/537: Loss=7.1010 (C:5.5364, R:0.0074, T:1.4637(w:0.560)ğŸš€)
Batch  75/537: Loss=7.1991 (C:5.6534, R:0.0074, T:1.4414(w:0.560)ğŸš€)
Batch 100/537: Loss=7.2185 (C:5.6768, R:0.0074, T:1.4334(w:0.560)ğŸš€)
Batch 125/537: Loss=7.2553 (C:5.6813, R:0.0075, T:1.4796(w:0.560)ğŸš€)
Batch 150/537: Loss=7.0848 (C:5.5748, R:0.0075, T:1.3625(w:0.560)ğŸš€)
Batch 175/537: Loss=7.0721 (C:5.5848, R:0.0075, T:1.3242(w:0.560)ğŸš€)
Batch 200/537: Loss=7.1480 (C:5.6501, R:0.0074, T:1.3545(w:0.560)ğŸš€)
Batch 225/537: Loss=7.1337 (C:5.5988, R:0.0075, T:1.4085(w:0.560)ğŸš€)
Batch 250/537: Loss=7.1355 (C:5.5876, R:0.0075, T:1.4290(w:0.560)ğŸš€)
Batch 275/537: Loss=7.1033 (C:5.6276, R:0.0074, T:1.3059(w:0.560)ğŸš€)
Batch 300/537: Loss=7.2231 (C:5.6145, R:0.0074, T:1.5492(w:0.560)ğŸš€)
Batch 325/537: Loss=7.1263 (C:5.6189, R:0.0074, T:1.3680(w:0.560)ğŸš€)
Batch 350/537: Loss=7.3071 (C:5.6615, R:0.0075, T:1.6072(w:0.560)ğŸš€)
Batch 375/537: Loss=7.2165 (C:5.5977, R:0.0075, T:1.5598(w:0.560)ğŸš€)
Batch 400/537: Loss=7.1493 (C:5.6443, R:0.0074, T:1.3603(w:0.560)ğŸš€)
Batch 425/537: Loss=7.1675 (C:5.6443, R:0.0074, T:1.3956(w:0.560)ğŸš€)
Batch 450/537: Loss=7.2697 (C:5.7092, R:0.0074, T:1.4618(w:0.560)ğŸš€)
Batch 475/537: Loss=7.2088 (C:5.6576, R:0.0075, T:1.4391(w:0.560)ğŸš€)
Batch 500/537: Loss=7.1616 (C:5.6175, R:0.0074, T:1.4312(w:0.560)ğŸš€)
Batch 525/537: Loss=7.0671 (C:5.5291, R:0.0074, T:1.4218(w:0.560)ğŸš€)
ğŸ“ˆ New best topological loss: 1.4584

ğŸ“Š EPOCH 29 TRAINING SUMMARY:
  Total Loss: 7.1704
  Contrastive: 5.6102
  Reconstruction: 0.0074
  Topological: 1.4584 (weight: 0.560)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.8474
  Contrastive: 5.6111
  Reconstruction: 0.0067
  Topological: 4.5749 (weight: 0.560)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 29/300 COMPLETE (48.3s)
Train Loss: 7.1704 (C:5.6102, R:0.0074, T:1.4584)
Val Loss:   8.8474 (C:5.6111, R:0.0067, T:4.5749)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 30 | Batches: 537 | Topological Weight: 0.5800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1824 (C:5.6414, R:0.0074, T:1.3742(w:0.580)ğŸš€)
Batch  25/537: Loss=7.1880 (C:5.5940, R:0.0075, T:1.4552(w:0.580)ğŸš€)
Batch  50/537: Loss=7.1522 (C:5.5584, R:0.0075, T:1.4634(w:0.580)ğŸš€)
Batch  75/537: Loss=7.1899 (C:5.5842, R:0.0074, T:1.4944(w:0.580)ğŸš€)
Batch 100/537: Loss=7.0591 (C:5.5384, R:0.0074, T:1.3396(w:0.580)ğŸš€)
Batch 125/537: Loss=7.2004 (C:5.5555, R:0.0075, T:1.5481(w:0.580)ğŸš€)
Batch 150/537: Loss=7.1668 (C:5.6219, R:0.0075, T:1.3754(w:0.580)ğŸš€)
Batch 175/537: Loss=7.1968 (C:5.6641, R:0.0075, T:1.3510(w:0.580)ğŸš€)
Batch 200/537: Loss=7.1589 (C:5.5432, R:0.0074, T:1.5044(w:0.580)ğŸš€)
Batch 225/537: Loss=7.1621 (C:5.5942, R:0.0075, T:1.4063(w:0.580)ğŸš€)
Batch 250/537: Loss=7.1282 (C:5.5737, R:0.0074, T:1.4021(w:0.580)ğŸš€)
Batch 275/537: Loss=7.1059 (C:5.5753, R:0.0074, T:1.3646(w:0.580)ğŸš€)
Batch 300/537: Loss=7.1835 (C:5.6060, R:0.0074, T:1.4390(w:0.580)ğŸš€)
Batch 325/537: Loss=7.1871 (C:5.6229, R:0.0074, T:1.4291(w:0.580)ğŸš€)
Batch 350/537: Loss=7.2858 (C:5.6947, R:0.0074, T:1.4687(w:0.580)ğŸš€)
Batch 375/537: Loss=7.1464 (C:5.6194, R:0.0074, T:1.3589(w:0.580)ğŸš€)
Batch 400/537: Loss=7.1225 (C:5.6234, R:0.0074, T:1.3119(w:0.580)ğŸš€)
Batch 425/537: Loss=7.0466 (C:5.4945, R:0.0075, T:1.3915(w:0.580)ğŸš€)
Batch 450/537: Loss=7.2054 (C:5.7041, R:0.0074, T:1.3049(w:0.580)ğŸš€)
Batch 475/537: Loss=7.1183 (C:5.6404, R:0.0074, T:1.2691(w:0.580)ğŸš€)
Batch 500/537: Loss=7.1777 (C:5.6028, R:0.0074, T:1.4334(w:0.580)ğŸš€)
Batch 525/537: Loss=7.2948 (C:5.6583, R:0.0074, T:1.5474(w:0.580)ğŸš€)
ğŸ“ˆ New best topological loss: 1.4274

ğŸ“Š EPOCH 30 TRAINING SUMMARY:
  Total Loss: 7.1841
  Contrastive: 5.6135
  Reconstruction: 0.0074
  Topological: 1.4274 (weight: 0.580)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.9905
  Contrastive: 5.5856
  Reconstruction: 0.0067
  Topological: 4.7118 (weight: 0.580)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 30/300 COMPLETE (48.2s)
Train Loss: 7.1841 (C:5.6135, R:0.0074, T:1.4274)
Val Loss:   8.9905 (C:5.5856, R:0.0067, T:4.7118)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 31
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 5.244 Â± 0.881
    Neg distances: 8.255 Â± 2.359
    Separation ratio: 1.57x
    Gap: -11.790
    âš ï¸  Moderate global separation

============================================================
EPOCH 31 | Batches: 537 | Topological Weight: 0.6000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1013 (C:5.5481, R:0.0075, T:1.3446(w:0.600)ğŸš€)
Batch  25/537: Loss=7.1390 (C:5.5681, R:0.0074, T:1.3814(w:0.600)ğŸš€)
Batch  50/537: Loss=7.0916 (C:5.5947, R:0.0074, T:1.2589(w:0.600)ğŸš€)
Batch  75/537: Loss=7.1605 (C:5.5283, R:0.0074, T:1.4796(w:0.600)ğŸš€)
Batch 100/537: Loss=7.0612 (C:5.5580, R:0.0074, T:1.2688(w:0.600)ğŸš€)
Batch 125/537: Loss=7.0590 (C:5.5240, R:0.0075, T:1.3116(w:0.600)ğŸš€)
Batch 150/537: Loss=7.2816 (C:5.6224, R:0.0075, T:1.5194(w:0.600)ğŸš€)
Batch 175/537: Loss=7.1949 (C:5.6449, R:0.0074, T:1.3423(w:0.600)ğŸš€)
Batch 200/537: Loss=7.0447 (C:5.5387, R:0.0074, T:1.2771(w:0.600)ğŸš€)
Batch 225/537: Loss=7.2693 (C:5.6488, R:0.0075, T:1.4574(w:0.600)ğŸš€)
Batch 250/537: Loss=7.1452 (C:5.6213, R:0.0075, T:1.2981(w:0.600)ğŸš€)
Batch 275/537: Loss=7.2298 (C:5.5872, R:0.0075, T:1.4944(w:0.600)ğŸš€)
Batch 300/537: Loss=7.2458 (C:5.6776, R:0.0074, T:1.3729(w:0.600)ğŸš€)
Batch 325/537: Loss=7.1411 (C:5.5864, R:0.0074, T:1.3608(w:0.600)ğŸš€)
Batch 350/537: Loss=7.2143 (C:5.5808, R:0.0074, T:1.4879(w:0.600)ğŸš€)
Batch 375/537: Loss=7.1223 (C:5.5798, R:0.0074, T:1.3330(w:0.600)ğŸš€)
Batch 400/537: Loss=7.2518 (C:5.6296, R:0.0074, T:1.4742(w:0.600)ğŸš€)
Batch 425/537: Loss=7.2290 (C:5.5825, R:0.0074, T:1.5098(w:0.600)ğŸš€)
Batch 450/537: Loss=7.1565 (C:5.5671, R:0.0074, T:1.4084(w:0.600)ğŸš€)
Batch 475/537: Loss=7.0709 (C:5.5378, R:0.0074, T:1.3226(w:0.600)ğŸš€)
Batch 500/537: Loss=7.2206 (C:5.6127, R:0.0075, T:1.4371(w:0.600)ğŸš€)
Batch 525/537: Loss=7.1328 (C:5.6676, R:0.0074, T:1.2094(w:0.600)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3980

ğŸ“Š EPOCH 31 TRAINING SUMMARY:
  Total Loss: 7.1582
  Contrastive: 5.5770
  Reconstruction: 0.0074
  Topological: 1.3980 (weight: 0.600)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.9076
  Contrastive: 5.5883
  Reconstruction: 0.0067
  Topological: 4.4113 (weight: 0.600)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 31/300 COMPLETE (55.5s)
Train Loss: 7.1582 (C:5.5770, R:0.0074, T:1.3980)
Val Loss:   8.9076 (C:5.5883, R:0.0067, T:4.4113)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 32 | Batches: 537 | Topological Weight: 0.6200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1778 (C:5.6753, R:0.0075, T:1.2212(w:0.620)ğŸš€)
Batch  25/537: Loss=7.1803 (C:5.5087, R:0.0074, T:1.4992(w:0.620)ğŸš€)
Batch  50/537: Loss=7.2178 (C:5.6022, R:0.0074, T:1.4150(w:0.620)ğŸš€)
Batch  75/537: Loss=7.2739 (C:5.5497, R:0.0074, T:1.5925(w:0.620)ğŸš€)
Batch 100/537: Loss=7.0743 (C:5.4877, R:0.0074, T:1.3609(w:0.620)ğŸš€)
Batch 125/537: Loss=7.2313 (C:5.5652, R:0.0074, T:1.4940(w:0.620)ğŸš€)
Batch 150/537: Loss=7.2576 (C:5.5798, R:0.0074, T:1.5158(w:0.620)ğŸš€)
Batch 175/537: Loss=7.1821 (C:5.6047, R:0.0074, T:1.3532(w:0.620)ğŸš€)
Batch 200/537: Loss=7.2204 (C:5.5781, R:0.0074, T:1.4523(w:0.620)ğŸš€)
Batch 225/537: Loss=7.1436 (C:5.5436, R:0.0074, T:1.3864(w:0.620)ğŸš€)
Batch 250/537: Loss=7.1577 (C:5.6516, R:0.0074, T:1.2388(w:0.620)ğŸš€)
Batch 275/537: Loss=7.1664 (C:5.5156, R:0.0075, T:1.4552(w:0.620)ğŸš€)
Batch 300/537: Loss=7.1717 (C:5.5511, R:0.0074, T:1.4204(w:0.620)ğŸš€)
Batch 325/537: Loss=7.0098 (C:5.4284, R:0.0074, T:1.3584(w:0.620)ğŸš€)
Batch 350/537: Loss=7.1379 (C:5.5422, R:0.0074, T:1.3805(w:0.620)ğŸš€)
Batch 375/537: Loss=7.2570 (C:5.6203, R:0.0074, T:1.4491(w:0.620)ğŸš€)
Batch 400/537: Loss=7.2436 (C:5.6420, R:0.0074, T:1.3879(w:0.620)ğŸš€)
Batch 425/537: Loss=7.2256 (C:5.5798, R:0.0074, T:1.4619(w:0.620)ğŸš€)
Batch 450/537: Loss=7.1465 (C:5.5654, R:0.0074, T:1.3616(w:0.620)ğŸš€)
Batch 475/537: Loss=7.2478 (C:5.6866, R:0.0074, T:1.3290(w:0.620)ğŸš€)
Batch 500/537: Loss=7.2080 (C:5.6723, R:0.0074, T:1.2856(w:0.620)ğŸš€)
Batch 525/537: Loss=7.2417 (C:5.5522, R:0.0074, T:1.5269(w:0.620)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3716

ğŸ“Š EPOCH 32 TRAINING SUMMARY:
  Total Loss: 7.1635
  Contrastive: 5.5719
  Reconstruction: 0.0074
  Topological: 1.3716 (weight: 0.620)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.0908
  Contrastive: 5.5724
  Reconstruction: 0.0067
  Topological: 4.5918 (weight: 0.620)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 32/300 COMPLETE (48.3s)
Train Loss: 7.1635 (C:5.5719, R:0.0074, T:1.3716)
Val Loss:   9.0908 (C:5.5724, R:0.0067, T:4.5918)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 33 | Batches: 537 | Topological Weight: 0.6400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1412 (C:5.5528, R:0.0074, T:1.3253(w:0.640)ğŸš€)
Batch  25/537: Loss=7.0091 (C:5.5040, R:0.0074, T:1.1939(w:0.640)ğŸš€)
Batch  50/537: Loss=7.0670 (C:5.5597, R:0.0074, T:1.2023(w:0.640)ğŸš€)
Batch  75/537: Loss=7.3206 (C:5.6602, R:0.0074, T:1.4338(w:0.640)ğŸš€)
Batch 100/537: Loss=7.2085 (C:5.5887, R:0.0074, T:1.3727(w:0.640)ğŸš€)
Batch 125/537: Loss=7.3564 (C:5.6029, R:0.0074, T:1.5823(w:0.640)ğŸš€)
Batch 150/537: Loss=7.2998 (C:5.6210, R:0.0074, T:1.4655(w:0.640)ğŸš€)
Batch 175/537: Loss=7.3186 (C:5.5400, R:0.0074, T:1.6200(w:0.640)ğŸš€)
Batch 200/537: Loss=7.1856 (C:5.5756, R:0.0075, T:1.3501(w:0.640)ğŸš€)
Batch 225/537: Loss=7.1164 (C:5.5756, R:0.0074, T:1.2452(w:0.640)ğŸš€)
Batch 250/537: Loss=7.1381 (C:5.5798, R:0.0074, T:1.2818(w:0.640)ğŸš€)
Batch 275/537: Loss=7.1433 (C:5.5344, R:0.0074, T:1.3553(w:0.640)ğŸš€)
Batch 300/537: Loss=7.0390 (C:5.5645, R:0.0074, T:1.1467(w:0.640)ğŸš€)
Batch 325/537: Loss=7.1375 (C:5.5800, R:0.0074, T:1.2783(w:0.640)ğŸš€)
Batch 350/537: Loss=7.2901 (C:5.6391, R:0.0074, T:1.4238(w:0.640)ğŸš€)
Batch 375/537: Loss=7.0995 (C:5.5539, R:0.0075, T:1.2497(w:0.640)ğŸš€)
Batch 400/537: Loss=7.2073 (C:5.5467, R:0.0074, T:1.4368(w:0.640)ğŸš€)
Batch 425/537: Loss=7.2993 (C:5.6718, R:0.0074, T:1.3916(w:0.640)ğŸš€)
Batch 450/537: Loss=7.3813 (C:5.6508, R:0.0074, T:1.5398(w:0.640)ğŸš€)
Batch 475/537: Loss=7.1095 (C:5.5947, R:0.0074, T:1.2163(w:0.640)ğŸš€)
Batch 500/537: Loss=7.1358 (C:5.5747, R:0.0074, T:1.2858(w:0.640)ğŸš€)
Batch 525/537: Loss=7.3591 (C:5.6299, R:0.0074, T:1.5526(w:0.640)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3466

ğŸ“Š EPOCH 33 TRAINING SUMMARY:
  Total Loss: 7.1803
  Contrastive: 5.5775
  Reconstruction: 0.0074
  Topological: 1.3466 (weight: 0.640)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.1678
  Contrastive: 5.5635
  Reconstruction: 0.0067
  Topological: 4.5832 (weight: 0.640)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 33/300 COMPLETE (47.9s)
Train Loss: 7.1803 (C:5.5775, R:0.0074, T:1.3466)
Val Loss:   9.1678 (C:5.5635, R:0.0067, T:4.5832)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸ›‘ Early stopping triggered after 33 epochs
Best model was at epoch 13 with Val Loss: 7.8000

======================================================================
ğŸ“ˆ FINAL TOPOLOGICAL LEARNING ANALYSIS
======================================================================
First topological learning: Epoch 1
Epochs with topology: 33/33
Max consecutive topology epochs: 33
Best topological loss: 1.3466
Final topological loss: 1.3466
âœ… SUCCESS: Topological learning achieved!
ğŸš€ EXCELLENT: Very consistent topological learning (>80%)
ğŸ“ˆ Topological learning appears stable

======================================================================
ğŸ¯ TOPOLOGICAL AUTOENCODER TRAINING COMPLETED
======================================================================
âœ… Topological training completed successfully!
Creating topological loss plots...
Topological loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/plots/moor_topo-contrastive_autoencoder_attention_topological_training_losses.png
Curriculum learning analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/plots/moor_topo-contrastive_autoencoder_attention_curriculum_analysis.png
Main training plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/plots/moor_topo-contrastive_autoencoder_attention_topological_training_losses.png
Curriculum analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/plots/moor_topo-contrastive_autoencoder_attention_curriculum_analysis.png
Training summary saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/moor_topo-contrastive_autoencoder_attention_training_summary.txt
Saving results...
Loading best model from entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/checkpoints/best_model.pt
Starting model evaluation...
========================================
GlobalContrastiveEvaluator initialized on cuda
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.1248
  Adjusted Rand Score: 0.4881
  Clustering Accuracy: 0.7878
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.7809
  Per-class F1: [0.7913623372499206, 0.6899474487201221, 0.8573746552003896]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.006944
Evaluating separation quality...
Separation Results:
  Positive distances: 5.370 Â± 1.275
  Negative distances: 7.464 Â± 2.034
  Separation ratio: 1.39x
  Gap: -11.784
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.1248
  Clustering Accuracy: 0.7878
  Adjusted Rand Score: 0.4881

Classification Performance:
  Accuracy: 0.7809

Separation Quality:
  Separation Ratio: 1.39x
  Gap: -11.784
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.006944
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/results/evaluation_results_20250725_133732.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/results/evaluation_results_20250725_133732.json

Key Results:
  Separation ratio: 1.39x
  Perfect separation: False
  Classification accuracy: 0.7809

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

ğŸ“ˆ TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 33
  Epochs with topological learning: 33
  Current topological loss: 1.3466
  Current topological weight: 0.6400
  âœ… Topological loss is decreasing (good progress)
ğŸš€ EXCELLENT: Consistent topological learning achieved!
Final topological loss: 1.3466
Epochs with topology: 33/33
ğŸ‘ Good clustering accuracy: 0.788

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853/results/final_analysis.json
Experiment saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_attention_20250725_130853

Analysis completed with exit code: 0
Time: Fri 25 Jul 13:37:33 BST 2025

=== ANALYSIS SUCCESSFUL ===
Regularization successful!


Job finished.
