Starting Surface Distance Metric Analysis job...
Job ID: 186369
Node: gpuvm14
Time: Fri 25 Jul 17:05:33 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Fri Jul 25 17:05:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   37C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 3,045,451
FullDatasetContrastiveLoss initialized:
  Positive Margin: 2.0
  Negative Margin: 10.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 100.0
MoorTopologicalLoss Initialized: Using 0-dimensional persistence pairings (MST edges).
No prototypes being used for topological loss - whole dataset instead.
TopologicalTrainer initialized on device: cuda
Model parameters: 3,045,451
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.2
  Reconstruction weight: 100.0

======================================================================
ğŸ§  TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

ğŸŒ Updating global dataset at epoch 1
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 0.005 Â± 0.001
    Neg distances: 0.005 Â± 0.001
    Separation ratio: 1.00x
    Gap: -0.009
    âŒ Poor global separation

============================================================
EPOCH 1 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=20.9264 (C:9.9490, R:0.0159, T:46.9304(w:0.200)âš ï¸)
Batch  25/537: Loss=13.8976 (C:8.7082, R:0.0317, T:10.0980(w:0.200)âš ï¸)
Batch  50/537: Loss=12.5143 (C:8.7056, R:0.0241, T:6.9686(w:0.200)ğŸš€)
Batch  75/537: Loss=11.4749 (C:8.7068, R:0.0171, T:5.3025(w:0.200)ğŸš€)
Batch 100/537: Loss=10.9946 (C:8.7040, R:0.0129, T:5.0141(w:0.200)ğŸš€)
Batch 125/537: Loss=10.7502 (C:8.7029, R:0.0113, T:4.5637(w:0.200)ğŸš€)
Batch 150/537: Loss=10.7176 (C:8.7067, R:0.0109, T:4.6257(w:0.200)ğŸš€)
Batch 175/537: Loss=10.6500 (C:8.7045, R:0.0105, T:4.4797(w:0.200)ğŸš€)
Batch 200/537: Loss=10.6163 (C:8.7054, R:0.0103, T:4.3973(w:0.200)ğŸš€)
Batch 225/537: Loss=10.5998 (C:8.7061, R:0.0102, T:4.3470(w:0.200)ğŸš€)
Batch 250/537: Loss=10.5088 (C:8.7023, R:0.0102, T:3.9510(w:0.200)ğŸš€)
Batch 275/537: Loss=10.4622 (C:8.7060, R:0.0099, T:3.8122(w:0.200)ğŸš€)
Batch 300/537: Loss=10.3665 (C:8.7015, R:0.0097, T:3.4798(w:0.200)ğŸš€)
Batch 325/537: Loss=10.3459 (C:8.7072, R:0.0097, T:3.3565(w:0.200)ğŸš€)
Batch 350/537: Loss=10.3257 (C:8.7059, R:0.0096, T:3.2971(w:0.200)ğŸš€)
Batch 375/537: Loss=10.3572 (C:8.7020, R:0.0096, T:3.4828(w:0.200)ğŸš€)
Batch 400/537: Loss=10.2838 (C:8.7043, R:0.0095, T:3.1377(w:0.200)ğŸš€)
Batch 425/537: Loss=10.2720 (C:8.7033, R:0.0094, T:3.1516(w:0.200)ğŸš€)
Batch 450/537: Loss=10.1980 (C:8.7068, R:0.0093, T:2.8147(w:0.200)ğŸš€)
Batch 475/537: Loss=10.2044 (C:8.7069, R:0.0093, T:2.8142(w:0.200)ğŸš€)
Batch 500/537: Loss=10.2073 (C:8.7058, R:0.0093, T:2.8736(w:0.200)ğŸš€)
Batch 525/537: Loss=10.1865 (C:8.7025, R:0.0093, T:2.7932(w:0.200)ğŸš€)
ğŸ‰ MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 4.7887
ğŸ“ˆ New best topological loss: 4.7887

ğŸ“Š EPOCH 1 TRAINING SUMMARY:
  Total Loss: 10.9262
  Contrastive: 8.7151
  Reconstruction: 0.0125
  Topological: 4.7887 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 16.5419
  Contrastive: 8.7506
  Reconstruction: 0.0090
  Topological: 34.4447 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 1/100 COMPLETE (56.4s)
Train Loss: 10.9262 (C:8.7151, R:0.0125, T:4.7887)
Val Loss:   16.5419 (C:8.7506, R:0.0090, T:34.4447)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=10.1759 (C:8.7032, R:0.0092, T:2.7388(w:0.200)ğŸš€)
Batch  25/537: Loss=10.1767 (C:8.7063, R:0.0092, T:2.7736(w:0.200)ğŸš€)
Batch  50/537: Loss=10.1443 (C:8.7052, R:0.0092, T:2.6194(w:0.200)ğŸš€)
Batch  75/537: Loss=10.1089 (C:8.7029, R:0.0091, T:2.4926(w:0.200)ğŸš€)
Batch 100/537: Loss=10.1223 (C:8.7060, R:0.0091, T:2.5304(w:0.200)ğŸš€)
Batch 125/537: Loss=10.0908 (C:8.7077, R:0.0090, T:2.4152(w:0.200)ğŸš€)
Batch 150/537: Loss=10.0887 (C:8.7067, R:0.0090, T:2.4193(w:0.200)ğŸš€)
Batch 175/537: Loss=10.0729 (C:8.7028, R:0.0089, T:2.3809(w:0.200)ğŸš€)
Batch 200/537: Loss=10.0689 (C:8.6969, R:0.0090, T:2.3818(w:0.200)ğŸš€)
Batch 225/537: Loss=10.0585 (C:8.7071, R:0.0090, T:2.2713(w:0.200)ğŸš€)
Batch 250/537: Loss=10.0646 (C:8.7024, R:0.0089, T:2.3646(w:0.200)ğŸš€)
Batch 275/537: Loss=10.0328 (C:8.7028, R:0.0089, T:2.2201(w:0.200)ğŸš€)
Batch 300/537: Loss=10.0400 (C:8.7030, R:0.0088, T:2.2845(w:0.200)ğŸš€)
Batch 325/537: Loss=10.0250 (C:8.7019, R:0.0088, T:2.2310(w:0.200)ğŸš€)
Batch 350/537: Loss=10.0245 (C:8.7061, R:0.0088, T:2.2142(w:0.200)ğŸš€)
Batch 375/537: Loss=9.9951 (C:8.7054, R:0.0087, T:2.0759(w:0.200)ğŸš€)
Batch 400/537: Loss=9.9756 (C:8.7031, R:0.0087, T:2.0173(w:0.200)ğŸš€)
Batch 425/537: Loss=9.9697 (C:8.7057, R:0.0086, T:2.0082(w:0.200)ğŸš€)
Batch 450/537: Loss=9.9639 (C:8.6988, R:0.0087, T:1.9922(w:0.200)ğŸš€)
Batch 475/537: Loss=9.9560 (C:8.7029, R:0.0086, T:1.9782(w:0.200)ğŸš€)
Batch 500/537: Loss=9.9379 (C:8.7035, R:0.0085, T:1.9009(w:0.200)ğŸš€)
Batch 525/537: Loss=9.9418 (C:8.7022, R:0.0086, T:1.9033(w:0.200)ğŸš€)
ğŸ“ˆ New best topological loss: 2.2736

ğŸ“Š EPOCH 2 TRAINING SUMMARY:
  Total Loss: 10.0458
  Contrastive: 8.7037
  Reconstruction: 0.0089
  Topological: 2.2736 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 13.8802
  Contrastive: 8.7021
  Reconstruction: 0.0082
  Topological: 21.8031 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 2/100 COMPLETE (45.9s)
Train Loss: 10.0458 (C:8.7037, R:0.0089, T:2.2736)
Val Loss:   13.8802 (C:8.7021, R:0.0082, T:21.8031)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=9.9316 (C:8.7036, R:0.0086, T:1.8645(w:0.200)ğŸš€)
Batch  25/537: Loss=9.9251 (C:8.7039, R:0.0086, T:1.8279(w:0.200)ğŸš€)
Batch  50/537: Loss=9.9128 (C:8.6987, R:0.0085, T:1.8379(w:0.200)ğŸš€)
Batch  75/537: Loss=9.9266 (C:8.6994, R:0.0085, T:1.8806(w:0.200)ğŸš€)
Batch 100/537: Loss=9.9106 (C:8.7040, R:0.0084, T:1.8187(w:0.200)ğŸš€)
Batch 125/537: Loss=9.9301 (C:8.6972, R:0.0085, T:1.9338(w:0.200)ğŸš€)
Batch 150/537: Loss=9.9054 (C:8.7012, R:0.0085, T:1.7759(w:0.200)ğŸš€)
Batch 175/537: Loss=9.8962 (C:8.7008, R:0.0085, T:1.7433(w:0.200)ğŸš€)
Batch 200/537: Loss=9.9193 (C:8.6981, R:0.0084, T:1.9027(w:0.200)ğŸš€)
Batch 225/537: Loss=9.8946 (C:8.6985, R:0.0084, T:1.7990(w:0.200)ğŸš€)
Batch 250/537: Loss=9.9035 (C:8.7007, R:0.0084, T:1.8194(w:0.200)ğŸš€)
Batch 275/537: Loss=9.8763 (C:8.6965, R:0.0084, T:1.7127(w:0.200)ğŸš€)
Batch 300/537: Loss=9.8669 (C:8.6885, R:0.0083, T:1.7201(w:0.200)ğŸš€)
Batch 325/537: Loss=9.8846 (C:8.7009, R:0.0083, T:1.7628(w:0.200)ğŸš€)
Batch 350/537: Loss=9.8687 (C:8.6939, R:0.0083, T:1.7298(w:0.200)ğŸš€)
Batch 375/537: Loss=9.8428 (C:8.6968, R:0.0082, T:1.6072(w:0.200)ğŸš€)
Batch 400/537: Loss=9.8563 (C:8.7002, R:0.0083, T:1.6535(w:0.200)ğŸš€)
Batch 425/537: Loss=9.8711 (C:8.6978, R:0.0082, T:1.7471(w:0.200)ğŸš€)
Batch 450/537: Loss=9.8518 (C:8.6981, R:0.0083, T:1.6427(w:0.200)ğŸš€)
Batch 475/537: Loss=9.8424 (C:8.6982, R:0.0082, T:1.6026(w:0.200)ğŸš€)
Batch 500/537: Loss=9.8458 (C:8.6922, R:0.0082, T:1.6680(w:0.200)ğŸš€)
Batch 525/537: Loss=9.8220 (C:8.6910, R:0.0081, T:1.5881(w:0.200)ğŸš€)
ğŸ“ˆ New best topological loss: 1.7483

ğŸ“Š EPOCH 3 TRAINING SUMMARY:
  Total Loss: 9.8831
  Contrastive: 8.6985
  Reconstruction: 0.0083
  Topological: 1.7483 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.5095
  Contrastive: 8.6869
  Reconstruction: 0.0077
  Topological: 15.2705 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 3/100 COMPLETE (48.3s)
Train Loss: 9.8831 (C:8.6985, R:0.0083, T:1.7483)
Val Loss:   12.5095 (C:8.6869, R:0.0077, T:15.2705)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 4
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.035 Â± 0.814
    Neg distances: 4.152 Â± 0.804
    Separation ratio: 1.03x
    Gap: -6.729
    âŒ Poor global separation

============================================================
EPOCH 4 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=9.0412 (C:7.9137, R:0.0082, T:1.5432(w:0.200)ğŸš€)
Batch  25/537: Loss=9.0398 (C:7.8741, R:0.0082, T:1.7397(w:0.200)ğŸš€)
Batch  50/537: Loss=9.0153 (C:7.8453, R:0.0082, T:1.7590(w:0.200)ğŸš€)
Batch  75/537: Loss=9.0197 (C:7.8406, R:0.0081, T:1.8227(w:0.200)ğŸš€)
Batch 100/537: Loss=9.0187 (C:7.8406, R:0.0082, T:1.7867(w:0.200)ğŸš€)
Batch 125/537: Loss=9.0299 (C:7.8432, R:0.0082, T:1.8397(w:0.200)ğŸš€)
Batch 150/537: Loss=8.9853 (C:7.8001, R:0.0082, T:1.8409(w:0.200)ğŸš€)
Batch 175/537: Loss=8.9949 (C:7.8044, R:0.0082, T:1.8487(w:0.200)ğŸš€)
Batch 200/537: Loss=8.9844 (C:7.7781, R:0.0081, T:1.9623(w:0.200)ğŸš€)
Batch 225/537: Loss=8.9818 (C:7.7615, R:0.0082, T:1.9946(w:0.200)ğŸš€)
Batch 250/537: Loss=8.9773 (C:7.7851, R:0.0081, T:1.8872(w:0.200)ğŸš€)
Batch 275/537: Loss=9.0264 (C:7.7909, R:0.0082, T:2.0604(w:0.200)ğŸš€)
Batch 300/537: Loss=8.9717 (C:7.7833, R:0.0081, T:1.8994(w:0.200)ğŸš€)
Batch 325/537: Loss=8.9647 (C:7.7791, R:0.0081, T:1.8611(w:0.200)ğŸš€)
Batch 350/537: Loss=8.9698 (C:7.7422, R:0.0082, T:2.0498(w:0.200)ğŸš€)
Batch 375/537: Loss=8.9527 (C:7.7814, R:0.0081, T:1.7828(w:0.200)ğŸš€)
Batch 400/537: Loss=8.9458 (C:7.7349, R:0.0081, T:1.9941(w:0.200)ğŸš€)
Batch 425/537: Loss=8.9634 (C:7.7624, R:0.0081, T:1.9484(w:0.200)ğŸš€)
Batch 450/537: Loss=8.9617 (C:7.7489, R:0.0081, T:1.9944(w:0.200)ğŸš€)
Batch 475/537: Loss=8.9898 (C:7.7620, R:0.0082, T:2.0601(w:0.200)ğŸš€)
Batch 500/537: Loss=8.9070 (C:7.7348, R:0.0081, T:1.8197(w:0.200)ğŸš€)
Batch 525/537: Loss=8.9204 (C:7.7389, R:0.0080, T:1.8868(w:0.200)ğŸš€)

ğŸ“Š EPOCH 4 TRAINING SUMMARY:
  Total Loss: 8.9859
  Contrastive: 7.7934
  Reconstruction: 0.0082
  Topological: 1.8863 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.7860
  Contrastive: 7.6564
  Reconstruction: 0.0076
  Topological: 11.8691 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 4/100 COMPLETE (54.2s)
Train Loss: 8.9859 (C:7.7934, R:0.0082, T:1.8863)
Val Loss:   10.7860 (C:7.6564, R:0.0076, T:11.8691)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=8.9372 (C:7.7306, R:0.0081, T:1.9781(w:0.200)ğŸš€)
Batch  25/537: Loss=8.9077 (C:7.7238, R:0.0081, T:1.8703(w:0.200)ğŸš€)
Batch  50/537: Loss=8.9408 (C:7.7366, R:0.0081, T:1.9563(w:0.200)ğŸš€)
Batch  75/537: Loss=8.9299 (C:7.7562, R:0.0080, T:1.8557(w:0.200)ğŸš€)
Batch 100/537: Loss=8.9013 (C:7.7195, R:0.0081, T:1.8743(w:0.200)ğŸš€)
Batch 125/537: Loss=8.8717 (C:7.7011, R:0.0080, T:1.8482(w:0.200)ğŸš€)
Batch 150/537: Loss=8.9577 (C:7.7552, R:0.0080, T:2.0063(w:0.200)ğŸš€)
Batch 175/537: Loss=8.8971 (C:7.7304, R:0.0081, T:1.7988(w:0.200)ğŸš€)
Batch 200/537: Loss=8.8955 (C:7.6958, R:0.0080, T:2.0018(w:0.200)ğŸš€)
Batch 225/537: Loss=8.8830 (C:7.6967, R:0.0080, T:1.9144(w:0.200)ğŸš€)
Batch 250/537: Loss=8.9136 (C:7.7125, R:0.0080, T:1.9985(w:0.200)ğŸš€)
Batch 275/537: Loss=8.9206 (C:7.7352, R:0.0081, T:1.8804(w:0.200)ğŸš€)
Batch 300/537: Loss=8.8446 (C:7.6726, R:0.0080, T:1.8426(w:0.200)ğŸš€)
Batch 325/537: Loss=8.9348 (C:7.7564, R:0.0081, T:1.8660(w:0.200)ğŸš€)
Batch 350/537: Loss=8.8802 (C:7.7181, R:0.0080, T:1.8148(w:0.200)ğŸš€)
Batch 375/537: Loss=8.8739 (C:7.7067, R:0.0079, T:1.8618(w:0.200)ğŸš€)
Batch 400/537: Loss=8.8982 (C:7.7100, R:0.0080, T:1.9484(w:0.200)ğŸš€)
Batch 425/537: Loss=8.8512 (C:7.6768, R:0.0080, T:1.8842(w:0.200)ğŸš€)
Batch 450/537: Loss=8.8820 (C:7.7094, R:0.0079, T:1.8883(w:0.200)ğŸš€)
Batch 475/537: Loss=8.9052 (C:7.7563, R:0.0079, T:1.7794(w:0.200)ğŸš€)
Batch 500/537: Loss=8.9086 (C:7.6946, R:0.0080, T:2.0891(w:0.200)ğŸš€)
Batch 525/537: Loss=8.9034 (C:7.7176, R:0.0079, T:1.9558(w:0.200)ğŸš€)

ğŸ“Š EPOCH 5 TRAINING SUMMARY:
  Total Loss: 8.9015
  Contrastive: 7.7148
  Reconstruction: 0.0080
  Topological: 1.9257 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.5593
  Contrastive: 7.6219
  Reconstruction: 0.0074
  Topological: 10.9947 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 5/100 COMPLETE (45.6s)
Train Loss: 8.9015 (C:7.7148, R:0.0080, T:1.9257)
Val Loss:   10.5593 (C:7.6219, R:0.0074, T:10.9947)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=8.8483 (C:7.6970, R:0.0079, T:1.8200(w:0.200)ğŸš€)
Batch  25/537: Loss=8.8341 (C:7.6611, R:0.0079, T:1.8998(w:0.200)ğŸš€)
Batch  50/537: Loss=8.8994 (C:7.7047, R:0.0079, T:2.0168(w:0.200)ğŸš€)
Batch  75/537: Loss=8.8366 (C:7.6960, R:0.0079, T:1.7548(w:0.200)ğŸš€)
Batch 100/537: Loss=8.8772 (C:7.6705, R:0.0079, T:2.0776(w:0.200)ğŸš€)
Batch 125/537: Loss=8.8714 (C:7.6854, R:0.0079, T:2.0050(w:0.200)ğŸš€)
Batch 150/537: Loss=8.8635 (C:7.6814, R:0.0078, T:1.9875(w:0.200)ğŸš€)
Batch 175/537: Loss=8.8381 (C:7.6971, R:0.0078, T:1.7908(w:0.200)ğŸš€)
Batch 200/537: Loss=8.8453 (C:7.6911, R:0.0079, T:1.8244(w:0.200)ğŸš€)
Batch 225/537: Loss=8.8629 (C:7.6628, R:0.0079, T:2.0316(w:0.200)ğŸš€)
Batch 250/537: Loss=8.8737 (C:7.6918, R:0.0078, T:1.9925(w:0.200)ğŸš€)
Batch 275/537: Loss=8.8347 (C:7.6942, R:0.0078, T:1.7865(w:0.200)ğŸš€)
Batch 300/537: Loss=8.8372 (C:7.6738, R:0.0079, T:1.8859(w:0.200)ğŸš€)
Batch 325/537: Loss=8.8633 (C:7.6918, R:0.0078, T:1.9377(w:0.200)ğŸš€)
Batch 350/537: Loss=8.8485 (C:7.6760, R:0.0078, T:1.9389(w:0.200)ğŸš€)
Batch 375/537: Loss=8.8796 (C:7.6929, R:0.0079, T:2.0072(w:0.200)ğŸš€)
Batch 400/537: Loss=8.8274 (C:7.6717, R:0.0079, T:1.8517(w:0.200)ğŸš€)
Batch 425/537: Loss=8.8226 (C:7.6666, R:0.0078, T:1.8653(w:0.200)ğŸš€)
Batch 450/537: Loss=8.7770 (C:7.6612, R:0.0078, T:1.6952(w:0.200)ğŸš€)
Batch 475/537: Loss=8.8701 (C:7.6762, R:0.0079, T:2.0440(w:0.200)ğŸš€)
Batch 500/537: Loss=8.8039 (C:7.6445, R:0.0078, T:1.8759(w:0.200)ğŸš€)
Batch 525/537: Loss=8.7816 (C:7.6394, R:0.0078, T:1.8343(w:0.200)ğŸš€)

ğŸ“Š EPOCH 6 TRAINING SUMMARY:
  Total Loss: 8.8315
  Contrastive: 7.6733
  Reconstruction: 0.0079
  Topological: 1.8656 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.3155
  Contrastive: 7.6134
  Reconstruction: 0.0072
  Topological: 9.9050 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 6/100 COMPLETE (46.7s)
Train Loss: 8.8315 (C:7.6733, R:0.0079, T:1.8656)
Val Loss:   10.3155 (C:7.6134, R:0.0072, T:9.9050)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 7
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.350 Â± 0.866
    Neg distances: 5.513 Â± 1.342
    Separation ratio: 1.27x
    Gap: -8.932
    âŒ Poor global separation

============================================================
EPOCH 7 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=8.0844 (C:6.9721, R:0.0078, T:1.6744(w:0.200)ğŸš€)
Batch  25/537: Loss=8.0559 (C:6.8375, R:0.0079, T:2.1560(w:0.200)ğŸš€)
Batch  50/537: Loss=8.0173 (C:6.7984, R:0.0078, T:2.1993(w:0.200)ğŸš€)
Batch  75/537: Loss=8.1218 (C:6.8543, R:0.0079, T:2.3944(w:0.200)ğŸš€)
Batch 100/537: Loss=8.0947 (C:6.8573, R:0.0079, T:2.2456(w:0.200)ğŸš€)
Batch 125/537: Loss=8.0483 (C:6.7811, R:0.0079, T:2.4056(w:0.200)ğŸš€)
Batch 150/537: Loss=8.0958 (C:6.8281, R:0.0078, T:2.4502(w:0.200)ğŸš€)
Batch 175/537: Loss=8.1141 (C:6.8316, R:0.0078, T:2.5002(w:0.200)ğŸš€)
Batch 200/537: Loss=8.0758 (C:6.7853, R:0.0079, T:2.5175(w:0.200)ğŸš€)
Batch 225/537: Loss=8.0273 (C:6.7440, R:0.0078, T:2.5044(w:0.200)ğŸš€)
Batch 250/537: Loss=8.1036 (C:6.8399, R:0.0078, T:2.4244(w:0.200)ğŸš€)
Batch 275/537: Loss=8.1267 (C:6.7983, R:0.0078, T:2.7608(w:0.200)ğŸš€)
Batch 300/537: Loss=8.0124 (C:6.7120, R:0.0078, T:2.5836(w:0.200)ğŸš€)
Batch 325/537: Loss=7.9905 (C:6.7188, R:0.0078, T:2.4380(w:0.200)ğŸš€)
Batch 350/537: Loss=7.9910 (C:6.7370, R:0.0078, T:2.3553(w:0.200)ğŸš€)
Batch 375/537: Loss=8.0657 (C:6.7897, R:0.0079, T:2.4515(w:0.200)ğŸš€)
Batch 400/537: Loss=7.9884 (C:6.7223, R:0.0078, T:2.4188(w:0.200)ğŸš€)
Batch 425/537: Loss=8.0375 (C:6.7486, R:0.0078, T:2.5367(w:0.200)ğŸš€)
Batch 450/537: Loss=7.9797 (C:6.7034, R:0.0078, T:2.4741(w:0.200)ğŸš€)
Batch 475/537: Loss=8.1005 (C:6.8445, R:0.0078, T:2.3601(w:0.200)ğŸš€)
Batch 500/537: Loss=8.0540 (C:6.7981, R:0.0078, T:2.3636(w:0.200)ğŸš€)
Batch 525/537: Loss=7.9123 (C:6.6752, R:0.0078, T:2.2837(w:0.200)ğŸš€)

ğŸ“Š EPOCH 7 TRAINING SUMMARY:
  Total Loss: 8.0484
  Contrastive: 6.7705
  Reconstruction: 0.0078
  Topological: 2.4678 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.4638
  Contrastive: 6.5674
  Reconstruction: 0.0072
  Topological: 10.8723 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 7/100 COMPLETE (53.0s)
Train Loss: 8.0484 (C:6.7705, R:0.0078, T:2.4678)
Val Loss:   9.4638 (C:6.5674, R:0.0072, T:10.8723)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.9423 (C:6.6857, R:0.0078, T:2.3777(w:0.200)ğŸš€)
Batch  25/537: Loss=7.9956 (C:6.7197, R:0.0079, T:2.4407(w:0.200)ğŸš€)
Batch  50/537: Loss=7.9493 (C:6.7357, R:0.0078, T:2.1768(w:0.200)ğŸš€)
Batch  75/537: Loss=7.9977 (C:6.7375, R:0.0079, T:2.3604(w:0.200)ğŸš€)
Batch 100/537: Loss=8.0211 (C:6.7248, R:0.0078, T:2.5592(w:0.200)ğŸš€)
Batch 125/537: Loss=7.9689 (C:6.6920, R:0.0078, T:2.4968(w:0.200)ğŸš€)
Batch 150/537: Loss=7.9509 (C:6.6940, R:0.0078, T:2.3622(w:0.200)ğŸš€)
Batch 175/537: Loss=7.9628 (C:6.6988, R:0.0078, T:2.4331(w:0.200)ğŸš€)
Batch 200/537: Loss=8.0494 (C:6.7006, R:0.0078, T:2.8387(w:0.200)ğŸš€)
Batch 225/537: Loss=7.9868 (C:6.7476, R:0.0078, T:2.3123(w:0.200)ğŸš€)
Batch 250/537: Loss=7.9667 (C:6.6983, R:0.0078, T:2.4352(w:0.200)ğŸš€)
Batch 275/537: Loss=7.9268 (C:6.6727, R:0.0077, T:2.3974(w:0.200)ğŸš€)
Batch 300/537: Loss=7.9214 (C:6.6623, R:0.0077, T:2.4338(w:0.200)ğŸš€)
Batch 325/537: Loss=7.9720 (C:6.6903, R:0.0078, T:2.5228(w:0.200)ğŸš€)
Batch 350/537: Loss=8.0002 (C:6.7440, R:0.0078, T:2.3813(w:0.200)ğŸš€)
Batch 375/537: Loss=7.9538 (C:6.6568, R:0.0079, T:2.5485(w:0.200)ğŸš€)
Batch 400/537: Loss=8.0049 (C:6.7674, R:0.0078, T:2.2854(w:0.200)ğŸš€)
Batch 425/537: Loss=7.9886 (C:6.7230, R:0.0078, T:2.4369(w:0.200)ğŸš€)
Batch 450/537: Loss=7.9702 (C:6.6656, R:0.0078, T:2.6390(w:0.200)ğŸš€)
Batch 475/537: Loss=8.0251 (C:6.7114, R:0.0078, T:2.6792(w:0.200)ğŸš€)
Batch 500/537: Loss=7.9461 (C:6.6953, R:0.0078, T:2.3473(w:0.200)ğŸš€)
Batch 525/537: Loss=8.0226 (C:6.7140, R:0.0078, T:2.6505(w:0.200)ğŸš€)

ğŸ“Š EPOCH 8 TRAINING SUMMARY:
  Total Loss: 7.9741
  Contrastive: 6.7057
  Reconstruction: 0.0078
  Topological: 2.4487 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.3595
  Contrastive: 6.5886
  Reconstruction: 0.0071
  Topological: 10.2820 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 8/100 COMPLETE (46.6s)
Train Loss: 7.9741 (C:6.7057, R:0.0078, T:2.4487)
Val Loss:   9.3595 (C:6.5886, R:0.0071, T:10.2820)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.8861 (C:6.6909, R:0.0077, T:2.1015(w:0.200)ğŸš€)
Batch  25/537: Loss=7.9696 (C:6.6392, R:0.0078, T:2.7609(w:0.200)ğŸš€)
Batch  50/537: Loss=7.9836 (C:6.6985, R:0.0077, T:2.5757(w:0.200)ğŸš€)
Batch  75/537: Loss=7.9289 (C:6.6810, R:0.0077, T:2.3921(w:0.200)ğŸš€)
Batch 100/537: Loss=7.9461 (C:6.6864, R:0.0078, T:2.4187(w:0.200)ğŸš€)
Batch 125/537: Loss=7.9372 (C:6.6391, R:0.0078, T:2.6005(w:0.200)ğŸš€)
Batch 150/537: Loss=7.9678 (C:6.7158, R:0.0078, T:2.3811(w:0.200)ğŸš€)
Batch 175/537: Loss=7.9181 (C:6.6416, R:0.0078, T:2.5020(w:0.200)ğŸš€)
Batch 200/537: Loss=7.9118 (C:6.6917, R:0.0077, T:2.2498(w:0.200)ğŸš€)
Batch 225/537: Loss=7.9376 (C:6.6929, R:0.0078, T:2.3352(w:0.200)ğŸš€)
Batch 250/537: Loss=7.8662 (C:6.6149, R:0.0077, T:2.3964(w:0.200)ğŸš€)
Batch 275/537: Loss=8.0063 (C:6.7135, R:0.0077, T:2.6097(w:0.200)ğŸš€)
Batch 300/537: Loss=7.9157 (C:6.6366, R:0.0077, T:2.5659(w:0.200)ğŸš€)
Batch 325/537: Loss=7.9319 (C:6.6488, R:0.0077, T:2.5564(w:0.200)ğŸš€)
Batch 350/537: Loss=7.8584 (C:6.6298, R:0.0077, T:2.3143(w:0.200)ğŸš€)
Batch 375/537: Loss=7.9562 (C:6.6589, R:0.0077, T:2.6425(w:0.200)ğŸš€)
Batch 400/537: Loss=8.0060 (C:6.7582, R:0.0077, T:2.4029(w:0.200)ğŸš€)
Batch 425/537: Loss=7.9540 (C:6.6340, R:0.0078, T:2.7219(w:0.200)ğŸš€)
Batch 450/537: Loss=7.9082 (C:6.6262, R:0.0078, T:2.5226(w:0.200)ğŸš€)
Batch 475/537: Loss=7.8011 (C:6.6190, R:0.0077, T:2.0520(w:0.200)ğŸš€)
Batch 500/537: Loss=8.0484 (C:6.7183, R:0.0077, T:2.8188(w:0.200)ğŸš€)
Batch 525/537: Loss=7.8787 (C:6.6475, R:0.0077, T:2.3141(w:0.200)ğŸš€)

ğŸ“Š EPOCH 9 TRAINING SUMMARY:
  Total Loss: 7.9200
  Contrastive: 6.6622
  Reconstruction: 0.0077
  Topological: 2.4199 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.2282
  Contrastive: 6.5678
  Reconstruction: 0.0071
  Topological: 9.7660 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 9/100 COMPLETE (45.7s)
Train Loss: 7.9200 (C:6.6622, R:0.0077, T:2.4199)
Val Loss:   9.2282 (C:6.5678, R:0.0071, T:9.7660)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 10
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.294 Â± 1.093
    Neg distances: 6.468 Â± 1.997
    Separation ratio: 1.51x
    Gap: -10.612
    âš ï¸  Moderate global separation

============================================================
EPOCH 10 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2226 (C:6.0700, R:0.0077, T:1.9256(w:0.200)ğŸš€)
Batch  25/537: Loss=7.2186 (C:5.9323, R:0.0078, T:2.5478(w:0.200)ğŸš€)
Batch  50/537: Loss=7.3328 (C:5.9930, R:0.0077, T:2.8543(w:0.200)ğŸš€)
Batch  75/537: Loss=7.3202 (C:5.9795, R:0.0078, T:2.8018(w:0.200)ğŸš€)
Batch 100/537: Loss=7.3038 (C:5.9780, R:0.0077, T:2.7545(w:0.200)ğŸš€)
Batch 125/537: Loss=7.3711 (C:6.0256, R:0.0078, T:2.8037(w:0.200)ğŸš€)
Batch 150/537: Loss=7.3684 (C:6.0371, R:0.0077, T:2.7937(w:0.200)ğŸš€)
Batch 175/537: Loss=7.2656 (C:5.9524, R:0.0077, T:2.7033(w:0.200)ğŸš€)
Batch 200/537: Loss=7.2519 (C:5.9298, R:0.0077, T:2.7481(w:0.200)ğŸš€)
Batch 225/537: Loss=7.2369 (C:5.9889, R:0.0077, T:2.3988(w:0.200)ğŸš€)
Batch 250/537: Loss=7.2977 (C:5.9613, R:0.0078, T:2.7968(w:0.200)ğŸš€)
Batch 275/537: Loss=7.1862 (C:5.8811, R:0.0077, T:2.6513(w:0.200)ğŸš€)
Batch 300/537: Loss=7.2714 (C:6.0013, R:0.0077, T:2.4855(w:0.200)ğŸš€)
Batch 325/537: Loss=7.3764 (C:6.0313, R:0.0077, T:2.8806(w:0.200)ğŸš€)
Batch 350/537: Loss=7.1910 (C:5.8991, R:0.0078, T:2.5695(w:0.200)ğŸš€)
Batch 375/537: Loss=7.2655 (C:6.0014, R:0.0077, T:2.4696(w:0.200)ğŸš€)
Batch 400/537: Loss=7.1285 (C:5.8956, R:0.0077, T:2.3146(w:0.200)ğŸš€)
Batch 425/537: Loss=7.1461 (C:5.8846, R:0.0077, T:2.4565(w:0.200)ğŸš€)
Batch 450/537: Loss=7.2819 (C:5.9803, R:0.0077, T:2.6601(w:0.200)ğŸš€)
Batch 475/537: Loss=7.2274 (C:5.9515, R:0.0077, T:2.5432(w:0.200)ğŸš€)
Batch 500/537: Loss=7.3367 (C:6.0854, R:0.0077, T:2.4054(w:0.200)ğŸš€)
Batch 525/537: Loss=7.3036 (C:6.0556, R:0.0077, T:2.3847(w:0.200)ğŸš€)

ğŸ“Š EPOCH 10 TRAINING SUMMARY:
  Total Loss: 7.2866
  Contrastive: 5.9868
  Reconstruction: 0.0077
  Topological: 2.6360 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.5925
  Contrastive: 5.8351
  Reconstruction: 0.0071
  Topological: 10.2574 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 10/100 COMPLETE (53.2s)
Train Loss: 7.2866 (C:5.9868, R:0.0077, T:2.6360)
Val Loss:   8.5925 (C:5.8351, R:0.0071, T:10.2574)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 537 | Topological Weight: 0.2000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2202 (C:5.9927, R:0.0078, T:2.2615(w:0.200)ğŸš€)
Batch  25/537: Loss=7.2216 (C:5.9461, R:0.0077, T:2.5365(w:0.200)ğŸš€)
Batch  50/537: Loss=7.3403 (C:6.0369, R:0.0077, T:2.6551(w:0.200)ğŸš€)
Batch  75/537: Loss=7.2848 (C:5.9957, R:0.0077, T:2.6128(w:0.200)ğŸš€)
Batch 100/537: Loss=7.2344 (C:5.9443, R:0.0077, T:2.6022(w:0.200)ğŸš€)
Batch 125/537: Loss=7.3622 (C:6.0771, R:0.0077, T:2.5850(w:0.200)ğŸš€)
Batch 150/537: Loss=7.2068 (C:5.9227, R:0.0077, T:2.5864(w:0.200)ğŸš€)
Batch 175/537: Loss=7.2199 (C:5.9162, R:0.0077, T:2.6885(w:0.200)ğŸš€)
Batch 200/537: Loss=7.2895 (C:5.9651, R:0.0077, T:2.7888(w:0.200)ğŸš€)
Batch 225/537: Loss=7.2768 (C:5.9699, R:0.0076, T:2.7259(w:0.200)ğŸš€)
Batch 250/537: Loss=7.1523 (C:5.9291, R:0.0077, T:2.2880(w:0.200)ğŸš€)
Batch 275/537: Loss=7.2860 (C:5.9626, R:0.0077, T:2.7779(w:0.200)ğŸš€)
Batch 300/537: Loss=7.2225 (C:5.8929, R:0.0077, T:2.8042(w:0.200)ğŸš€)
Batch 325/537: Loss=7.2882 (C:6.0386, R:0.0076, T:2.4234(w:0.200)ğŸš€)
Batch 350/537: Loss=7.2354 (C:5.9852, R:0.0077, T:2.4212(w:0.200)ğŸš€)
Batch 375/537: Loss=7.1869 (C:5.9621, R:0.0077, T:2.2938(w:0.200)ğŸš€)
Batch 400/537: Loss=7.1831 (C:5.8989, R:0.0077, T:2.5756(w:0.200)ğŸš€)
Batch 425/537: Loss=7.3018 (C:6.0351, R:0.0077, T:2.5072(w:0.200)ğŸš€)
Batch 450/537: Loss=7.2877 (C:5.9784, R:0.0076, T:2.7270(w:0.200)ğŸš€)
Batch 475/537: Loss=7.2610 (C:5.9476, R:0.0077, T:2.7340(w:0.200)ğŸš€)
Batch 500/537: Loss=7.2739 (C:5.9919, R:0.0076, T:2.5882(w:0.200)ğŸš€)
Batch 525/537: Loss=7.2130 (C:5.8928, R:0.0077, T:2.7672(w:0.200)ğŸš€)

ğŸ“Š EPOCH 11 TRAINING SUMMARY:
  Total Loss: 7.2391
  Contrastive: 5.9488
  Reconstruction: 0.0077
  Topological: 2.6112 (weight: 0.200)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.5081
  Contrastive: 5.7723
  Reconstruction: 0.0070
  Topological: 10.1698 (weight: 0.200)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 11/100 COMPLETE (48.0s)
Train Loss: 7.2391 (C:5.9488, R:0.0077, T:2.6112)
Val Loss:   8.5081 (C:5.7723, R:0.0070, T:10.1698)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 537 | Topological Weight: 0.2200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2912 (C:5.9472, R:0.0077, T:2.6029(w:0.220)ğŸš€)
Batch  25/537: Loss=7.1792 (C:5.9202, R:0.0076, T:2.2552(w:0.220)ğŸš€)
Batch  50/537: Loss=7.2271 (C:5.8839, R:0.0078, T:2.5810(w:0.220)ğŸš€)
Batch  75/537: Loss=7.2059 (C:5.8765, R:0.0076, T:2.5873(w:0.220)ğŸš€)
Batch 100/537: Loss=7.2264 (C:5.9344, R:0.0077, T:2.3776(w:0.220)ğŸš€)
Batch 125/537: Loss=7.1871 (C:5.9443, R:0.0076, T:2.1747(w:0.220)ğŸš€)
Batch 150/537: Loss=7.2048 (C:5.8513, R:0.0077, T:2.6700(w:0.220)ğŸš€)
Batch 175/537: Loss=7.2262 (C:5.9095, R:0.0077, T:2.4892(w:0.220)ğŸš€)
Batch 200/537: Loss=7.2292 (C:5.9299, R:0.0076, T:2.4616(w:0.220)ğŸš€)
Batch 225/537: Loss=7.3269 (C:5.9905, R:0.0077, T:2.5865(w:0.220)ğŸš€)
Batch 250/537: Loss=7.3020 (C:5.9650, R:0.0077, T:2.5938(w:0.220)ğŸš€)
Batch 275/537: Loss=7.2138 (C:5.9442, R:0.0076, T:2.3080(w:0.220)ğŸš€)
Batch 300/537: Loss=7.3132 (C:5.9620, R:0.0076, T:2.6686(w:0.220)ğŸš€)
Batch 325/537: Loss=7.3289 (C:6.0078, R:0.0076, T:2.5302(w:0.220)ğŸš€)
Batch 350/537: Loss=7.2956 (C:6.0473, R:0.0076, T:2.2063(w:0.220)ğŸš€)
Batch 375/537: Loss=7.2310 (C:5.9545, R:0.0077, T:2.3134(w:0.220)ğŸš€)
Batch 400/537: Loss=7.2599 (C:5.9305, R:0.0077, T:2.5553(w:0.220)ğŸš€)
Batch 425/537: Loss=7.1139 (C:5.8707, R:0.0077, T:2.1639(w:0.220)ğŸš€)
Batch 450/537: Loss=7.2597 (C:5.9766, R:0.0077, T:2.3521(w:0.220)ğŸš€)
Batch 475/537: Loss=7.2067 (C:5.8859, R:0.0076, T:2.5269(w:0.220)ğŸš€)
Batch 500/537: Loss=7.2482 (C:5.9279, R:0.0077, T:2.5213(w:0.220)ğŸš€)
Batch 525/537: Loss=7.1990 (C:5.9301, R:0.0076, T:2.2924(w:0.220)ğŸš€)

ğŸ“Š EPOCH 12 TRAINING SUMMARY:
  Total Loss: 7.2455
  Contrastive: 5.9360
  Reconstruction: 0.0077
  Topological: 2.4734 (weight: 0.220)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.6468
  Contrastive: 5.7212
  Reconstruction: 0.0070
  Topological: 10.1190 (weight: 0.220)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 12/100 COMPLETE (48.6s)
Train Loss: 7.2455 (C:5.9360, R:0.0077, T:2.4734)
Val Loss:   8.6468 (C:5.7212, R:0.0070, T:10.1190)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 13
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.203 Â± 1.264
    Neg distances: 7.066 Â± 2.427
    Separation ratio: 1.68x
    Gap: -11.427
    âš ï¸  Moderate global separation

============================================================
EPOCH 13 | Batches: 537 | Topological Weight: 0.2400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.9294 (C:5.5736, R:0.0077, T:2.4517(w:0.240)ğŸš€)
Batch  25/537: Loss=6.8787 (C:5.5427, R:0.0077, T:2.3683(w:0.240)ğŸš€)
Batch  50/537: Loss=6.9075 (C:5.4974, R:0.0076, T:2.6891(w:0.240)ğŸš€)
Batch  75/537: Loss=6.8317 (C:5.5097, R:0.0077, T:2.3210(w:0.240)ğŸš€)
Batch 100/537: Loss=6.8043 (C:5.5059, R:0.0077, T:2.2036(w:0.240)ğŸš€)
Batch 125/537: Loss=7.0370 (C:5.6052, R:0.0077, T:2.7566(w:0.240)ğŸš€)
Batch 150/537: Loss=6.8320 (C:5.5091, R:0.0076, T:2.3254(w:0.240)ğŸš€)
Batch 175/537: Loss=7.0080 (C:5.6551, R:0.0077, T:2.4378(w:0.240)ğŸš€)
Batch 200/537: Loss=6.9401 (C:5.6076, R:0.0076, T:2.3892(w:0.240)ğŸš€)
Batch 225/537: Loss=6.9949 (C:5.6164, R:0.0077, T:2.5460(w:0.240)ğŸš€)
Batch 250/537: Loss=6.9150 (C:5.6364, R:0.0076, T:2.1532(w:0.240)ğŸš€)
Batch 275/537: Loss=6.8775 (C:5.5595, R:0.0076, T:2.3074(w:0.240)ğŸš€)
Batch 300/537: Loss=6.8393 (C:5.5128, R:0.0077, T:2.3390(w:0.240)ğŸš€)
Batch 325/537: Loss=7.0497 (C:5.6602, R:0.0076, T:2.6075(w:0.240)ğŸš€)
Batch 350/537: Loss=6.9565 (C:5.6007, R:0.0077, T:2.4555(w:0.240)ğŸš€)
Batch 375/537: Loss=6.8739 (C:5.5330, R:0.0076, T:2.4053(w:0.240)ğŸš€)
Batch 400/537: Loss=6.8882 (C:5.5195, R:0.0077, T:2.5115(w:0.240)ğŸš€)
Batch 425/537: Loss=6.9093 (C:5.5830, R:0.0076, T:2.3424(w:0.240)ğŸš€)
Batch 450/537: Loss=6.9469 (C:5.5817, R:0.0077, T:2.4910(w:0.240)ğŸš€)
Batch 475/537: Loss=7.0656 (C:5.6649, R:0.0076, T:2.6653(w:0.240)ğŸš€)
Batch 500/537: Loss=7.0281 (C:5.6508, R:0.0076, T:2.5733(w:0.240)ğŸš€)
Batch 525/537: Loss=6.9635 (C:5.6279, R:0.0077, T:2.3749(w:0.240)ğŸš€)

ğŸ“Š EPOCH 13 TRAINING SUMMARY:
  Total Loss: 6.9127
  Contrastive: 5.5648
  Reconstruction: 0.0077
  Topological: 2.4282 (weight: 0.240)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.3996
  Contrastive: 5.3599
  Reconstruction: 0.0070
  Topological: 9.7542 (weight: 0.240)
  Batches with topology: 9/9 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 13/100 COMPLETE (54.2s)
Train Loss: 6.9127 (C:5.5648, R:0.0077, T:2.4282)
Val Loss:   8.3996 (C:5.3599, R:0.0070, T:9.7542)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 537 | Topological Weight: 0.2600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.9594 (C:5.6056, R:0.0076, T:2.2835(w:0.260)ğŸš€)
Batch  25/537: Loss=6.7407 (C:5.4477, R:0.0077, T:2.0226(w:0.260)ğŸš€)
Batch  50/537: Loss=6.9009 (C:5.5126, R:0.0077, T:2.3689(w:0.260)ğŸš€)
Batch  75/537: Loss=6.8826 (C:5.5512, R:0.0076, T:2.1975(w:0.260)ğŸš€)
Batch 100/537: Loss=7.0122 (C:5.5810, R:0.0076, T:2.5682(w:0.260)ğŸš€)
Batch 125/537: Loss=6.9446 (C:5.5843, R:0.0077, T:2.2700(w:0.260)ğŸš€)
Batch 150/537: Loss=7.0337 (C:5.6444, R:0.0077, T:2.3879(w:0.260)ğŸš€)
Batch 175/537: Loss=6.8874 (C:5.5711, R:0.0076, T:2.1335(w:0.260)ğŸš€)
Batch 200/537: Loss=6.7161 (C:5.4100, R:0.0077, T:2.0773(w:0.260)ğŸš€)
Batch 225/537: Loss=6.9977 (C:5.5865, R:0.0077, T:2.4547(w:0.260)ğŸš€)
Batch 250/537: Loss=6.8728 (C:5.5532, R:0.0076, T:2.1452(w:0.260)ğŸš€)
Batch 275/537: Loss=6.8170 (C:5.4470, R:0.0077, T:2.3182(w:0.260)ğŸš€)
Batch 300/537: Loss=6.8947 (C:5.5699, R:0.0077, T:2.1522(w:0.260)ğŸš€)
Batch 325/537: Loss=6.8088 (C:5.4232, R:0.0076, T:2.3962(w:0.260)ğŸš€)
Batch 350/537: Loss=6.8928 (C:5.6426, R:0.0076, T:1.8834(w:0.260)ğŸš€)
Batch 375/537: Loss=6.8879 (C:5.4929, R:0.0076, T:2.4346(w:0.260)ğŸš€)
Batch 400/537: Loss=6.8798 (C:5.5406, R:0.0077, T:2.2057(w:0.260)ğŸš€)
Batch 425/537: Loss=6.8524 (C:5.5763, R:0.0076, T:1.9872(w:0.260)ğŸš€)
Batch 450/537: Loss=6.9545 (C:5.5750, R:0.0076, T:2.3754(w:0.260)ğŸš€)
Batch 475/537: Loss=7.2097 (C:5.7032, R:0.0076, T:2.8636(w:0.260)ğŸš€)
Batch 500/537: Loss=6.8735 (C:5.5837, R:0.0076, T:2.0469(w:0.260)ğŸš€)
Batch 525/537: Loss=6.7840 (C:5.4485, R:0.0076, T:2.2079(w:0.260)ğŸš€)

ğŸ“Š EPOCH 14 TRAINING SUMMARY:
  Total Loss: 6.9219
  Contrastive: 5.5565
  Reconstruction: 0.0076
  Topological: 2.3158 (weight: 0.260)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.5984
  Contrastive: 5.4225
  Reconstruction: 0.0070
  Topological: 9.5379 (weight: 0.260)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 14/100 COMPLETE (47.7s)
Train Loss: 6.9219 (C:5.5565, R:0.0076, T:2.3158)
Val Loss:   8.5984 (C:5.4225, R:0.0070, T:9.5379)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 537 | Topological Weight: 0.2800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.8777 (C:5.5468, R:0.0076, T:2.0440(w:0.280)ğŸš€)
Batch  25/537: Loss=6.8733 (C:5.5071, R:0.0077, T:2.1411(w:0.280)ğŸš€)
Batch  50/537: Loss=6.8936 (C:5.5776, R:0.0076, T:1.9844(w:0.280)ğŸš€)
Batch  75/537: Loss=6.9642 (C:5.5396, R:0.0076, T:2.3679(w:0.280)ğŸš€)
Batch 100/537: Loss=7.1013 (C:5.5690, R:0.0077, T:2.7332(w:0.280)ğŸš€)
Batch 125/537: Loss=7.1008 (C:5.6071, R:0.0076, T:2.6131(w:0.280)ğŸš€)
Batch 150/537: Loss=6.9284 (C:5.4878, R:0.0077, T:2.4017(w:0.280)ğŸš€)
Batch 175/537: Loss=6.9696 (C:5.5237, R:0.0077, T:2.4217(w:0.280)ğŸš€)
Batch 200/537: Loss=6.7947 (C:5.4896, R:0.0076, T:1.9596(w:0.280)ğŸš€)
Batch 225/537: Loss=7.0693 (C:5.6538, R:0.0076, T:2.3273(w:0.280)ğŸš€)
Batch 250/537: Loss=6.9501 (C:5.5797, R:0.0076, T:2.1727(w:0.280)ğŸš€)
Batch 275/537: Loss=7.0797 (C:5.6604, R:0.0076, T:2.3576(w:0.280)ğŸš€)
Batch 300/537: Loss=6.7642 (C:5.4354, R:0.0076, T:2.0380(w:0.280)ğŸš€)
Batch 325/537: Loss=6.8650 (C:5.4874, R:0.0076, T:2.1976(w:0.280)ğŸš€)
Batch 350/537: Loss=6.8501 (C:5.4830, R:0.0076, T:2.1508(w:0.280)ğŸš€)
Batch 375/537: Loss=7.0329 (C:5.5968, R:0.0077, T:2.3923(w:0.280)ğŸš€)
Batch 400/537: Loss=7.0042 (C:5.6420, R:0.0076, T:2.1567(w:0.280)ğŸš€)
Batch 425/537: Loss=7.0340 (C:5.6335, R:0.0076, T:2.2771(w:0.280)ğŸš€)
Batch 450/537: Loss=6.9545 (C:5.5412, R:0.0076, T:2.3428(w:0.280)ğŸš€)
Batch 475/537: Loss=6.9299 (C:5.6346, R:0.0076, T:1.9231(w:0.280)ğŸš€)
Batch 500/537: Loss=7.0566 (C:5.6124, R:0.0076, T:2.4359(w:0.280)ğŸš€)
Batch 525/537: Loss=6.8987 (C:5.5657, R:0.0076, T:2.0429(w:0.280)ğŸš€)

ğŸ“Š EPOCH 15 TRAINING SUMMARY:
  Total Loss: 6.9329
  Contrastive: 5.5512
  Reconstruction: 0.0076
  Topological: 2.2153 (weight: 0.280)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.6439
  Contrastive: 5.3826
  Reconstruction: 0.0069
  Topological: 9.1709 (weight: 0.280)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 15/100 COMPLETE (47.7s)
Train Loss: 6.9329 (C:5.5512, R:0.0076, T:2.2153)
Val Loss:   8.6439 (C:5.3826, R:0.0069, T:9.1709)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 16
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.141 Â± 1.064
    Neg distances: 6.995 Â± 2.237
    Separation ratio: 1.69x
    Gap: -10.520
    âš ï¸  Moderate global separation

============================================================
EPOCH 16 | Batches: 537 | Topological Weight: 0.3000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.7618 (C:5.4328, R:0.0077, T:1.8724(w:0.300)ğŸš€)
Batch  25/537: Loss=7.0712 (C:5.5823, R:0.0076, T:2.4316(w:0.300)ğŸš€)
Batch  50/537: Loss=6.8974 (C:5.5220, R:0.0076, T:2.0385(w:0.300)ğŸš€)
Batch  75/537: Loss=6.9648 (C:5.5737, R:0.0076, T:2.0906(w:0.300)ğŸš€)
Batch 100/537: Loss=6.9803 (C:5.6007, R:0.0076, T:2.0720(w:0.300)ğŸš€)
Batch 125/537: Loss=7.0690 (C:5.6141, R:0.0076, T:2.3091(w:0.300)ğŸš€)
Batch 150/537: Loss=6.8544 (C:5.4691, R:0.0076, T:2.0860(w:0.300)ğŸš€)
Batch 175/537: Loss=7.0312 (C:5.6620, R:0.0076, T:2.0340(w:0.300)ğŸš€)
Batch 200/537: Loss=6.9672 (C:5.5064, R:0.0076, T:2.3260(w:0.300)ğŸš€)
Batch 225/537: Loss=6.9520 (C:5.5593, R:0.0076, T:2.1027(w:0.300)ğŸš€)
Batch 250/537: Loss=7.0277 (C:5.5953, R:0.0075, T:2.2622(w:0.300)ğŸš€)
Batch 275/537: Loss=6.8867 (C:5.5211, R:0.0076, T:2.0048(w:0.300)ğŸš€)
Batch 300/537: Loss=6.9761 (C:5.5771, R:0.0076, T:2.1368(w:0.300)ğŸš€)
Batch 325/537: Loss=7.0170 (C:5.5930, R:0.0076, T:2.2194(w:0.300)ğŸš€)
Batch 350/537: Loss=6.9245 (C:5.5548, R:0.0076, T:2.0474(w:0.300)ğŸš€)
Batch 375/537: Loss=6.9072 (C:5.5067, R:0.0076, T:2.1349(w:0.300)ğŸš€)
Batch 400/537: Loss=7.1055 (C:5.5640, R:0.0076, T:2.6031(w:0.300)ğŸš€)
Batch 425/537: Loss=6.8495 (C:5.4925, R:0.0076, T:1.9974(w:0.300)ğŸš€)
Batch 450/537: Loss=7.0101 (C:5.5764, R:0.0076, T:2.2474(w:0.300)ğŸš€)
Batch 475/537: Loss=7.0045 (C:5.5753, R:0.0076, T:2.2218(w:0.300)ğŸš€)
Batch 500/537: Loss=6.8715 (C:5.5128, R:0.0076, T:1.9973(w:0.300)ğŸš€)
Batch 525/537: Loss=7.0597 (C:5.6761, R:0.0076, T:2.0951(w:0.300)ğŸš€)

ğŸ“Š EPOCH 16 TRAINING SUMMARY:
  Total Loss: 6.9277
  Contrastive: 5.5252
  Reconstruction: 0.0076
  Topological: 2.1389 (weight: 0.300)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.8073
  Contrastive: 5.3596
  Reconstruction: 0.0069
  Topological: 9.1782 (weight: 0.300)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 16/100 COMPLETE (51.6s)
Train Loss: 6.9277 (C:5.5252, R:0.0076, T:2.1389)
Val Loss:   8.8073 (C:5.3596, R:0.0069, T:9.1782)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 537 | Topological Weight: 0.3200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.8597 (C:5.4377, R:0.0075, T:2.0900(w:0.320)ğŸš€)
Batch  25/537: Loss=6.9026 (C:5.5119, R:0.0076, T:1.9711(w:0.320)ğŸš€)
Batch  50/537: Loss=6.8767 (C:5.4688, R:0.0076, T:2.0171(w:0.320)ğŸš€)
Batch  75/537: Loss=6.8809 (C:5.5166, R:0.0076, T:1.8976(w:0.320)ğŸš€)
Batch 100/537: Loss=6.8433 (C:5.4240, R:0.0076, T:2.0504(w:0.320)ğŸš€)
Batch 125/537: Loss=6.9529 (C:5.5358, R:0.0075, T:2.0780(w:0.320)ğŸš€)
Batch 150/537: Loss=6.8620 (C:5.4564, R:0.0076, T:2.0211(w:0.320)ğŸš€)
Batch 175/537: Loss=6.9921 (C:5.5488, R:0.0076, T:2.1344(w:0.320)ğŸš€)
Batch 200/537: Loss=7.0337 (C:5.5947, R:0.0076, T:2.1069(w:0.320)ğŸš€)
Batch 225/537: Loss=7.0342 (C:5.5621, R:0.0076, T:2.2154(w:0.320)ğŸš€)
Batch 250/537: Loss=6.8684 (C:5.4569, R:0.0076, T:2.0312(w:0.320)ğŸš€)
Batch 275/537: Loss=6.9902 (C:5.5519, R:0.0076, T:2.1168(w:0.320)ğŸš€)
Batch 300/537: Loss=6.9497 (C:5.5397, R:0.0076, T:2.0311(w:0.320)ğŸš€)
Batch 325/537: Loss=6.9771 (C:5.5484, R:0.0076, T:2.1030(w:0.320)ğŸš€)
Batch 350/537: Loss=7.1606 (C:5.6240, R:0.0077, T:2.4077(w:0.320)ğŸš€)
Batch 375/537: Loss=6.9178 (C:5.5695, R:0.0075, T:1.8594(w:0.320)ğŸš€)
Batch 400/537: Loss=6.9334 (C:5.5200, R:0.0076, T:2.0566(w:0.320)ğŸš€)
Batch 425/537: Loss=7.0984 (C:5.5447, R:0.0075, T:2.5003(w:0.320)ğŸš€)
Batch 450/537: Loss=6.8811 (C:5.5634, R:0.0076, T:1.7470(w:0.320)ğŸš€)
Batch 475/537: Loss=7.1293 (C:5.6135, R:0.0077, T:2.3324(w:0.320)ğŸš€)
Batch 500/537: Loss=7.0115 (C:5.5200, R:0.0076, T:2.2959(w:0.320)ğŸš€)
Batch 525/537: Loss=6.8542 (C:5.5155, R:0.0076, T:1.8153(w:0.320)ğŸš€)

ğŸ“Š EPOCH 17 TRAINING SUMMARY:
  Total Loss: 6.9389
  Contrastive: 5.5213
  Reconstruction: 0.0076
  Topological: 2.0540 (weight: 0.320)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 8.9382
  Contrastive: 5.4081
  Reconstruction: 0.0069
  Topological: 8.8664 (weight: 0.320)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 17/100 COMPLETE (45.1s)
Train Loss: 6.9389 (C:5.5213, R:0.0076, T:2.0540)
Val Loss:   8.9382 (C:5.4081, R:0.0069, T:8.8664)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 537 | Topological Weight: 0.3400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.9817 (C:5.5114, R:0.0076, T:2.0917(w:0.340)ğŸš€)
Batch  25/537: Loss=6.7615 (C:5.4113, R:0.0076, T:1.7275(w:0.340)ğŸš€)
Batch  50/537: Loss=6.8745 (C:5.4829, R:0.0076, T:1.8490(w:0.340)ğŸš€)
Batch  75/537: Loss=7.1113 (C:5.5250, R:0.0076, T:2.4164(w:0.340)ğŸš€)
Batch 100/537: Loss=6.8881 (C:5.4598, R:0.0076, T:1.9545(w:0.340)ğŸš€)
Batch 125/537: Loss=6.9482 (C:5.5634, R:0.0076, T:1.8349(w:0.340)ğŸš€)
Batch 150/537: Loss=6.9764 (C:5.5541, R:0.0076, T:1.9499(w:0.340)ğŸš€)
Batch 175/537: Loss=6.9450 (C:5.4640, R:0.0077, T:2.1031(w:0.340)ğŸš€)
Batch 200/537: Loss=6.9804 (C:5.4877, R:0.0076, T:2.1604(w:0.340)ğŸš€)
Batch 225/537: Loss=6.8572 (C:5.4686, R:0.0076, T:1.8508(w:0.340)ğŸš€)
Batch 250/537: Loss=6.9636 (C:5.5676, R:0.0076, T:1.8604(w:0.340)ğŸš€)
Batch 275/537: Loss=6.9800 (C:5.5499, R:0.0076, T:1.9686(w:0.340)ğŸš€)
Batch 300/537: Loss=6.8786 (C:5.4847, R:0.0076, T:1.8525(w:0.340)ğŸš€)
Batch 325/537: Loss=6.9643 (C:5.5672, R:0.0076, T:1.8829(w:0.340)ğŸš€)
Batch 350/537: Loss=7.1089 (C:5.5841, R:0.0076, T:2.2350(w:0.340)ğŸš€)
Batch 375/537: Loss=6.9739 (C:5.5498, R:0.0076, T:1.9641(w:0.340)ğŸš€)
Batch 400/537: Loss=6.9334 (C:5.5004, R:0.0076, T:1.9799(w:0.340)ğŸš€)
Batch 425/537: Loss=6.9741 (C:5.5743, R:0.0076, T:1.8835(w:0.340)ğŸš€)
Batch 450/537: Loss=6.9669 (C:5.5145, R:0.0076, T:2.0252(w:0.340)ğŸš€)
Batch 475/537: Loss=6.9115 (C:5.4857, R:0.0076, T:1.9685(w:0.340)ğŸš€)
Batch 500/537: Loss=6.9308 (C:5.4941, R:0.0076, T:1.9876(w:0.340)ğŸš€)
Batch 525/537: Loss=6.9480 (C:5.4815, R:0.0076, T:2.0717(w:0.340)ğŸš€)

ğŸ“Š EPOCH 18 TRAINING SUMMARY:
  Total Loss: 6.9505
  Contrastive: 5.5153
  Reconstruction: 0.0076
  Topological: 1.9849 (weight: 0.340)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.0683
  Contrastive: 5.4041
  Reconstruction: 0.0069
  Topological: 8.7437 (weight: 0.340)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 18/100 COMPLETE (45.0s)
Train Loss: 6.9505 (C:5.5153, R:0.0076, T:1.9849)
Val Loss:   9.0683 (C:5.4041, R:0.0069, T:8.7437)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 19
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.181 Â± 0.986
    Neg distances: 6.912 Â± 2.140
    Separation ratio: 1.65x
    Gap: -10.264
    âš ï¸  Moderate global separation

============================================================
EPOCH 19 | Batches: 537 | Topological Weight: 0.3600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.9320 (C:5.5591, R:0.0075, T:1.7200(w:0.360)ğŸš€)
Batch  25/537: Loss=6.8981 (C:5.4435, R:0.0077, T:1.9099(w:0.360)ğŸš€)
Batch  50/537: Loss=7.0014 (C:5.5451, R:0.0077, T:1.9122(w:0.360)ğŸš€)
Batch  75/537: Loss=7.0547 (C:5.5384, R:0.0076, T:2.1022(w:0.360)ğŸš€)
Batch 100/537: Loss=7.1665 (C:5.6533, R:0.0076, T:2.0958(w:0.360)ğŸš€)
Batch 125/537: Loss=6.9251 (C:5.4260, R:0.0076, T:2.0489(w:0.360)ğŸš€)
Batch 150/537: Loss=6.9943 (C:5.6023, R:0.0076, T:1.7526(w:0.360)ğŸš€)
Batch 175/537: Loss=7.1326 (C:5.6213, R:0.0076, T:2.0749(w:0.360)ğŸš€)
Batch 200/537: Loss=6.8760 (C:5.5109, R:0.0076, T:1.6864(w:0.360)ğŸš€)
Batch 225/537: Loss=7.0169 (C:5.5772, R:0.0076, T:1.8754(w:0.360)ğŸš€)
Batch 250/537: Loss=6.9543 (C:5.5367, R:0.0076, T:1.8395(w:0.360)ğŸš€)
Batch 275/537: Loss=7.1083 (C:5.6593, R:0.0076, T:1.9247(w:0.360)ğŸš€)
Batch 300/537: Loss=7.1940 (C:5.6276, R:0.0076, T:2.2423(w:0.360)ğŸš€)
Batch 325/537: Loss=6.9761 (C:5.5764, R:0.0076, T:1.7737(w:0.360)ğŸš€)
Batch 350/537: Loss=7.0166 (C:5.5937, R:0.0076, T:1.8474(w:0.360)ğŸš€)
Batch 375/537: Loss=6.9935 (C:5.5478, R:0.0075, T:1.9195(w:0.360)ğŸš€)
Batch 400/537: Loss=7.0883 (C:5.6960, R:0.0076, T:1.7645(w:0.360)ğŸš€)
Batch 425/537: Loss=7.1074 (C:5.6297, R:0.0076, T:1.9835(w:0.360)ğŸš€)
Batch 450/537: Loss=7.0234 (C:5.5910, R:0.0076, T:1.8671(w:0.360)ğŸš€)
Batch 475/537: Loss=7.0658 (C:5.5418, R:0.0076, T:2.1219(w:0.360)ğŸš€)
Batch 500/537: Loss=7.1233 (C:5.6420, R:0.0076, T:2.0129(w:0.360)ğŸš€)
Batch 525/537: Loss=7.0118 (C:5.5895, R:0.0076, T:1.8430(w:0.360)ğŸš€)

ğŸ“Š EPOCH 19 TRAINING SUMMARY:
  Total Loss: 7.0235
  Contrastive: 5.5700
  Reconstruction: 0.0076
  Topological: 1.9281 (weight: 0.360)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.3023
  Contrastive: 5.4627
  Reconstruction: 0.0069
  Topological: 8.7435 (weight: 0.360)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 19/100 COMPLETE (52.1s)
Train Loss: 7.0235 (C:5.5700, R:0.0076, T:1.9281)
Val Loss:   9.3023 (C:5.4627, R:0.0069, T:8.7435)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 537 | Topological Weight: 0.3800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0279 (C:5.6042, R:0.0076, T:1.7576(w:0.380)ğŸš€)
Batch  25/537: Loss=6.8878 (C:5.5058, R:0.0076, T:1.6368(w:0.380)ğŸš€)
Batch  50/537: Loss=7.0060 (C:5.5065, R:0.0076, T:1.9529(w:0.380)ğŸš€)
Batch  75/537: Loss=6.9778 (C:5.5399, R:0.0076, T:1.7890(w:0.380)ğŸš€)
Batch 100/537: Loss=7.0778 (C:5.6353, R:0.0076, T:1.7997(w:0.380)ğŸš€)
Batch 125/537: Loss=6.9464 (C:5.5530, R:0.0076, T:1.6783(w:0.380)ğŸš€)
Batch 150/537: Loss=7.1263 (C:5.6821, R:0.0075, T:1.8319(w:0.380)ğŸš€)
Batch 175/537: Loss=6.9679 (C:5.5620, R:0.0076, T:1.7001(w:0.380)ğŸš€)
Batch 200/537: Loss=7.0516 (C:5.5482, R:0.0076, T:1.9556(w:0.380)ğŸš€)
Batch 225/537: Loss=7.0118 (C:5.5452, R:0.0076, T:1.8539(w:0.380)ğŸš€)
Batch 250/537: Loss=7.0226 (C:5.5477, R:0.0076, T:1.8808(w:0.380)ğŸš€)
Batch 275/537: Loss=7.1274 (C:5.6074, R:0.0076, T:1.9967(w:0.380)ğŸš€)
Batch 300/537: Loss=7.0254 (C:5.5953, R:0.0075, T:1.7841(w:0.380)ğŸš€)
Batch 325/537: Loss=7.0298 (C:5.5776, R:0.0076, T:1.8295(w:0.380)ğŸš€)
Batch 350/537: Loss=7.0665 (C:5.5789, R:0.0076, T:1.9222(w:0.380)ğŸš€)
Batch 375/537: Loss=6.9824 (C:5.5680, R:0.0076, T:1.7255(w:0.380)ğŸš€)
Batch 400/537: Loss=7.0786 (C:5.6252, R:0.0075, T:1.8400(w:0.380)ğŸš€)
Batch 425/537: Loss=7.0395 (C:5.5553, R:0.0076, T:1.9089(w:0.380)ğŸš€)
Batch 450/537: Loss=7.1468 (C:5.6374, R:0.0076, T:1.9835(w:0.380)ğŸš€)
Batch 475/537: Loss=7.1059 (C:5.6400, R:0.0076, T:1.8553(w:0.380)ğŸš€)
Batch 500/537: Loss=7.0339 (C:5.5594, R:0.0075, T:1.8971(w:0.380)ğŸš€)
Batch 525/537: Loss=7.0453 (C:5.5421, R:0.0076, T:1.9480(w:0.380)ğŸš€)

ğŸ“Š EPOCH 20 TRAINING SUMMARY:
  Total Loss: 7.0327
  Contrastive: 5.5641
  Reconstruction: 0.0076
  Topological: 1.8695 (weight: 0.380)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.4908
  Contrastive: 5.4623
  Reconstruction: 0.0069
  Topological: 8.7847 (weight: 0.380)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 20/100 COMPLETE (46.1s)
Train Loss: 7.0327 (C:5.5641, R:0.0076, T:1.8695)
Val Loss:   9.4908 (C:5.4623, R:0.0069, T:8.7847)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 537 | Topological Weight: 0.4000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0159 (C:5.5037, R:0.0076, T:1.8835(w:0.400)ğŸš€)
Batch  25/537: Loss=6.9732 (C:5.5113, R:0.0076, T:1.7527(w:0.400)ğŸš€)
Batch  50/537: Loss=6.9644 (C:5.4321, R:0.0076, T:1.9315(w:0.400)ğŸš€)
Batch  75/537: Loss=7.0438 (C:5.6404, R:0.0076, T:1.6143(w:0.400)ğŸš€)
Batch 100/537: Loss=7.0002 (C:5.6094, R:0.0075, T:1.5934(w:0.400)ğŸš€)
Batch 125/537: Loss=7.1067 (C:5.5675, R:0.0076, T:1.9558(w:0.400)ğŸš€)
Batch 150/537: Loss=6.9901 (C:5.5568, R:0.0076, T:1.6908(w:0.400)ğŸš€)
Batch 175/537: Loss=7.0503 (C:5.5505, R:0.0075, T:1.8673(w:0.400)ğŸš€)
Batch 200/537: Loss=7.0610 (C:5.6548, R:0.0075, T:1.6290(w:0.400)ğŸš€)
Batch 225/537: Loss=7.0983 (C:5.5854, R:0.0075, T:1.9029(w:0.400)ğŸš€)
Batch 250/537: Loss=7.1354 (C:5.5718, R:0.0076, T:2.0070(w:0.400)ğŸš€)
Batch 275/537: Loss=7.0350 (C:5.5686, R:0.0076, T:1.7730(w:0.400)ğŸš€)
Batch 300/537: Loss=7.0489 (C:5.5801, R:0.0076, T:1.7689(w:0.400)ğŸš€)
Batch 325/537: Loss=7.1357 (C:5.6184, R:0.0076, T:1.9051(w:0.400)ğŸš€)
Batch 350/537: Loss=7.1023 (C:5.5874, R:0.0077, T:1.8682(w:0.400)ğŸš€)
Batch 375/537: Loss=7.0621 (C:5.6063, R:0.0075, T:1.7601(w:0.400)ğŸš€)
Batch 400/537: Loss=7.0209 (C:5.6085, R:0.0075, T:1.6468(w:0.400)ğŸš€)
Batch 425/537: Loss=7.1329 (C:5.5745, R:0.0075, T:2.0108(w:0.400)ğŸš€)
Batch 450/537: Loss=7.1609 (C:5.6533, R:0.0076, T:1.8794(w:0.400)ğŸš€)
Batch 475/537: Loss=7.0446 (C:5.5999, R:0.0075, T:1.7328(w:0.400)ğŸš€)
Batch 500/537: Loss=7.0210 (C:5.4691, R:0.0076, T:1.9812(w:0.400)ğŸš€)
Batch 525/537: Loss=6.9775 (C:5.5766, R:0.0075, T:1.6153(w:0.400)ğŸš€)

ğŸ“Š EPOCH 21 TRAINING SUMMARY:
  Total Loss: 7.0481
  Contrastive: 5.5621
  Reconstruction: 0.0076
  Topological: 1.8213 (weight: 0.400)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.5698
  Contrastive: 5.5338
  Reconstruction: 0.0069
  Topological: 8.3680 (weight: 0.400)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 21/100 COMPLETE (47.4s)
Train Loss: 7.0481 (C:5.5621, R:0.0076, T:1.8213)
Val Loss:   9.5698 (C:5.5338, R:0.0069, T:8.3680)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 22
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.176 Â± 0.922
    Neg distances: 6.902 Â± 2.077
    Separation ratio: 1.65x
    Gap: -10.013
    âš ï¸  Moderate global separation

============================================================
EPOCH 22 | Batches: 537 | Topological Weight: 0.4200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1303 (C:5.6676, R:0.0076, T:1.6825(w:0.420)ğŸš€)
Batch  25/537: Loss=6.8898 (C:5.4919, R:0.0076, T:1.5250(w:0.420)ğŸš€)
Batch  50/537: Loss=7.1018 (C:5.5355, R:0.0076, T:1.9295(w:0.420)ğŸš€)
Batch  75/537: Loss=7.0627 (C:5.5680, R:0.0075, T:1.7689(w:0.420)ğŸš€)
Batch 100/537: Loss=6.9617 (C:5.5330, R:0.0076, T:1.5955(w:0.420)ğŸš€)
Batch 125/537: Loss=6.9967 (C:5.4814, R:0.0076, T:1.8019(w:0.420)ğŸš€)
Batch 150/537: Loss=7.0226 (C:5.5522, R:0.0075, T:1.7037(w:0.420)ğŸš€)
Batch 175/537: Loss=7.0178 (C:5.5408, R:0.0076, T:1.7099(w:0.420)ğŸš€)
Batch 200/537: Loss=6.9261 (C:5.4978, R:0.0075, T:1.6097(w:0.420)ğŸš€)
Batch 225/537: Loss=6.9825 (C:5.5214, R:0.0076, T:1.6754(w:0.420)ğŸš€)
Batch 250/537: Loss=7.0575 (C:5.5026, R:0.0076, T:1.8957(w:0.420)ğŸš€)
Batch 275/537: Loss=6.9841 (C:5.5167, R:0.0075, T:1.7072(w:0.420)ğŸš€)
Batch 300/537: Loss=7.0009 (C:5.5483, R:0.0076, T:1.6517(w:0.420)ğŸš€)
Batch 325/537: Loss=7.0372 (C:5.5247, R:0.0075, T:1.8057(w:0.420)ğŸš€)
Batch 350/537: Loss=7.0356 (C:5.5557, R:0.0076, T:1.7228(w:0.420)ğŸš€)
Batch 375/537: Loss=7.1179 (C:5.5500, R:0.0076, T:1.9275(w:0.420)ğŸš€)
Batch 400/537: Loss=6.9843 (C:5.5733, R:0.0075, T:1.5634(w:0.420)ğŸš€)
Batch 425/537: Loss=6.9734 (C:5.5217, R:0.0075, T:1.6602(w:0.420)ğŸš€)
Batch 450/537: Loss=7.0762 (C:5.5754, R:0.0076, T:1.7754(w:0.420)ğŸš€)
Batch 475/537: Loss=7.1212 (C:5.6102, R:0.0076, T:1.7820(w:0.420)ğŸš€)
Batch 500/537: Loss=7.1405 (C:5.6411, R:0.0075, T:1.7768(w:0.420)ğŸš€)
Batch 525/537: Loss=7.0924 (C:5.5947, R:0.0076, T:1.7496(w:0.420)ğŸš€)

ğŸ“Š EPOCH 22 TRAINING SUMMARY:
  Total Loss: 7.0568
  Contrastive: 5.5524
  Reconstruction: 0.0076
  Topological: 1.7793 (weight: 0.420)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.6091
  Contrastive: 5.4797
  Reconstruction: 0.0069
  Topological: 8.1942 (weight: 0.420)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 22/100 COMPLETE (54.3s)
Train Loss: 7.0568 (C:5.5524, R:0.0076, T:1.7793)
Val Loss:   9.6091 (C:5.4797, R:0.0069, T:8.1942)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 23 | Batches: 537 | Topological Weight: 0.4400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0031 (C:5.5683, R:0.0076, T:1.5395(w:0.440)ğŸš€)
Batch  25/537: Loss=6.9101 (C:5.4339, R:0.0076, T:1.6247(w:0.440)ğŸš€)
Batch  50/537: Loss=6.9101 (C:5.4784, R:0.0076, T:1.5299(w:0.440)ğŸš€)
Batch  75/537: Loss=6.8431 (C:5.4498, R:0.0075, T:1.4578(w:0.440)ğŸš€)
Batch 100/537: Loss=6.8587 (C:5.4919, R:0.0075, T:1.3941(w:0.440)ğŸš€)
Batch 125/537: Loss=6.9963 (C:5.5948, R:0.0075, T:1.4700(w:0.440)ğŸš€)
Batch 150/537: Loss=6.9977 (C:5.4960, R:0.0076, T:1.6940(w:0.440)ğŸš€)
Batch 175/537: Loss=7.0364 (C:5.5569, R:0.0076, T:1.6387(w:0.440)ğŸš€)
Batch 200/537: Loss=6.9841 (C:5.4466, R:0.0076, T:1.7761(w:0.440)ğŸš€)
Batch 225/537: Loss=7.1962 (C:5.6286, R:0.0076, T:1.8286(w:0.440)ğŸš€)
Batch 250/537: Loss=7.0314 (C:5.5748, R:0.0076, T:1.5934(w:0.440)ğŸš€)
Batch 275/537: Loss=7.1720 (C:5.6803, R:0.0076, T:1.6627(w:0.440)ğŸš€)
Batch 300/537: Loss=7.1593 (C:5.5784, R:0.0076, T:1.8670(w:0.440)ğŸš€)
Batch 325/537: Loss=7.1685 (C:5.6653, R:0.0076, T:1.6955(w:0.440)ğŸš€)
Batch 350/537: Loss=7.1517 (C:5.5649, R:0.0076, T:1.8837(w:0.440)ğŸš€)
Batch 375/537: Loss=7.0851 (C:5.4903, R:0.0076, T:1.9071(w:0.440)ğŸš€)
Batch 400/537: Loss=7.0475 (C:5.6093, R:0.0076, T:1.5457(w:0.440)ğŸš€)
Batch 425/537: Loss=7.1672 (C:5.6213, R:0.0076, T:1.7934(w:0.440)ğŸš€)
Batch 450/537: Loss=6.9206 (C:5.4599, R:0.0075, T:1.6146(w:0.440)ğŸš€)
Batch 475/537: Loss=7.1850 (C:5.6354, R:0.0076, T:1.8038(w:0.440)ğŸš€)
Batch 500/537: Loss=6.9616 (C:5.5300, R:0.0075, T:1.5470(w:0.440)ğŸš€)
Batch 525/537: Loss=7.0455 (C:5.5637, R:0.0076, T:1.6501(w:0.440)ğŸš€)
ğŸ“ˆ New best topological loss: 1.7386

ğŸ“Š EPOCH 23 TRAINING SUMMARY:
  Total Loss: 7.0742
  Contrastive: 5.5519
  Reconstruction: 0.0076
  Topological: 1.7386 (weight: 0.440)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.8060
  Contrastive: 5.5117
  Reconstruction: 0.0069
  Topological: 8.1953 (weight: 0.440)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 23/100 COMPLETE (47.6s)
Train Loss: 7.0742 (C:5.5519, R:0.0076, T:1.7386)
Val Loss:   9.8060 (C:5.5117, R:0.0069, T:8.1953)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 24 | Batches: 537 | Topological Weight: 0.4600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0174 (C:5.5280, R:0.0075, T:1.6025(w:0.460)ğŸš€)
Batch  25/537: Loss=7.1606 (C:5.6253, R:0.0076, T:1.6961(w:0.460)ğŸš€)
Batch  50/537: Loss=7.0589 (C:5.5495, R:0.0076, T:1.6317(w:0.460)ğŸš€)
Batch  75/537: Loss=7.0944 (C:5.5551, R:0.0076, T:1.7038(w:0.460)ğŸš€)
Batch 100/537: Loss=7.0927 (C:5.5909, R:0.0076, T:1.6107(w:0.460)ğŸš€)
Batch 125/537: Loss=7.0150 (C:5.5243, R:0.0076, T:1.5991(w:0.460)ğŸš€)
Batch 150/537: Loss=7.1003 (C:5.4913, R:0.0075, T:1.8583(w:0.460)ğŸš€)
Batch 175/537: Loss=7.0714 (C:5.5653, R:0.0075, T:1.6338(w:0.460)ğŸš€)
Batch 200/537: Loss=7.1473 (C:5.5558, R:0.0075, T:1.8193(w:0.460)ğŸš€)
Batch 225/537: Loss=7.0995 (C:5.5927, R:0.0076, T:1.6283(w:0.460)ğŸš€)
Batch 250/537: Loss=7.1133 (C:5.6318, R:0.0075, T:1.5806(w:0.460)ğŸš€)
Batch 275/537: Loss=7.1657 (C:5.5240, R:0.0076, T:1.9225(w:0.460)ğŸš€)
Batch 300/537: Loss=6.9626 (C:5.4931, R:0.0076, T:1.5483(w:0.460)ğŸš€)
Batch 325/537: Loss=7.1262 (C:5.5968, R:0.0076, T:1.6811(w:0.460)ğŸš€)
Batch 350/537: Loss=7.0588 (C:5.5362, R:0.0076, T:1.6686(w:0.460)ğŸš€)
Batch 375/537: Loss=7.1279 (C:5.6922, R:0.0076, T:1.4791(w:0.460)ğŸš€)
Batch 400/537: Loss=7.0684 (C:5.5448, R:0.0075, T:1.6753(w:0.460)ğŸš€)
Batch 425/537: Loss=7.0667 (C:5.5913, R:0.0075, T:1.5661(w:0.460)ğŸš€)
Batch 450/537: Loss=7.1108 (C:5.6103, R:0.0075, T:1.6284(w:0.460)ğŸš€)
Batch 475/537: Loss=7.0700 (C:5.5546, R:0.0075, T:1.6633(w:0.460)ğŸš€)
Batch 500/537: Loss=7.0012 (C:5.5152, R:0.0076, T:1.5789(w:0.460)ğŸš€)
Batch 525/537: Loss=7.2551 (C:5.6088, R:0.0077, T:1.9092(w:0.460)ğŸš€)
ğŸ“ˆ New best topological loss: 1.6969

ğŸ“Š EPOCH 24 TRAINING SUMMARY:
  Total Loss: 7.0903
  Contrastive: 5.5525
  Reconstruction: 0.0076
  Topological: 1.6969 (weight: 0.460)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.0369
  Contrastive: 5.4973
  Reconstruction: 0.0069
  Topological: 8.3703 (weight: 0.460)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 24/100 COMPLETE (47.2s)
Train Loss: 7.0903 (C:5.5525, R:0.0076, T:1.6969)
Val Loss:   10.0369 (C:5.4973, R:0.0069, T:8.3703)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 25
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.166 Â± 0.864
    Neg distances: 6.908 Â± 2.154
    Separation ratio: 1.66x
    Gap: -9.889
    âš ï¸  Moderate global separation

============================================================
EPOCH 25 | Batches: 537 | Topological Weight: 0.4800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1481 (C:5.5751, R:0.0076, T:1.6970(w:0.480)ğŸš€)
Batch  25/537: Loss=6.9827 (C:5.4800, R:0.0076, T:1.5556(w:0.480)ğŸš€)
Batch  50/537: Loss=7.0856 (C:5.5082, R:0.0075, T:1.7188(w:0.480)ğŸš€)
Batch  75/537: Loss=7.2761 (C:5.5987, R:0.0076, T:1.9114(w:0.480)ğŸš€)
Batch 100/537: Loss=7.0732 (C:5.5159, R:0.0075, T:1.6726(w:0.480)ğŸš€)
Batch 125/537: Loss=7.0445 (C:5.5510, R:0.0075, T:1.5465(w:0.480)ğŸš€)
Batch 150/537: Loss=6.9092 (C:5.4118, R:0.0076, T:1.5354(w:0.480)ğŸš€)
Batch 175/537: Loss=7.1202 (C:5.5361, R:0.0076, T:1.7200(w:0.480)ğŸš€)
Batch 200/537: Loss=7.0693 (C:5.5075, R:0.0076, T:1.6721(w:0.480)ğŸš€)
Batch 225/537: Loss=7.0963 (C:5.5413, R:0.0076, T:1.6588(w:0.480)ğŸš€)
Batch 250/537: Loss=6.9943 (C:5.4966, R:0.0076, T:1.5357(w:0.480)ğŸš€)
Batch 275/537: Loss=7.1283 (C:5.5406, R:0.0075, T:1.7407(w:0.480)ğŸš€)
Batch 300/537: Loss=7.1037 (C:5.5226, R:0.0076, T:1.7113(w:0.480)ğŸš€)
Batch 325/537: Loss=7.2347 (C:5.6023, R:0.0076, T:1.8211(w:0.480)ğŸš€)
Batch 350/537: Loss=7.2248 (C:5.6419, R:0.0076, T:1.7193(w:0.480)ğŸš€)
Batch 375/537: Loss=7.1606 (C:5.5877, R:0.0076, T:1.7026(w:0.480)ğŸš€)
Batch 400/537: Loss=7.1523 (C:5.5062, R:0.0076, T:1.8465(w:0.480)ğŸš€)
Batch 425/537: Loss=7.0679 (C:5.5282, R:0.0076, T:1.6236(w:0.480)ğŸš€)
Batch 450/537: Loss=7.2390 (C:5.5604, R:0.0076, T:1.9184(w:0.480)ğŸš€)
Batch 475/537: Loss=6.9767 (C:5.4611, R:0.0075, T:1.5863(w:0.480)ğŸš€)
Batch 500/537: Loss=7.1422 (C:5.6000, R:0.0075, T:1.6450(w:0.480)ğŸš€)
Batch 525/537: Loss=7.0914 (C:5.5784, R:0.0076, T:1.5787(w:0.480)ğŸš€)
ğŸ“ˆ New best topological loss: 1.6656

ğŸ“Š EPOCH 25 TRAINING SUMMARY:
  Total Loss: 7.0979
  Contrastive: 5.5413
  Reconstruction: 0.0076
  Topological: 1.6656 (weight: 0.480)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 9.9868
  Contrastive: 5.5439
  Reconstruction: 0.0069
  Topological: 7.8261 (weight: 0.480)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 25/100 COMPLETE (53.9s)
Train Loss: 7.0979 (C:5.5413, R:0.0076, T:1.6656)
Val Loss:   9.9868 (C:5.5439, R:0.0069, T:7.8261)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 26 | Batches: 537 | Topological Weight: 0.5000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0997 (C:5.6270, R:0.0075, T:1.4362(w:0.500)ğŸš€)
Batch  25/537: Loss=7.0724 (C:5.5289, R:0.0076, T:1.5676(w:0.500)ğŸš€)
Batch  50/537: Loss=7.0225 (C:5.4813, R:0.0076, T:1.5540(w:0.500)ğŸš€)
Batch  75/537: Loss=7.1098 (C:5.4571, R:0.0076, T:1.7777(w:0.500)ğŸš€)
Batch 100/537: Loss=7.1562 (C:5.5646, R:0.0076, T:1.6673(w:0.500)ğŸš€)
Batch 125/537: Loss=7.0458 (C:5.4768, R:0.0076, T:1.6173(w:0.500)ğŸš€)
Batch 150/537: Loss=7.1210 (C:5.5015, R:0.0076, T:1.7243(w:0.500)ğŸš€)
Batch 175/537: Loss=7.0986 (C:5.5587, R:0.0076, T:1.5689(w:0.500)ğŸš€)
Batch 200/537: Loss=7.1406 (C:5.6375, R:0.0076, T:1.4922(w:0.500)ğŸš€)
Batch 225/537: Loss=7.0166 (C:5.4540, R:0.0076, T:1.6093(w:0.500)ğŸš€)
Batch 250/537: Loss=7.1803 (C:5.5903, R:0.0076, T:1.6657(w:0.500)ğŸš€)
Batch 275/537: Loss=7.1259 (C:5.5195, R:0.0075, T:1.7122(w:0.500)ğŸš€)
Batch 300/537: Loss=7.2912 (C:5.6479, R:0.0076, T:1.7583(w:0.500)ğŸš€)
Batch 325/537: Loss=7.1661 (C:5.5959, R:0.0075, T:1.6412(w:0.500)ğŸš€)
Batch 350/537: Loss=6.9899 (C:5.4437, R:0.0076, T:1.5748(w:0.500)ğŸš€)
Batch 375/537: Loss=7.0917 (C:5.5052, R:0.0075, T:1.6654(w:0.500)ğŸš€)
Batch 400/537: Loss=7.1638 (C:5.6184, R:0.0076, T:1.5727(w:0.500)ğŸš€)
Batch 425/537: Loss=7.2535 (C:5.6181, R:0.0076, T:1.7473(w:0.500)ğŸš€)
Batch 450/537: Loss=7.0899 (C:5.5423, R:0.0075, T:1.5867(w:0.500)ğŸš€)
Batch 475/537: Loss=7.1658 (C:5.6359, R:0.0075, T:1.5517(w:0.500)ğŸš€)
Batch 500/537: Loss=7.2302 (C:5.5381, R:0.0076, T:1.8651(w:0.500)ğŸš€)
Batch 525/537: Loss=7.1614 (C:5.5350, R:0.0076, T:1.7282(w:0.500)ğŸš€)
ğŸ“ˆ New best topological loss: 1.6267

ğŸ“Š EPOCH 26 TRAINING SUMMARY:
  Total Loss: 7.1066
  Contrastive: 5.5364
  Reconstruction: 0.0076
  Topological: 1.6267 (weight: 0.500)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.2890
  Contrastive: 5.4952
  Reconstruction: 0.0069
  Topological: 8.2114 (weight: 0.500)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 26/100 COMPLETE (45.3s)
Train Loss: 7.1066 (C:5.5364, R:0.0076, T:1.6267)
Val Loss:   10.2890 (C:5.4952, R:0.0069, T:8.2114)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 27 | Batches: 537 | Topological Weight: 0.5200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1111 (C:5.5400, R:0.0076, T:1.5582(w:0.520)ğŸš€)
Batch  25/537: Loss=7.0386 (C:5.5118, R:0.0076, T:1.4784(w:0.520)ğŸš€)
Batch  50/537: Loss=6.9244 (C:5.3997, R:0.0076, T:1.4745(w:0.520)ğŸš€)
Batch  75/537: Loss=7.1060 (C:5.5037, R:0.0077, T:1.6075(w:0.520)ğŸš€)
Batch 100/537: Loss=7.0242 (C:5.5181, R:0.0075, T:1.4462(w:0.520)ğŸš€)
Batch 125/537: Loss=7.0661 (C:5.4540, R:0.0076, T:1.6450(w:0.520)ğŸš€)
Batch 150/537: Loss=7.0805 (C:5.5393, R:0.0076, T:1.5018(w:0.520)ğŸš€)
Batch 175/537: Loss=6.9930 (C:5.5194, R:0.0077, T:1.3620(w:0.520)ğŸš€)
Batch 200/537: Loss=7.0469 (C:5.5056, R:0.0075, T:1.5170(w:0.520)ğŸš€)
Batch 225/537: Loss=7.2105 (C:5.5488, R:0.0075, T:1.7451(w:0.520)ğŸš€)
Batch 250/537: Loss=7.0328 (C:5.5023, R:0.0076, T:1.4889(w:0.520)ğŸš€)
Batch 275/537: Loss=7.1163 (C:5.5631, R:0.0076, T:1.5286(w:0.520)ğŸš€)
Batch 300/537: Loss=7.1555 (C:5.4850, R:0.0076, T:1.7560(w:0.520)ğŸš€)
Batch 325/537: Loss=7.2095 (C:5.5154, R:0.0076, T:1.8038(w:0.520)ğŸš€)
Batch 350/537: Loss=7.3377 (C:5.6635, R:0.0075, T:1.7702(w:0.520)ğŸš€)
Batch 375/537: Loss=6.9817 (C:5.4539, R:0.0076, T:1.4841(w:0.520)ğŸš€)
Batch 400/537: Loss=7.2229 (C:5.5703, R:0.0075, T:1.7264(w:0.520)ğŸš€)
Batch 425/537: Loss=7.0586 (C:5.5664, R:0.0076, T:1.4164(w:0.520)ğŸš€)
Batch 450/537: Loss=7.2055 (C:5.6050, R:0.0076, T:1.6173(w:0.520)ğŸš€)
Batch 475/537: Loss=7.1733 (C:5.5609, R:0.0076, T:1.6401(w:0.520)ğŸš€)
Batch 500/537: Loss=7.0125 (C:5.5358, R:0.0076, T:1.3862(w:0.520)ğŸš€)
Batch 525/537: Loss=7.0535 (C:5.5226, R:0.0076, T:1.4855(w:0.520)ğŸš€)
ğŸ“ˆ New best topological loss: 1.5926

ğŸ“Š EPOCH 27 TRAINING SUMMARY:
  Total Loss: 7.1194
  Contrastive: 5.5346
  Reconstruction: 0.0076
  Topological: 1.5926 (weight: 0.520)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.3107
  Contrastive: 5.5085
  Reconstruction: 0.0069
  Topological: 7.9110 (weight: 0.520)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 27/100 COMPLETE (45.5s)
Train Loss: 7.1194 (C:5.5346, R:0.0076, T:1.5926)
Val Loss:   10.3107 (C:5.5085, R:0.0069, T:7.9110)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 28
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.149 Â± 0.771
    Neg distances: 7.021 Â± 2.192
    Separation ratio: 1.69x
    Gap: -10.492
    âš ï¸  Moderate global separation

============================================================
EPOCH 28 | Batches: 537 | Topological Weight: 0.5400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=6.9190 (C:5.4308, R:0.0076, T:1.3552(w:0.540)ğŸš€)
Batch  25/537: Loss=7.0415 (C:5.4257, R:0.0076, T:1.5821(w:0.540)ğŸš€)
Batch  50/537: Loss=7.0761 (C:5.4714, R:0.0076, T:1.5626(w:0.540)ğŸš€)
Batch  75/537: Loss=6.9414 (C:5.3405, R:0.0076, T:1.5644(w:0.540)ğŸš€)
Batch 100/537: Loss=7.0721 (C:5.5169, R:0.0075, T:1.4828(w:0.540)ğŸš€)
Batch 125/537: Loss=7.0393 (C:5.3899, R:0.0076, T:1.6476(w:0.540)ğŸš€)
Batch 150/537: Loss=7.1852 (C:5.4715, R:0.0076, T:1.7722(w:0.540)ğŸš€)
Batch 175/537: Loss=6.9292 (C:5.4350, R:0.0076, T:1.3664(w:0.540)ğŸš€)
Batch 200/537: Loss=6.9142 (C:5.4126, R:0.0075, T:1.3872(w:0.540)ğŸš€)
Batch 225/537: Loss=6.9762 (C:5.4227, R:0.0077, T:1.4579(w:0.540)ğŸš€)
Batch 250/537: Loss=7.0496 (C:5.4298, R:0.0076, T:1.6012(w:0.540)ğŸš€)
Batch 275/537: Loss=6.9324 (C:5.4547, R:0.0076, T:1.3366(w:0.540)ğŸš€)
Batch 300/537: Loss=7.0301 (C:5.5263, R:0.0075, T:1.3924(w:0.540)ğŸš€)
Batch 325/537: Loss=7.1002 (C:5.4133, R:0.0076, T:1.7203(w:0.540)ğŸš€)
Batch 350/537: Loss=7.0170 (C:5.4314, R:0.0076, T:1.5359(w:0.540)ğŸš€)
Batch 375/537: Loss=7.2963 (C:5.5674, R:0.0076, T:1.7969(w:0.540)ğŸš€)
Batch 400/537: Loss=7.1012 (C:5.5029, R:0.0076, T:1.5463(w:0.540)ğŸš€)
Batch 425/537: Loss=7.0628 (C:5.4890, R:0.0076, T:1.5085(w:0.540)ğŸš€)
Batch 450/537: Loss=7.0897 (C:5.4285, R:0.0076, T:1.6693(w:0.540)ğŸš€)
Batch 475/537: Loss=7.0028 (C:5.4935, R:0.0075, T:1.4056(w:0.540)ğŸš€)
Batch 500/537: Loss=6.9915 (C:5.4828, R:0.0076, T:1.3926(w:0.540)ğŸš€)
Batch 525/537: Loss=7.1260 (C:5.4420, R:0.0075, T:1.7231(w:0.540)ğŸš€)
ğŸ“ˆ New best topological loss: 1.5685

ğŸ“Š EPOCH 28 TRAINING SUMMARY:
  Total Loss: 7.0635
  Contrastive: 5.4597
  Reconstruction: 0.0076
  Topological: 1.5685 (weight: 0.540)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.2853
  Contrastive: 5.4446
  Reconstruction: 0.0069
  Topological: 7.6917 (weight: 0.540)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 28/100 COMPLETE (54.5s)
Train Loss: 7.0635 (C:5.4597, R:0.0076, T:1.5685)
Val Loss:   10.2853 (C:5.4446, R:0.0069, T:7.6917)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 29 | Batches: 537 | Topological Weight: 0.5600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0932 (C:5.5240, R:0.0075, T:1.4564(w:0.560)ğŸš€)
Batch  25/537: Loss=7.0865 (C:5.4144, R:0.0076, T:1.6201(w:0.560)ğŸš€)
Batch  50/537: Loss=7.1597 (C:5.5050, R:0.0076, T:1.5928(w:0.560)ğŸš€)
Batch  75/537: Loss=7.1712 (C:5.4024, R:0.0076, T:1.8069(w:0.560)ğŸš€)
Batch 100/537: Loss=6.8922 (C:5.3706, R:0.0076, T:1.3625(w:0.560)ğŸš€)
Batch 125/537: Loss=7.0625 (C:5.4864, R:0.0076, T:1.4606(w:0.560)ğŸš€)
Batch 150/537: Loss=6.8717 (C:5.3751, R:0.0075, T:1.3291(w:0.560)ğŸš€)
Batch 175/537: Loss=7.1559 (C:5.5321, R:0.0076, T:1.5451(w:0.560)ğŸš€)
Batch 200/537: Loss=7.1010 (C:5.4945, R:0.0076, T:1.5137(w:0.560)ğŸš€)
Batch 225/537: Loss=7.0700 (C:5.4640, R:0.0075, T:1.5302(w:0.560)ğŸš€)
Batch 250/537: Loss=7.3068 (C:5.5237, R:0.0075, T:1.8390(w:0.560)ğŸš€)
Batch 275/537: Loss=7.1234 (C:5.4740, R:0.0076, T:1.5950(w:0.560)ğŸš€)
Batch 300/537: Loss=7.0107 (C:5.4227, R:0.0076, T:1.4802(w:0.560)ğŸš€)
Batch 325/537: Loss=7.0913 (C:5.4727, R:0.0076, T:1.5244(w:0.560)ğŸš€)
Batch 350/537: Loss=7.1084 (C:5.5050, R:0.0076, T:1.5147(w:0.560)ğŸš€)
Batch 375/537: Loss=6.9311 (C:5.3649, R:0.0075, T:1.4541(w:0.560)ğŸš€)
Batch 400/537: Loss=7.1408 (C:5.5171, R:0.0075, T:1.5523(w:0.560)ğŸš€)
Batch 425/537: Loss=7.0689 (C:5.4975, R:0.0075, T:1.4592(w:0.560)ğŸš€)
Batch 450/537: Loss=7.0721 (C:5.4274, R:0.0076, T:1.5768(w:0.560)ğŸš€)
Batch 475/537: Loss=7.0543 (C:5.4677, R:0.0076, T:1.4805(w:0.560)ğŸš€)
Batch 500/537: Loss=7.0681 (C:5.4691, R:0.0076, T:1.4979(w:0.560)ğŸš€)
Batch 525/537: Loss=7.1763 (C:5.5035, R:0.0076, T:1.6269(w:0.560)ğŸš€)
ğŸ“ˆ New best topological loss: 1.5425

ğŸ“Š EPOCH 29 TRAINING SUMMARY:
  Total Loss: 7.0847
  Contrastive: 5.4641
  Reconstruction: 0.0076
  Topological: 1.5425 (weight: 0.560)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.5000
  Contrastive: 5.4605
  Reconstruction: 0.0069
  Topological: 7.7713 (weight: 0.560)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 29/100 COMPLETE (47.2s)
Train Loss: 7.0847 (C:5.4641, R:0.0076, T:1.5425)
Val Loss:   10.5000 (C:5.4605, R:0.0069, T:7.7713)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 30 | Batches: 537 | Topological Weight: 0.5800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0264 (C:5.4153, R:0.0076, T:1.4662(w:0.580)ğŸš€)
Batch  25/537: Loss=7.0968 (C:5.4226, R:0.0075, T:1.5900(w:0.580)ğŸš€)
Batch  50/537: Loss=7.0922 (C:5.3963, R:0.0076, T:1.6143(w:0.580)ğŸš€)
Batch  75/537: Loss=7.1450 (C:5.5364, R:0.0075, T:1.4730(w:0.580)ğŸš€)
Batch 100/537: Loss=6.9750 (C:5.4687, R:0.0076, T:1.2880(w:0.580)ğŸš€)
Batch 125/537: Loss=6.9998 (C:5.3962, R:0.0076, T:1.4619(w:0.580)ğŸš€)
Batch 150/537: Loss=7.2338 (C:5.4220, R:0.0076, T:1.8182(w:0.580)ğŸš€)
Batch 175/537: Loss=7.0627 (C:5.4710, R:0.0076, T:1.4308(w:0.580)ğŸš€)
Batch 200/537: Loss=6.9902 (C:5.4458, R:0.0075, T:1.3633(w:0.580)ğŸš€)
Batch 225/537: Loss=7.0618 (C:5.3953, R:0.0075, T:1.5796(w:0.580)ğŸš€)
Batch 250/537: Loss=7.0097 (C:5.4467, R:0.0076, T:1.3906(w:0.580)ğŸš€)
Batch 275/537: Loss=7.1364 (C:5.4297, R:0.0075, T:1.6458(w:0.580)ğŸš€)
Batch 300/537: Loss=7.1672 (C:5.5064, R:0.0076, T:1.5573(w:0.580)ğŸš€)
Batch 325/537: Loss=7.0563 (C:5.4865, R:0.0076, T:1.3955(w:0.580)ğŸš€)
Batch 350/537: Loss=7.1264 (C:5.4702, R:0.0076, T:1.5421(w:0.580)ğŸš€)
Batch 375/537: Loss=7.1482 (C:5.4509, R:0.0075, T:1.6274(w:0.580)ğŸš€)
Batch 400/537: Loss=7.2156 (C:5.4903, R:0.0076, T:1.6608(w:0.580)ğŸš€)
Batch 425/537: Loss=7.1924 (C:5.4730, R:0.0076, T:1.6596(w:0.580)ğŸš€)
Batch 450/537: Loss=7.1419 (C:5.5464, R:0.0076, T:1.4433(w:0.580)ğŸš€)
Batch 475/537: Loss=7.0474 (C:5.3914, R:0.0076, T:1.5389(w:0.580)ğŸš€)
Batch 500/537: Loss=7.0378 (C:5.4026, R:0.0076, T:1.5075(w:0.580)ğŸš€)
Batch 525/537: Loss=7.0960 (C:5.4425, R:0.0076, T:1.5381(w:0.580)ğŸš€)
ğŸ“ˆ New best topological loss: 1.5149

ğŸ“Š EPOCH 30 TRAINING SUMMARY:
  Total Loss: 7.0999
  Contrastive: 5.4642
  Reconstruction: 0.0076
  Topological: 1.5149 (weight: 0.580)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.5355
  Contrastive: 5.4592
  Reconstruction: 0.0069
  Topological: 7.5701 (weight: 0.580)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 30/100 COMPLETE (47.9s)
Train Loss: 7.0999 (C:5.4642, R:0.0076, T:1.5149)
Val Loss:   10.5355 (C:5.4592, R:0.0069, T:7.5701)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 31
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.215 Â± 0.764
    Neg distances: 6.921 Â± 2.132
    Separation ratio: 1.64x
    Gap: -9.883
    âš ï¸  Moderate global separation

============================================================
EPOCH 31 | Batches: 537 | Topological Weight: 0.6000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.0895 (C:5.5726, R:0.0075, T:1.2779(w:0.600)ğŸš€)
Batch  25/537: Loss=7.2689 (C:5.5620, R:0.0076, T:1.5848(w:0.600)ğŸš€)
Batch  50/537: Loss=7.1346 (C:5.5623, R:0.0076, T:1.3596(w:0.600)ğŸš€)
Batch  75/537: Loss=6.9705 (C:5.4255, R:0.0075, T:1.3200(w:0.600)ğŸš€)
Batch 100/537: Loss=7.0298 (C:5.4685, R:0.0075, T:1.3449(w:0.600)ğŸš€)
Batch 125/537: Loss=7.2028 (C:5.5686, R:0.0076, T:1.4607(w:0.600)ğŸš€)
Batch 150/537: Loss=7.4261 (C:5.6039, R:0.0075, T:1.7849(w:0.600)ğŸš€)
Batch 175/537: Loss=7.2573 (C:5.6039, R:0.0076, T:1.4908(w:0.600)ğŸš€)
Batch 200/537: Loss=7.2582 (C:5.4422, R:0.0076, T:1.7644(w:0.600)ğŸš€)
Batch 225/537: Loss=7.2970 (C:5.6102, R:0.0076, T:1.5502(w:0.600)ğŸš€)
Batch 250/537: Loss=7.1463 (C:5.5513, R:0.0075, T:1.4073(w:0.600)ğŸš€)
Batch 275/537: Loss=7.1897 (C:5.6403, R:0.0075, T:1.3249(w:0.600)ğŸš€)
Batch 300/537: Loss=7.2244 (C:5.5552, R:0.0076, T:1.5224(w:0.600)ğŸš€)
Batch 325/537: Loss=7.1011 (C:5.5544, R:0.0076, T:1.3123(w:0.600)ğŸš€)
Batch 350/537: Loss=7.2505 (C:5.5171, R:0.0076, T:1.6296(w:0.600)ğŸš€)
Batch 375/537: Loss=7.3569 (C:5.6331, R:0.0075, T:1.6197(w:0.600)ğŸš€)
Batch 400/537: Loss=7.3897 (C:5.5819, R:0.0076, T:1.7418(w:0.600)ğŸš€)
Batch 425/537: Loss=7.1420 (C:5.5731, R:0.0076, T:1.3472(w:0.600)ğŸš€)
Batch 450/537: Loss=7.1010 (C:5.4688, R:0.0075, T:1.4717(w:0.600)ğŸš€)
Batch 475/537: Loss=7.1428 (C:5.5686, R:0.0076, T:1.3592(w:0.600)ğŸš€)
Batch 500/537: Loss=7.1510 (C:5.4930, R:0.0076, T:1.4976(w:0.600)ğŸš€)
Batch 525/537: Loss=7.1556 (C:5.5132, R:0.0076, T:1.4774(w:0.600)ğŸš€)
ğŸ“ˆ New best topological loss: 1.4802

ğŸ“Š EPOCH 31 TRAINING SUMMARY:
  Total Loss: 7.1933
  Contrastive: 5.5485
  Reconstruction: 0.0076
  Topological: 1.4802 (weight: 0.600)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.7907
  Contrastive: 5.5517
  Reconstruction: 0.0069
  Topological: 7.5873 (weight: 0.600)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 31/100 COMPLETE (55.4s)
Train Loss: 7.1933 (C:5.5485, R:0.0076, T:1.4802)
Val Loss:   10.7907 (C:5.5517, R:0.0069, T:7.5873)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 32 | Batches: 537 | Topological Weight: 0.6200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1087 (C:5.5255, R:0.0076, T:1.3352(w:0.620)ğŸš€)
Batch  25/537: Loss=7.1574 (C:5.5407, R:0.0076, T:1.3893(w:0.620)ğŸš€)
Batch  50/537: Loss=7.0502 (C:5.4724, R:0.0076, T:1.3235(w:0.620)ğŸš€)
Batch  75/537: Loss=7.1668 (C:5.5513, R:0.0075, T:1.3891(w:0.620)ğŸš€)
Batch 100/537: Loss=7.2535 (C:5.4963, R:0.0076, T:1.6031(w:0.620)ğŸš€)
Batch 125/537: Loss=7.2809 (C:5.6088, R:0.0076, T:1.4768(w:0.620)ğŸš€)
Batch 150/537: Loss=7.2890 (C:5.5856, R:0.0076, T:1.5243(w:0.620)ğŸš€)
Batch 175/537: Loss=7.1625 (C:5.5263, R:0.0076, T:1.4165(w:0.620)ğŸš€)
Batch 200/537: Loss=7.1027 (C:5.5395, R:0.0076, T:1.2953(w:0.620)ğŸš€)
Batch 225/537: Loss=7.2641 (C:5.5103, R:0.0076, T:1.6092(w:0.620)ğŸš€)
Batch 250/537: Loss=7.2269 (C:5.6049, R:0.0075, T:1.3993(w:0.620)ğŸš€)
Batch 275/537: Loss=7.3168 (C:5.5859, R:0.0075, T:1.5752(w:0.620)ğŸš€)
Batch 300/537: Loss=7.3806 (C:5.6113, R:0.0075, T:1.6380(w:0.620)ğŸš€)
Batch 325/537: Loss=7.1322 (C:5.5512, R:0.0076, T:1.3280(w:0.620)ğŸš€)
Batch 350/537: Loss=7.3049 (C:5.5624, R:0.0076, T:1.5833(w:0.620)ğŸš€)
Batch 375/537: Loss=7.1341 (C:5.5277, R:0.0076, T:1.3681(w:0.620)ğŸš€)
Batch 400/537: Loss=7.1656 (C:5.5519, R:0.0076, T:1.3700(w:0.620)ğŸš€)
Batch 425/537: Loss=7.2346 (C:5.5608, R:0.0076, T:1.4795(w:0.620)ğŸš€)
Batch 450/537: Loss=7.1882 (C:5.5612, R:0.0076, T:1.4038(w:0.620)ğŸš€)
Batch 475/537: Loss=7.1203 (C:5.5419, R:0.0075, T:1.3391(w:0.620)ğŸš€)
Batch 500/537: Loss=7.2674 (C:5.5629, R:0.0076, T:1.5163(w:0.620)ğŸš€)
Batch 525/537: Loss=7.2310 (C:5.5675, R:0.0075, T:1.4662(w:0.620)ğŸš€)
ğŸ“ˆ New best topological loss: 1.4599

ğŸ“Š EPOCH 32 TRAINING SUMMARY:
  Total Loss: 7.2094
  Contrastive: 5.5471
  Reconstruction: 0.0076
  Topological: 1.4599 (weight: 0.620)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 10.8362
  Contrastive: 5.5592
  Reconstruction: 0.0069
  Topological: 7.4045 (weight: 0.620)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 32/100 COMPLETE (48.9s)
Train Loss: 7.2094 (C:5.5471, R:0.0076, T:1.4599)
Val Loss:   10.8362 (C:5.5592, R:0.0069, T:7.4045)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 33 | Batches: 537 | Topological Weight: 0.6400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1175 (C:5.5272, R:0.0076, T:1.3024(w:0.640)ğŸš€)
Batch  25/537: Loss=7.2502 (C:5.4923, R:0.0076, T:1.5569(w:0.640)ğŸš€)
Batch  50/537: Loss=7.2596 (C:5.5777, R:0.0076, T:1.4378(w:0.640)ğŸš€)
Batch  75/537: Loss=7.3422 (C:5.5405, R:0.0075, T:1.6365(w:0.640)ğŸš€)
Batch 100/537: Loss=7.3060 (C:5.6280, R:0.0075, T:1.4464(w:0.640)ğŸš€)
Batch 125/537: Loss=7.2312 (C:5.4838, R:0.0076, T:1.5481(w:0.640)ğŸš€)
Batch 150/537: Loss=7.0932 (C:5.4785, R:0.0076, T:1.3415(w:0.640)ğŸš€)
Batch 175/537: Loss=7.2709 (C:5.6159, R:0.0076, T:1.4014(w:0.640)ğŸš€)
Batch 200/537: Loss=7.3807 (C:5.6147, R:0.0075, T:1.5818(w:0.640)ğŸš€)
Batch 225/537: Loss=7.1642 (C:5.5273, R:0.0076, T:1.3745(w:0.640)ğŸš€)
Batch 250/537: Loss=7.3250 (C:5.6532, R:0.0075, T:1.4344(w:0.640)ğŸš€)
Batch 275/537: Loss=7.2242 (C:5.6051, R:0.0076, T:1.3411(w:0.640)ğŸš€)
Batch 300/537: Loss=7.2662 (C:5.5683, R:0.0075, T:1.4735(w:0.640)ğŸš€)
Batch 325/537: Loss=7.3840 (C:5.5989, R:0.0076, T:1.5968(w:0.640)ğŸš€)
Batch 350/537: Loss=7.1846 (C:5.5712, R:0.0076, T:1.3385(w:0.640)ğŸš€)
Batch 375/537: Loss=7.3387 (C:5.5949, R:0.0076, T:1.5418(w:0.640)ğŸš€)
Batch 400/537: Loss=7.1751 (C:5.4620, R:0.0075, T:1.4979(w:0.640)ğŸš€)
Batch 425/537: Loss=7.2582 (C:5.6462, R:0.0076, T:1.3314(w:0.640)ğŸš€)
Batch 450/537: Loss=7.1526 (C:5.5338, R:0.0076, T:1.3470(w:0.640)ğŸš€)
Batch 475/537: Loss=7.1052 (C:5.5623, R:0.0075, T:1.2334(w:0.640)ğŸš€)
Batch 500/537: Loss=7.0846 (C:5.5124, R:0.0076, T:1.2725(w:0.640)ğŸš€)
Batch 525/537: Loss=7.1558 (C:5.5083, R:0.0076, T:1.3859(w:0.640)ğŸš€)
ğŸ“ˆ New best topological loss: 1.4361

ğŸ“Š EPOCH 33 TRAINING SUMMARY:
  Total Loss: 7.2259
  Contrastive: 5.5493
  Reconstruction: 0.0076
  Topological: 1.4361 (weight: 0.640)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.0058
  Contrastive: 5.5967
  Reconstruction: 0.0069
  Topological: 7.3771 (weight: 0.640)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 33/100 COMPLETE (47.6s)
Train Loss: 7.2259 (C:5.5493, R:0.0076, T:1.4361)
Val Loss:   11.0058 (C:5.5967, R:0.0069, T:7.3771)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 34
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.217 Â± 0.718
    Neg distances: 6.878 Â± 2.013
    Separation ratio: 1.63x
    Gap: -9.758
    âš ï¸  Moderate global separation

============================================================
EPOCH 34 | Batches: 537 | Topological Weight: 0.6600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2079 (C:5.6439, R:0.0076, T:1.2173(w:0.660)ğŸš€)
Batch  25/537: Loss=7.1532 (C:5.4883, R:0.0076, T:1.3687(w:0.660)ğŸš€)
Batch  50/537: Loss=7.2621 (C:5.4990, R:0.0076, T:1.5264(w:0.660)ğŸš€)
Batch  75/537: Loss=7.1953 (C:5.5511, R:0.0076, T:1.3409(w:0.660)ğŸš€)
Batch 100/537: Loss=7.2081 (C:5.5950, R:0.0075, T:1.3069(w:0.660)ğŸš€)
Batch 125/537: Loss=7.1157 (C:5.5144, R:0.0076, T:1.2798(w:0.660)ğŸš€)
Batch 150/537: Loss=7.2225 (C:5.5211, R:0.0076, T:1.4235(w:0.660)ğŸš€)
Batch 175/537: Loss=7.2440 (C:5.5132, R:0.0076, T:1.4712(w:0.660)ğŸš€)
Batch 200/537: Loss=7.3145 (C:5.6139, R:0.0076, T:1.4300(w:0.660)ğŸš€)
Batch 225/537: Loss=7.1904 (C:5.5454, R:0.0076, T:1.3417(w:0.660)ğŸš€)
Batch 250/537: Loss=7.2951 (C:5.5661, R:0.0076, T:1.4747(w:0.660)ğŸš€)
Batch 275/537: Loss=7.1972 (C:5.5492, R:0.0076, T:1.3484(w:0.660)ğŸš€)
Batch 300/537: Loss=7.1948 (C:5.5885, R:0.0076, T:1.2801(w:0.660)ğŸš€)
Batch 325/537: Loss=7.2224 (C:5.5397, R:0.0076, T:1.3962(w:0.660)ğŸš€)
Batch 350/537: Loss=7.3018 (C:5.6027, R:0.0076, T:1.4290(w:0.660)ğŸš€)
Batch 375/537: Loss=7.3112 (C:5.6307, R:0.0075, T:1.4043(w:0.660)ğŸš€)
Batch 400/537: Loss=7.2604 (C:5.6121, R:0.0076, T:1.3487(w:0.660)ğŸš€)
Batch 425/537: Loss=7.3460 (C:5.5830, R:0.0076, T:1.5197(w:0.660)ğŸš€)
Batch 450/537: Loss=7.2635 (C:5.6125, R:0.0076, T:1.3527(w:0.660)ğŸš€)
Batch 475/537: Loss=7.1594 (C:5.5313, R:0.0076, T:1.3198(w:0.660)ğŸš€)
Batch 500/537: Loss=7.3346 (C:5.5564, R:0.0076, T:1.5402(w:0.660)ğŸš€)
Batch 525/537: Loss=7.2247 (C:5.6717, R:0.0075, T:1.2132(w:0.660)ğŸš€)
ğŸ“ˆ New best topological loss: 1.4179

ğŸ“Š EPOCH 34 TRAINING SUMMARY:
  Total Loss: 7.2644
  Contrastive: 5.5709
  Reconstruction: 0.0076
  Topological: 1.4179 (weight: 0.660)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.3092
  Contrastive: 5.5452
  Reconstruction: 0.0069
  Topological: 7.6902 (weight: 0.660)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 34/100 COMPLETE (52.0s)
Train Loss: 7.2644 (C:5.5709, R:0.0076, T:1.4179)
Val Loss:   11.3092 (C:5.5452, R:0.0069, T:7.6902)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 35 | Batches: 537 | Topological Weight: 0.6800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1550 (C:5.5269, R:0.0075, T:1.2850(w:0.680)ğŸš€)
Batch  25/537: Loss=7.3245 (C:5.5242, R:0.0075, T:1.5385(w:0.680)ğŸš€)
Batch  50/537: Loss=7.1191 (C:5.4816, R:0.0076, T:1.2890(w:0.680)ğŸš€)
Batch  75/537: Loss=7.2734 (C:5.5128, R:0.0076, T:1.4754(w:0.680)ğŸš€)
Batch 100/537: Loss=7.3533 (C:5.6323, R:0.0076, T:1.4093(w:0.680)ğŸš€)
Batch 125/537: Loss=7.1691 (C:5.4802, R:0.0076, T:1.3637(w:0.680)ğŸš€)
Batch 150/537: Loss=7.2313 (C:5.4874, R:0.0076, T:1.4509(w:0.680)ğŸš€)
Batch 175/537: Loss=7.3595 (C:5.5606, R:0.0077, T:1.5188(w:0.680)ğŸš€)
Batch 200/537: Loss=7.3498 (C:5.6494, R:0.0076, T:1.3800(w:0.680)ğŸš€)
Batch 225/537: Loss=7.3553 (C:5.5946, R:0.0076, T:1.4763(w:0.680)ğŸš€)
Batch 250/537: Loss=7.1527 (C:5.5244, R:0.0076, T:1.2713(w:0.680)ğŸš€)
Batch 275/537: Loss=7.2993 (C:5.5791, R:0.0075, T:1.4205(w:0.680)ğŸš€)
Batch 300/537: Loss=7.2799 (C:5.6347, R:0.0077, T:1.2921(w:0.680)ğŸš€)
Batch 325/537: Loss=7.3144 (C:5.5665, R:0.0076, T:1.4524(w:0.680)ğŸš€)
Batch 350/537: Loss=7.3050 (C:5.6390, R:0.0076, T:1.3286(w:0.680)ğŸš€)
Batch 375/537: Loss=7.3611 (C:5.6234, R:0.0075, T:1.4463(w:0.680)ğŸš€)
Batch 400/537: Loss=7.4430 (C:5.5551, R:0.0075, T:1.6677(w:0.680)ğŸš€)
Batch 425/537: Loss=7.2729 (C:5.5966, R:0.0076, T:1.3456(w:0.680)ğŸš€)
Batch 450/537: Loss=7.4826 (C:5.7525, R:0.0076, T:1.4334(w:0.680)ğŸš€)
Batch 475/537: Loss=7.2005 (C:5.5862, R:0.0075, T:1.2650(w:0.680)ğŸš€)
Batch 500/537: Loss=7.4720 (C:5.5982, R:0.0076, T:1.6418(w:0.680)ğŸš€)
Batch 525/537: Loss=7.3787 (C:5.6170, R:0.0075, T:1.4820(w:0.680)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3958

ğŸ“Š EPOCH 35 TRAINING SUMMARY:
  Total Loss: 7.2727
  Contrastive: 5.5657
  Reconstruction: 0.0076
  Topological: 1.3958 (weight: 0.680)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.1187
  Contrastive: 5.6654
  Reconstruction: 0.0069
  Topological: 7.0095 (weight: 0.680)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 35/100 COMPLETE (47.4s)
Train Loss: 7.2727 (C:5.5657, R:0.0076, T:1.3958)
Val Loss:   11.1187 (C:5.6654, R:0.0069, T:7.0095)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 36 | Batches: 537 | Topological Weight: 0.7000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2585 (C:5.6345, R:0.0076, T:1.2321(w:0.700)ğŸš€)
Batch  25/537: Loss=7.2382 (C:5.5046, R:0.0076, T:1.3965(w:0.700)ğŸš€)
Batch  50/537: Loss=7.3047 (C:5.5046, R:0.0076, T:1.4924(w:0.700)ğŸš€)
Batch  75/537: Loss=7.1518 (C:5.4642, R:0.0076, T:1.3221(w:0.700)ğŸš€)
Batch 100/537: Loss=7.4188 (C:5.5394, R:0.0075, T:1.6077(w:0.700)ğŸš€)
Batch 125/537: Loss=7.1721 (C:5.5127, R:0.0075, T:1.2937(w:0.700)ğŸš€)
Batch 150/537: Loss=7.3609 (C:5.6007, R:0.0075, T:1.4428(w:0.700)ğŸš€)
Batch 175/537: Loss=7.3655 (C:5.6496, R:0.0076, T:1.3692(w:0.700)ğŸš€)
Batch 200/537: Loss=7.2948 (C:5.5782, R:0.0076, T:1.3655(w:0.700)ğŸš€)
Batch 225/537: Loss=7.1842 (C:5.4901, R:0.0076, T:1.3383(w:0.700)ğŸš€)
Batch 250/537: Loss=7.3246 (C:5.6515, R:0.0076, T:1.3078(w:0.700)ğŸš€)
Batch 275/537: Loss=7.2276 (C:5.5909, R:0.0076, T:1.2536(w:0.700)ğŸš€)
Batch 300/537: Loss=7.1930 (C:5.4783, R:0.0076, T:1.3679(w:0.700)ğŸš€)
Batch 325/537: Loss=7.3027 (C:5.5175, R:0.0076, T:1.4687(w:0.700)ğŸš€)
Batch 350/537: Loss=7.3290 (C:5.5641, R:0.0076, T:1.4347(w:0.700)ğŸš€)
Batch 375/537: Loss=7.1693 (C:5.5647, R:0.0076, T:1.2103(w:0.700)ğŸš€)
Batch 400/537: Loss=7.2546 (C:5.5755, R:0.0076, T:1.3169(w:0.700)ğŸš€)
Batch 425/537: Loss=7.3488 (C:5.6191, R:0.0076, T:1.3897(w:0.700)ğŸš€)
Batch 450/537: Loss=7.3582 (C:5.6563, R:0.0076, T:1.3481(w:0.700)ğŸš€)
Batch 475/537: Loss=7.2000 (C:5.5925, R:0.0075, T:1.2187(w:0.700)ğŸš€)
Batch 500/537: Loss=7.3386 (C:5.6040, R:0.0075, T:1.4027(w:0.700)ğŸš€)
Batch 525/537: Loss=7.2611 (C:5.6458, R:0.0076, T:1.2218(w:0.700)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3796

ğŸ“Š EPOCH 36 TRAINING SUMMARY:
  Total Loss: 7.2928
  Contrastive: 5.5689
  Reconstruction: 0.0076
  Topological: 1.3796 (weight: 0.700)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.4297
  Contrastive: 5.6312
  Reconstruction: 0.0069
  Topological: 7.3005 (weight: 0.700)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 36/100 COMPLETE (46.5s)
Train Loss: 7.2928 (C:5.5689, R:0.0076, T:1.3796)
Val Loss:   11.4297 (C:5.6312, R:0.0069, T:7.3005)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 37
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.204 Â± 0.710
    Neg distances: 6.849 Â± 2.043
    Separation ratio: 1.63x
    Gap: -9.839
    âš ï¸  Moderate global separation

============================================================
EPOCH 37 | Batches: 537 | Topological Weight: 0.7200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3155 (C:5.6920, R:0.0076, T:1.1996(w:0.720)ğŸš€)
Batch  25/537: Loss=7.4270 (C:5.5221, R:0.0076, T:1.5862(w:0.720)ğŸš€)
Batch  50/537: Loss=7.2253 (C:5.4653, R:0.0076, T:1.3905(w:0.720)ğŸš€)
Batch  75/537: Loss=7.2514 (C:5.5198, R:0.0076, T:1.3493(w:0.720)ğŸš€)
Batch 100/537: Loss=7.3074 (C:5.5200, R:0.0076, T:1.4291(w:0.720)ğŸš€)
Batch 125/537: Loss=7.2892 (C:5.5656, R:0.0076, T:1.3450(w:0.720)ğŸš€)
Batch 150/537: Loss=7.1736 (C:5.5357, R:0.0076, T:1.2160(w:0.720)ğŸš€)
Batch 175/537: Loss=7.2742 (C:5.5662, R:0.0077, T:1.3069(w:0.720)ğŸš€)
Batch 200/537: Loss=7.2557 (C:5.4470, R:0.0076, T:1.4573(w:0.720)ğŸš€)
Batch 225/537: Loss=7.2667 (C:5.6143, R:0.0075, T:1.2509(w:0.720)ğŸš€)
Batch 250/537: Loss=7.3562 (C:5.6367, R:0.0076, T:1.3293(w:0.720)ğŸš€)
Batch 275/537: Loss=7.4256 (C:5.5829, R:0.0077, T:1.4961(w:0.720)ğŸš€)
Batch 300/537: Loss=7.3108 (C:5.6189, R:0.0076, T:1.2916(w:0.720)ğŸš€)
Batch 325/537: Loss=7.2417 (C:5.5401, R:0.0076, T:1.3144(w:0.720)ğŸš€)
Batch 350/537: Loss=7.2512 (C:5.5769, R:0.0076, T:1.2634(w:0.720)ğŸš€)
Batch 375/537: Loss=7.3673 (C:5.5801, R:0.0076, T:1.4208(w:0.720)ğŸš€)
Batch 400/537: Loss=7.4025 (C:5.5912, R:0.0076, T:1.4637(w:0.720)ğŸš€)
Batch 425/537: Loss=7.2631 (C:5.5462, R:0.0075, T:1.3362(w:0.720)ğŸš€)
Batch 450/537: Loss=7.3958 (C:5.6168, R:0.0076, T:1.4202(w:0.720)ğŸš€)
Batch 475/537: Loss=7.4104 (C:5.6411, R:0.0076, T:1.4083(w:0.720)ğŸš€)
Batch 500/537: Loss=7.3616 (C:5.6486, R:0.0076, T:1.3274(w:0.720)ğŸš€)
Batch 525/537: Loss=7.4760 (C:5.5991, R:0.0076, T:1.5499(w:0.720)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3596

ğŸ“Š EPOCH 37 TRAINING SUMMARY:
  Total Loss: 7.3134
  Contrastive: 5.5763
  Reconstruction: 0.0076
  Topological: 1.3596 (weight: 0.720)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.2560
  Contrastive: 5.6303
  Reconstruction: 0.0069
  Topological: 6.8602 (weight: 0.720)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 37/100 COMPLETE (53.2s)
Train Loss: 7.3134 (C:5.5763, R:0.0076, T:1.3596)
Val Loss:   11.2560 (C:5.6303, R:0.0069, T:6.8602)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 38 | Batches: 537 | Topological Weight: 0.7400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3766 (C:5.6548, R:0.0076, T:1.3017(w:0.740)ğŸš€)
Batch  25/537: Loss=7.2803 (C:5.4509, R:0.0075, T:1.4572(w:0.740)ğŸš€)
Batch  50/537: Loss=7.4499 (C:5.6158, R:0.0076, T:1.4554(w:0.740)ğŸš€)
Batch  75/537: Loss=7.4076 (C:5.6813, R:0.0076, T:1.3066(w:0.740)ğŸš€)
Batch 100/537: Loss=7.3453 (C:5.5895, R:0.0075, T:1.3550(w:0.740)ğŸš€)
Batch 125/537: Loss=7.2258 (C:5.5352, R:0.0076, T:1.2571(w:0.740)ğŸš€)
Batch 150/537: Loss=7.3053 (C:5.5726, R:0.0076, T:1.3174(w:0.740)ğŸš€)
Batch 175/537: Loss=7.3329 (C:5.6537, R:0.0076, T:1.2448(w:0.740)ğŸš€)
Batch 200/537: Loss=7.4961 (C:5.6481, R:0.0076, T:1.4769(w:0.740)ğŸš€)
Batch 225/537: Loss=7.2662 (C:5.5318, R:0.0076, T:1.3172(w:0.740)ğŸš€)
Batch 250/537: Loss=7.4112 (C:5.6453, R:0.0076, T:1.3662(w:0.740)ğŸš€)
Batch 275/537: Loss=7.2359 (C:5.5147, R:0.0076, T:1.2960(w:0.740)ğŸš€)
Batch 300/537: Loss=7.3933 (C:5.5190, R:0.0076, T:1.5067(w:0.740)ğŸš€)
Batch 325/537: Loss=7.2419 (C:5.4959, R:0.0076, T:1.3375(w:0.740)ğŸš€)
Batch 350/537: Loss=7.5208 (C:5.6363, R:0.0076, T:1.5232(w:0.740)ğŸš€)
Batch 375/537: Loss=7.3551 (C:5.6275, R:0.0076, T:1.3140(w:0.740)ğŸš€)
Batch 400/537: Loss=7.3161 (C:5.5819, R:0.0076, T:1.3156(w:0.740)ğŸš€)
Batch 425/537: Loss=7.4865 (C:5.6257, R:0.0076, T:1.4891(w:0.740)ğŸš€)
Batch 450/537: Loss=7.3690 (C:5.5652, R:0.0076, T:1.4135(w:0.740)ğŸš€)
Batch 475/537: Loss=7.3931 (C:5.6203, R:0.0076, T:1.3663(w:0.740)ğŸš€)
Batch 500/537: Loss=7.5277 (C:5.5794, R:0.0076, T:1.6042(w:0.740)ğŸš€)
Batch 525/537: Loss=7.3264 (C:5.6605, R:0.0075, T:1.2329(w:0.740)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3440

ğŸ“Š EPOCH 38 TRAINING SUMMARY:
  Total Loss: 7.3320
  Contrastive: 5.5793
  Reconstruction: 0.0076
  Topological: 1.3440 (weight: 0.740)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.6866
  Contrastive: 5.5695
  Reconstruction: 0.0069
  Topological: 7.3352 (weight: 0.740)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 38/100 COMPLETE (48.0s)
Train Loss: 7.3320 (C:5.5793, R:0.0076, T:1.3440)
Val Loss:   11.6866 (C:5.5695, R:0.0069, T:7.3352)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 39 | Batches: 537 | Topological Weight: 0.7600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2201 (C:5.5157, R:0.0076, T:1.2479(w:0.760)ğŸš€)
Batch  25/537: Loss=7.2426 (C:5.5152, R:0.0076, T:1.2744(w:0.760)ğŸš€)
Batch  50/537: Loss=7.4004 (C:5.5931, R:0.0076, T:1.3778(w:0.760)ğŸš€)
Batch  75/537: Loss=7.4186 (C:5.5792, R:0.0076, T:1.4268(w:0.760)ğŸš€)
Batch 100/537: Loss=7.3556 (C:5.6563, R:0.0076, T:1.2378(w:0.760)ğŸš€)
Batch 125/537: Loss=7.3144 (C:5.6156, R:0.0075, T:1.2446(w:0.760)ğŸš€)
Batch 150/537: Loss=7.4195 (C:5.6645, R:0.0076, T:1.3152(w:0.760)ğŸš€)
Batch 175/537: Loss=7.3714 (C:5.6191, R:0.0075, T:1.3154(w:0.760)ğŸš€)
Batch 200/537: Loss=7.2026 (C:5.6114, R:0.0075, T:1.1043(w:0.760)ğŸš€)
Batch 225/537: Loss=7.2603 (C:5.5685, R:0.0076, T:1.2216(w:0.760)ğŸš€)
Batch 250/537: Loss=7.4141 (C:5.6256, R:0.0075, T:1.3601(w:0.760)ğŸš€)
Batch 275/537: Loss=7.1407 (C:5.5978, R:0.0075, T:1.0377(w:0.760)ğŸš€)
Batch 300/537: Loss=7.5141 (C:5.6813, R:0.0076, T:1.4141(w:0.760)ğŸš€)
Batch 325/537: Loss=7.4498 (C:5.5577, R:0.0076, T:1.4861(w:0.760)ğŸš€)
Batch 350/537: Loss=7.3398 (C:5.6061, R:0.0076, T:1.2861(w:0.760)ğŸš€)
Batch 375/537: Loss=7.2807 (C:5.5561, R:0.0076, T:1.2712(w:0.760)ğŸš€)
Batch 400/537: Loss=7.3710 (C:5.6313, R:0.0076, T:1.2954(w:0.760)ğŸš€)
Batch 425/537: Loss=7.2858 (C:5.5150, R:0.0076, T:1.3334(w:0.760)ğŸš€)
Batch 450/537: Loss=7.3690 (C:5.6120, R:0.0075, T:1.3225(w:0.760)ğŸš€)
Batch 475/537: Loss=7.3680 (C:5.6333, R:0.0076, T:1.2846(w:0.760)ğŸš€)
Batch 500/537: Loss=7.4670 (C:5.6898, R:0.0076, T:1.3448(w:0.760)ğŸš€)
Batch 525/537: Loss=7.3823 (C:5.6808, R:0.0076, T:1.2431(w:0.760)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3329

ğŸ“Š EPOCH 39 TRAINING SUMMARY:
  Total Loss: 7.3526
  Contrastive: 5.5812
  Reconstruction: 0.0076
  Topological: 1.3329 (weight: 0.760)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.8400
  Contrastive: 5.5866
  Reconstruction: 0.0069
  Topological: 7.3224 (weight: 0.760)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 39/100 COMPLETE (47.8s)
Train Loss: 7.3526 (C:5.5812, R:0.0076, T:1.3329)
Val Loss:   11.8400 (C:5.5866, R:0.0069, T:7.3224)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 40
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.229 Â± 0.750
    Neg distances: 7.147 Â± 2.214
    Separation ratio: 1.69x
    Gap: -9.874
    âš ï¸  Moderate global separation

============================================================
EPOCH 40 | Batches: 537 | Topological Weight: 0.7800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1045 (C:5.3792, R:0.0076, T:1.2374(w:0.780)ğŸš€)
Batch  25/537: Loss=7.1973 (C:5.3347, R:0.0076, T:1.4128(w:0.780)ğŸš€)
Batch  50/537: Loss=7.2099 (C:5.3416, R:0.0076, T:1.4177(w:0.780)ğŸš€)
Batch  75/537: Loss=7.2203 (C:5.3667, R:0.0076, T:1.4003(w:0.780)ğŸš€)
Batch 100/537: Loss=7.1589 (C:5.3890, R:0.0076, T:1.2924(w:0.780)ğŸš€)
Batch 125/537: Loss=7.2685 (C:5.4009, R:0.0076, T:1.4242(w:0.780)ğŸš€)
Batch 150/537: Loss=7.3245 (C:5.4407, R:0.0076, T:1.4386(w:0.780)ğŸš€)
Batch 175/537: Loss=7.2094 (C:5.4444, R:0.0076, T:1.2870(w:0.780)ğŸš€)
Batch 200/537: Loss=7.2450 (C:5.3669, R:0.0076, T:1.4272(w:0.780)ğŸš€)
Batch 225/537: Loss=7.1410 (C:5.3768, R:0.0076, T:1.2886(w:0.780)ğŸš€)
Batch 250/537: Loss=7.1578 (C:5.4153, R:0.0075, T:1.2662(w:0.780)ğŸš€)
Batch 275/537: Loss=7.2492 (C:5.4820, R:0.0076, T:1.2896(w:0.780)ğŸš€)
Batch 300/537: Loss=7.3090 (C:5.4203, R:0.0077, T:1.4362(w:0.780)ğŸš€)
Batch 325/537: Loss=7.3383 (C:5.5836, R:0.0077, T:1.2663(w:0.780)ğŸš€)
Batch 350/537: Loss=7.2051 (C:5.4222, R:0.0076, T:1.3158(w:0.780)ğŸš€)
Batch 375/537: Loss=7.1884 (C:5.4199, R:0.0077, T:1.2851(w:0.780)ğŸš€)
Batch 400/537: Loss=7.1554 (C:5.4235, R:0.0076, T:1.2468(w:0.780)ğŸš€)
Batch 425/537: Loss=7.3729 (C:5.5280, R:0.0076, T:1.3874(w:0.780)ğŸš€)
Batch 450/537: Loss=7.1770 (C:5.4059, R:0.0076, T:1.2958(w:0.780)ğŸš€)
Batch 475/537: Loss=7.2590 (C:5.4216, R:0.0076, T:1.3841(w:0.780)ğŸš€)
Batch 500/537: Loss=7.1413 (C:5.4234, R:0.0076, T:1.2319(w:0.780)ğŸš€)
Batch 525/537: Loss=7.1705 (C:5.3819, R:0.0075, T:1.3309(w:0.780)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3275

ğŸ“Š EPOCH 40 TRAINING SUMMARY:
  Total Loss: 7.2241
  Contrastive: 5.4296
  Reconstruction: 0.0076
  Topological: 1.3275 (weight: 0.780)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.6053
  Contrastive: 5.4856
  Reconstruction: 0.0069
  Topological: 6.9633 (weight: 0.780)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 40/100 COMPLETE (55.5s)
Train Loss: 7.2241 (C:5.4296, R:0.0076, T:1.3275)
Val Loss:   11.6053 (C:5.4856, R:0.0069, T:6.9633)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 41 | Batches: 537 | Topological Weight: 0.8000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2106 (C:5.4448, R:0.0076, T:1.2562(w:0.800)ğŸš€)
Batch  25/537: Loss=7.3199 (C:5.3429, R:0.0075, T:1.5286(w:0.800)ğŸš€)
Batch  50/537: Loss=7.1085 (C:5.3783, R:0.0076, T:1.2182(w:0.800)ğŸš€)
Batch  75/537: Loss=7.1096 (C:5.3702, R:0.0076, T:1.2228(w:0.800)ğŸš€)
Batch 100/537: Loss=7.1435 (C:5.3827, R:0.0076, T:1.2461(w:0.800)ğŸš€)
Batch 125/537: Loss=7.0700 (C:5.3671, R:0.0076, T:1.1834(w:0.800)ğŸš€)
Batch 150/537: Loss=7.3250 (C:5.4370, R:0.0076, T:1.4071(w:0.800)ğŸš€)
Batch 175/537: Loss=7.1778 (C:5.4340, R:0.0076, T:1.2276(w:0.800)ğŸš€)
Batch 200/537: Loss=7.2128 (C:5.4800, R:0.0076, T:1.2187(w:0.800)ğŸš€)
Batch 225/537: Loss=7.3249 (C:5.4526, R:0.0076, T:1.3941(w:0.800)ğŸš€)
Batch 250/537: Loss=7.2315 (C:5.4269, R:0.0076, T:1.3088(w:0.800)ğŸš€)
Batch 275/537: Loss=7.2819 (C:5.4935, R:0.0076, T:1.2838(w:0.800)ğŸš€)
Batch 300/537: Loss=7.2226 (C:5.4133, R:0.0076, T:1.3057(w:0.800)ğŸš€)
Batch 325/537: Loss=7.3130 (C:5.5271, R:0.0076, T:1.2864(w:0.800)ğŸš€)
Batch 350/537: Loss=7.2074 (C:5.4919, R:0.0076, T:1.1981(w:0.800)ğŸš€)
Batch 375/537: Loss=7.1741 (C:5.4266, R:0.0076, T:1.2346(w:0.800)ğŸš€)
Batch 400/537: Loss=7.2170 (C:5.4856, R:0.0076, T:1.2122(w:0.800)ğŸš€)
Batch 425/537: Loss=7.3592 (C:5.6342, R:0.0076, T:1.2076(w:0.800)ğŸš€)
Batch 450/537: Loss=7.1102 (C:5.4364, R:0.0076, T:1.1381(w:0.800)ğŸš€)
Batch 475/537: Loss=7.2679 (C:5.4987, R:0.0076, T:1.2662(w:0.800)ğŸš€)
Batch 500/537: Loss=7.3353 (C:5.4820, R:0.0076, T:1.3643(w:0.800)ğŸš€)
Batch 525/537: Loss=7.1423 (C:5.3296, R:0.0076, T:1.3183(w:0.800)ğŸš€)
ğŸ“ˆ New best topological loss: 1.3125

ğŸ“Š EPOCH 41 TRAINING SUMMARY:
  Total Loss: 7.2343
  Contrastive: 5.4251
  Reconstruction: 0.0076
  Topological: 1.3125 (weight: 0.800)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 11.8426
  Contrastive: 5.5360
  Reconstruction: 0.0069
  Topological: 7.0221 (weight: 0.800)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 41/100 COMPLETE (47.5s)
Train Loss: 7.2343 (C:5.4251, R:0.0076, T:1.3125)
Val Loss:   11.8426 (C:5.5360, R:0.0069, T:7.0221)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 42 | Batches: 537 | Topological Weight: 0.8200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2664 (C:5.4898, R:0.0076, T:1.2450(w:0.820)ğŸš€)
Batch  25/537: Loss=7.2755 (C:5.4503, R:0.0076, T:1.2972(w:0.820)ğŸš€)
Batch  50/537: Loss=7.2095 (C:5.3782, R:0.0076, T:1.3111(w:0.820)ğŸš€)
Batch  75/537: Loss=7.1972 (C:5.2700, R:0.0075, T:1.4321(w:0.820)ğŸš€)
Batch 100/537: Loss=7.2208 (C:5.3421, R:0.0076, T:1.3623(w:0.820)ğŸš€)
Batch 125/537: Loss=7.2473 (C:5.3513, R:0.0076, T:1.3890(w:0.820)ğŸš€)
Batch 150/537: Loss=7.1957 (C:5.4074, R:0.0076, T:1.2546(w:0.820)ğŸš€)
Batch 175/537: Loss=7.2674 (C:5.4267, R:0.0076, T:1.3218(w:0.820)ğŸš€)
Batch 200/537: Loss=7.2861 (C:5.4189, R:0.0076, T:1.3542(w:0.820)ğŸš€)
Batch 225/537: Loss=7.3312 (C:5.4487, R:0.0076, T:1.3687(w:0.820)ğŸš€)
Batch 250/537: Loss=7.2919 (C:5.4149, R:0.0075, T:1.3721(w:0.820)ğŸš€)
Batch 275/537: Loss=7.3001 (C:5.4176, R:0.0077, T:1.3567(w:0.820)ğŸš€)
Batch 300/537: Loss=7.2610 (C:5.4304, R:0.0077, T:1.2994(w:0.820)ğŸš€)
Batch 325/537: Loss=7.2844 (C:5.3774, R:0.0076, T:1.3992(w:0.820)ğŸš€)
Batch 350/537: Loss=7.1537 (C:5.4587, R:0.0076, T:1.1438(w:0.820)ğŸš€)
Batch 375/537: Loss=7.1930 (C:5.4162, R:0.0076, T:1.2384(w:0.820)ğŸš€)
Batch 400/537: Loss=7.1454 (C:5.4091, R:0.0076, T:1.1943(w:0.820)ğŸš€)
Batch 425/537: Loss=7.3564 (C:5.4360, R:0.0077, T:1.4064(w:0.820)ğŸš€)
Batch 450/537: Loss=7.3764 (C:5.4871, R:0.0076, T:1.3810(w:0.820)ğŸš€)
Batch 475/537: Loss=7.2598 (C:5.3894, R:0.0077, T:1.3479(w:0.820)ğŸš€)
Batch 500/537: Loss=7.3729 (C:5.4119, R:0.0076, T:1.4684(w:0.820)ğŸš€)
Batch 525/537: Loss=7.3140 (C:5.5090, R:0.0077, T:1.2671(w:0.820)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2962

ğŸ“Š EPOCH 42 TRAINING SUMMARY:
  Total Loss: 7.2502
  Contrastive: 5.4280
  Reconstruction: 0.0076
  Topological: 1.2962 (weight: 0.820)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.0513
  Contrastive: 5.4712
  Reconstruction: 0.0069
  Topological: 7.1837 (weight: 0.820)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 42/100 COMPLETE (47.8s)
Train Loss: 7.2502 (C:5.4280, R:0.0076, T:1.2962)
Val Loss:   12.0513 (C:5.4712, R:0.0069, T:7.1837)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 43
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.229 Â± 0.671
    Neg distances: 6.986 Â± 2.063
    Separation ratio: 1.65x
    Gap: -9.934
    âš ï¸  Moderate global separation

============================================================
EPOCH 43 | Batches: 537 | Topological Weight: 0.8400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2510 (C:5.5106, R:0.0076, T:1.1684(w:0.840)ğŸš€)
Batch  25/537: Loss=7.3336 (C:5.4973, R:0.0075, T:1.2879(w:0.840)ğŸš€)
Batch  50/537: Loss=7.1330 (C:5.3745, R:0.0076, T:1.1903(w:0.840)ğŸš€)
Batch  75/537: Loss=7.3305 (C:5.4646, R:0.0076, T:1.3160(w:0.840)ğŸš€)
Batch 100/537: Loss=7.4578 (C:5.4473, R:0.0076, T:1.4875(w:0.840)ğŸš€)
Batch 125/537: Loss=7.1809 (C:5.4665, R:0.0076, T:1.1383(w:0.840)ğŸš€)
Batch 150/537: Loss=7.3848 (C:5.5552, R:0.0076, T:1.2694(w:0.840)ğŸš€)
Batch 175/537: Loss=7.3696 (C:5.5451, R:0.0076, T:1.2700(w:0.840)ğŸš€)
Batch 200/537: Loss=7.4336 (C:5.4474, R:0.0076, T:1.4569(w:0.840)ğŸš€)
Batch 225/537: Loss=7.4340 (C:5.4577, R:0.0076, T:1.4423(w:0.840)ğŸš€)
Batch 250/537: Loss=7.2035 (C:5.5099, R:0.0076, T:1.1150(w:0.840)ğŸš€)
Batch 275/537: Loss=7.3460 (C:5.5048, R:0.0076, T:1.2880(w:0.840)ğŸš€)
Batch 300/537: Loss=7.4743 (C:5.5362, R:0.0076, T:1.4077(w:0.840)ğŸš€)
Batch 325/537: Loss=7.3801 (C:5.5921, R:0.0076, T:1.2245(w:0.840)ğŸš€)
Batch 350/537: Loss=7.5114 (C:5.5718, R:0.0076, T:1.4028(w:0.840)ğŸš€)
Batch 375/537: Loss=7.4272 (C:5.5725, R:0.0075, T:1.3114(w:0.840)ğŸš€)
Batch 400/537: Loss=7.3781 (C:5.5107, R:0.0076, T:1.3168(w:0.840)ğŸš€)
Batch 425/537: Loss=7.3510 (C:5.4957, R:0.0076, T:1.2993(w:0.840)ğŸš€)
Batch 450/537: Loss=7.3012 (C:5.5783, R:0.0075, T:1.1550(w:0.840)ğŸš€)
Batch 475/537: Loss=7.5871 (C:5.5580, R:0.0076, T:1.5115(w:0.840)ğŸš€)
Batch 500/537: Loss=7.5109 (C:5.5059, R:0.0076, T:1.4835(w:0.840)ğŸš€)
Batch 525/537: Loss=7.2692 (C:5.5281, R:0.0076, T:1.1716(w:0.840)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2866

ğŸ“Š EPOCH 43 TRAINING SUMMARY:
  Total Loss: 7.3539
  Contrastive: 5.5142
  Reconstruction: 0.0076
  Topological: 1.2866 (weight: 0.840)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.1505
  Contrastive: 5.5035
  Reconstruction: 0.0069
  Topological: 7.0927 (weight: 0.840)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 43/100 COMPLETE (54.8s)
Train Loss: 7.3539 (C:5.5142, R:0.0076, T:1.2866)
Val Loss:   12.1505 (C:5.5035, R:0.0069, T:7.0927)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 44 | Batches: 537 | Topological Weight: 0.8600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3596 (C:5.4926, R:0.0076, T:1.2881(w:0.860)ğŸš€)
Batch  25/537: Loss=7.3129 (C:5.5135, R:0.0077, T:1.2026(w:0.860)ğŸš€)
Batch  50/537: Loss=7.3767 (C:5.4473, R:0.0076, T:1.3588(w:0.860)ğŸš€)
Batch  75/537: Loss=7.3380 (C:5.4522, R:0.0076, T:1.3083(w:0.860)ğŸš€)
Batch 100/537: Loss=7.3208 (C:5.4749, R:0.0076, T:1.2581(w:0.860)ğŸš€)
Batch 125/537: Loss=7.3175 (C:5.4390, R:0.0076, T:1.3024(w:0.860)ğŸš€)
Batch 150/537: Loss=7.3607 (C:5.5427, R:0.0076, T:1.2298(w:0.860)ğŸš€)
Batch 175/537: Loss=7.3902 (C:5.5840, R:0.0076, T:1.2149(w:0.860)ğŸš€)
Batch 200/537: Loss=7.3087 (C:5.4517, R:0.0076, T:1.2744(w:0.860)ğŸš€)
Batch 225/537: Loss=7.3201 (C:5.4575, R:0.0076, T:1.2808(w:0.860)ğŸš€)
Batch 250/537: Loss=7.4918 (C:5.6009, R:0.0075, T:1.3225(w:0.860)ğŸš€)
Batch 275/537: Loss=7.4343 (C:5.6034, R:0.0076, T:1.2502(w:0.860)ğŸš€)
Batch 300/537: Loss=7.4962 (C:5.5117, R:0.0076, T:1.4242(w:0.860)ğŸš€)
Batch 325/537: Loss=7.1915 (C:5.5164, R:0.0076, T:1.0666(w:0.860)ğŸš€)
Batch 350/537: Loss=7.2777 (C:5.4356, R:0.0076, T:1.2555(w:0.860)ğŸš€)
Batch 375/537: Loss=7.1851 (C:5.4588, R:0.0075, T:1.1332(w:0.860)ğŸš€)
Batch 400/537: Loss=7.4329 (C:5.5341, R:0.0076, T:1.3263(w:0.860)ğŸš€)
Batch 425/537: Loss=7.3845 (C:5.6010, R:0.0076, T:1.1951(w:0.860)ğŸš€)
Batch 450/537: Loss=7.3487 (C:5.5620, R:0.0076, T:1.1967(w:0.860)ğŸš€)
Batch 475/537: Loss=7.4827 (C:5.5603, R:0.0076, T:1.3547(w:0.860)ğŸš€)
Batch 500/537: Loss=7.4054 (C:5.5140, R:0.0076, T:1.3114(w:0.860)ğŸš€)
Batch 525/537: Loss=7.3439 (C:5.5030, R:0.0076, T:1.2533(w:0.860)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2725

ğŸ“Š EPOCH 44 TRAINING SUMMARY:
  Total Loss: 7.3613
  Contrastive: 5.5079
  Reconstruction: 0.0076
  Topological: 1.2725 (weight: 0.860)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.1331
  Contrastive: 5.5757
  Reconstruction: 0.0069
  Topological: 6.8253 (weight: 0.860)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 44/100 COMPLETE (46.3s)
Train Loss: 7.3613 (C:5.5079, R:0.0076, T:1.2725)
Val Loss:   12.1331 (C:5.5757, R:0.0069, T:6.8253)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 45 | Batches: 537 | Topological Weight: 0.8800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.4208 (C:5.5880, R:0.0076, T:1.2187(w:0.880)ğŸš€)
Batch  25/537: Loss=7.3887 (C:5.4882, R:0.0076, T:1.2933(w:0.880)ğŸš€)
Batch  50/537: Loss=7.2971 (C:5.4120, R:0.0076, T:1.2758(w:0.880)ğŸš€)
Batch  75/537: Loss=7.2148 (C:5.4152, R:0.0076, T:1.1833(w:0.880)ğŸš€)
Batch 100/537: Loss=7.2994 (C:5.5059, R:0.0075, T:1.1805(w:0.880)ğŸš€)
Batch 125/537: Loss=7.3686 (C:5.4561, R:0.0075, T:1.3168(w:0.880)ğŸš€)
Batch 150/537: Loss=7.4220 (C:5.5132, R:0.0076, T:1.3029(w:0.880)ğŸš€)
Batch 175/537: Loss=7.3963 (C:5.5767, R:0.0076, T:1.2038(w:0.880)ğŸš€)
Batch 200/537: Loss=7.3601 (C:5.5063, R:0.0076, T:1.2402(w:0.880)ğŸš€)
Batch 225/537: Loss=7.3999 (C:5.4672, R:0.0076, T:1.3342(w:0.880)ğŸš€)
Batch 250/537: Loss=7.4635 (C:5.5015, R:0.0076, T:1.3679(w:0.880)ğŸš€)
Batch 275/537: Loss=7.3626 (C:5.4985, R:0.0076, T:1.2585(w:0.880)ğŸš€)
Batch 300/537: Loss=7.3713 (C:5.5709, R:0.0076, T:1.1770(w:0.880)ğŸš€)
Batch 325/537: Loss=7.5748 (C:5.6374, R:0.0076, T:1.3414(w:0.880)ğŸš€)
Batch 350/537: Loss=7.3249 (C:5.5360, R:0.0076, T:1.1682(w:0.880)ğŸš€)
Batch 375/537: Loss=7.3009 (C:5.4241, R:0.0076, T:1.2727(w:0.880)ğŸš€)
Batch 400/537: Loss=7.5802 (C:5.5047, R:0.0076, T:1.4917(w:0.880)ğŸš€)
Batch 425/537: Loss=7.4467 (C:5.5219, R:0.0076, T:1.3265(w:0.880)ğŸš€)
Batch 450/537: Loss=7.4666 (C:5.7058, R:0.0076, T:1.1419(w:0.880)ğŸš€)
Batch 475/537: Loss=7.5482 (C:5.5218, R:0.0076, T:1.4352(w:0.880)ğŸš€)
Batch 500/537: Loss=7.2238 (C:5.4746, R:0.0076, T:1.1249(w:0.880)ğŸš€)
Batch 525/537: Loss=7.5310 (C:5.4910, R:0.0076, T:1.4552(w:0.880)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2645

ğŸ“Š EPOCH 45 TRAINING SUMMARY:
  Total Loss: 7.3864
  Contrastive: 5.5145
  Reconstruction: 0.0076
  Topological: 1.2645 (weight: 0.880)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.3308
  Contrastive: 5.5974
  Reconstruction: 0.0069
  Topological: 6.8694 (weight: 0.880)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 45/100 COMPLETE (48.7s)
Train Loss: 7.3864 (C:5.5145, R:0.0076, T:1.2645)
Val Loss:   12.3308 (C:5.5974, R:0.0069, T:6.8694)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 46
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.274 Â± 0.644
    Neg distances: 6.998 Â± 2.037
    Separation ratio: 1.64x
    Gap: -9.917
    âš ï¸  Moderate global separation

============================================================
EPOCH 46 | Batches: 537 | Topological Weight: 0.9000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2174 (C:5.4572, R:0.0076, T:1.1147(w:0.900)ğŸš€)
Batch  25/537: Loss=7.2583 (C:5.4584, R:0.0076, T:1.1518(w:0.900)ğŸš€)
Batch  50/537: Loss=7.5702 (C:5.5814, R:0.0077, T:1.3598(w:0.900)ğŸš€)
Batch  75/537: Loss=7.3998 (C:5.5162, R:0.0076, T:1.2445(w:0.900)ğŸš€)
Batch 100/537: Loss=7.3950 (C:5.4507, R:0.0076, T:1.3166(w:0.900)ğŸš€)
Batch 125/537: Loss=7.3452 (C:5.5166, R:0.0076, T:1.1914(w:0.900)ğŸš€)
Batch 150/537: Loss=7.4130 (C:5.5142, R:0.0076, T:1.2644(w:0.900)ğŸš€)
Batch 175/537: Loss=7.3619 (C:5.4766, R:0.0076, T:1.2481(w:0.900)ğŸš€)
Batch 200/537: Loss=7.5558 (C:5.5296, R:0.0076, T:1.4060(w:0.900)ğŸš€)
Batch 225/537: Loss=7.3350 (C:5.4599, R:0.0076, T:1.2376(w:0.900)ğŸš€)
Batch 250/537: Loss=7.2435 (C:5.4781, R:0.0076, T:1.1146(w:0.900)ğŸš€)
Batch 275/537: Loss=7.4441 (C:5.5530, R:0.0076, T:1.2548(w:0.900)ğŸš€)
Batch 300/537: Loss=7.3783 (C:5.4931, R:0.0076, T:1.2497(w:0.900)ğŸš€)
Batch 325/537: Loss=7.4901 (C:5.5862, R:0.0076, T:1.2706(w:0.900)ğŸš€)
Batch 350/537: Loss=7.2810 (C:5.5024, R:0.0076, T:1.1313(w:0.900)ğŸš€)
Batch 375/537: Loss=7.3266 (C:5.5559, R:0.0076, T:1.1238(w:0.900)ğŸš€)
Batch 400/537: Loss=7.5751 (C:5.6015, R:0.0076, T:1.3462(w:0.900)ğŸš€)
Batch 425/537: Loss=7.5049 (C:5.6293, R:0.0076, T:1.2413(w:0.900)ğŸš€)
Batch 450/537: Loss=7.4888 (C:5.6677, R:0.0075, T:1.1858(w:0.900)ğŸš€)
Batch 475/537: Loss=7.4808 (C:5.5692, R:0.0076, T:1.2764(w:0.900)ğŸš€)
Batch 500/537: Loss=7.3183 (C:5.4937, R:0.0076, T:1.1804(w:0.900)ğŸš€)
Batch 525/537: Loss=7.5966 (C:5.5868, R:0.0076, T:1.3931(w:0.900)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2505

ğŸ“Š EPOCH 46 TRAINING SUMMARY:
  Total Loss: 7.4164
  Contrastive: 5.5317
  Reconstruction: 0.0076
  Topological: 1.2505 (weight: 0.900)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.5795
  Contrastive: 5.6049
  Reconstruction: 0.0069
  Topological: 6.9831 (weight: 0.900)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 46/100 COMPLETE (56.8s)
Train Loss: 7.4164 (C:5.5317, R:0.0076, T:1.2505)
Val Loss:   12.5795 (C:5.6049, R:0.0069, T:6.9831)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 47 | Batches: 537 | Topological Weight: 0.9200
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2670 (C:5.4665, R:0.0076, T:1.1289(w:0.920)ğŸš€)
Batch  25/537: Loss=7.2295 (C:5.3673, R:0.0076, T:1.1967(w:0.920)ğŸš€)
Batch  50/537: Loss=7.3266 (C:5.4708, R:0.0076, T:1.1939(w:0.920)ğŸš€)
Batch  75/537: Loss=7.4454 (C:5.5594, R:0.0076, T:1.2289(w:0.920)ğŸš€)
Batch 100/537: Loss=7.4959 (C:5.5598, R:0.0076, T:1.2783(w:0.920)ğŸš€)
Batch 125/537: Loss=7.5256 (C:5.5997, R:0.0076, T:1.2637(w:0.920)ğŸš€)
Batch 150/537: Loss=7.4273 (C:5.6051, R:0.0076, T:1.1574(w:0.920)ğŸš€)
Batch 175/537: Loss=7.2631 (C:5.4768, R:0.0075, T:1.1215(w:0.920)ğŸš€)
Batch 200/537: Loss=7.4348 (C:5.5312, R:0.0076, T:1.2461(w:0.920)ğŸš€)
Batch 225/537: Loss=7.3772 (C:5.5283, R:0.0076, T:1.1834(w:0.920)ğŸš€)
Batch 250/537: Loss=7.3566 (C:5.5545, R:0.0076, T:1.1326(w:0.920)ğŸš€)
Batch 275/537: Loss=7.5434 (C:5.5763, R:0.0075, T:1.3176(w:0.920)ğŸš€)
Batch 300/537: Loss=7.4437 (C:5.5309, R:0.0076, T:1.2492(w:0.920)ğŸš€)
Batch 325/537: Loss=7.4557 (C:5.4538, R:0.0076, T:1.3504(w:0.920)ğŸš€)
Batch 350/537: Loss=7.3943 (C:5.5113, R:0.0076, T:1.2228(w:0.920)ğŸš€)
Batch 375/537: Loss=7.4399 (C:5.5217, R:0.0076, T:1.2593(w:0.920)ğŸš€)
Batch 400/537: Loss=7.3205 (C:5.5148, R:0.0076, T:1.1397(w:0.920)ğŸš€)
Batch 425/537: Loss=7.4780 (C:5.6564, R:0.0076, T:1.1574(w:0.920)ğŸš€)
Batch 450/537: Loss=7.5951 (C:5.5841, R:0.0076, T:1.3604(w:0.920)ğŸš€)
Batch 475/537: Loss=7.2711 (C:5.4958, R:0.0076, T:1.1026(w:0.920)ğŸš€)
Batch 500/537: Loss=7.5528 (C:5.5456, R:0.0076, T:1.3581(w:0.920)ğŸš€)
Batch 525/537: Loss=7.4178 (C:5.5331, R:0.0075, T:1.2287(w:0.920)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2368

ğŸ“Š EPOCH 47 TRAINING SUMMARY:
  Total Loss: 7.4294
  Contrastive: 5.5324
  Reconstruction: 0.0076
  Topological: 1.2368 (weight: 0.920)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.5071
  Contrastive: 5.6069
  Reconstruction: 0.0069
  Topological: 6.7504 (weight: 0.920)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 47/100 COMPLETE (50.1s)
Train Loss: 7.4294 (C:5.5324, R:0.0076, T:1.2368)
Val Loss:   12.5071 (C:5.6069, R:0.0069, T:6.7504)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 48 | Batches: 537 | Topological Weight: 0.9400
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3454 (C:5.4922, R:0.0076, T:1.1633(w:0.940)ğŸš€)
Batch  25/537: Loss=7.3600 (C:5.4331, R:0.0076, T:1.2420(w:0.940)ğŸš€)
Batch  50/537: Loss=7.2741 (C:5.4831, R:0.0075, T:1.1022(w:0.940)ğŸš€)
Batch  75/537: Loss=7.3494 (C:5.4852, R:0.0076, T:1.1696(w:0.940)ğŸš€)
Batch 100/537: Loss=7.4654 (C:5.6044, R:0.0076, T:1.1719(w:0.940)ğŸš€)
Batch 125/537: Loss=7.4579 (C:5.5502, R:0.0076, T:1.2242(w:0.940)ğŸš€)
Batch 150/537: Loss=7.4361 (C:5.5198, R:0.0076, T:1.2251(w:0.940)ğŸš€)
Batch 175/537: Loss=7.3595 (C:5.5558, R:0.0076, T:1.1142(w:0.940)ğŸš€)
Batch 200/537: Loss=7.4118 (C:5.4822, R:0.0076, T:1.2447(w:0.940)ğŸš€)
Batch 225/537: Loss=7.5357 (C:5.6040, R:0.0076, T:1.2481(w:0.940)ğŸš€)
Batch 250/537: Loss=7.4801 (C:5.5527, R:0.0076, T:1.2412(w:0.940)ğŸš€)
Batch 275/537: Loss=7.3308 (C:5.4413, R:0.0076, T:1.2026(w:0.940)ğŸš€)
Batch 300/537: Loss=7.6265 (C:5.5520, R:0.0076, T:1.3982(w:0.940)ğŸš€)
Batch 325/537: Loss=7.3742 (C:5.5073, R:0.0076, T:1.1806(w:0.940)ğŸš€)
Batch 350/537: Loss=7.4454 (C:5.5578, R:0.0076, T:1.1991(w:0.940)ğŸš€)
Batch 375/537: Loss=7.5854 (C:5.5929, R:0.0076, T:1.3124(w:0.940)ğŸš€)
Batch 400/537: Loss=7.4510 (C:5.4981, R:0.0076, T:1.2682(w:0.940)ğŸš€)
Batch 425/537: Loss=7.4678 (C:5.6193, R:0.0076, T:1.1589(w:0.940)ğŸš€)
Batch 450/537: Loss=7.4063 (C:5.5695, R:0.0076, T:1.1429(w:0.940)ğŸš€)
Batch 475/537: Loss=7.5080 (C:5.5196, R:0.0076, T:1.3053(w:0.940)ğŸš€)
Batch 500/537: Loss=7.3649 (C:5.5715, R:0.0076, T:1.0987(w:0.940)ğŸš€)
Batch 525/537: Loss=7.4895 (C:5.5884, R:0.0076, T:1.2117(w:0.940)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2273

ğŸ“Š EPOCH 48 TRAINING SUMMARY:
  Total Loss: 7.4441
  Contrastive: 5.5310
  Reconstruction: 0.0076
  Topological: 1.2273 (weight: 0.940)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.5819
  Contrastive: 5.5678
  Reconstruction: 0.0069
  Topological: 6.7285 (weight: 0.940)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 48/100 COMPLETE (48.8s)
Train Loss: 7.4441 (C:5.5310, R:0.0076, T:1.2273)
Val Loss:   12.5819 (C:5.5678, R:0.0069, T:6.7285)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 49
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.285 Â± 0.633
    Neg distances: 7.056 Â± 2.092
    Separation ratio: 1.65x
    Gap: -10.531
    âš ï¸  Moderate global separation

============================================================
EPOCH 49 | Batches: 537 | Topological Weight: 0.9600
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3442 (C:5.5072, R:0.0076, T:1.1249(w:0.960)ğŸš€)
Batch  25/537: Loss=7.5973 (C:5.5313, R:0.0076, T:1.3607(w:0.960)ğŸš€)
Batch  50/537: Loss=7.4615 (C:5.5321, R:0.0076, T:1.2223(w:0.960)ğŸš€)
Batch  75/537: Loss=7.4620 (C:5.4680, R:0.0076, T:1.2831(w:0.960)ğŸš€)
Batch 100/537: Loss=7.3186 (C:5.4609, R:0.0076, T:1.1423(w:0.960)ğŸš€)
Batch 125/537: Loss=7.5949 (C:5.4603, R:0.0075, T:1.4373(w:0.960)ğŸš€)
Batch 150/537: Loss=7.3906 (C:5.4475, R:0.0076, T:1.2282(w:0.960)ğŸš€)
Batch 175/537: Loss=7.4708 (C:5.4099, R:0.0076, T:1.3543(w:0.960)ğŸš€)
Batch 200/537: Loss=7.4588 (C:5.4871, R:0.0077, T:1.2561(w:0.960)ğŸš€)
Batch 225/537: Loss=7.3729 (C:5.5127, R:0.0076, T:1.1469(w:0.960)ğŸš€)
Batch 250/537: Loss=7.4467 (C:5.5512, R:0.0076, T:1.1851(w:0.960)ğŸš€)
Batch 275/537: Loss=7.3361 (C:5.3991, R:0.0076, T:1.2290(w:0.960)ğŸš€)
Batch 300/537: Loss=7.3498 (C:5.4655, R:0.0076, T:1.1693(w:0.960)ğŸš€)
Batch 325/537: Loss=7.3813 (C:5.5176, R:0.0076, T:1.1454(w:0.960)ğŸš€)
Batch 350/537: Loss=7.2997 (C:5.4733, R:0.0076, T:1.1151(w:0.960)ğŸš€)
Batch 375/537: Loss=7.6451 (C:5.5493, R:0.0076, T:1.3962(w:0.960)ğŸš€)
Batch 400/537: Loss=7.5253 (C:5.5476, R:0.0076, T:1.2699(w:0.960)ğŸš€)
Batch 425/537: Loss=7.4757 (C:5.5265, R:0.0076, T:1.2437(w:0.960)ğŸš€)
Batch 450/537: Loss=7.5296 (C:5.4989, R:0.0076, T:1.3187(w:0.960)ğŸš€)
Batch 475/537: Loss=7.4266 (C:5.4882, R:0.0076, T:1.2246(w:0.960)ğŸš€)
Batch 500/537: Loss=7.2775 (C:5.4839, R:0.0076, T:1.0775(w:0.960)ğŸš€)
Batch 525/537: Loss=7.5093 (C:5.5728, R:0.0076, T:1.2218(w:0.960)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2226

ğŸ“Š EPOCH 49 TRAINING SUMMARY:
  Total Loss: 7.4438
  Contrastive: 5.5105
  Reconstruction: 0.0076
  Topological: 1.2226 (weight: 0.960)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.8086
  Contrastive: 5.5666
  Reconstruction: 0.0069
  Topological: 6.8264 (weight: 0.960)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 49/100 COMPLETE (54.4s)
Train Loss: 7.4438 (C:5.5105, R:0.0076, T:1.2226)
Val Loss:   12.8086 (C:5.5666, R:0.0069, T:6.8264)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 50 | Batches: 537 | Topological Weight: 0.9800
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.4932 (C:5.5305, R:0.0076, T:1.2245(w:0.980)ğŸš€)
Batch  25/537: Loss=7.5656 (C:5.6095, R:0.0076, T:1.2210(w:0.980)ğŸš€)
Batch  50/537: Loss=7.3274 (C:5.5348, R:0.0076, T:1.0528(w:0.980)ğŸš€)
Batch  75/537: Loss=7.4387 (C:5.5145, R:0.0076, T:1.1858(w:0.980)ğŸš€)
Batch 100/537: Loss=7.5792 (C:5.5035, R:0.0076, T:1.3461(w:0.980)ğŸš€)
Batch 125/537: Loss=7.5898 (C:5.5608, R:0.0077, T:1.2859(w:0.980)ğŸš€)
Batch 150/537: Loss=7.6260 (C:5.5343, R:0.0076, T:1.3581(w:0.980)ğŸš€)
Batch 175/537: Loss=7.3848 (C:5.5251, R:0.0075, T:1.1287(w:0.980)ğŸš€)
Batch 200/537: Loss=7.4464 (C:5.5799, R:0.0076, T:1.1295(w:0.980)ğŸš€)
Batch 225/537: Loss=7.6080 (C:5.5376, R:0.0076, T:1.3362(w:0.980)ğŸš€)
Batch 250/537: Loss=7.3952 (C:5.5358, R:0.0075, T:1.1284(w:0.980)ğŸš€)
Batch 275/537: Loss=7.6781 (C:5.5227, R:0.0076, T:1.4254(w:0.980)ğŸš€)
Batch 300/537: Loss=7.4566 (C:5.4859, R:0.0076, T:1.2342(w:0.980)ğŸš€)
Batch 325/537: Loss=7.5278 (C:5.4983, R:0.0076, T:1.2953(w:0.980)ğŸš€)
Batch 350/537: Loss=7.5222 (C:5.5508, R:0.0076, T:1.2352(w:0.980)ğŸš€)
Batch 375/537: Loss=7.5278 (C:5.4575, R:0.0076, T:1.3328(w:0.980)ğŸš€)
Batch 400/537: Loss=7.5854 (C:5.5400, R:0.0076, T:1.3130(w:0.980)ğŸš€)
Batch 425/537: Loss=7.4299 (C:5.5660, R:0.0076, T:1.1243(w:0.980)ğŸš€)
Batch 450/537: Loss=7.5875 (C:5.5981, R:0.0076, T:1.2551(w:0.980)ğŸš€)
Batch 475/537: Loss=7.5896 (C:5.5632, R:0.0077, T:1.2866(w:0.980)ğŸš€)
Batch 500/537: Loss=7.4645 (C:5.5573, R:0.0076, T:1.1691(w:0.980)ğŸš€)
Batch 525/537: Loss=7.5037 (C:5.5007, R:0.0077, T:1.2613(w:0.980)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2117

ğŸ“Š EPOCH 50 TRAINING SUMMARY:
  Total Loss: 7.4645
  Contrastive: 5.5178
  Reconstruction: 0.0076
  Topological: 1.2117 (weight: 0.980)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.8186
  Contrastive: 5.5963
  Reconstruction: 0.0069
  Topological: 6.6666 (weight: 0.980)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 50/100 COMPLETE (46.1s)
Train Loss: 7.4645 (C:5.5178, R:0.0076, T:1.2117)
Val Loss:   12.8186 (C:5.5963, R:0.0069, T:6.6666)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 51 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.4602 (C:5.5224, R:0.0075, T:1.1831(w:1.000)ğŸš€)
Batch  25/537: Loss=7.3917 (C:5.4541, R:0.0076, T:1.1761(w:1.000)ğŸš€)
Batch  50/537: Loss=7.3570 (C:5.4538, R:0.0076, T:1.1459(w:1.000)ğŸš€)
Batch  75/537: Loss=7.2808 (C:5.4935, R:0.0075, T:1.0351(w:1.000)ğŸš€)
Batch 100/537: Loss=7.3714 (C:5.5058, R:0.0076, T:1.1058(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4209 (C:5.5447, R:0.0076, T:1.1194(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3785 (C:5.4896, R:0.0076, T:1.1295(w:1.000)ğŸš€)
Batch 175/537: Loss=7.2332 (C:5.4453, R:0.0076, T:1.0287(w:1.000)ğŸš€)
Batch 200/537: Loss=7.4443 (C:5.5248, R:0.0076, T:1.1611(w:1.000)ğŸš€)
Batch 225/537: Loss=7.6015 (C:5.4628, R:0.0076, T:1.3761(w:1.000)ğŸš€)
Batch 250/537: Loss=7.3398 (C:5.4874, R:0.0076, T:1.0911(w:1.000)ğŸš€)
Batch 275/537: Loss=7.5816 (C:5.4966, R:0.0076, T:1.3227(w:1.000)ğŸš€)
Batch 300/537: Loss=7.5798 (C:5.4996, R:0.0076, T:1.3179(w:1.000)ğŸš€)
Batch 325/537: Loss=7.4795 (C:5.5117, R:0.0076, T:1.2039(w:1.000)ğŸš€)
Batch 350/537: Loss=7.5397 (C:5.5943, R:0.0075, T:1.1911(w:1.000)ğŸš€)
Batch 375/537: Loss=7.4049 (C:5.5155, R:0.0076, T:1.1307(w:1.000)ğŸš€)
Batch 400/537: Loss=7.3610 (C:5.5381, R:0.0075, T:1.0686(w:1.000)ğŸš€)
Batch 425/537: Loss=7.3864 (C:5.5360, R:0.0076, T:1.0905(w:1.000)ğŸš€)
Batch 450/537: Loss=7.5278 (C:5.6139, R:0.0076, T:1.1568(w:1.000)ğŸš€)
Batch 475/537: Loss=7.4424 (C:5.5014, R:0.0076, T:1.1826(w:1.000)ğŸš€)
Batch 500/537: Loss=7.4662 (C:5.5192, R:0.0076, T:1.1895(w:1.000)ğŸš€)
Batch 525/537: Loss=7.4747 (C:5.5176, R:0.0076, T:1.1959(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.2014

ğŸ“Š EPOCH 51 TRAINING SUMMARY:
  Total Loss: 7.4781
  Contrastive: 5.5171
  Reconstruction: 0.0076
  Topological: 1.2014 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 13.1107
  Contrastive: 5.5929
  Reconstruction: 0.0069
  Topological: 6.8277 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 51/100 COMPLETE (46.9s)
Train Loss: 7.4781 (C:5.5171, R:0.0076, T:1.2014)
Val Loss:   13.1107 (C:5.5929, R:0.0069, T:6.8277)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 52
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.274 Â± 0.621
    Neg distances: 7.034 Â± 2.096
    Separation ratio: 1.65x
    Gap: -10.003
    âš ï¸  Moderate global separation

============================================================
EPOCH 52 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.5411 (C:5.5876, R:0.0076, T:1.1937(w:1.000)ğŸš€)
Batch  25/537: Loss=7.5762 (C:5.4493, R:0.0076, T:1.3704(w:1.000)ğŸš€)
Batch  50/537: Loss=7.4416 (C:5.5543, R:0.0076, T:1.1269(w:1.000)ğŸš€)
Batch  75/537: Loss=7.6395 (C:5.5062, R:0.0076, T:1.3759(w:1.000)ğŸš€)
Batch 100/537: Loss=7.3950 (C:5.5969, R:0.0076, T:1.0400(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4343 (C:5.4352, R:0.0076, T:1.2394(w:1.000)ğŸš€)
Batch 150/537: Loss=7.5400 (C:5.5190, R:0.0077, T:1.2557(w:1.000)ğŸš€)
Batch 175/537: Loss=7.4234 (C:5.4452, R:0.0076, T:1.2198(w:1.000)ğŸš€)
Batch 200/537: Loss=7.4783 (C:5.5260, R:0.0077, T:1.1854(w:1.000)ğŸš€)
Batch 225/537: Loss=7.2159 (C:5.4439, R:0.0076, T:1.0114(w:1.000)ğŸš€)
Batch 250/537: Loss=7.4601 (C:5.4846, R:0.0076, T:1.2166(w:1.000)ğŸš€)
Batch 275/537: Loss=7.5968 (C:5.5286, R:0.0076, T:1.3088(w:1.000)ğŸš€)
Batch 300/537: Loss=7.6136 (C:5.6075, R:0.0076, T:1.2470(w:1.000)ğŸš€)
Batch 325/537: Loss=7.5179 (C:5.6862, R:0.0076, T:1.0702(w:1.000)ğŸš€)
Batch 350/537: Loss=7.5644 (C:5.4521, R:0.0076, T:1.3476(w:1.000)ğŸš€)
Batch 375/537: Loss=7.4476 (C:5.4886, R:0.0076, T:1.2011(w:1.000)ğŸš€)
Batch 400/537: Loss=7.2588 (C:5.4767, R:0.0076, T:1.0252(w:1.000)ğŸš€)
Batch 425/537: Loss=7.7619 (C:5.4753, R:0.0076, T:1.5254(w:1.000)ğŸš€)
Batch 450/537: Loss=7.5628 (C:5.4088, R:0.0076, T:1.3961(w:1.000)ğŸš€)
Batch 475/537: Loss=7.5170 (C:5.6038, R:0.0076, T:1.1535(w:1.000)ğŸš€)
Batch 500/537: Loss=7.6248 (C:5.5739, R:0.0076, T:1.2880(w:1.000)ğŸš€)
Batch 525/537: Loss=7.4757 (C:5.6356, R:0.0076, T:1.0834(w:1.000)ğŸš€)

ğŸ“Š EPOCH 52 TRAINING SUMMARY:
  Total Loss: 7.4781
  Contrastive: 5.5138
  Reconstruction: 0.0076
  Topological: 1.2045 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 13.0420
  Contrastive: 5.4940
  Reconstruction: 0.0069
  Topological: 6.8589 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 52/100 COMPLETE (55.6s)
Train Loss: 7.4781 (C:5.5138, R:0.0076, T:1.2045)
Val Loss:   13.0420 (C:5.4940, R:0.0069, T:6.8589)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 53 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.6132 (C:5.4617, R:0.0076, T:1.3950(w:1.000)ğŸš€)
Batch  25/537: Loss=7.5382 (C:5.4942, R:0.0076, T:1.2833(w:1.000)ğŸš€)
Batch  50/537: Loss=7.3332 (C:5.5023, R:0.0075, T:1.0767(w:1.000)ğŸš€)
Batch  75/537: Loss=7.4985 (C:5.5114, R:0.0076, T:1.2317(w:1.000)ğŸš€)
Batch 100/537: Loss=7.3868 (C:5.4489, R:0.0076, T:1.1773(w:1.000)ğŸš€)
Batch 125/537: Loss=7.3590 (C:5.4649, R:0.0076, T:1.1377(w:1.000)ğŸš€)
Batch 150/537: Loss=7.4061 (C:5.4561, R:0.0076, T:1.1921(w:1.000)ğŸš€)
Batch 175/537: Loss=7.4245 (C:5.4903, R:0.0076, T:1.1718(w:1.000)ğŸš€)
Batch 200/537: Loss=7.5875 (C:5.5629, R:0.0076, T:1.2651(w:1.000)ğŸš€)
Batch 225/537: Loss=7.4824 (C:5.6240, R:0.0075, T:1.1043(w:1.000)ğŸš€)
Batch 250/537: Loss=7.4124 (C:5.4681, R:0.0076, T:1.1888(w:1.000)ğŸš€)
Batch 275/537: Loss=7.3662 (C:5.4874, R:0.0076, T:1.1151(w:1.000)ğŸš€)
Batch 300/537: Loss=7.4015 (C:5.5230, R:0.0076, T:1.1214(w:1.000)ğŸš€)
Batch 325/537: Loss=7.4188 (C:5.4318, R:0.0076, T:1.2258(w:1.000)ğŸš€)
Batch 350/537: Loss=7.3607 (C:5.4564, R:0.0076, T:1.1489(w:1.000)ğŸš€)
Batch 375/537: Loss=7.4600 (C:5.5715, R:0.0075, T:1.1344(w:1.000)ğŸš€)
Batch 400/537: Loss=7.4493 (C:5.5163, R:0.0076, T:1.1760(w:1.000)ğŸš€)
Batch 425/537: Loss=7.4110 (C:5.4947, R:0.0076, T:1.1550(w:1.000)ğŸš€)
Batch 450/537: Loss=7.3628 (C:5.5040, R:0.0075, T:1.1039(w:1.000)ğŸš€)
Batch 475/537: Loss=7.4664 (C:5.4698, R:0.0076, T:1.2376(w:1.000)ğŸš€)
Batch 500/537: Loss=7.5148 (C:5.5253, R:0.0076, T:1.2328(w:1.000)ğŸš€)
Batch 525/537: Loss=7.6457 (C:5.4725, R:0.0076, T:1.4143(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.1991

ğŸ“Š EPOCH 53 TRAINING SUMMARY:
  Total Loss: 7.4651
  Contrastive: 5.5066
  Reconstruction: 0.0076
  Topological: 1.1991 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.9336
  Contrastive: 5.5990
  Reconstruction: 0.0069
  Topological: 6.6456 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 53/100 COMPLETE (47.3s)
Train Loss: 7.4651 (C:5.5066, R:0.0076, T:1.1991)
Val Loss:   12.9336 (C:5.5990, R:0.0069, T:6.6456)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 54 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3114 (C:5.5465, R:0.0076, T:1.0078(w:1.000)ğŸš€)
Batch  25/537: Loss=7.4612 (C:5.5078, R:0.0076, T:1.1922(w:1.000)ğŸš€)
Batch  50/537: Loss=7.4197 (C:5.3992, R:0.0076, T:1.2622(w:1.000)ğŸš€)
Batch  75/537: Loss=7.4141 (C:5.5966, R:0.0076, T:1.0603(w:1.000)ğŸš€)
Batch 100/537: Loss=7.4410 (C:5.5585, R:0.0076, T:1.1244(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4681 (C:5.3752, R:0.0076, T:1.3337(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3662 (C:5.4220, R:0.0076, T:1.1802(w:1.000)ğŸš€)
Batch 175/537: Loss=7.4682 (C:5.5292, R:0.0076, T:1.1790(w:1.000)ğŸš€)
Batch 200/537: Loss=7.5259 (C:5.5656, R:0.0076, T:1.1998(w:1.000)ğŸš€)
Batch 225/537: Loss=7.3959 (C:5.5173, R:0.0076, T:1.1205(w:1.000)ğŸš€)
Batch 250/537: Loss=7.4647 (C:5.4785, R:0.0077, T:1.2193(w:1.000)ğŸš€)
Batch 275/537: Loss=7.2886 (C:5.5583, R:0.0075, T:0.9770(w:1.000)ğŸ‰)
Batch 300/537: Loss=7.5561 (C:5.5084, R:0.0076, T:1.2866(w:1.000)ğŸš€)
Batch 325/537: Loss=7.3317 (C:5.4827, R:0.0076, T:1.0904(w:1.000)ğŸš€)
Batch 350/537: Loss=7.4591 (C:5.4231, R:0.0076, T:1.2774(w:1.000)ğŸš€)
Batch 375/537: Loss=7.3799 (C:5.4982, R:0.0076, T:1.1244(w:1.000)ğŸš€)
Batch 400/537: Loss=7.4915 (C:5.5656, R:0.0076, T:1.1664(w:1.000)ğŸš€)
Batch 425/537: Loss=7.4391 (C:5.5474, R:0.0076, T:1.1344(w:1.000)ğŸš€)
Batch 450/537: Loss=7.5045 (C:5.5200, R:0.0076, T:1.2269(w:1.000)ğŸš€)
Batch 475/537: Loss=7.4129 (C:5.4838, R:0.0076, T:1.1658(w:1.000)ğŸš€)
Batch 500/537: Loss=7.4383 (C:5.5032, R:0.0075, T:1.1802(w:1.000)ğŸš€)
Batch 525/537: Loss=7.5241 (C:5.5350, R:0.0076, T:1.2289(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.1987

ğŸ“Š EPOCH 54 TRAINING SUMMARY:
  Total Loss: 7.4619
  Contrastive: 5.5040
  Reconstruction: 0.0076
  Topological: 1.1987 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.7177
  Contrastive: 5.6476
  Reconstruction: 0.0069
  Topological: 6.3819 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 54/100 COMPLETE (46.3s)
Train Loss: 7.4619 (C:5.5040, R:0.0076, T:1.1987)
Val Loss:   12.7177 (C:5.6476, R:0.0069, T:6.3819)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 55
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.313 Â± 0.576
    Neg distances: 6.868 Â± 1.911
    Separation ratio: 1.59x
    Gap: -9.361
    âš ï¸  Moderate global separation

============================================================
EPOCH 55 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.5633 (C:5.6357, R:0.0076, T:1.1661(w:1.000)ğŸš€)
Batch  25/537: Loss=7.6546 (C:5.6078, R:0.0077, T:1.2816(w:1.000)ğŸš€)
Batch  50/537: Loss=7.5120 (C:5.5711, R:0.0076, T:1.1822(w:1.000)ğŸš€)
Batch  75/537: Loss=7.5565 (C:5.5349, R:0.0077, T:1.2544(w:1.000)ğŸš€)
Batch 100/537: Loss=7.6291 (C:5.6243, R:0.0076, T:1.2442(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4241 (C:5.5801, R:0.0076, T:1.0826(w:1.000)ğŸš€)
Batch 150/537: Loss=7.4389 (C:5.6256, R:0.0076, T:1.0505(w:1.000)ğŸš€)
Batch 175/537: Loss=7.4790 (C:5.6127, R:0.0076, T:1.1030(w:1.000)ğŸš€)
Batch 200/537: Loss=7.5611 (C:5.6059, R:0.0076, T:1.1967(w:1.000)ğŸš€)
Batch 225/537: Loss=7.6566 (C:5.6394, R:0.0076, T:1.2572(w:1.000)ğŸš€)
Batch 250/537: Loss=7.5859 (C:5.7009, R:0.0076, T:1.1284(w:1.000)ğŸš€)
Batch 275/537: Loss=7.5043 (C:5.6617, R:0.0076, T:1.0807(w:1.000)ğŸš€)
Batch 300/537: Loss=7.5041 (C:5.5537, R:0.0076, T:1.1923(w:1.000)ğŸš€)
Batch 325/537: Loss=7.4191 (C:5.5059, R:0.0076, T:1.1499(w:1.000)ğŸš€)
Batch 350/537: Loss=7.5354 (C:5.5607, R:0.0076, T:1.2168(w:1.000)ğŸš€)
Batch 375/537: Loss=7.3960 (C:5.5843, R:0.0077, T:1.0447(w:1.000)ğŸš€)
Batch 400/537: Loss=7.7399 (C:5.6665, R:0.0076, T:1.3147(w:1.000)ğŸš€)
Batch 425/537: Loss=7.7223 (C:5.7064, R:0.0076, T:1.2543(w:1.000)ğŸš€)
Batch 450/537: Loss=7.4816 (C:5.6147, R:0.0076, T:1.1103(w:1.000)ğŸš€)
Batch 475/537: Loss=7.5303 (C:5.5901, R:0.0076, T:1.1808(w:1.000)ğŸš€)
Batch 500/537: Loss=7.5181 (C:5.5901, R:0.0076, T:1.1709(w:1.000)ğŸš€)
Batch 525/537: Loss=7.6200 (C:5.7151, R:0.0076, T:1.1464(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.1946

ğŸ“Š EPOCH 55 TRAINING SUMMARY:
  Total Loss: 7.5685
  Contrastive: 5.6148
  Reconstruction: 0.0076
  Topological: 1.1946 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.9265
  Contrastive: 5.7001
  Reconstruction: 0.0069
  Topological: 6.5381 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 55/100 COMPLETE (54.0s)
Train Loss: 7.5685 (C:5.6148, R:0.0076, T:1.1946)
Val Loss:   12.9265 (C:5.7001, R:0.0069, T:6.5381)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 56 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.5401 (C:5.6316, R:0.0076, T:1.1471(w:1.000)ğŸš€)
Batch  25/537: Loss=7.3129 (C:5.5315, R:0.0076, T:1.0183(w:1.000)ğŸš€)
Batch  50/537: Loss=7.5534 (C:5.6216, R:0.0075, T:1.1814(w:1.000)ğŸš€)
Batch  75/537: Loss=7.5316 (C:5.6009, R:0.0076, T:1.1703(w:1.000)ğŸš€)
Batch 100/537: Loss=7.5693 (C:5.6763, R:0.0076, T:1.1333(w:1.000)ğŸš€)
Batch 125/537: Loss=7.5714 (C:5.6397, R:0.0075, T:1.1769(w:1.000)ğŸš€)
Batch 150/537: Loss=7.6057 (C:5.5736, R:0.0076, T:1.2699(w:1.000)ğŸš€)
Batch 175/537: Loss=7.4153 (C:5.5035, R:0.0076, T:1.1509(w:1.000)ğŸš€)
Batch 200/537: Loss=7.6389 (C:5.6953, R:0.0076, T:1.1855(w:1.000)ğŸš€)
Batch 225/537: Loss=7.5149 (C:5.6499, R:0.0076, T:1.1052(w:1.000)ğŸš€)
Batch 250/537: Loss=7.4766 (C:5.5996, R:0.0076, T:1.1202(w:1.000)ğŸš€)
Batch 275/537: Loss=7.5840 (C:5.6551, R:0.0076, T:1.1676(w:1.000)ğŸš€)
Batch 300/537: Loss=7.5585 (C:5.6198, R:0.0076, T:1.1805(w:1.000)ğŸš€)
Batch 325/537: Loss=7.5178 (C:5.5010, R:0.0076, T:1.2576(w:1.000)ğŸš€)
Batch 350/537: Loss=7.5407 (C:5.6412, R:0.0076, T:1.1344(w:1.000)ğŸš€)
Batch 375/537: Loss=7.5853 (C:5.5947, R:0.0076, T:1.2288(w:1.000)ğŸš€)
Batch 400/537: Loss=7.6290 (C:5.6487, R:0.0076, T:1.2195(w:1.000)ğŸš€)
Batch 425/537: Loss=7.7671 (C:5.6824, R:0.0075, T:1.3330(w:1.000)ğŸš€)
Batch 450/537: Loss=7.4524 (C:5.5698, R:0.0076, T:1.1197(w:1.000)ğŸš€)
Batch 475/537: Loss=7.4843 (C:5.5669, R:0.0076, T:1.1562(w:1.000)ğŸš€)
Batch 500/537: Loss=7.6255 (C:5.6309, R:0.0076, T:1.2367(w:1.000)ğŸš€)
Batch 525/537: Loss=7.3577 (C:5.5385, R:0.0075, T:1.0683(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.1913

ğŸ“Š EPOCH 56 TRAINING SUMMARY:
  Total Loss: 7.5565
  Contrastive: 5.6061
  Reconstruction: 0.0076
  Topological: 1.1913 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 13.0878
  Contrastive: 5.6700
  Reconstruction: 0.0069
  Topological: 6.7290 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 56/100 COMPLETE (46.3s)
Train Loss: 7.5565 (C:5.6061, R:0.0076, T:1.1913)
Val Loss:   13.0878 (C:5.6700, R:0.0069, T:6.7290)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 57 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.6255 (C:5.6326, R:0.0076, T:1.2373(w:1.000)ğŸš€)
Batch  25/537: Loss=7.6145 (C:5.4858, R:0.0076, T:1.3692(w:1.000)ğŸš€)
Batch  50/537: Loss=7.4072 (C:5.4720, R:0.0076, T:1.1768(w:1.000)ğŸš€)
Batch  75/537: Loss=7.5511 (C:5.6750, R:0.0076, T:1.1170(w:1.000)ğŸš€)
Batch 100/537: Loss=7.4891 (C:5.5727, R:0.0076, T:1.1549(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4951 (C:5.5016, R:0.0076, T:1.2324(w:1.000)ğŸš€)
Batch 150/537: Loss=7.6327 (C:5.6013, R:0.0076, T:1.2693(w:1.000)ğŸš€)
Batch 175/537: Loss=7.5010 (C:5.5862, R:0.0076, T:1.1535(w:1.000)ğŸš€)
Batch 200/537: Loss=7.5318 (C:5.5335, R:0.0076, T:1.2356(w:1.000)ğŸš€)
Batch 225/537: Loss=7.5207 (C:5.6105, R:0.0076, T:1.1536(w:1.000)ğŸš€)
Batch 250/537: Loss=7.6964 (C:5.6479, R:0.0076, T:1.2879(w:1.000)ğŸš€)
Batch 275/537: Loss=7.5307 (C:5.5881, R:0.0077, T:1.1771(w:1.000)ğŸš€)
Batch 300/537: Loss=7.5486 (C:5.7625, R:0.0076, T:1.0283(w:1.000)ğŸš€)
Batch 325/537: Loss=7.6231 (C:5.6058, R:0.0076, T:1.2573(w:1.000)ğŸš€)
Batch 350/537: Loss=7.6151 (C:5.5372, R:0.0076, T:1.3182(w:1.000)ğŸš€)
Batch 375/537: Loss=7.5392 (C:5.6003, R:0.0075, T:1.1846(w:1.000)ğŸš€)
Batch 400/537: Loss=7.5393 (C:5.6199, R:0.0076, T:1.1606(w:1.000)ğŸš€)
Batch 425/537: Loss=7.5505 (C:5.5894, R:0.0075, T:1.2062(w:1.000)ğŸš€)
Batch 450/537: Loss=7.4650 (C:5.5526, R:0.0075, T:1.1608(w:1.000)ğŸš€)
Batch 475/537: Loss=7.5599 (C:5.5899, R:0.0076, T:1.2148(w:1.000)ğŸš€)
Batch 500/537: Loss=7.6449 (C:5.6829, R:0.0076, T:1.2028(w:1.000)ğŸš€)
Batch 525/537: Loss=7.5278 (C:5.6856, R:0.0076, T:1.0867(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.1906

ğŸ“Š EPOCH 57 TRAINING SUMMARY:
  Total Loss: 7.5500
  Contrastive: 5.6005
  Reconstruction: 0.0076
  Topological: 1.1906 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.9350
  Contrastive: 5.7154
  Reconstruction: 0.0069
  Topological: 6.5307 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 57/100 COMPLETE (47.8s)
Train Loss: 7.5500 (C:5.6005, R:0.0076, T:1.1906)
Val Loss:   12.9350 (C:5.7154, R:0.0069, T:6.5307)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 58
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.341 Â± 0.630
    Neg distances: 7.030 Â± 2.041
    Separation ratio: 1.62x
    Gap: -10.095
    âš ï¸  Moderate global separation

============================================================
EPOCH 58 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3715 (C:5.5564, R:0.0076, T:1.0592(w:1.000)ğŸš€)
Batch  25/537: Loss=7.4363 (C:5.4118, R:0.0076, T:1.2658(w:1.000)ğŸš€)
Batch  50/537: Loss=7.7103 (C:5.5833, R:0.0076, T:1.3707(w:1.000)ğŸš€)
Batch  75/537: Loss=7.4360 (C:5.4895, R:0.0075, T:1.1927(w:1.000)ğŸš€)
Batch 100/537: Loss=7.3626 (C:5.4525, R:0.0076, T:1.1488(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4584 (C:5.4885, R:0.0076, T:1.2128(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3610 (C:5.5268, R:0.0076, T:1.0745(w:1.000)ğŸš€)
Batch 175/537: Loss=7.4796 (C:5.4813, R:0.0076, T:1.2403(w:1.000)ğŸš€)
Batch 200/537: Loss=7.4095 (C:5.5035, R:0.0076, T:1.1505(w:1.000)ğŸš€)
Batch 225/537: Loss=7.6921 (C:5.5518, R:0.0076, T:1.3803(w:1.000)ğŸš€)
Batch 250/537: Loss=7.5987 (C:5.6533, R:0.0075, T:1.1909(w:1.000)ğŸš€)
Batch 275/537: Loss=7.5101 (C:5.5031, R:0.0076, T:1.2515(w:1.000)ğŸš€)
Batch 300/537: Loss=7.4035 (C:5.5115, R:0.0076, T:1.1364(w:1.000)ğŸš€)
Batch 325/537: Loss=7.4441 (C:5.4877, R:0.0076, T:1.1989(w:1.000)ğŸš€)
Batch 350/537: Loss=7.3761 (C:5.4690, R:0.0076, T:1.1484(w:1.000)ğŸš€)
Batch 375/537: Loss=7.5254 (C:5.4721, R:0.0076, T:1.2930(w:1.000)ğŸš€)
Batch 400/537: Loss=7.4769 (C:5.4601, R:0.0076, T:1.2617(w:1.000)ğŸš€)
Batch 425/537: Loss=7.4892 (C:5.5461, R:0.0075, T:1.1895(w:1.000)ğŸš€)
Batch 450/537: Loss=7.5053 (C:5.5571, R:0.0076, T:1.1892(w:1.000)ğŸš€)
Batch 475/537: Loss=7.4423 (C:5.5014, R:0.0076, T:1.1777(w:1.000)ğŸš€)
Batch 500/537: Loss=7.6149 (C:5.5473, R:0.0076, T:1.3064(w:1.000)ğŸš€)
Batch 525/537: Loss=7.3819 (C:5.5318, R:0.0076, T:1.0922(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.1899

ğŸ“Š EPOCH 58 TRAINING SUMMARY:
  Total Loss: 7.4789
  Contrastive: 5.5302
  Reconstruction: 0.0076
  Topological: 1.1899 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 13.1024
  Contrastive: 5.5944
  Reconstruction: 0.0069
  Topological: 6.8188 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 58/100 COMPLETE (53.9s)
Train Loss: 7.4789 (C:5.5302, R:0.0076, T:1.1899)
Val Loss:   13.1024 (C:5.5944, R:0.0069, T:6.8188)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 59 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.4862 (C:5.5210, R:0.0076, T:1.2036(w:1.000)ğŸš€)
Batch  25/537: Loss=7.5611 (C:5.5780, R:0.0076, T:1.2256(w:1.000)ğŸš€)
Batch  50/537: Loss=7.2985 (C:5.4697, R:0.0076, T:1.0736(w:1.000)ğŸš€)
Batch  75/537: Loss=7.3225 (C:5.4679, R:0.0076, T:1.0930(w:1.000)ğŸš€)
Batch 100/537: Loss=7.6333 (C:5.5356, R:0.0076, T:1.3395(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4562 (C:5.5299, R:0.0076, T:1.1689(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3911 (C:5.5765, R:0.0076, T:1.0569(w:1.000)ğŸš€)
Batch 175/537: Loss=7.5089 (C:5.5343, R:0.0076, T:1.2112(w:1.000)ğŸš€)
Batch 200/537: Loss=7.4049 (C:5.4949, R:0.0076, T:1.1499(w:1.000)ğŸš€)
Batch 225/537: Loss=7.5855 (C:5.5361, R:0.0076, T:1.2910(w:1.000)ğŸš€)
Batch 250/537: Loss=7.4392 (C:5.4835, R:0.0077, T:1.1900(w:1.000)ğŸš€)
Batch 275/537: Loss=7.3557 (C:5.5078, R:0.0076, T:1.0912(w:1.000)ğŸš€)
Batch 300/537: Loss=7.4204 (C:5.5750, R:0.0076, T:1.0852(w:1.000)ğŸš€)
Batch 325/537: Loss=7.4863 (C:5.5346, R:0.0076, T:1.1916(w:1.000)ğŸš€)
Batch 350/537: Loss=7.4637 (C:5.5074, R:0.0075, T:1.2017(w:1.000)ğŸš€)
Batch 375/537: Loss=7.4135 (C:5.5253, R:0.0075, T:1.1361(w:1.000)ğŸš€)
Batch 400/537: Loss=7.5301 (C:5.6480, R:0.0076, T:1.1260(w:1.000)ğŸš€)
Batch 425/537: Loss=7.6049 (C:5.5665, R:0.0075, T:1.2844(w:1.000)ğŸš€)
Batch 450/537: Loss=7.5983 (C:5.4436, R:0.0076, T:1.3906(w:1.000)ğŸš€)
Batch 475/537: Loss=7.6909 (C:5.5994, R:0.0076, T:1.3283(w:1.000)ğŸš€)
Batch 500/537: Loss=7.7557 (C:5.5438, R:0.0076, T:1.4503(w:1.000)ğŸš€)
Batch 525/537: Loss=7.4221 (C:5.5934, R:0.0076, T:1.0691(w:1.000)ğŸš€)

ğŸ“Š EPOCH 59 TRAINING SUMMARY:
  Total Loss: 7.4712
  Contrastive: 5.5210
  Reconstruction: 0.0076
  Topological: 1.1912 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.7961
  Contrastive: 5.6286
  Reconstruction: 0.0069
  Topological: 6.4786 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 59/100 COMPLETE (48.5s)
Train Loss: 7.4712 (C:5.5210, R:0.0076, T:1.1912)
Val Loss:   12.7961 (C:5.6286, R:0.0069, T:6.4786)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 60 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.5137 (C:5.5471, R:0.0075, T:1.2137(w:1.000)ğŸš€)
Batch  25/537: Loss=7.4061 (C:5.4003, R:0.0076, T:1.2474(w:1.000)ğŸš€)
Batch  50/537: Loss=7.4083 (C:5.4779, R:0.0076, T:1.1745(w:1.000)ğŸš€)
Batch  75/537: Loss=7.4260 (C:5.4887, R:0.0076, T:1.1779(w:1.000)ğŸš€)
Batch 100/537: Loss=7.5546 (C:5.4574, R:0.0077, T:1.3263(w:1.000)ğŸš€)
Batch 125/537: Loss=7.5990 (C:5.5279, R:0.0076, T:1.3129(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3460 (C:5.5082, R:0.0076, T:1.0777(w:1.000)ğŸš€)
Batch 175/537: Loss=7.5078 (C:5.4481, R:0.0076, T:1.2986(w:1.000)ğŸš€)
Batch 200/537: Loss=7.5100 (C:5.5200, R:0.0076, T:1.2252(w:1.000)ğŸš€)
Batch 225/537: Loss=7.4561 (C:5.5325, R:0.0076, T:1.1685(w:1.000)ğŸš€)
Batch 250/537: Loss=7.5395 (C:5.4953, R:0.0076, T:1.2802(w:1.000)ğŸš€)
Batch 275/537: Loss=7.4194 (C:5.5485, R:0.0076, T:1.1100(w:1.000)ğŸš€)
Batch 300/537: Loss=7.5410 (C:5.5515, R:0.0076, T:1.2318(w:1.000)ğŸš€)
Batch 325/537: Loss=7.5432 (C:5.5402, R:0.0076, T:1.2453(w:1.000)ğŸš€)
Batch 350/537: Loss=7.5719 (C:5.6069, R:0.0076, T:1.2046(w:1.000)ğŸš€)
Batch 375/537: Loss=7.5941 (C:5.6158, R:0.0076, T:1.2143(w:1.000)ğŸš€)
Batch 400/537: Loss=7.3008 (C:5.4397, R:0.0077, T:1.0951(w:1.000)ğŸš€)
Batch 425/537: Loss=7.4486 (C:5.4855, R:0.0076, T:1.2033(w:1.000)ğŸš€)
Batch 450/537: Loss=7.4669 (C:5.5692, R:0.0077, T:1.1325(w:1.000)ğŸš€)
Batch 475/537: Loss=7.2528 (C:5.3973, R:0.0075, T:1.1024(w:1.000)ğŸš€)
Batch 500/537: Loss=7.4216 (C:5.5690, R:0.0076, T:1.0932(w:1.000)ğŸš€)
Batch 525/537: Loss=7.6056 (C:5.5993, R:0.0076, T:1.2448(w:1.000)ğŸš€)
ğŸ“ˆ New best topological loss: 1.1891

ğŸ“Š EPOCH 60 TRAINING SUMMARY:
  Total Loss: 7.4610
  Contrastive: 5.5126
  Reconstruction: 0.0076
  Topological: 1.1891 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 13.0772
  Contrastive: 5.6041
  Reconstruction: 0.0069
  Topological: 6.7834 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 60/100 COMPLETE (48.3s)
Train Loss: 7.4610 (C:5.5126, R:0.0076, T:1.1891)
Val Loss:   13.0772 (C:5.6041, R:0.0069, T:6.7834)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 61
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.256 Â± 0.586
    Neg distances: 7.123 Â± 2.170
    Separation ratio: 1.67x
    Gap: -9.655
    âš ï¸  Moderate global separation

============================================================
EPOCH 61 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2994 (C:5.4489, R:0.0076, T:1.0953(w:1.000)ğŸš€)
Batch  25/537: Loss=7.1321 (C:5.2592, R:0.0076, T:1.1114(w:1.000)ğŸš€)
Batch  50/537: Loss=7.4085 (C:5.3666, R:0.0076, T:1.2854(w:1.000)ğŸš€)
Batch  75/537: Loss=7.3893 (C:5.3876, R:0.0076, T:1.2457(w:1.000)ğŸš€)
Batch 100/537: Loss=7.3435 (C:5.3881, R:0.0076, T:1.1940(w:1.000)ğŸš€)
Batch 125/537: Loss=7.4672 (C:5.3294, R:0.0076, T:1.3773(w:1.000)ğŸš€)
Batch 150/537: Loss=7.2213 (C:5.4053, R:0.0075, T:1.0612(w:1.000)ğŸš€)
Batch 175/537: Loss=7.4526 (C:5.4125, R:0.0076, T:1.2816(w:1.000)ğŸš€)
Batch 200/537: Loss=7.2507 (C:5.3951, R:0.0076, T:1.0953(w:1.000)ğŸš€)
Batch 225/537: Loss=7.3781 (C:5.4038, R:0.0076, T:1.2162(w:1.000)ğŸš€)
Batch 250/537: Loss=7.3622 (C:5.3928, R:0.0076, T:1.2046(w:1.000)ğŸš€)
Batch 275/537: Loss=7.4717 (C:5.4711, R:0.0076, T:1.2448(w:1.000)ğŸš€)
Batch 300/537: Loss=7.3164 (C:5.4088, R:0.0075, T:1.1551(w:1.000)ğŸš€)
Batch 325/537: Loss=7.3511 (C:5.4042, R:0.0076, T:1.1825(w:1.000)ğŸš€)
Batch 350/537: Loss=7.3382 (C:5.4309, R:0.0076, T:1.1520(w:1.000)ğŸš€)
Batch 375/537: Loss=7.4389 (C:5.5070, R:0.0075, T:1.1775(w:1.000)ğŸš€)
Batch 400/537: Loss=7.2475 (C:5.3678, R:0.0076, T:1.1216(w:1.000)ğŸš€)
Batch 425/537: Loss=7.1529 (C:5.3755, R:0.0076, T:1.0183(w:1.000)ğŸš€)
Batch 450/537: Loss=7.4220 (C:5.4360, R:0.0076, T:1.2257(w:1.000)ğŸš€)
Batch 475/537: Loss=7.3538 (C:5.4764, R:0.0076, T:1.1194(w:1.000)ğŸš€)
Batch 500/537: Loss=7.2728 (C:5.3782, R:0.0076, T:1.1316(w:1.000)ğŸš€)
Batch 525/537: Loss=7.3060 (C:5.4349, R:0.0076, T:1.1096(w:1.000)ğŸš€)

ğŸ“Š EPOCH 61 TRAINING SUMMARY:
  Total Loss: 7.3570
  Contrastive: 5.4046
  Reconstruction: 0.0076
  Topological: 1.1930 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.8022
  Contrastive: 5.4419
  Reconstruction: 0.0069
  Topological: 6.6714 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 61/100 COMPLETE (53.2s)
Train Loss: 7.3570 (C:5.4046, R:0.0076, T:1.1930)
Val Loss:   12.8022 (C:5.4419, R:0.0069, T:6.6714)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 62 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3302 (C:5.3991, R:0.0076, T:1.1670(w:1.000)ğŸš€)
Batch  25/537: Loss=7.3870 (C:5.4113, R:0.0076, T:1.2159(w:1.000)ğŸš€)
Batch  50/537: Loss=7.2290 (C:5.3669, R:0.0076, T:1.1042(w:1.000)ğŸš€)
Batch  75/537: Loss=7.3301 (C:5.3056, R:0.0076, T:1.2635(w:1.000)ğŸš€)
Batch 100/537: Loss=7.2947 (C:5.3202, R:0.0076, T:1.2137(w:1.000)ğŸš€)
Batch 125/537: Loss=7.2757 (C:5.3949, R:0.0076, T:1.1234(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3575 (C:5.3454, R:0.0076, T:1.2523(w:1.000)ğŸš€)
Batch 175/537: Loss=7.5422 (C:5.4619, R:0.0076, T:1.3252(w:1.000)ğŸš€)
Batch 200/537: Loss=7.1741 (C:5.3218, R:0.0076, T:1.0945(w:1.000)ğŸš€)
Batch 225/537: Loss=7.2341 (C:5.3896, R:0.0076, T:1.0832(w:1.000)ğŸš€)
Batch 250/537: Loss=7.3558 (C:5.4065, R:0.0075, T:1.1955(w:1.000)ğŸš€)
Batch 275/537: Loss=7.4250 (C:5.4023, R:0.0076, T:1.2617(w:1.000)ğŸš€)
Batch 300/537: Loss=7.2781 (C:5.3354, R:0.0076, T:1.1826(w:1.000)ğŸš€)
Batch 325/537: Loss=7.5287 (C:5.3681, R:0.0076, T:1.3986(w:1.000)ğŸš€)
Batch 350/537: Loss=7.3674 (C:5.4066, R:0.0076, T:1.2019(w:1.000)ğŸš€)
Batch 375/537: Loss=7.4108 (C:5.5538, R:0.0075, T:1.1040(w:1.000)ğŸš€)
Batch 400/537: Loss=7.4764 (C:5.4300, R:0.0076, T:1.2823(w:1.000)ğŸš€)
Batch 425/537: Loss=7.5227 (C:5.4359, R:0.0076, T:1.3239(w:1.000)ğŸš€)
Batch 450/537: Loss=7.5570 (C:5.4143, R:0.0076, T:1.3815(w:1.000)ğŸš€)
Batch 475/537: Loss=7.3459 (C:5.4037, R:0.0075, T:1.1933(w:1.000)ğŸš€)
Batch 500/537: Loss=7.4183 (C:5.4214, R:0.0076, T:1.2353(w:1.000)ğŸš€)
Batch 525/537: Loss=7.3351 (C:5.3812, R:0.0076, T:1.1931(w:1.000)ğŸš€)

ğŸ“Š EPOCH 62 TRAINING SUMMARY:
  Total Loss: 7.3454
  Contrastive: 5.3937
  Reconstruction: 0.0076
  Topological: 1.1921 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.8527
  Contrastive: 5.5107
  Reconstruction: 0.0069
  Topological: 6.6521 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 62/100 COMPLETE (45.8s)
Train Loss: 7.3454 (C:5.3937, R:0.0076, T:1.1921)
Val Loss:   12.8527 (C:5.5107, R:0.0069, T:6.6521)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 63 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2088 (C:5.4137, R:0.0076, T:1.0389(w:1.000)ğŸš€)
Batch  25/537: Loss=7.2116 (C:5.3210, R:0.0076, T:1.1316(w:1.000)ğŸš€)
Batch  50/537: Loss=7.2427 (C:5.3474, R:0.0076, T:1.1368(w:1.000)ğŸš€)
Batch  75/537: Loss=7.2273 (C:5.2913, R:0.0076, T:1.1728(w:1.000)ğŸš€)
Batch 100/537: Loss=7.2827 (C:5.3486, R:0.0076, T:1.1702(w:1.000)ğŸš€)
Batch 125/537: Loss=7.5608 (C:5.4578, R:0.0076, T:1.3382(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3138 (C:5.4013, R:0.0077, T:1.1470(w:1.000)ğŸš€)
Batch 175/537: Loss=7.2198 (C:5.3706, R:0.0076, T:1.0902(w:1.000)ğŸš€)
Batch 200/537: Loss=7.2766 (C:5.4754, R:0.0076, T:1.0419(w:1.000)ğŸš€)
Batch 225/537: Loss=7.1114 (C:5.3388, R:0.0076, T:1.0175(w:1.000)ğŸš€)
Batch 250/537: Loss=7.5845 (C:5.4593, R:0.0076, T:1.3698(w:1.000)ğŸš€)
Batch 275/537: Loss=7.2941 (C:5.3823, R:0.0076, T:1.1548(w:1.000)ğŸš€)
Batch 300/537: Loss=7.2766 (C:5.3948, R:0.0076, T:1.1229(w:1.000)ğŸš€)
Batch 325/537: Loss=7.4278 (C:5.3941, R:0.0076, T:1.2717(w:1.000)ğŸš€)
Batch 350/537: Loss=7.2441 (C:5.4088, R:0.0075, T:1.0827(w:1.000)ğŸš€)
Batch 375/537: Loss=7.2812 (C:5.3660, R:0.0076, T:1.1557(w:1.000)ğŸš€)
Batch 400/537: Loss=7.2834 (C:5.4716, R:0.0076, T:1.0529(w:1.000)ğŸš€)
Batch 425/537: Loss=7.2438 (C:5.3772, R:0.0076, T:1.1075(w:1.000)ğŸš€)
Batch 450/537: Loss=7.4412 (C:5.4030, R:0.0076, T:1.2742(w:1.000)ğŸš€)
Batch 475/537: Loss=7.3273 (C:5.4087, R:0.0076, T:1.1609(w:1.000)ğŸš€)
Batch 500/537: Loss=7.3800 (C:5.4480, R:0.0076, T:1.1743(w:1.000)ğŸš€)
Batch 525/537: Loss=7.1631 (C:5.2698, R:0.0076, T:1.1343(w:1.000)ğŸš€)

ğŸ“Š EPOCH 63 TRAINING SUMMARY:
  Total Loss: 7.3434
  Contrastive: 5.3913
  Reconstruction: 0.0076
  Topological: 1.1926 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.7031
  Contrastive: 5.5198
  Reconstruction: 0.0069
  Topological: 6.4962 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 63/100 COMPLETE (45.2s)
Train Loss: 7.3434 (C:5.3913, R:0.0076, T:1.1926)
Val Loss:   12.7031 (C:5.5198, R:0.0069, T:6.4962)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 64
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.307 Â± 0.614
    Neg distances: 7.178 Â± 2.080
    Separation ratio: 1.67x
    Gap: -10.154
    âš ï¸  Moderate global separation

============================================================
EPOCH 64 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3621 (C:5.4413, R:0.0076, T:1.1607(w:1.000)ğŸš€)
Batch  25/537: Loss=7.3664 (C:5.3737, R:0.0076, T:1.2370(w:1.000)ğŸš€)
Batch  50/537: Loss=7.3419 (C:5.3509, R:0.0076, T:1.2278(w:1.000)ğŸš€)
Batch  75/537: Loss=7.1407 (C:5.3637, R:0.0076, T:1.0179(w:1.000)ğŸš€)
Batch 100/537: Loss=7.1991 (C:5.3626, R:0.0076, T:1.0799(w:1.000)ğŸš€)
Batch 125/537: Loss=7.2486 (C:5.3534, R:0.0076, T:1.1368(w:1.000)ğŸš€)
Batch 150/537: Loss=7.5200 (C:5.4233, R:0.0076, T:1.3372(w:1.000)ğŸš€)
Batch 175/537: Loss=7.2474 (C:5.3602, R:0.0076, T:1.1240(w:1.000)ğŸš€)
Batch 200/537: Loss=7.3808 (C:5.3594, R:0.0076, T:1.2594(w:1.000)ğŸš€)
Batch 225/537: Loss=7.1457 (C:5.3664, R:0.0075, T:1.0248(w:1.000)ğŸš€)
Batch 250/537: Loss=7.3692 (C:5.4895, R:0.0076, T:1.1185(w:1.000)ğŸš€)
Batch 275/537: Loss=7.3569 (C:5.3252, R:0.0076, T:1.2765(w:1.000)ğŸš€)
Batch 300/537: Loss=7.3666 (C:5.4142, R:0.0076, T:1.1929(w:1.000)ğŸš€)
Batch 325/537: Loss=7.4876 (C:5.3729, R:0.0076, T:1.3503(w:1.000)ğŸš€)
Batch 350/537: Loss=7.4473 (C:5.4080, R:0.0076, T:1.2787(w:1.000)ğŸš€)
Batch 375/537: Loss=7.4381 (C:5.4164, R:0.0076, T:1.2625(w:1.000)ğŸš€)
Batch 400/537: Loss=7.2538 (C:5.4728, R:0.0076, T:1.0238(w:1.000)ğŸš€)
Batch 425/537: Loss=7.4921 (C:5.4139, R:0.0076, T:1.3188(w:1.000)ğŸš€)
Batch 450/537: Loss=7.3211 (C:5.4201, R:0.0076, T:1.1421(w:1.000)ğŸš€)
Batch 475/537: Loss=7.3205 (C:5.3665, R:0.0076, T:1.1943(w:1.000)ğŸš€)
Batch 500/537: Loss=7.4652 (C:5.4534, R:0.0076, T:1.2527(w:1.000)ğŸš€)
Batch 525/537: Loss=7.2977 (C:5.4711, R:0.0076, T:1.0710(w:1.000)ğŸš€)

ğŸ“Š EPOCH 64 TRAINING SUMMARY:
  Total Loss: 7.3314
  Contrastive: 5.3788
  Reconstruction: 0.0076
  Topological: 1.1934 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.7378
  Contrastive: 5.4899
  Reconstruction: 0.0069
  Topological: 6.5584 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 64/100 COMPLETE (52.0s)
Train Loss: 7.3314 (C:5.3788, R:0.0076, T:1.1934)
Val Loss:   12.7378 (C:5.4899, R:0.0069, T:6.5584)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 65 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.3123 (C:5.4354, R:0.0076, T:1.1187(w:1.000)ğŸš€)
Batch  25/537: Loss=7.3267 (C:5.3674, R:0.0076, T:1.1999(w:1.000)ğŸš€)
Batch  50/537: Loss=7.2573 (C:5.2768, R:0.0076, T:1.2194(w:1.000)ğŸš€)
Batch  75/537: Loss=7.5612 (C:5.3468, R:0.0076, T:1.4515(w:1.000)ğŸš€)
Batch 100/537: Loss=7.2233 (C:5.3458, R:0.0076, T:1.1183(w:1.000)ğŸš€)
Batch 125/537: Loss=7.2660 (C:5.3076, R:0.0076, T:1.2025(w:1.000)ğŸš€)
Batch 150/537: Loss=7.2979 (C:5.4198, R:0.0076, T:1.1217(w:1.000)ğŸš€)
Batch 175/537: Loss=7.2657 (C:5.4014, R:0.0075, T:1.1093(w:1.000)ğŸš€)
Batch 200/537: Loss=7.5345 (C:5.3395, R:0.0076, T:1.4341(w:1.000)ğŸš€)
Batch 225/537: Loss=7.2312 (C:5.3211, R:0.0076, T:1.1488(w:1.000)ğŸš€)
Batch 250/537: Loss=7.2934 (C:5.3671, R:0.0077, T:1.1600(w:1.000)ğŸš€)
Batch 275/537: Loss=7.4158 (C:5.4259, R:0.0076, T:1.2347(w:1.000)ğŸš€)
Batch 300/537: Loss=7.4268 (C:5.4076, R:0.0076, T:1.2639(w:1.000)ğŸš€)
Batch 325/537: Loss=7.5533 (C:5.4475, R:0.0076, T:1.3465(w:1.000)ğŸš€)
Batch 350/537: Loss=7.1694 (C:5.3700, R:0.0076, T:1.0392(w:1.000)ğŸš€)
Batch 375/537: Loss=7.3011 (C:5.4184, R:0.0076, T:1.1256(w:1.000)ğŸš€)
Batch 400/537: Loss=7.1752 (C:5.3841, R:0.0076, T:1.0294(w:1.000)ğŸš€)
Batch 425/537: Loss=7.3195 (C:5.3476, R:0.0075, T:1.2173(w:1.000)ğŸš€)
Batch 450/537: Loss=7.2674 (C:5.3169, R:0.0076, T:1.1925(w:1.000)ğŸš€)
Batch 475/537: Loss=7.1987 (C:5.3291, R:0.0076, T:1.1119(w:1.000)ğŸš€)
Batch 500/537: Loss=7.4084 (C:5.3816, R:0.0077, T:1.2606(w:1.000)ğŸš€)
Batch 525/537: Loss=7.3080 (C:5.3962, R:0.0076, T:1.1510(w:1.000)ğŸš€)

ğŸ“Š EPOCH 65 TRAINING SUMMARY:
  Total Loss: 7.3283
  Contrastive: 5.3743
  Reconstruction: 0.0076
  Topological: 1.1943 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.6795
  Contrastive: 5.5291
  Reconstruction: 0.0069
  Topological: 6.4611 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 65/100 COMPLETE (47.6s)
Train Loss: 7.3283 (C:5.3743, R:0.0076, T:1.1943)
Val Loss:   12.6795 (C:5.5291, R:0.0069, T:6.4611)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 66 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.2576 (C:5.4517, R:0.0076, T:1.0458(w:1.000)ğŸš€)
Batch  25/537: Loss=7.1571 (C:5.2673, R:0.0076, T:1.1315(w:1.000)ğŸš€)
Batch  50/537: Loss=7.1831 (C:5.2924, R:0.0076, T:1.1288(w:1.000)ğŸš€)
Batch  75/537: Loss=7.3559 (C:5.4070, R:0.0076, T:1.1907(w:1.000)ğŸš€)
Batch 100/537: Loss=7.5534 (C:5.3706, R:0.0076, T:1.4199(w:1.000)ğŸš€)
Batch 125/537: Loss=7.3526 (C:5.4041, R:0.0076, T:1.1897(w:1.000)ğŸš€)
Batch 150/537: Loss=7.3333 (C:5.3849, R:0.0076, T:1.1859(w:1.000)ğŸš€)
Batch 175/537: Loss=7.3616 (C:5.3483, R:0.0077, T:1.2435(w:1.000)ğŸš€)
Batch 200/537: Loss=7.2344 (C:5.3312, R:0.0076, T:1.1401(w:1.000)ğŸš€)
Batch 225/537: Loss=7.1871 (C:5.2511, R:0.0076, T:1.1718(w:1.000)ğŸš€)
Batch 250/537: Loss=7.3853 (C:5.3718, R:0.0076, T:1.2526(w:1.000)ğŸš€)
Batch 275/537: Loss=7.2251 (C:5.3263, R:0.0076, T:1.1363(w:1.000)ğŸš€)
Batch 300/537: Loss=7.3978 (C:5.4235, R:0.0076, T:1.2148(w:1.000)ğŸš€)
Batch 325/537: Loss=7.3744 (C:5.4845, R:0.0075, T:1.1371(w:1.000)ğŸš€)
Batch 350/537: Loss=7.2846 (C:5.4061, R:0.0076, T:1.1154(w:1.000)ğŸš€)
Batch 375/537: Loss=7.3396 (C:5.3472, R:0.0076, T:1.2332(w:1.000)ğŸš€)
Batch 400/537: Loss=7.2501 (C:5.3479, R:0.0076, T:1.1431(w:1.000)ğŸš€)
Batch 425/537: Loss=7.2708 (C:5.2990, R:0.0076, T:1.2136(w:1.000)ğŸš€)
Batch 450/537: Loss=7.3398 (C:5.5026, R:0.0076, T:1.0768(w:1.000)ğŸš€)
Batch 475/537: Loss=7.6246 (C:5.3998, R:0.0076, T:1.4628(w:1.000)ğŸš€)
Batch 500/537: Loss=7.1687 (C:5.3298, R:0.0076, T:1.0773(w:1.000)ğŸš€)
Batch 525/537: Loss=7.3354 (C:5.3285, R:0.0076, T:1.2484(w:1.000)ğŸš€)

ğŸ“Š EPOCH 66 TRAINING SUMMARY:
  Total Loss: 7.3264
  Contrastive: 5.3721
  Reconstruction: 0.0076
  Topological: 1.1942 (weight: 1.000)
  Batches with topology: 537/537 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 12.9309
  Contrastive: 5.4789
  Reconstruction: 0.0069
  Topological: 6.7611 (weight: 1.000)
  Batches with topology: 9/9 (100.0%)

ğŸ¯ EPOCH 66/100 COMPLETE (47.5s)
Train Loss: 7.3264 (C:5.3721, R:0.0076, T:1.1942)
Val Loss:   12.9309 (C:5.4789, R:0.0069, T:6.7611)
ğŸš€ Good topological learning progress
------------------------------------------------------------

ğŸŒ Updating global dataset at epoch 67
ğŸŒ Extracting features for entire dataset...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
  Subsampled to 5000 samples
  Global dataset updated: 5000 samples
  Analyzing global separation...
  GLOBAL ANALYSIS:
    Pos distances: 4.273 Â± 0.618
    Neg distances: 7.281 Â± 2.179
    Separation ratio: 1.70x
    Gap: -10.001
    âš ï¸  Moderate global separation

============================================================
EPOCH 67 | Batches: 537 | Topological Weight: 1.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/537: Loss=7.1481 (C:5.2162, R:0.0076, T:1.1700(w:1.000)ğŸš€)
Batch  25/537: Loss=7.1961 (C:5.2316, R:0.0076, T:1.2028(w:1.000)ğŸš€)
Batch  50/537: Loss=7.3372 (C:5.2826, R:0.0076, T:1.2940(w:1.000)ğŸš€)
Batch  75/537: Loss=7.2904 (C:5.3538, R:0.0076, T:1.1752(w:1.000)ğŸš€)
Batch 100/537: Loss=7.2516 (C:5.2599, R:0.0077, T:1.2252(w:1.000)ğŸš€)
Batch 125/537: Loss=7.1109 (C:5.2482, R:0.0077, T:1.0943(w:1.000)ğŸš€)
Batch 150/537: Loss=7.0857 (C:5.2530, R:0.0076, T:1.0700(w:1.000)ğŸš€)
Batch 175/537: Loss=7.1421 (C:5.2946, R:0.0076, T:1.0913(w:1.000)ğŸš€)


TOOK THE EPOCH 50 MODEL AS BEST: 

ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 3,045,451
FullDatasetContrastiveLoss initialized:
  Positive Margin: 2.0
  Negative Margin: 10.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 100.0
MoorTopologicalLoss Initialized: Using 0-dimensional persistence pairings (MST edges).
No prototypes being used for topological loss - whole dataset instead.
TopologicalTrainer initialized on device: cpu
Model parameters: 3,045,451
Enhanced with topological loss monitoring
âœ… Model and trainer loaded successfully
Creating topological loss plots...
Topological loss plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/plots/moor_topo-contrastive_autoencoder_noattention_20250725_170549_topological_training_losses.png
Curriculum learning analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/plots/moor_topo-contrastive_autoencoder_noattention_20250725_170549_curriculum_analysis.png
Main training plots saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/plots/moor_topo-contrastive_autoencoder_noattention_20250725_170549_topological_training_losses.png
Curriculum analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/plots/moor_topo-contrastive_autoencoder_noattention_20250725_170549_curriculum_analysis.png
Training summary saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/moor_topo-contrastive_autoencoder_noattention_20250725_170549_training_summary.txt
âœ… Plots and summary created successfully
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating concat embeddings on cpu
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cpu
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9842, 1536])
Generating embeddings for test...
Generating concat embeddings on cpu
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated concat embeddings: torch.Size([9824, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1536])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1536])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
âœ… Data loaded successfully
Starting model evaluation...
GlobalContrastiveEvaluator initialized on cpu
Starting comprehensive evaluation...
============================================================
Extracting latent representations...
  Processed 1/10 batches
Extracted representations: torch.Size([9824, 75])
Evaluating clustering performance...
Clustering Results:
  Silhouette Score: 0.1034
  Adjusted Rand Score: 0.3783
  Clustering Accuracy: 0.6390
Evaluating classification performance...
  Extracting training representations...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
  Extracting validation representations...
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Subsampled training to 50000 samples
  Training on 50000 samples, evaluating on 9180 samples
Classification Results:
  Accuracy: 0.7674
  Per-class F1: [0.7635732323232324, 0.6674645238502308, 0.8660728744939272]
Evaluating reconstruction quality...
Reconstruction Results:
  Average MSE: 0.006859
Evaluating separation quality...
Separation Results:
  Positive distances: 4.699 Â± 1.244
  Negative distances: 6.626 Â± 2.050
  Separation ratio: 1.41x
  Gap: -10.435
  Perfect separation: No

Comprehensive evaluation completed!

============================================================
EVALUATION SUMMARY
============================================================
Clustering Performance:
  Silhouette Score: 0.1034
  Clustering Accuracy: 0.6390
  Adjusted Rand Score: 0.3783

Classification Performance:
  Accuracy: 0.7674

Separation Quality:
  Separation Ratio: 1.41x
  Gap: -10.435
  Perfect Separation: False

Reconstruction Quality:
  Average MSE: 0.006859
============================================================
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/results/evaluation_results_20250725_180159.json
Evaluation results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/results/evaluation_results_20250725_180159.json
âœ… Model evaluation completed successfully

Key Results:
  Separation ratio: 1.41x
  Perfect separation: False
  Classification accuracy: 0.7674

============================================================
TOPOLOGICAL TRAINING ANALYSIS
============================================================

ğŸ“ˆ TOPOLOGICAL LEARNING DIAGNOSIS:
  Total epochs: 50
  Epochs with topological learning: 50
  Current topological loss: 1.2117
  Current topological weight: 0.9800
  âœ… Topological loss is decreasing (good progress)
ğŸš€ EXCELLENT: Consistent topological learning achieved!
Final topological loss: 1.2117
Epochs with topology: 50/50
âš ï¸  Poor clustering accuracy: 0.639

Final analysis saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549/results/final_analysis.json
âœ… Analysis completed for: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549

============================================================
ğŸ‰ ANALYSIS COMPLETED SUCCESSFULLY!
============================================================
Results saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/moor_topo-contrastive_autoencoder_noattention_20250725_170549