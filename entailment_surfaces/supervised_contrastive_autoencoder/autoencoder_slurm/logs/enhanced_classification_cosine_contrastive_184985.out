Starting Surface Distance Metric Analysis job...
Job ID: 184985
Node: gpuvm16
Time: Tue 22 Jul 19:31:21 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Tue Jul 22 19:31:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   38C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

Running Enhanced Classification Evaluation on Trained Model
======================================================================
Using device: cuda
Loading model from: entailment_surfaces/supervised_contrastive_autoencoder/experiments/coarse_embeddingcosine_concat_hiddendims[1024, 768, 512, 256, 128]_dropout0.2_optimAdam_lr0.0001_20250715_204239/checkpoints/best_model.pt
ContrastiveAutoencoder initialized:
  Input dim: 1537
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,860,940
Model loaded successfully

Loading SNLI data...
Loading data...
========================================
FlexibleEmbedder initialized with type: 'cosine_concat'
Output dimension will be: 1537
GlobalDataLoader initialized:
  Embedding type: cosine_concat
  Output dimension: 1537
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 9824 test samples
Generating embeddings for training...
Generating cosine_concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated cosine_concat embeddings: torch.Size([549367, 1537])
Generating embeddings for validation...
Generating cosine_concat embeddings on cuda
Processing 9842 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9842, 1537])
Generating embeddings for test...
Generating cosine_concat embeddings on cuda
Processing 9824 samples in batches of 1000
  Processing batch 1/10
  Processing batch 6/10
Generated cosine_concat embeddings: torch.Size([9824, 1537])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1537])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 9842 samples
  Embedding shape: torch.Size([9842, 1537])
  Class distribution: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
EntailmentDataset created: 9824 samples
  Embedding shape: torch.Size([9824, 1537])
  Class distribution: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Output embedding dimension: 1537
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 537
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 340
  Effective batch size: 1020
  Number of batches: 9
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
DataLoaders created:
  Batch size: 1020
  Balanced sampling: True
  Train batches: 537
  Val batches: 9
  Test batches: 10
Data loading completed!
  Train: 549367 samples, 537 batches
  Val: 9842 samples, 9 batches
  Test: 9824 samples, 10 batches
Data loaded: 537 train batches, 9 val batches

Creating evaluator...
GlobalContrastiveEvaluator initialized on cuda

======================================================================
RUNNING ENHANCED CLASSIFICATION EVALUATION
======================================================================
Running enhanced classification evaluation...
Extracting latent representations...
  Processed 1/537 batches
  Processed 51/537 batches
  Processed 101/537 batches
  Processed 151/537 batches
  Processed 201/537 batches
  Processed 251/537 batches
  Processed 301/537 batches
  Processed 351/537 batches
  Processed 401/537 batches
  Processed 451/537 batches
  Processed 501/537 batches
Extracted representations: torch.Size([547740, 75])
Extracting latent representations...
  Processed 1/9 batches
Extracted representations: torch.Size([9180, 75])
Starting comprehensive classification evaluation...
============================================================
Dataset info:
  Training: 547740 samples, 75 features
  Validation: 9180 samples
  Class distribution (train): [182580 182580 182580]
  Class distribution (val): [3060 3060 3060]
  Using all 547740 training samples

1. Random Forest Classification:
  Training Random Forest classifier...
    Random Forest Accuracy: 0.8363
    Random Forest F1-macro: 0.8367
    Random Forest CV: 0.9246 ± 0.0014

2. SVM with RBF Kernel Classification:
  Note: SVM limited to 100000 samples for computational efficiency
  Subsampled to 100000 samples for svm_rbf
  Training SVM with RBF kernel...
    SVM RBF Accuracy: 0.8375
    SVM RBF F1-macro: 0.8375
    SVM RBF CV: 0.9309 ± 0.0091

3. Neural Network Classification:
  Training Neural Network classifier...
      Epoch 0/50, Loss: 1.0923
      Epoch 10/50, Loss: 1.0130
      Epoch 20/50, Loss: 0.9037
      Epoch 30/50, Loss: 0.7532
      Epoch 40/50, Loss: 0.5950
    Neural Network Accuracy: 0.8375
    Neural Network F1-macro: 0.8372
    Neural Network Train Acc: 0.9292

============================================================
ENHANCED CLASSIFICATION SUMMARY
============================================================
Method          Accuracy   F1-Macro   Balanced Acc
--------------------------------------------------
Random Forest   0.8363     0.8367     0.8363      
SVM RBF         0.8375     0.8375     0.8375      
Neural Network  0.8375     0.8372     0.8375      

Best performing method: Svm Rbf (0.8375)
============================================================

================================================================================
DETAILED CLASSIFICATION RESULTS
================================================================================

RANDOM FOREST:
----------------------------------------
  Accuracy: 0.8363
  Balanced Accuracy: 0.8363
  F1-Macro: 0.8367
  F1 per class: ['0.8586', '0.7849', '0.8666']
  Cross-validation: 0.9246 ± 0.0014
  Feature importance (mean): 0.013333
  Top 5 important features: [40, 58, 67, 33, 69]

SVM WITH RBF KERNEL:
----------------------------------------
  Accuracy: 0.8375
  Balanced Accuracy: 0.8375
  F1-Macro: 0.8375
  F1 per class: ['0.8614', '0.7839', '0.8672']
  Cross-validation: 0.9309 ± 0.0091
  Total support vectors: 13392
  Support vectors per class: [4033, 5989, 3370]

NEURAL NETWORK:
----------------------------------------
  Accuracy: 0.8375
  Balanced Accuracy: 0.8375
  F1-Macro: 0.8372
  F1 per class: ['0.8619', '0.7824', '0.8673']
  Training accuracy: 0.9292
  Model parameters: 7,043
================================================================================

Enhanced classification results saved to: enhanced_classification_results/enhanced_classification_20250722_203142.json

======================================================================
EVALUATION COMPLETED SUCCESSFULLY!
======================================================================
 Best performing method: Svm Rbf
 Best accuracy: 0.8375

For detailed results, check the saved JSON file or run with detailed printing enabled.

Analysis completed with exit code: 0
Time: Tue 22 Jul 20:31:43 BST 2025

=== ANALYSIS SUCCESSFUL ===
Evaluation successful!


Job finished.
