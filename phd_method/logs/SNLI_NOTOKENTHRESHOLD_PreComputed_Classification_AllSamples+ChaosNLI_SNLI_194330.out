Job ID: 194330
Node: parrot
Time: Thu 14 Aug 07:34:56 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Thu Aug 14 07:35:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A16                     Off |   00000000:95:00.0 Off |                    0 |
|  0%   25C    P8             14W /   62W |       0MiB /  15356MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: NVIDIA A16
GPU memory: 15.7 GB
PyTorch setup verified!

Starting Model training...

================================================================================
SNLI CLASSIFICATION EXPERIMENT
================================================================================
Loading precomputed topological features...
Topological features: Train (150000, 21), Val (9842, 21)
Computing SBERT baseline features from /vol/bitbucket/ahb24/tda_entailment_new/snli_train_sbert_tokens.pkl
Processing entailment: 50214 samples
  Processed 1000/50214 entailment samples
  Processed 2000/50214 entailment samples
  Processed 3000/50214 entailment samples
  Processed 4000/50214 entailment samples
  Processed 5000/50214 entailment samples
  Processed 6000/50214 entailment samples
  Processed 7000/50214 entailment samples
  Processed 8000/50214 entailment samples
  Processed 9000/50214 entailment samples
  Processed 10000/50214 entailment samples
  Processed 11000/50214 entailment samples
  Processed 12000/50214 entailment samples
  Processed 13000/50214 entailment samples
  Processed 14000/50214 entailment samples
  Processed 15000/50214 entailment samples
  Processed 16000/50214 entailment samples
  Processed 17000/50214 entailment samples
  Processed 18000/50214 entailment samples
  Processed 19000/50214 entailment samples
  Processed 20000/50214 entailment samples
  Processed 21000/50214 entailment samples
  Processed 22000/50214 entailment samples
  Processed 23000/50214 entailment samples
  Processed 24000/50214 entailment samples
  Processed 25000/50214 entailment samples
  Processed 26000/50214 entailment samples
  Processed 27000/50214 entailment samples
  Processed 28000/50214 entailment samples
  Processed 29000/50214 entailment samples
  Processed 30000/50214 entailment samples
  Processed 31000/50214 entailment samples
  Processed 32000/50214 entailment samples
  Processed 33000/50214 entailment samples
  Processed 34000/50214 entailment samples
  Processed 35000/50214 entailment samples
  Processed 36000/50214 entailment samples
  Processed 37000/50214 entailment samples
  Processed 38000/50214 entailment samples
  Processed 39000/50214 entailment samples
  Processed 40000/50214 entailment samples
  Processed 41000/50214 entailment samples
  Processed 42000/50214 entailment samples
  Processed 43000/50214 entailment samples
  Processed 44000/50214 entailment samples
  Processed 45000/50214 entailment samples
  Processed 46000/50214 entailment samples
  Processed 47000/50214 entailment samples
  Processed 48000/50214 entailment samples
  Processed 49000/50214 entailment samples
  Processed 50000/50214 entailment samples
Processing neutral: 50069 samples
  Processed 1000/50069 neutral samples
  Processed 2000/50069 neutral samples
  Processed 3000/50069 neutral samples
  Processed 4000/50069 neutral samples
  Processed 5000/50069 neutral samples
  Processed 6000/50069 neutral samples
  Processed 7000/50069 neutral samples
  Processed 8000/50069 neutral samples
  Processed 9000/50069 neutral samples
  Processed 10000/50069 neutral samples
  Processed 11000/50069 neutral samples
  Processed 12000/50069 neutral samples
  Processed 13000/50069 neutral samples
  Processed 14000/50069 neutral samples
  Processed 15000/50069 neutral samples
  Processed 16000/50069 neutral samples
  Processed 17000/50069 neutral samples
  Processed 18000/50069 neutral samples
  Processed 19000/50069 neutral samples
  Processed 20000/50069 neutral samples
  Processed 21000/50069 neutral samples
  Processed 22000/50069 neutral samples
  Processed 23000/50069 neutral samples
  Processed 24000/50069 neutral samples
  Processed 25000/50069 neutral samples
  Processed 26000/50069 neutral samples
  Processed 27000/50069 neutral samples
  Processed 28000/50069 neutral samples
  Processed 29000/50069 neutral samples
  Processed 30000/50069 neutral samples
  Processed 31000/50069 neutral samples
  Processed 32000/50069 neutral samples
  Processed 33000/50069 neutral samples
  Processed 34000/50069 neutral samples
  Processed 35000/50069 neutral samples
  Processed 36000/50069 neutral samples
  Processed 37000/50069 neutral samples
  Processed 38000/50069 neutral samples
  Processed 39000/50069 neutral samples
  Processed 40000/50069 neutral samples
  Processed 41000/50069 neutral samples
  Processed 42000/50069 neutral samples
  Processed 43000/50069 neutral samples
  Processed 44000/50069 neutral samples
  Processed 45000/50069 neutral samples
  Processed 46000/50069 neutral samples
  Processed 47000/50069 neutral samples
  Processed 48000/50069 neutral samples
  Processed 49000/50069 neutral samples
  Processed 50000/50069 neutral samples
Processing contradiction: 49717 samples
  Processed 1000/49717 contradiction samples
  Processed 2000/49717 contradiction samples
  Processed 3000/49717 contradiction samples
  Processed 4000/49717 contradiction samples
  Processed 5000/49717 contradiction samples
  Processed 6000/49717 contradiction samples
  Processed 7000/49717 contradiction samples
  Processed 8000/49717 contradiction samples
  Processed 9000/49717 contradiction samples
  Processed 10000/49717 contradiction samples
  Processed 11000/49717 contradiction samples
  Processed 12000/49717 contradiction samples
  Processed 13000/49717 contradiction samples
  Processed 14000/49717 contradiction samples
  Processed 15000/49717 contradiction samples
  Processed 16000/49717 contradiction samples
  Processed 17000/49717 contradiction samples
  Processed 18000/49717 contradiction samples
  Processed 19000/49717 contradiction samples
  Processed 20000/49717 contradiction samples
  Processed 21000/49717 contradiction samples
  Processed 22000/49717 contradiction samples
  Processed 23000/49717 contradiction samples
  Processed 24000/49717 contradiction samples
  Processed 25000/49717 contradiction samples
  Processed 26000/49717 contradiction samples
  Processed 27000/49717 contradiction samples
  Processed 28000/49717 contradiction samples
  Processed 29000/49717 contradiction samples
  Processed 30000/49717 contradiction samples
  Processed 31000/49717 contradiction samples
  Processed 32000/49717 contradiction samples
  Processed 33000/49717 contradiction samples
  Processed 34000/49717 contradiction samples
  Processed 35000/49717 contradiction samples
  Processed 36000/49717 contradiction samples
  Processed 37000/49717 contradiction samples
  Processed 38000/49717 contradiction samples
  Processed 39000/49717 contradiction samples
  Processed 40000/49717 contradiction samples
  Processed 41000/49717 contradiction samples
  Processed 42000/49717 contradiction samples
  Processed 43000/49717 contradiction samples
  Processed 44000/49717 contradiction samples
  Processed 45000/49717 contradiction samples
  Processed 46000/49717 contradiction samples
  Processed 47000/49717 contradiction samples
  Processed 48000/49717 contradiction samples
  Processed 49000/49717 contradiction samples
SBERT features: (150000, 3072), class distribution: {0: 50214, 1: 50069, 2: 49717}
Computing SBERT baseline features from /vol/bitbucket/ahb24/tda_entailment_new/snli_val_sbert_tokens.pkl
Processing entailment: 3329 samples
  Processed 1000/3329 entailment samples
  Processed 2000/3329 entailment samples
  Processed 3000/3329 entailment samples
Processing neutral: 3235 samples
  Processed 1000/3235 neutral samples
  Processed 2000/3235 neutral samples
  Processed 3000/3235 neutral samples
Processing contradiction: 3278 samples
  Processed 1000/3278 contradiction samples
  Processed 2000/3278 contradiction samples
  Processed 3000/3278 contradiction samples
SBERT features: (9842, 3072), class distribution: {0: 3329, 1: 3235, 2: 3278}
Aligning topological and SBERT datasets...
Class 0: topo=50214, sbert=50214, using=50214
Class 1: topo=50069, sbert=50069, using=50069
Class 2: topo=49717, sbert=49717, using=49717
Final aligned datasets:
  Train: topo=(150000, 21), sbert=(150000, 3072)
  Val: topo=(9842, 21), sbert=(9842, 3072)
Creating hybrid features: (150000, 21) + (150000, 3072)
Hybrid features: (150000, 3093)
Creating hybrid features: (9842, 21) + (9842, 3072)
Hybrid features: (9842, 3093)

Running Topological classification...
Training set class distribution: {0: 50214, 1: 50069, 2: 49717}
Standardizing (150000, 21) in-place...
Training Random Forest...
Random Forest: 0.662
Training Logistic Regression...
Logistic Regression: 0.648
Training on 150000 samples with 21 features
Using batch size: 32
Epoch   0: Train Loss 0.8177 (Acc 0.624), Val Loss 0.7641 (Acc 0.658), Best Val 0.658
Epoch   1: Train Loss 0.8021 (Acc 0.633), Val Loss 0.7628 (Acc 0.661), Best Val 0.661
Epoch   2: Train Loss 0.8007 (Acc 0.634), Val Loss 0.7652 (Acc 0.660), Best Val 0.661
Epoch   3: Train Loss 0.7981 (Acc 0.635), Val Loss 0.7567 (Acc 0.664), Best Val 0.664
Epoch   4: Train Loss 0.7981 (Acc 0.635), Val Loss 0.7594 (Acc 0.664), Best Val 0.664
Epoch   5: Train Loss 0.7964 (Acc 0.637), Val Loss 0.7556 (Acc 0.663), Best Val 0.663
Epoch   6: Train Loss 0.7955 (Acc 0.636), Val Loss 0.7584 (Acc 0.663), Best Val 0.663
Epoch   7: Train Loss 0.7960 (Acc 0.638), Val Loss 0.7610 (Acc 0.662), Best Val 0.663
Epoch   8: Train Loss 0.7960 (Acc 0.636), Val Loss 0.7614 (Acc 0.662), Best Val 0.663
Epoch   9: Train Loss 0.7959 (Acc 0.637), Val Loss 0.7613 (Acc 0.662), Best Val 0.663
Epoch  20: Train Loss 0.7920 (Acc 0.639), Val Loss 0.7547 (Acc 0.666), Best Val 0.663
Epoch  40: Train Loss 0.7857 (Acc 0.644), Val Loss 0.7478 (Acc 0.668), Best Val 0.668
Epoch  60: Train Loss 0.7846 (Acc 0.644), Val Loss 0.7506 (Acc 0.666), Best Val 0.667
Epoch  80: Train Loss 0.7818 (Acc 0.644), Val Loss 0.7502 (Acc 0.668), Best Val 0.669
Epoch 100: Train Loss 0.7809 (Acc 0.647), Val Loss 0.7470 (Acc 0.668), Best Val 0.670
Epoch 120: Train Loss 0.7805 (Acc 0.646), Val Loss 0.7452 (Acc 0.669), Best Val 0.668
Epoch 140: Train Loss 0.7785 (Acc 0.648), Val Loss 0.7431 (Acc 0.668), Best Val 0.669
Epoch 160: Train Loss 0.7793 (Acc 0.648), Val Loss 0.7455 (Acc 0.668), Best Val 0.668
Early stopping at epoch 167 (patience=25)

Final Results:
  Best validation accuracy: 0.6684
  Final validation accuracy: 0.6693
  Total epochs: 168
PyTorch NN: 0.668

Running SBERT Baseline classification...
Training set class distribution: {0: 50214, 1: 50069, 2: 49717}
Standardizing (150000, 3072) in-place...
Training Random Forest...
Random Forest: 0.714
Training Logistic Regression...
Logistic Regression: 0.785
Training on 150000 samples with 3072 features
Using batch size: 32
Epoch   0: Train Loss 0.6800 (Acc 0.712), Val Loss 0.5548 (Acc 0.774), Best Val 0.774
Epoch   1: Train Loss 0.5996 (Acc 0.751), Val Loss 0.5384 (Acc 0.781), Best Val 0.781
Epoch   2: Train Loss 0.5791 (Acc 0.762), Val Loss 0.5254 (Acc 0.784), Best Val 0.784
Epoch   3: Train Loss 0.5617 (Acc 0.770), Val Loss 0.5155 (Acc 0.789), Best Val 0.789
Epoch   4: Train Loss 0.5518 (Acc 0.775), Val Loss 0.5051 (Acc 0.797), Best Val 0.797
Epoch   5: Train Loss 0.5435 (Acc 0.779), Val Loss 0.5107 (Acc 0.792), Best Val 0.797
Epoch   6: Train Loss 0.5386 (Acc 0.781), Val Loss 0.5059 (Acc 0.799), Best Val 0.797
Epoch   7: Train Loss 0.5327 (Acc 0.784), Val Loss 0.5090 (Acc 0.794), Best Val 0.797
Epoch   8: Train Loss 0.5281 (Acc 0.786), Val Loss 0.5043 (Acc 0.800), Best Val 0.800
Epoch   9: Train Loss 0.5256 (Acc 0.790), Val Loss 0.5012 (Acc 0.798), Best Val 0.798
Epoch  20: Train Loss 0.5112 (Acc 0.796), Val Loss 0.5054 (Acc 0.797), Best Val 0.802
Epoch  40: Train Loss 0.4548 (Acc 0.824), Val Loss 0.5041 (Acc 0.799), Best Val 0.805
Epoch  60: Train Loss 0.4071 (Acc 0.847), Val Loss 0.5036 (Acc 0.799), Best Val 0.805
Early stopping at epoch 63 (patience=25)

Final Results:
  Best validation accuracy: 0.8049
  Final validation accuracy: 0.8020
  Total epochs: 64
PyTorch NN: 0.805

Running Hybrid classification...
Training set class distribution: {0: 50214, 1: 50069, 2: 49717}
Standardizing (150000, 3093) in-place...
Training Random Forest...
Random Forest: 0.686
Training Logistic Regression...
Logistic Regression: 0.778
Training on 150000 samples with 3093 features
Using batch size: 32
Epoch   0: Train Loss 0.6013 (Acc 0.751), Val Loss 0.5635 (Acc 0.768), Best Val 0.768
Epoch   1: Train Loss 0.5087 (Acc 0.792), Val Loss 0.5496 (Acc 0.779), Best Val 0.779
Epoch   2: Train Loss 0.4842 (Acc 0.803), Val Loss 0.5442 (Acc 0.781), Best Val 0.781
Epoch   3: Train Loss 0.4708 (Acc 0.810), Val Loss 0.5312 (Acc 0.787), Best Val 0.787
Epoch   4: Train Loss 0.4620 (Acc 0.814), Val Loss 0.5323 (Acc 0.787), Best Val 0.787
Epoch   5: Train Loss 0.4555 (Acc 0.818), Val Loss 0.5290 (Acc 0.788), Best Val 0.788
Epoch   6: Train Loss 0.4507 (Acc 0.820), Val Loss 0.5279 (Acc 0.792), Best Val 0.792
Epoch   7: Train Loss 0.4445 (Acc 0.822), Val Loss 0.5260 (Acc 0.791), Best Val 0.791
Epoch   8: Train Loss 0.4409 (Acc 0.824), Val Loss 0.5344 (Acc 0.786), Best Val 0.791
Epoch   9: Train Loss 0.4384 (Acc 0.825), Val Loss 0.5341 (Acc 0.787), Best Val 0.791
Epoch  20: Train Loss 0.4242 (Acc 0.833), Val Loss 0.5259 (Acc 0.788), Best Val 0.795
Epoch  40: Train Loss 0.4187 (Acc 0.835), Val Loss 0.5270 (Acc 0.793), Best Val 0.794
Epoch  60: Train Loss 0.3685 (Acc 0.859), Val Loss 0.5188 (Acc 0.797), Best Val 0.800
Epoch  80: Train Loss 0.3476 (Acc 0.871), Val Loss 0.5173 (Acc 0.797), Best Val 0.798
Early stopping at epoch 88 (patience=25)

Final Results:
  Best validation accuracy: 0.7975
  Final validation accuracy: 0.7988
  Total epochs: 89
PyTorch NN: 0.798

============================================================
SNLI CLASSIFICATION RESULTS
============================================================
Topological (21 features):     0.668 (PyTorch NN)
SBERT Baseline (3072 features): 0.805 (PyTorch NN)
Hybrid (3093 features):        0.798 (PyTorch NN)

Hybrid improvements:
  vs SBERT: -0.007
  vs Topological: +0.129

================================================================================
CHAOSNLI UNCERTAINTY QUANTIFICATION
================================================================================

Evaluating Topological PyTorch NN on ChaosNLI...

Analysis completed with exit code: 1
Time: Thu 14 Aug 10:55:57 BST 2025

=== ANALYSIS FAILED ===
Please check the error output above for debugging information.


Job finished.
