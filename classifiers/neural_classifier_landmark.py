"""
TDA Neural Network Classifier (for Landmark-Based Features)

This module implements a neural network classifier that uses a pre-computed feature
vector generated by the landmark-based TDA method.

Architecture: 7 input features -> 128 -> 64 -> 32 -> 16 -> 3 output classes
Features: The 7D vector from 'landmark_tda_integration.py', which includes:
  - 3 geometric features (cone energy, order energy, hyperbolic distance)
  - 4 topological features (H0/H1 persistence, etc.)
"""

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FeatureNormalizer:
    """
    A simple class to handle normalization of pre-computed features.
    This replaces the old, complex TDAFeatureExtractor. Its only job is to
    fit a StandardScaler on the training data and use it to transform any
    new data.
    """
    def __init__(self):
        """Initializes the feature normalizer."""
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.feature_names = []

    def fit(self, training_features: np.ndarray, feature_names: List[str]):
        """
        Fits the StandardScaler on the training feature data.

        Args:
            training_features (np.ndarray): The feature matrix from the training set.
            feature_names (List[str]): The names of the features.
        """
        logger.info("Fitting feature normalizer (StandardScaler)...")
        self.scaler.fit(training_features)
        self.feature_names = feature_names
        self.is_fitted = True
        logger.info(f"Normalizer fitted for {training_features.shape[1]} features.")
        logger.info(f"Feature names: {self.feature_names}")

    def transform(self, features: np.ndarray) -> np.ndarray:
        """
        Transforms features using the fitted scaler.

        Args:
            features (np.ndarray): The features to normalize. Can be a single sample or a batch.

        Returns:
            np.ndarray: The normalized features.
        """
        if not self.is_fitted:
            raise RuntimeError("FeatureNormalizer must be fitted before transforming data.")
        
        # Ensure input is 2D
        if features.ndim == 1:
            features = features.reshape(1, -1)
            
        return self.scaler.transform(features)

    def fit_transform(self, training_features: np.ndarray, feature_names: List[str]) -> np.ndarray:
        """Fits on and transforms the training data in one step."""
        self.fit(training_features, feature_names)
        return self.transform(training_features)


class TDANeuralClassifier(nn.Module):
    """
    Neural network classifier using the 7D geometric + topological feature vector.

    Architecture: 7 -> 128 -> 64 -> 32 -> 16 -> 3 (raw logits for CrossEntropyLoss)
    """
    def __init__(self, input_dim: int = 7, dropout_rate: float = 0.3):
        """
        Initialize the neural network.

        Args:
            input_dim (int): Number of input features. Should be 7 for the landmark method.
            dropout_rate (float): Dropout probability for regularization.
        """
        super(TDANeuralClassifier, self).__init__()

        self.input_dim = input_dim
        self.dropout_rate = dropout_rate

        # Network layers
        self.layer1 = nn.Linear(input_dim, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.layer2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.layer3 = nn.Linear(64, 32)
        self.bn3 = nn.BatchNorm1d(32)
        self.dropout3 = nn.Dropout(dropout_rate)

        self.layer4 = nn.Linear(32, 16)
        self.bn4 = nn.BatchNorm1d(16)
        self.dropout4 = nn.Dropout(dropout_rate)

        # Output layer - raw logits for CrossEntropyLoss
        self.output = nn.Linear(16, 3)

        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize network weights using Xavier initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the network."""
        x = F.gelu(self.bn1(self.layer1(x)))
        x = self.dropout1(x)

        x = F.gelu(self.bn2(self.layer2(x)))
        x = self.dropout2(x)

        x = F.gelu(self.bn3(self.layer3(x)))
        x = self.dropout3(x)

        x = F.gelu(self.bn4(self.layer4(x)))
        x = self.dropout4(x)

        x = self.output(x)
        return x

    def predict_proba(self, x: torch.Tensor) -> torch.Tensor:
        """Get prediction probabilities by applying softmax to logits."""
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            return F.softmax(logits, dim=1)

    def predict(self, x: torch.Tensor) -> torch.Tensor:
        """Get class predictions."""
        probabilities = self.predict_proba(x)
        return torch.argmax(probabilities, dim=1)


def load_classifier_data(data_path: str) -> Dict:
    """
    Loads data generated by 'landmark_method_tda.py'.
    """
    logger.info(f"Loading classifier data from {data_path}...")
    try:
        data = torch.load(data_path, map_location='cpu')
        required_fields = ['features', 'labels', 'feature_names']
        for field in required_fields:
            if field not in data:
                raise ValueError(f"Data file is missing required field: '{field}'")
        
        logger.info(f"Successfully loaded {data['features'].shape[0]} samples.")
        return data
    except Exception as e:
        logger.error(f"Failed to load data from {data_path}: {e}")
        raise


def prepare_training_data(
    features_matrix: np.ndarray,
    labels: List[str]
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Prepares data for PyTorch training by converting it to tensors.

    Args:
        features_matrix (np.ndarray): The final (normalized) feature matrix.
        labels (List[str]): The list of string labels.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: (features_tensor, labels_tensor)
    """
    X = torch.FloatTensor(features_matrix)
    label_to_idx = {'entailment': 0, 'neutral': 1, 'contradiction': 2}
    numeric_labels = [label_to_idx[label] for label in labels]
    y = torch.LongTensor(numeric_labels)
    
    logger.info(f"Prepared training data: {X.shape[0]} samples, {X.shape[1]} features")
    return X, y


if __name__ == "__main__":
    logger.info("Testing TDA Neural Classifier setup for landmark-based features...")

    # Define the path to the data file generated by the landmark script
    data_path = "results/tda_integration/landmark_tda_features/neural_network_features_snli_10k.pt"

    try:
        # 1. Load the pre-computed data
        classifier_data = load_classifier_data(data_path)
        features = classifier_data['features'].numpy()
        labels = classifier_data['labels']
        feature_names = classifier_data['feature_names']

        # 2. Initialize and fit the normalizer
        normalizer = FeatureNormalizer()
        normalized_features = normalizer.fit_transform(features, feature_names)
        
        logger.info("✓ Feature normalizer tested successfully.")

        # 3. Initialize the classifier
        input_dim = normalized_features.shape[1]
        classifier = TDANeuralClassifier(input_dim=input_dim)
        logger.info(f"✓ Classifier initialized with {input_dim} input features.")

        # 4. Test forward pass
        dummy_input = torch.randn(32, input_dim) # a dummy batch of 32 samples
        output = classifier(dummy_input)
        predictions = classifier.predict(dummy_input)

        logger.info(f"✓ Forward pass successful. Output shape: {output.shape}")
        logger.info(f"✓ Predictions shape: {predictions.shape}")
        logger.info("✓ Network is ready for CrossEntropyLoss training.")

        logger.info("\nSetup test completed successfully!")

    except FileNotFoundError:
        logger.error(f"Data file not found at '{data_path}'.")
        logger.error("Please run 'landmark_tda_integration.py' first to generate the feature file.")
    except Exception as e:
        logger.error(f"An error occurred during the test: {e}")









