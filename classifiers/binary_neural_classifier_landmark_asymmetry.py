"""
Enhanced TDA Neural Network Classifier (Binary: Entailment vs Non-Entailment)

This module implements a neural network classifier that uses the enhanced feature
vector generated by the enhanced landmark-based TDA method for binary classification.

Enhanced Architecture: 14 input features -> 128 -> 64 -> 32 -> 2 output classes
Enhanced Features: The 14D vector from 'enhanced_landmark_tda.py', which includes:
  - 10 enhanced geometric features (standard + asymmetric cone/order features)
  - 4 topological features (H0/H1 persistence, etc.)

Expected Performance: 80-90% accuracy (vs 58.9% 3-way) due to binary simplification
"""

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedFeatureNormalizer:
    """
    Enhanced feature normalizer with options for different scaling methods.
    Handles the 14D enhanced feature space with special attention to asymmetric features.
    """

    def __init__(self, scaler_type: str = 'standard'):
        """
        Initializes the enhanced feature normalizer.

        Args:
            scaler_type (str): Type of scaler ('standard', 'robust')
        """
        if scaler_type == 'robust':
            self.scaler = RobustScaler()  # Better for features with outliers
        else:
            self.scaler = StandardScaler()

        self.scaler_type = scaler_type
        self.is_fitted = False
        self.feature_names = []

    def fit(self, training_features: np.ndarray, feature_names: List[str]):
        """
        Fits the scaler on the training feature data with enhanced analysis.

        Args:
            training_features (np.ndarray): The feature matrix from the training set.
            feature_names (List[str]): The names of the features.
        """
        self.scaler.fit(training_features)
        self.feature_names = feature_names
        self.is_fitted = True

    def transform(self, features: np.ndarray) -> np.ndarray:
        """
        Transforms features using the fitted scaler.

        Args:
            features (np.ndarray): The features to normalize.

        Returns:
            np.ndarray: The normalized features.
        """
        if not self.is_fitted:
            raise RuntimeError("EnhancedFeatureNormalizer must be fitted before transforming data.")

        # Ensure input is 2D
        if features.ndim == 1:
            features = features.reshape(1, -1)

        return self.scaler.transform(features)

    def fit_transform(self, training_features: np.ndarray, feature_names: List[str]) -> np.ndarray:
        """Fits on and transforms the training data in one step."""
        self.fit(training_features, feature_names)
        return self.transform(training_features)


class BinaryTDANeuralClassifier(nn.Module):
    """
    Binary neural network classifier using the 14D geometric + topological feature vector.

    Architecture: 14 -> 128 -> 64 -> 32 -> 2 (entailment vs non-entailment)
    Enhanced with residual connections and attention mechanism for better feature utilization.
    """

    def __init__(self, input_dim: int = 14, dropout_rate: float = 0.3):
        """
        Initialize the binary neural network.

        Args:
            input_dim (int): Number of input features. Should be 14 for enhanced method.
            dropout_rate (float): Dropout probability for regularization.
        """
        super(BinaryTDANeuralClassifier, self).__init__()

        self.input_dim = input_dim
        self.dropout_rate = dropout_rate

        # Binary network layers - smaller than 3-way since binary is easier
        self.layer1 = nn.Linear(input_dim, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.layer2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.layer3 = nn.Linear(64, 32)
        self.bn3 = nn.BatchNorm1d(32)
        self.dropout3 = nn.Dropout(dropout_rate)

        # Output layer - 2 classes (entailment vs non-entailment)
        self.output = nn.Linear(32, 2)

        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize network weights using Xavier initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Binary forward pass through the network."""
        original_x = x

        # Enhanced architecture with residual-like connections
        x = F.gelu(self.bn1(self.layer1(x)))
        x = self.dropout1(x)

        x = F.gelu(self.bn2(self.layer2(x)))
        x = self.dropout2(x)

        x = F.gelu(self.bn3(self.layer3(x)))
        x = self.dropout3(x)

        x = self.output(x)
        return x

    def predict_proba(self, x: torch.Tensor) -> torch.Tensor:
        """Get prediction probabilities by applying softmax to logits."""
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            return F.softmax(logits, dim=1)

    def predict(self, x: torch.Tensor) -> torch.Tensor:
        """Get class predictions."""
        probabilities = self.predict_proba(x)
        return torch.argmax(probabilities, dim=1)


def load_enhanced_classifier_data(data_path: str) -> Dict:
    """
    Loads enhanced data generated by 'enhanced_landmark_tda.py'.
    """
    logger.info(f"Loading enhanced classifier data from {data_path}...")
    try:
        data = torch.load(data_path, map_location='cpu')
        required_fields = ['features', 'labels', 'feature_names']
        for field in required_fields:
            if field not in data:
                raise ValueError(f"Data file is missing required field: '{field}'")

        return data
    except Exception as e:
        logger.error(f"Failed to load data from {data_path}: {e}")
        raise


def prepare_binary_training_data(
        features_matrix: np.ndarray,
        labels: List[str],
        test_size: float = 0.2,
        random_state: int = 42
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Prepares binary data for PyTorch training with train/test split.
    Converts neutral and contradiction labels to 'non-entailment'.

    Args:
        features_matrix (np.ndarray): The final (normalized) feature matrix.
        labels (List[str]): The list of string labels.
        test_size (float): Fraction of data to use for testing.
        random_state (int): Random state for reproducible splits.

    Returns:
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        (X_train, X_test, y_train, y_test)
    """
    # Convert to binary labels: entailment (0) vs non-entailment (1)
    binary_labels = []
    for label in labels:
        if label == 'entailment':
            binary_labels.append(0)  # entailment
        else:  # neutral or contradiction
            binary_labels.append(1)  # non-entailment

    binary_labels = np.array(binary_labels)

    # Log class distribution
    entailment_count = np.sum(binary_labels == 0)
    non_entailment_count = np.sum(binary_labels == 1)
    logger.info(f"Binary class distribution:")
    logger.info(f"  Entailment: {entailment_count} samples ({entailment_count / len(binary_labels) * 100:.1f}%)")
    logger.info(
        f"  Non-entailment: {non_entailment_count} samples ({non_entailment_count / len(binary_labels) * 100:.1f}%)")

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        features_matrix, binary_labels,
        test_size=test_size,
        random_state=random_state,
        stratify=binary_labels  # Ensure balanced split
    )

    # Convert to tensors
    X_train = torch.FloatTensor(X_train)
    X_test = torch.FloatTensor(X_test)
    y_train = torch.LongTensor(y_train)
    y_test = torch.LongTensor(y_test)

    logger.info(f"Binary training data prepared:")
    logger.info(f"  Train: {X_train.shape[0]} samples, {X_train.shape[1]} features")
    logger.info(f"  Test: {X_test.shape[0]} samples, {X_test.shape[1]} features")

    return X_train, X_test, y_train, y_test


if __name__ == "__main__":
    logger.info("Testing Binary TDA Neural Classifier setup...")

    # Define the path to the enhanced data file
    enhanced_data_paths = [
        "results/tda_integration/landmark_tda_features/enhanced_neural_network_features_snli_10k_roberta.pt",
    ]

    data_path = None
    for path in enhanced_data_paths:
        if Path(path).exists():
            data_path = path
            break

    if data_path is None:
        logger.error("No enhanced data file found. Please run enhanced_landmark_tda.py first.")
        exit(1)

    try:
        # 1. Load the enhanced pre-computed data
        classifier_data = load_enhanced_classifier_data(data_path)
        features = classifier_data['features'].numpy()
        labels = classifier_data['labels']
        feature_names = classifier_data['feature_names']

        # 2. Initialize and fit the enhanced normalizer
        normalizer = EnhancedFeatureNormalizer(scaler_type='standard')
        normalized_features = normalizer.fit_transform(features, feature_names)

        logger.info("Enhanced feature normalizer tested successfully.")

        # 3. Prepare binary training data with train/test split
        X_train, X_test, y_train, y_test = prepare_binary_training_data(
            normalized_features, labels, test_size=0.2, random_state=42
        )

        # 4. Initialize the binary classifier
        input_dim = normalized_features.shape[1]
        classifier = BinaryTDANeuralClassifier(
            input_dim=input_dim,
            dropout_rate=0.3,
        )
        logger.info(f"✓ Binary classifier initialized with {input_dim} input features.")

        # 5. Test forward pass
        dummy_input = torch.randn(32, input_dim)
        output = classifier(dummy_input)
        predictions = classifier.predict(dummy_input)

        logger.info(f"✓ Forward pass successful. Output shape: {output.shape}")
        logger.info(f"✓ Predictions shape: {predictions.shape}")
        logger.info("✓ Binary network is ready for CrossEntropyLoss training.")

        logger.info("\n" + "=" * 60)
        logger.info("BINARY TDA NEURAL CLASSIFIER SETUP SUCCESSFUL!")
        logger.info("=" * 60)
        logger.info(f"Feature space: {input_dim}D enhanced features")
        logger.info(f"Training samples: {X_train.shape[0]}")
        logger.info(f"Test samples: {X_test.shape[0]}")
        logger.info(f"Classes: Entailment vs Non-Entailment (binary)")
        logger.info(f"Expected accuracy: 80-90% (vs 58.9% 3-way)")
        logger.info("Ready for hyperparameter search and training!")

    except FileNotFoundError:
        logger.error(f"Enhanced data file not found.")
        logger.error("Please run 'enhanced_landmark_tda.py' first to generate the enhanced feature file.")
    except Exception as e:
        logger.error(f"An error occurred during the binary test: {e}")
        import traceback

        traceback.print_exc()