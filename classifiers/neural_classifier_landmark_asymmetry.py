"""
Enhanced TDA Neural Network Classifier (with 14D Asymmetric Features)

This module implements a neural network classifier that uses the enhanced feature
vector generated by the enhanced landmark-based TDA method.

Enhanced Architecture: 14 input features -> 256 -> 128 -> 64 -> 32 -> 3 output classes
Enhanced Features: The 14D vector from 'enhanced_landmark_tda.py', which includes:
  - 10 enhanced geometric features (standard + asymmetric cone/order features)
  - 4 topological features (H0/H1 persistence, etc.)

Expected Performance: 70-85% accuracy (vs 52% baseline) due to richer feature space
"""

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedFeatureNormalizer:
    """
    Enhanced feature normalizer with options for different scaling methods.
    Handles the 14D enhanced feature space with special attention to asymmetric features.
    """
    def __init__(self, scaler_type: str = 'standard'):
        """
        Initializes the enhanced feature normalizer.
        
        Args:
            scaler_type (str): Type of scaler ('standard', 'robust')
        """
        if scaler_type == 'robust':
            self.scaler = RobustScaler()  # Better for features with outliers
        else:
            self.scaler = StandardScaler()
            
        self.scaler_type = scaler_type
        self.is_fitted = False
        self.feature_names = []

    def fit(self, training_features: np.ndarray, feature_names: List[str]):
        """
        Fits the scaler on the training feature data with enhanced analysis.

        Args:
            training_features (np.ndarray): The feature matrix from the training set.
            feature_names (List[str]): The names of the features.
        """
        self.scaler.fit(training_features)
        self.feature_names = feature_names
        self.is_fitted = True
    
        

    def transform(self, features: np.ndarray) -> np.ndarray:
        """
        Transforms features using the fitted scaler.

        Args:
            features (np.ndarray): The features to normalize.

        Returns:
            np.ndarray: The normalized features.
        """
        if not self.is_fitted:
            raise RuntimeError("EnhancedFeatureNormalizer must be fitted before transforming data.")
        
        # Ensure input is 2D
        if features.ndim == 1:
            features = features.reshape(1, -1)
            
        return self.scaler.transform(features)

    def fit_transform(self, training_features: np.ndarray, feature_names: List[str]) -> np.ndarray:
        """Fits on and transforms the training data in one step."""
        self.fit(training_features, feature_names)
        return self.transform(training_features)


class EnhancedTDANeuralClassifier(nn.Module):
    """
    Enhanced neural network classifier using the 14D geometric + topological feature vector.

    Architecture: 14 -> 256 -> 128 -> 64 -> 32 -> 3 (raw logits for CrossEntropyLoss)
    Enhanced with residual connections and attention mechanism for better feature utilization.
    """
    def __init__(self, input_dim: int = 14, dropout_rate: float = 0.3):
        """
        Initialize the enhanced neural network.

        Args:
            input_dim (int): Number of input features. Should be 14 for enhanced method.
            dropout_rate (float): Dropout probability for regularization.
            use_attention (bool): Whether to use attention mechanism for feature selection.
        """
        super(EnhancedTDANeuralClassifier, self).__init__()

        self.input_dim = input_dim
        self.dropout_rate = dropout_rate

        # Enhanced network layers with larger capacity for 14D features
        self.layer1 = nn.Linear(input_dim, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.layer2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.layer3 = nn.Linear(64, 32)
        self.bn3 = nn.BatchNorm1d(32)
        self.dropout3 = nn.Dropout(dropout_rate)

        # Output layer - raw logits for CrossEntropyLoss
        self.output = nn.Linear(32, 3)

        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize network weights using Xavier initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Enhanced forward pass through the network."""
        original_x = x

        # Enhanced architecture with residual-like connections
        x = F.gelu(self.bn1(self.layer1(x)))
        x = self.dropout1(x)

        x = F.gelu(self.bn2(self.layer2(x)))
        x = self.dropout2(x)

        x = F.gelu(self.bn3(self.layer3(x)))
        x = self.dropout3(x)

        x = self.output(x)
        return x

    def predict_proba(self, x: torch.Tensor) -> torch.Tensor:
        """Get prediction probabilities by applying softmax to logits."""
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            return F.softmax(logits, dim=1)

    def predict(self, x: torch.Tensor) -> torch.Tensor:
        """Get class predictions."""
        probabilities = self.predict_proba(x)
        return torch.argmax(probabilities, dim=1)



def load_enhanced_classifier_data(data_path: str) -> Dict:
    """
    Loads enhanced data generated by 'enhanced_landmark_tda.py'.
    """
    logger.info(f"Loading enhanced classifier data from {data_path}...")
    try:
        data = torch.load(data_path, map_location='cpu')
        required_fields = ['features', 'labels', 'feature_names']
        for field in required_fields:
            if field not in data:
                raise ValueError(f"Data file is missing required field: '{field}'")
        

            
        return data
    except Exception as e:
        logger.error(f"Failed to load data from {data_path}: {e}")
        raise


def prepare_enhanced_training_data(
    features_matrix: np.ndarray,
    labels: List[str],
    test_size: float = 0.2,
    random_state: int = 42
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Prepares enhanced data for PyTorch training with train/test split.

    Args:
        features_matrix (np.ndarray): The final (normalized) feature matrix.
        labels (List[str]): The list of string labels.
        test_size (float): Fraction of data to use for testing.
        random_state (int): Random state for reproducible splits.

    Returns:
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: 
        (X_train, X_test, y_train, y_test)
    """
    # Convert labels to numeric
    label_to_idx = {'entailment': 0, 'neutral': 1, 'contradiction': 2}
    numeric_labels = np.array([label_to_idx[label] for label in labels])
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        features_matrix, numeric_labels, 
        test_size=test_size, 
        random_state=random_state,
        stratify=numeric_labels  # Ensure balanced split
    )
    
    # Convert to tensors
    X_train = torch.FloatTensor(X_train)
    X_test = torch.FloatTensor(X_test)
    y_train = torch.LongTensor(y_train)
    y_test = torch.LongTensor(y_test)
    
    logger.info(f"Enhanced training data prepared:")
    logger.info(f"  Train: {X_train.shape[0]} samples, {X_train.shape[1]} features")
    logger.info(f"  Test: {X_test.shape[0]} samples, {X_test.shape[1]} features")
    
    return X_train, X_test, y_train, y_test



if __name__ == "__main__":
    logger.info("Testing Enhanced TDA Neural Classifier setup...")

    # Define the path to the enhanced data file
    enhanced_data_paths = [
        "results/tda_integration/landmark_tda_features/enhanced_neural_network_features_snli_10k_roberta.pt",
    ]

    data_path = None
    for path in enhanced_data_paths:
        if Path(path).exists():
            data_path = path
            break

    if data_path is None:
        logger.error("No enhanced data file found. Please run enhanced_landmark_tda.py first.")
        exit(1)

    try:
        # 1. Load the enhanced pre-computed data
        classifier_data = load_enhanced_classifier_data(data_path)
        features = classifier_data['features'].numpy()
        labels = classifier_data['labels']
        feature_names = classifier_data['feature_names']

        # 2. Initialize and fit the enhanced normalizer
        normalizer = EnhancedFeatureNormalizer(scaler_type='standard')
        normalized_features = normalizer.fit_transform(features, feature_names)
        
        logger.info("✓ Enhanced feature normalizer tested successfully.")

        # 3. Prepare training data with train/test split
        X_train, X_test, y_train, y_test = prepare_enhanced_training_data(
            normalized_features, labels, test_size=0.2, random_state=42
        )

        # 4. Initialize the enhanced classifier
        input_dim = normalized_features.shape[1]
        classifier = EnhancedTDANeuralClassifier(
            input_dim=input_dim, 
            dropout_rate=0.3,
        )
        logger.info(f"✓ Enhanced classifier initialized with {input_dim} input features.")

        # 5. Test forward pass
        dummy_input = torch.randn(32, input_dim)
        output = classifier(dummy_input)
        predictions = classifier.predict(dummy_input)

        logger.info(f"✓ Forward pass successful. Output shape: {output.shape}")
        logger.info(f"✓ Predictions shape: {predictions.shape}")
        logger.info("✓ Enhanced network is ready for CrossEntropyLoss training.")

        logger.info("\n" + "="*60)
        logger.info("ENHANCED TDA NEURAL CLASSIFIER SETUP SUCCESSFUL!")
        logger.info("="*60)
        logger.info(f"✓ Feature space: {input_dim}D enhanced features")
        logger.info(f"✓ Training samples: {X_train.shape[0]}")
        logger.info(f"✓ Test samples: {X_test.shape[0]}")
        logger.info(f"✓ Expected accuracy: 70-85% (vs 52% baseline)")
        logger.info("✓ Ready for hyperparameter search and training!")

    except FileNotFoundError:
        logger.error(f"Enhanced data file not found.")
        logger.error("Please run 'enhanced_landmark_tda.py' first to generate the enhanced feature file.")
    except Exception as e:
        logger.error(f"An error occurred during the enhanced test: {e}")
        import traceback
        traceback.print_exc()